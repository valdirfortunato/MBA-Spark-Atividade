series|location|keywords|numpages|pages|booktitle|abstract|doi|url|address|publisher|isbn|year|title|author|type_publication|ID|articleno|month|journal|issn|number|volume|issue_date|note|edition|editor|Sourceid|SJR|SJR_Best_Quartile|H_index|Total_Docs_2020|Total_Docs_3years|Total_Refs|Total_Cites_3years|Citable_Docs_3years|Cites_Doc_2years|Ref_Doc|Country|Region|Coverage|Categories|Total_Cites|Journal_Impact_Factor|Eigenfactor_Score|id_title|id_issn
||Data processing, Data quality, Usage control, Argumentation reasoning, Data manufacturing, Case scenarios||52-61||Data-intensive environments enable us to capture information and knowledge about the physical surroundings, to optimise our resources, enjoy personalised services and gain unprecedented insights into our lives. However, to obtain these endeavours extracted from the data, this data should be generated, collected and the insight should be exploited. Following an argumentation reasoning approach for data processing and building on the theoretical background of data management, we highlight the importance of data sharing agreements (DSAs) and quality attributes for the proposed data processing mechanism. The proposed approach is taking into account the DSAs and usage policies as well as the quality attributes of the data, which were previously neglected compared to existing methods in the data processing and management field. Previous research provided techniques towards this direction; however, a more intensive research approach for processing techniques should be introduced for the future to enhance the value creation from the data and new strategies should be formed around this data generated daily from various devices and sources.|https://doi.org/10.1016/j.compind.2017.09.002|https://www.sciencedirect.com/science/article/pii/S016636151730338X||||2018|An argumentation reasoning approach for data processing|Erisa Karafili and Konstantina Spanaki and Emil C. Lupu|article|KARAFILI201852|||Computers in Industry|01663615||94|||||19080|1,432|Q1|100|115|323|6926|3165|319|9,96|60,23|Netherlands|Western Europe|1979-2020|Computer Science (miscellaneous) (Q1); Engineering (miscellaneous) (Q1)|6,018|7.635|0.00584|1878047|605146181
||Software engineering, artificial intelligence, AI-based systems, systematic mapping study|59|||AI-based systems are software systems with functionalities enabled by at least one AI component (e.g., for image-, speech-recognition, and autonomous driving). AI-based systems are becoming pervasive in society due to advances in AI. However, there is limited synthesized knowledge on Software Engineering (SE) approaches for building, operating, and maintaining AI-based systems. To collect and analyze state-of-the-art knowledge about SE for AI-based systems, we conducted a systematic mapping study. We considered 248 studies published between January 2010 and March 2020. SE for AI-based systems is an emerging research area, where more than 2/3 of the studies have been published since 2018. The most studied properties of AI-based systems are dependability and safety. We identified multiple SE approaches for AI-based systems, which we classified according to the SWEBOK areas. Studies related to software testing and software quality are very prevalent, while areas like software maintenance seem neglected. Data-related issues are the most recurrent challenges. Our results are valuable for: researchers, to quickly understand the state-of-the-art and learn which topics need more research; practitioners, to learn about the approaches and challenges that SE entails for AI-based systems; and, educators, to bridge the gap among SE and AI in their curricula.|10.1145/3487043|https://doi.org/10.1145/3487043|New York, NY, USA|Association for Computing Machinery||2022|Software Engineering for AI-Based Systems: A Survey|Mart\'{\i}nez-Fern\'{a}ndez, Silverio and Bogner, Justus and Franch, Xavier and Oriol, Marc and Siebert, Julien and Trendowicz, Adam and Vollmer, Anna Maria and Wagner, Stefan|article|10.1145/3487043|37e|apr|ACM Trans. Softw. Eng. Methodol.|1049331X|2|31|April 2022||||||||||||||||||||||3220488|1429302734
||||||The Handbook of Multimodal-Multisensor Interfaces provides the first authoritative resource on what has become the dominant paradigm for new computer interfaces---user input involving new media (speech, multi-touch, hand and body gestures, facial expressions, writing) embedded in multimodal-multisensor interfaces.This three-volume handbook is written by international experts and pioneers in the field. It provides a textbook, reference, and technology roadmap for professionals working in this and related areas.This third volume focuses on state-of-the-art multimodal language and dialogue processing, including semantic integration of modalities. The development of increasingly expressive embodied agents and robots has become an active test-bed for coordinating multimodal dialogue input and output, including processing of language and nonverbal communication. In addition, major application areas are featured for commercializing multimodal-multisensor systems, including automotive, robotic, manufacturing, machine translation, banking, communications, and others. These systems rely heavily on software tools, data resources, and international standards to facilitate their development. For insights into the future, emerging multimodal-multisensor technology trends are highlighted for medicine, robotics, interaction with smart spaces, and similar topics. Finally, this volume discusses the societal impact of more widespread adoption of these systems, such as privacy risks and how to mitigate them. The handbook chapters provide a number of walk-through examples of system design and processing, information on practical resources for developing and evaluating new systems, and terminology and tutorial support for mastering this emerging field. In the final section of this volume, experts exchange views on a timely and controversial challenge topic, and how they believe multimodal-multisensor interfaces need to be equipped to most effectively advance human performance during the next decade.||||Association for Computing Machinery and Morgan &amp; Claypool|9781970001754|2019|The Handbook of Multimodal-Multisensor Interfaces: Language Processing, Software, Commercialization, and Emerging Directions||book|10.1145/3233795||||||||||"\"Oviatt, Sharon and Schuller, Bj\"\"{o}rn and Cohen, Philip R. and Sonntag, Daniel and Potamianos, Gerasimos and Kr\\\"\"{u}ger, Antonio\""|||||||||||||||||||4121568|42
|||6|85–90||The data gluttony of AI is well known: Data fuels the artificial intelligence. Technologies that help to gather the needed data are then essential, among which the IoT. However, the deployment of IoT solutions raises significant challenges, especially regarding the resource and financial costs at stake. It is our view that mobile crowdsensing, aka phone sensing, has a major role to play because it potentially contributes massive data at a relatively low cost. Still, crowdsensing is useless, and even harmful, if the contributed data are not properly analyzed. This paper surveys our work on the development of systems facing this challenge, which also illustrates the virtuous circles of AI. We specifically focus on how intelligent crowdsensing middleware leverages on-device machine learning to enhance the reported physical observations. Keywords: Crowdsensing, Middleware, Online learning.|10.1145/3352020.3352033|https://doi.org/10.1145/3352020.3352033|New York, NY, USA|Association for Computing Machinery||2019|When the Power of the Crowd Meets the Intelligence of the Middleware: The Mobile Phone Sensing Case|Du, Yifan and Issarny, Val\'{e}rie and Sailhan, Fran\c{c}oise|article|10.1145/3352020.3352033||jul|SIGOPS Oper. Syst. Rev.|01635980|1|53|July 2019||||19829|0,180|Q4|104|7|36|371|58|33|2,23|53,00|United States|Northern America|1980-1991, 1993, 1996-2010, 2013-2017, 2019|Computer Networks and Communications (Q4); Hardware and Architecture (Q4); Information Systems (Q4)||||5065549|892607647
ArabWIC 2019|Rabat, Morocco||8||Proceedings of the ArabWIC 6th Annual International Conference Research Track|Deep learning has been very successful in the past decades, especially in Computer Vision and Speech Recognition fields. It has been also used successfully in the Natural Language Processing field because of the availability of an enormous amount of online text data, such as social networks and reviews websites, which have gained a lot of popularity and success in the past years. Sentiment Analysis is one of the hottest applications of Natural Language Processing (NLP). Many researchers have done excellent work on Sentiment Analysis for English language. However, the amount of work on Sentiment Analysis for Arabic language is, in comparison, very limited due to the complexity of the Arabic language's morphology and orthography. Unlike the English language, Arabic has many different dialects which makes Sentiment Analysis for Arabic more difficult and challenging, especially when working on data collected from social networks, which is known to be unstructured and noisy. Most of the work that has been done on Sentiment Analysis of Arabic language, focused on using lexicons and basic machine learning models. In addition, most of the work has been done on small datasets because of the limited number of the available annotated datasets for Arabic language. This paper proposes state-of-the-art research for Sentiment Analysis of Arabic microblogging using new techniques, and a sophisticated Arabic text data preprocessing.|10.1145/3333165.3333185|https://doi.org/10.1145/3333165.3333185|New York, NY, USA|Association for Computing Machinery|9781450360890|2019|Deep Learning for Sentiment Analysis of Arabic Text|Soufan, Ayah|inproceedings|10.1145/3333165.3333185|20||||||||||||||||||||||||||||5635341|42
||Data quality, Text mining, Science mapping, Data mining, Trend analysis||280-287||The analysis of the transformation and changes in scientific disciplines has always been a critical path for policymakers and researchers. The current study examines the changes in the research areas of data and information quality (DIQ). The aim of this study was to detect different types of changes occurring in the scientific areas including birth, death, growth, decline, merge, and splitting. A model has been developed for this data mining. To test the model, all DIQ articles published in online scientific citation indexing service or Web of Science (WOS) between 1970 and 2016 were extracted and analyzed using the given model. The study is related to the Big Data as well as the integration methods in Big Data which is the most important area in DIQ. It is demonstrated that the first and second emerging research areas are sub-disciplines of entity resolution and record linkage. Accordingly, linkage and privacy are the first emerging research area and the entity resolution using ontology is the second in DIQ. This is followed by the social media issues and genetic related DIQ issues.|https://doi.org/10.1016/j.techfore.2018.08.003|https://www.sciencedirect.com/science/article/pii/S0040162517318140||||2018|Systematic method for finding emergence research areas as data quality|Babak Sohrabi and Ahmad Khalilijafarabad|article|SOHRABI2018280|||Technological Forecasting and Social Change|00401625||137|||||14704|2,226|Q1|117|448|1099|35581|10127|1061|9,01|79,42|United States|Northern America|1970-2020|Applied Psychology (Q1); Business and International Management (Q1); Management of Technology and Innovation (Q1)|21,116|8.593|0.02416|7629008|1949868303
||Accounting profession, Cloud, Big data, Blockchain, Artificial intelligence||100833||This paper reviews the accounting literature that focuses on four Internet-related technologies that have the potential to dramatically change and disrupt the work of accountants and accounting researchers in the near future. These include cloud, big data, blockchain, and artificial intelligence (AI). For instance, access to distributed ledgers (blockchain) and big data supported by cloud-based analytics tools and AI will automate decision making to a large extent. These technologies may significantly improve financial visibility and allow more timely intervention due to the perpetual nature of accounting. However, given the number of tasks technology has relieved of accountants, these technologies may also lead to concerns about the profession's legitimacy. The findings suggest that scholars have not given sufficient attention to these technologies and how these technologies affect the everyday work of accountants. Research is urgently needed to understand the new kinds of accounting required to manage firms in the changing digital economy and to determine the new skills and competencies accountants may need to master to remain relevant and add value. The paper outlines a set of questions to guide future research.|https://doi.org/10.1016/j.bar.2019.04.002|https://www.sciencedirect.com/science/article/pii/S0890838919300459||||2019|The role of internet-related technologies in shaping the work of accountants: New directions for accounting research|Jodie Moll and Ogan Yigitbasioglu|article|MOLL2019100833|||The British Accounting Review|08908389|6|51||Innovative Governance and Sustainable Pathways in a Disruptive Environment|||||||||||||||||||||10822654|563989249
||Pro-environmental behavior, Internet of Things, Measurement, Big data, Environmental psychology||100055||Promoting pro-environmental behavior is an effective means of reducing carbon emissions at the individual end, but the measurement of behavior has long been a problem for scholars. Especially in environmental psychology community, the complexity of social policies and habitat implies greater difficulty in measuring. Due to the limitations of traditional questionnaire, laboratory, and naturalistic observation methods, environmental psychologists need more realistic, accurate, and cost-effective ways to measure behavior. The rapid development of IoT technology lights up the hope for achieving this goal, and its large-scale popularization will bring great changes to the research community. This paper reviews the current methods and their limitations, proposes a framework for measuring behavior using IoT devices, and points out its future research directions.|https://doi.org/10.1016/j.crbeha.2021.100055|https://www.sciencedirect.com/science/article/pii/S2666518221000425||||2021|Aiding pro-environmental behavior measurement by Internet of Things|Ziqian Xia and Yurong Liu|article|XIA2021100055|||Current Research in Behavioral Sciences|26665182||2|||||||||||||||||||||||10887472|385753943
PCI 2020|Athens, Greece|Intensive Care Unit, Big Data Analysis, Machine Learning, Conceptual Framework|5|411–415|24th Pan-Hellenic Conference on Informatics|"\"The development of Big Data Analytics (BDA) technology and the maturity of the Machine Learning (ML) sector offer great opportunities for applications in Intensive Care Units (ICUs). This paper describes a Conceptual Framework and proposes its use in designing architectures and big data applications in ICUs. The Conceptual Framework is based on BDA,MLNatural Language Processing (NLP) and consists of the following subsystems: The \"\"Big Data Integration and ICUs\"\" module, the \"\"ICUs and critical care services\"\" module, the \"\"Use of standards and ICUs\"\" module, the \"\"Machine Learning and ICUs\"\" module, and the “NLP and ICUs” module. The framework is developed using Soft System Methodology (SSM) and Design Science Research Methodology (DSRM).\""|10.1145/3437120.3437352|https://doi.org/10.1145/3437120.3437352|New York, NY, USA|Association for Computing Machinery|9781450388979|2021|Towards the Design of a Conceptual Framework for the Operation of Intensive Care Units Based on Big Data Analysis|Markopoulos, Dimitris and Tsolakidis, Anastasios and N. Karanikolas, Nikitas and Skourlas, Christos|inproceedings|10.1145/3437120.3437352|||||||||||||||||||||||||||||11196646|42
||encrypted traffic, crowdsourcing, QoE management, data analytics, SDN, NFV, QoE monitoring, QoE modeling, monitoring probes|29|||Quality of Experience (QoE) has received much attention over the past years and has become a prominent issue for delivering services and applications. A significant amount of research has been devoted to understanding, measuring, and modelling QoE for a variety of media services. The next logical step is to actively exploit that accumulated knowledge to improve and manage the quality of multimedia services, while at the same time ensuring efficient and cost-effective network operations. Moreover, with many different players involved in the end-to-end service delivery chain, identifying the root causes of QoE impairments and finding effective solutions for meeting the end users’ requirements and expectations in terms of service quality is a challenging and complex problem. In this article, we survey state-of-the-art findings and present emerging concepts and challenges related to managing QoE for networked multimedia services. Going beyond a number of previously published survey articles addressing the topic of QoE management, we address QoE management in the context of ongoing developments, such as the move to softwarized networks, the exploitation of big data analytics and machine learning, and the steady rise of new and immersive services (e.g., augmented and virtual reality). We address the implications of such paradigm shifts in terms of new approaches in QoE modeling and the need for novel QoE monitoring and management infrastructures.|10.1145/3176648|https://doi.org/10.1145/3176648|New York, NY, USA|Association for Computing Machinery||2018|A Survey of Emerging Concepts and Challenges for QoE Management of Multimedia Services|Skorin-Kapov, Lea and Varela, Mart\'{\i}n and Ho\ss{}feld, Tobias and Chen, Kuan-Ta|article|10.1145/3176648|29|may|ACM Trans. Multimedia Comput. Commun. Appl.|15516857|2s|14|April 2018||||||||||||||||||||||12017370|343492898
ICEGOV 2021|Athens, Greece|enablers, smart city, challenges, sustainable city|7|422–428|14th International Conference on Theory and Practice of Electronic Governance|The term Smart Sustainable City (SSC) has been gaining popularity due to the growth of initiatives to address urban problems towards sustainable development. SSC can be considered as a combination of sustainable city and smart city, and some variance between the concepts may be expected. As this is a modern term, the literature falls short of studies presenting factors that hinder and/or facilitate the complex phenomenon of SSC development. Therefore, this paper aims to analyse scientific studies to identify aspects that influence the progress of smart sustainable cities. The methodological approach undertaken was a systematic literature review that included 169 papers. The results offer a comprehensive list of 57 drivers and 63 barriers, classified according to five main dimensions of a smart sustainable city, which are the three sustainability pillars (society, environment, and economy), combined to governance, and urban infrastructure. The findings revealed ‘governance’ as the most significant domain for SSC development, and multistakeholder engagement as one of the main challenges. This study shows that SSC is not a research field itself, but an interdisciplinary concept, contributing to academics, government, and policymakers for eradicating potential interferences in the development of smart and sustainable cities.|10.1145/3494193.3494250|https://doi.org/10.1145/3494193.3494250|New York, NY, USA|Association for Computing Machinery|9781450390118|2022|Drivers and Barriers for the Development of Smart Sustainable Cities: A Systematic Literature Review|Schuch de Azambuja, Luiza|inproceedings|10.1145/3494193.3494250|||||||||||||||||||||||||||||12181613|42
IMMS 2021|Chengdu, China|management dilemma, Chinese hotel groups, technology platform, digital transformation|6|13–18|2021 4th International Conference on Information Management and Management Science|The digital economy has been a hot spot in social development in recent years, and all walks of life are facing the opportunities and challenges of digital transformation. Successful digital transformation can enable traditional industries to gain dynamic capabilities in a changing environment, thereby gaining a leading competitive advantage. The previous literature paid more attention to the digital transformation of traditional industries, but lacked enough attention to the hotel industry. Through the case analysis of several major hotel groups in China, this article has gained profound insights in the digital transformation, enriched the influence of digital technology on the organization and management changes of the hotel industry, and has enlightening significance for guiding the hotel industry's practice.|10.1145/3485190.3485193|https://doi.org/10.1145/3485190.3485193|New York, NY, USA|Association for Computing Machinery|9781450384278|2021|The Dilemma of Digital Transformation of China's Hotel Industry and the Construction of Technology Platform: A Survey of Hotels Industry in China|Li, Yonghan and Lv, Hongjiang|inproceedings|10.1145/3485190.3485193|||||||||||||||||||||||||||||12226975|42
||Data Quality, Enterprise Systems, Accounting Information Systems, ERP, SCM, CRM, Big Data||442-449||Organisations are facing ever more diverse challenges in managing their enterprise systems as emerging technologies bring both added complexities as well as opportunities to the way they conduct their business. Underpinning this ever-increasing volatility is the importance of having quality data to provide information to make those important enterprise-wide decisions. Numerous studies suggest that many organisations are not paying enough attention to their data and that a major cause of this is their failure to measure its quality and value and/or evaluate the costs of having poor data. This study proposes an integrated framework that organisations can adopt as part of their financial and management control processes to provide a mechanism for quantifying data problems, costing potential solutions and monitoring the on-going costs and benefits, to assist them in improving and then sustaining the quality of their data.|https://doi.org/10.1016/j.procs.2015.08.539|https://www.sciencedirect.com/science/article/pii/S1877050915026745||||2015|‘Accounting’ for Data Quality in Enterprise Systems|Tony O’Brien|article|OBRIEN2015442|||Procedia Computer Science|18770509||64||Conference on ENTERprise Information Systems/International Conference on Project MANagement/Conference on Health and Social Care Information Systems and Technologies, CENTERIS/ProjMAN / HCist 2015 October 7-9, 2015|||19700182801|0,334|-|76|1907|6483|38511|13722|6359|2,09|20,19|Netherlands|Western Europe|2010-2020|Computer Science (miscellaneous)||||14119165|2108686752
||education, co-design, big data, community engagement|4|41–44||University of Colorado Anschutz Medical Campus' Data Science to Patient Value Program and 2040 Partners for Health sought to create open learning materials for engaged citizens and community leaders regarding big data and big data methods to support their collaboration in patient-centered and participatorybased community research and evaluation. 2040 is a local nonprofit organization that cultivates partnerships in Aurora, Colorado neighborhoods to tackle critical health needs. Our goal was to co-design and co-create a series of big data learning modules accessible to community laypeople, so they might better understand big data topics and be empowered more actively engage in health research and evaluation that uses big data methods.|10.1145/3331651.3331659|https://doi.org/10.1145/3331651.3331659|New York, NY, USA|Association for Computing Machinery||2019|Co-Designing Learning Materials to Empower Laypersons to Better Understand Big Data and Big Data Methods|Schilling, Lisa M. and Pena-Jackson, Griselda and Russell, Seth and Corral, Janet and Kwan, Bethany and Ressalam, Julie|article|10.1145/3331651.3331659||may|SIGKDD Explor. Newsl.|19310145|1|21|June 2019||||||||||||||||||||||17261266|22833317
||Resource description framework;Medical diagnostic imaging;Medical services;Big data;Sensors;Biomedical monitoring;RDF;Medical Data;Map / Reduce;Joins||737-746|2015 IEEE International Conference on Big Data (Big Data)|Recent technological advances in modern healthcare have lead to the ability to collect a vast wealth of patient monitoring data. This data can be utilised for patient diagnosis but it also holds the potential for use within medical research. However, these datasets often contain errors which limit their value to medical research, with one study finding error rates ranging from 2.3%-26.9% in a selection of medical databases. Previous methods for automatically assessing data quality normally rely on threshold rules, which are often unable to correctly identify errors, as further complex domain knowledge is required. To combat this, a semantic web based framework has previously been developed to assess the quality of medical data. However, early work, based solely on traditional semantic web technologies, revealed they are either unable or inefficient at scaling to the vast volumes of medical data. In this paper we present a new method for storing and querying medical RDF datasets using Hadoop Map / Reduce. This approach exploits the inherent parallelism found within RDF datasets and queries, allowing us to scale with both dataset and system size. Unlike previous solutions, this framework uses highly optimised (SPARQL) joining strategies, intelligent data caching and the use of a super-query to enable the completion of eight distinct SPARQL lookups, comprising over eighty distinct joins, in only two Map / Reduce iterations. Results are presented comparing both the Jena and a previous Hadoop implementation demonstrating the superior performance of the new methodology. The new method is shown to be five times faster than Jena and twice as fast as the previous approach.|10.1109/BigData.2015.7363818|||||2015|Data quality assessment and anomaly detection via map/reduce and linked data: A case study in the medical domain|Bonner, Stephen and McGough, Andrew Stephen and Kureshi, Ibad and Brennan, John and Theodoropoulos, Georgios and Moss, Laura and Corsar, David and Antoniou, Grigoris|inproceedings|7363818||Oct|||||||||||||||||||||||||||17483129|42
||Customer services;Blockchain;Companies;Machine learning;Personnel;Data models;Smart contracts;Automated customer service;automated machine learning (AutoML);blockchain;open customer service||3642-3651||Customer service is transforming from traditional manual service toward automated service, which utilizes different computational informatics to achieve a higher efficient and quality services. Automated customer service requires big data and expertise in data analysis as prerequisites. However, many companies, especially small and medium enterprises, do not have sufficient data and experience due to their limited scale and resources. They need to rely on third parties, and this reliance results in the lack of development of core customer service competency. In order to overcome these challenges, an open and automated customer service platform based on Internet of things (IoT), blockchain, and automated machine learning (AutoML) is proposed. The data are gathered with the use of IoT devices during the customer service. An open but secured environment to achieve data trading is ensured by using blockchain. AutoML is adopted to automate the data analysis processes for reducing the reliance of costly experts. The proposed platform is analyzed through use case evaluation. A prototype system has also been developed and evaluated. The simulation results show that our platform is scalable and efficient.|10.1109/TII.2019.2900987|||||2019|A Blockchain and AutoML Approach for Open and Automated Customer Service|Li, Zhi and Guo, Hanyang and Wang, Wai Ming and Guan, Yijiang and Barenji, Ali Vatankhah and Huang, George Q. and McFall, Kevin S. and Chen, Xin|article|8649758||June|IEEE Transactions on Industrial Informatics|19410050|6|15|||||||||||||||||||||||20178983|75062013
CAIN '22|Pittsburgh, Pennsylvania||11|159–169|Proceedings of the 1st International Conference on AI Engineering: Software Engineering for AI|Manufacturing has enabled the mechanized mass production of the same (or similar) products by replacing craftsmen with assembly lines of machines. The quality of each product in an assembly line greatly hinges on continual observation and error compensation during machining using sensors that measure quantities such as position and torque of a cutting tool and vibrations due to possible imperfections in the cutting tool and raw material. Patterns observed in sensor data from a (near-)optimal production cycle should ideally recur in subsequent production cycles with minimal deviation. Manually labeling and comparing such patterns is an insurmountable task due to the massive amount of streaming data that can be generated from a production process. We present UDAVA, an unsupervised machine learning pipeline that automatically discovers process behavior patterns in sensor data for a reference production cycle. UDAVA performs clustering of reduced dimensionality summary statistics of raw sensor data to enable high-speed clustering of dense time-series data. It deploys the model as a service to verify batch data from subsequent production cycles to detect recurring behavior patterns and quantify deviation from the reference behavior. We have evaluated UDAVA from an AI Engineering perspective using two industrial case studies.|10.1145/3522664.3528603|https://doi.org/10.1145/3522664.3528603|New York, NY, USA|Association for Computing Machinery|9781450392754|2022|UDAVA: An Unsupervised Learning Pipeline for Sensor Data Validation in Manufacturing|Husom, Erik Johannes and Tverdal, Simeon and Goknil, Arda and Sen, Sagar|inproceedings|10.1145/3522664.3528603|||||||||||||||||||||||||||||22929854|42
||||3-16||In common with much contemporary discourse around big data, recent discussion of datafication in the Journal of Strategic Information Systems has focused on its effects on individuals, organisations and society. Generally missing from such analysis, however, is any consideration of data themselves. What is it that is having these effects? In this Viewpoint article I therefore present a critical analysis of a number of widely-held assumptions about data in general and big data in particular. Rather than being a referential, natural, foundational, objective and equal representation of the world, it will be argued, data are partial and contingent and are brought into being through situated practices of conceptualization, recording and use. Big data are also not as revolutionary voluminous, universal or exhaustive as they are often presented. Some initial implications of this reconceptualization of data are explored. A distinction is made between “data in principle” as they are recorded, and the “data in practice” as they are used. It is only the latter, typically a small and not necessarily representative subset of the former, that will contribute directly to the effects of datafication.|https://doi.org/10.1016/j.jsis.2018.10.005|https://www.sciencedirect.com/science/article/pii/S0963868718302622||||2019|What we talk about when we talk about (big) data|Matthew Jones|article|JONES20193|||The Journal of Strategic Information Systems|09638687|1|28|||||12396|3,133|Q1|88|24|75|2111|965|61|12,45|87,96|Netherlands|Western Europe|1991-2020|Information Systems (Q1); Information Systems and Management (Q1); Management Information Systems (Q1)|2,945|11.022|0.00233|24208360|1997649283
||Technology delivery system, Tech mining, Emerging technology, Big Data, Technology assessment, Impact assessment||165-176||While there is a general recognition that breakthrough innovation is non-linear and requires an alignment between producers (supply) and users (demand), there is still a need for strategic intelligence about the emerging supply chains of new technological innovations. This technology delivery system (TDS) is an updated form of the TDS model and provides a promising chain-link approach to the supply side of innovation. Building on early research into supply-side TDS studies, we present a systematic approach to building a TDS model that includes four phases: (1) identifying the macroeconomic and policy environment, including market competition, financial investment, and industrial policy; (2) specifying the key public and private institutions; (3) addressing the core technical complements and their owners, then tracing their interactions through information linkages and technology transfers; and (4) depicting the market prospects and evaluating the potential profound influences on technological change and social developments. Our TDS methodology is illustrated using the field of Big Data & Analytics (“BDA”).|https://doi.org/10.1016/j.techfore.2017.09.012|https://www.sciencedirect.com/science/article/pii/S0040162517311927||||2018|A technology delivery system for characterizing the supply side of technology emergence: Illustrated for Big Data & Analytics|Ying Huang and Alan L. Porter and Scott W. Cunningham and Douglas K.R. Robinson and Jianhua Liu and Donghua Zhu|article|HUANG2018165|||Technological Forecasting and Social Change|00401625||130|||||14704|2,226|Q1|117|448|1099|35581|10127|1061|9,01|79,42|United States|Northern America|1970-2020|Applied Psychology (Q1); Business and International Management (Q1); Management of Technology and Innovation (Q1)|21,116|8.593|0.02416|25428587|1949868303
SIET '21|Malang, Indonesia|Precission Agriculture, Unmanned Aerial Vehicle, Vegetation Index, Image Processing|6|173–178|6th International Conference on Sustainable Information Engineering and Technology 2021|Agriculture assumes a vital role in human life because it provides food, feed for livestock, and bioenergy. The agricultural sector is expected to meet the needs of secure and nutritious food for the community at all times to boost productivity. Providing nutrition, water and light precisely and measuredly is an important effort in plant cultivation to produce quality. This effort can be materialized by implementing smart farming involving devices and information technology. Vast field surveillance or monitoring is made easy with the advent of unmanned aerial vehicle (UAV). Detection of plant condition can be achieved by obtaining Vegetation Index (VI) through camera imaging in UAVs which are more economic compared to multispectral or hyperspectral cameras. This study aims to obtain VI that is accurate but still economical, so that it can be utilized even by small-scale agriculture. The work that will be done is to conduct repair experiments at several stages of image processing to produce a new, more accurate VI. The research stages started from experiments on previous research, to finding new research opportunities in VI. Furthermore, the experiment was carried out with the addition of white balance value parameters and other UAV sensor parameters at the Pre-Processing stage to improve its quality. The hypothesis of adding white balance parameters should prove to be more accurate in correcting shooting in various light conditions. Next, try to modify the feature extraction algorithm using Color Extraction Edge Detection. Followed by modifying it using Back Propagation Neural Network to increase accuracy at the image processing stage. After synthesizing some of these experiments, a new formula or model VI using the camera on the UAV is expected to be produced. This research will contribute to the modification of methods or algorithms at the image processing stage to produce a corrected image in producing a new VI that is more accurate using a camera on a more economical UAV.|10.1145/3479645.3479661|https://doi.org/10.1145/3479645.3479661|New York, NY, USA|Association for Computing Machinery|9781450384070|2021|Camera-Based Vegetation Index from Unmanned Aerial Vehicles|Kusnandar, Toni and Surendro, Kridanto|inproceedings|10.1145/3479645.3479661|||||||||||||||||||||||||||||25789580|42
ICITEE2021|Changde, Hunan, China|intelligence, digitization, financial robot, RPA, electric power enterprise|5||The 4th International Conference on Information Technologies and Electrical Engineering|Under the background of the new technology era of cloud, big things, mobile intelligence, RPA (RoboticsProcessAutomation) technology, as an important and mature application in the field of artificial intelligence, can help financial personnel to free themselves from a large number of simple and complex transactional work and invest in Financial analysis, scientific decision-making and other high value-added work. At present, financial robot products based on RPA technology can be extended to be compatible with OCR, voice, intelligent customer service, deep learning and other functions, supporting the establishment of risk management and control systems and intelligent application scenarios, and ultimately improve the cross-business collaboration capabilities and operation automation efficiency of financial management. Effectively control financial risks, improve the efficiency of data asset use and financial analysis and decision-making capabilities, and provide power companies with good management and economic benefits. This article first analyzes the advantages and technical characteristics of RPA technology, then summarizes the practical application of financial robotics technology in power companies, explores the role of RPA technology in financial digital transformation, and studies its risk management and control models, which are of great significance to improving the comprehensive management level of power grid companies.|10.1145/3513142.3513235|https://doi.org/10.1145/3513142.3513235|New York, NY, USA|Association for Computing Machinery|9781450386494|2022|Analysis of The Advancement of Rpa Technology and Its Application in the Financial Field of Electric Power Enterprises|Zhang, Le and Ren, Junda and Yang, Zhi and Yin, Zenan and Chen, Yiting and Gu, Yiming|inproceedings|10.1145/3513142.3513235|91||||||||||||||||||||||||||||27440550|42
MSIE '22|Chiang Mai, Thailand||10|373–382|Proceedings of the 4th International Conference on Management Science and Industrial Engineering|This research presents the development of an Expert System to predict Canned Motor Pump (CMP) Status by applying a machine learning (ML) algorithm with domain expert knowledge in the case study plant. A Case study plant is a petrochemical plant that uses CMP to transfer process medium within inside plant battery limit (ISBL). At present, The CMP maintenance strategy is improving from condition-based maintenance to predictive maintenance. To archive desired level of predictive maintenance need CMP domain expert knowledge to find potential failure signs. This expert system is contributing to reducing expertise human load by substitution with the system. The research contains identifying system framework, experiment steps, including dataset preparation and model testing. The experiment result shows Random Forest (RF) algorithm is suitable for this system due to model performance evaluation comparing four algorithms with confusion matrix and similar data resampling and hyperparameter tuning method. Further on, this contribution is a role model, and enrolling in other equipment in the case study plant is a benefit of this work. Recommendation and key success factors found during this research are also mentioned in the conclusion for further work as a continuous improvement process cycle.|10.1145/3535782.3535831|https://doi.org/10.1145/3535782.3535831|New York, NY, USA|Association for Computing Machinery|9781450395816|2022|Expert System Development to Predict Canned Motor Pump Status|Thuensuwan, Komkrish and Chutima, Parames|inproceedings|10.1145/3535782.3535831|||||||||||||||||||||||||||||30571464|42
||unsupervised learning, copula fitting, Anomaly scoring|26|||The anomaly detection method presented by this article has a special feature: it not only indicates whether or not an observation is anomalous but also tells what exactly makes an anomalous observation unusual. Hence, it provides support to localize the reason of the anomaly.The proposed approach is model based; it relies on the multivariate probability distribution associated with the observations. Since the rare events are present in the tails of the probability distributions, we use copula functions, which are able to model the fat-tailed distributions well. The presented procedure scales well; it can cope with a large number of high-dimensional samples. Furthermore, our procedure can cope with missing values as well, which occur frequently in high-dimensional datasets.In the second part of the article, we demonstrate the usability of the method through a case study, where we analyze a large dataset consisting of the performance counters of a real mobile telecommunication network. Since such networks are complex systems, the signs of sub-optimal operation can remain hidden for a potentially long time. With the proposed procedure, many such hidden issues can be isolated and indicated to the network operator.|10.1145/3372274|https://doi.org/10.1145/3372274|New York, NY, USA|Association for Computing Machinery||2020|Copula-Based Anomaly Scoring and Localization for Large-Scale, High-Dimensional Continuous Data|Horv\'{a}th, G\'{a}bor and Kov\'{a}cs, Edith and Molontay, Roland and Nov\'{a}czki, Szabolcs|article|10.1145/3372274|26|apr|ACM Trans. Intell. Syst. Technol.|21576904|3|11|June 2020||||||||||||||||||||||33343108|273436860
ICSE '20|Seoul, South Korea|evidence-based software engineering, survey, empirical software engineering, systematic (literature) review, grey literature|13|1422–1434|Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering|Context: Following on other scientific disciplines, such as health sciences, the use of Grey Literature (GL) has become widespread in Software Engineering (SE) research. Whilst the number of papers incorporating GL in SE is increasing, there is little empirically known about different aspects of the use of GL in SE research.Method: We used a mixed-methods approach for this research. We carried out a Systematic Literature Review (SLR) of the use of GL in SE, and surveyed the authors of the selected papers included in the SLR (as GL users) and the invited experts in SE community on the use of GL in SE research. Results: We systematically selected and reviewed 102 SE secondary studies that incorporate GL in SE research, from which we identified two groups based on their reporting: 1) 76 reviews only claim their use of GL; 2) 26 reviews report the results by including GL. We also obtained 20 replies from the GL users and 24 replies from the invited SE experts. Conclusion: There is no common understanding of the meaning of GL in SE. Researchers define the scopes and the definitions of GL in a variety of ways. We found five main reasons of using GL in SE research. The findings have enabled us to propose a conceptual model for how GL works in SE research lifecycle. There is an apparent need for research to develop guidelines for using GL in SE and for assessing quality of GL. The current work can provide a panorama of the state-of-the-art of using GL in SE for the follow-up research, as to determine the important position of GL in SE research.|10.1145/3377811.3380336|https://doi.org/10.1145/3377811.3380336|New York, NY, USA|Association for Computing Machinery|9781450371216|2020|An Evidence-Based Inquiry into the Use of Grey Literature in Software Engineering|Zhang, He and Zhou, Xin and Huang, Xin and Huang, Huang and Babar, Muhammad Ali|inproceedings|10.1145/3377811.3380336|||||||||||||||||||||||||||||34322662|42
||Source selection, Data integration, Data cleaning||197-213||In big data era, information integration often requires abundant data extracted from massive data sources. Due to a large number of data sources, data source selection plays a crucial role in information integration, since it is costly and even impossible to access all data sources. Data Source selection should consider both efficiency and effectiveness issues. For efficiency, the approach should scale to large data source amount. From effectiveness aspect, data quality and overlapping of sources are to be considered. In this paper, we study source selection problem in Big Data and propose methods which can scale to datasets with up to millions of data sources and guarantee the quality of results. Motivated by this, we propose a new metric taking the expected number of true values a source can provide as a criteria to evaluate the contribution of a data source. Based on our proposed index, we present a scalable algorithm and two pruning strategies to improve the efficiency without sacrificing precision. Experimental results on both real world and synthetic data sets show that our methods can select sources providing a large proportion of true values efficiently and can scale to massive data sources.|https://doi.org/10.1016/j.ins.2018.11.029|https://www.sciencedirect.com/science/article/pii/S0020025518309162||||2019|Data source selection for information integration in big data era|Yiming Lin and Hongzhi Wang and Jianzhong Li and Hong Gao|article|LIN2019197|||Information Sciences|00200255||479|||||15134|1,524|Q1|184|928|2184|40007|17554|2168|7,89|43,11|United States|Northern America|1968-2021|Artificial Intelligence (Q1); Computer Science Applications (Q1); Control and Systems Engineering (Q1); Information Systems and Management (Q1); Software (Q1); Theoretical Computer Science (Q1)|44,038|6.795|0.04908|34732837|1633962588
||Big data, Machine learning, Data size, Prediction accuracy, Social media||120175||The access of machine learning techniques in popular programming languages and the exponentially expanding big data from social media, news, surveys, and markets provide exciting challenges and invaluable opportunities for organizations and individuals to explore implicit information for decision making. Nevertheless, the users of machine learning usually find that these sophisticated techniques could incur a high level of tensions caused by the selection of the appropriate size of the training data set among other factors. In this paper, we provide a systematic way of resolving such tensions by examining practical examples of predicting popularity and sentiment of posts on Twitter and Facebook, blogs on Mashable, news on Google and Yahoo, the US house survey, and Bitcoin prices. Interesting results show that for the case of big data, using around 20% of the full sample often leads to a better prediction accuracy than opting for the full sample. Our conclusion is found to be consistent across a series of experiments. The managerial implication is that using more is not necessarily the best and users need to be cautious about such an important sensitivity as the simplistic approach may easily lead to inferior solutions with potentially detrimental consequences.|https://doi.org/10.1016/j.techfore.2020.120175|https://www.sciencedirect.com/science/article/pii/S0040162520310015||||2020|Tension in big data using machine learning: Analysis and applications|Huamao Wang and Yumei Yao and Said Salhi|article|WANG2020120175|||Technological Forecasting and Social Change|00401625||158|||||14704|2,226|Q1|117|448|1099|35581|10127|1061|9,01|79,42|United States|Northern America|1970-2020|Applied Psychology (Q1); Business and International Management (Q1); Management of Technology and Innovation (Q1)|21,116|8.593|0.02416|34763257|1949868303
||Visualization;Data analysis;Databases;Data integrity;Semantics;Big Data;Internet;Big Data;E-commerce;Internet marketing;Marketing design||77-81|2020 International Conference on E-Commerce and Internet Technology (ECIT)|In the process of big data processing, big data analysis is the core work content. After obtaining a large amount of data, we use related analysis technology to perform data processing and analysis to obtain knowledge. Its related content includes visual analysis, data mining, predictive analysis, semantic analysis and data quality management. How to obtain big data, classify and store according to data types, mine valuable information from big data, and effectively apply big data in the field of precision marketing are hot topics of research. On the basis of researching the key technologies of big data analysis, this paper uses Hadoop big data to analyze and store the massive online logs generated by users' mobile terminals, and calculates and builds user characteristic databases. We use relevant analysis technology to analyze the user's location information, browsing and usage habits, hobbies, and focus content. At the same time, a precise marketing model is established according to user behavior characteristics and attributes, thereby improving the marketing effect of the enterprise.|10.1109/ECIT50008.2020.00026|||||2020|Design of Network Precision Marketing Based on Big Data Analysis Technology|Gan, Wenting|inproceedings|9134140||April|||||||||||||||||||||||||||38114093|42
||Big Data;Task analysis;Cloud computing;Business;Security;Monitoring;Data mining||1-9|Designing Big Data Platforms: How to Use, Deploy, and Maintain Big Data Systems|This chapter discusses the different aspects of designing Big Data platforms, in order to define what makes a big platform and to set expectations for these platforms. The solutions for Big Data processing vary based on the company strategy. A modern Big Data platform has several requirements, and to meet them correctly, expectations with regard to data should be set. Securing data has become a crucial aspect of a modern Big Data platform. The data quality depends on factors such as accuracy, consistency, reliability, and visibility. One of the hard problems of Big Data is backups as the vast amount of storage needed is overwhelming for backups. The Big Data platform should provide an extract, transform, and load (ETL) solution/s that manages the experience end to end. ETL developers should be able to develop, test, stage, and deploy their changes. Big Data platforms are quite complex as they are built based on distributed systems.|10.1002/9781119690962.ch1|https://ieeexplore.ieee.org/document/9821748||Wiley|9781119690948|2021|An Introduction: What's a Modern Big Data Platform|Aytas, Yusuf|inbook|9821748|||||||||||||||||||||||||||||40250399|42
||technology, software, computational biology, history, bioinformatics, data protocols, DNA sequencing, tools, third-generation sequencing (TGS)|30|||The recent advances in DNA sequencing technology, from first-generation sequencing (FGS) to third-generation sequencing (TGS), have constantly transformed the genome research landscape. Its data throughput is unprecedented and severalfold as compared with past technologies. DNA sequencing technologies generate sequencing data that are big, sparse, and heterogeneous. This results in the rapid development of various data protocols and bioinformatics tools for handling sequencing data.In this review, a historical snapshot of DNA sequencing is taken with an emphasis on data manipulation and tools. The technological history of DNA sequencing is described and reviewed in thorough detail. To manipulate the sequencing data generated, different data protocols are introduced and reviewed. In particular, data compression methods are highlighted and discussed to provide readers a practical perspective in the real-world setting. A large variety of bioinformatics tools are also reviewed to help readers extract the most from their sequencing data in different aspects, such as sequencing quality control, genomic visualization, single-nucleotide variant calling, INDEL calling, structural variation calling, and integrative analysis. Toward the end of the article, we critically discuss the existing DNA sequencing technologies for their pitfalls and potential solutions.|10.1145/3340286|https://doi.org/10.1145/3340286|New York, NY, USA|Association for Computing Machinery||2019|DNA Sequencing Technologies: Sequencing Data Protocols and Bioinformatics Tools|Wong, Ka-Chun and Zhang, Jiao and Yan, Shankai and Li, Xiangtao and Lin, Qiuzhen and Kwong, Sam and Liang, Cheng|article|10.1145/3340286|98|sep|ACM Comput. Surv.|03600300|5|52|September 2020||||23038|2,079|Q1|163|112|365|15859|6978|365|19,16|141,60|United States|Northern America|1969-2020|Computer Science (miscellaneous) (Q1); Theoretical Computer Science (Q1)|10,790|10.282|0.01438|40899244|1517405264
||||||The Handbook of Multimodal-Multisensor Interfaces provides the first authoritative resource on what has become the dominant paradigm for new computer interfaces-user input involving new media (speech, multi-touch, gestures, writing) embedded in multimodal-multisensor interfaces. These interfaces support smartphones, wearables, in-vehicle, robotic, and many other applications that are now highly competitive commercially. This edited collection is written by international experts and pioneers in the field. It provides a textbook for students, and a reference and technology roadmap for professionals working in this rapidly emerging area. Volume 1 of the handbook presents relevant theory and neuroscience foundations for guiding the development of high-performance systems. Additional chapters discuss approaches to user modeling, interface design that supports user choice, synergistic combination of modalities with sensors, and blending of multimodal input and output. They also highlight an in-depth look at the most common multimodal-multisensor combinations- for example, touch and pen input, haptic and non-speech audio output, and speech co-processed with visible lip movements, gaze, gestures, or pen input. A common theme throughout is support for mobility and individual differences among users-including the world's rapidly growing population of seniors. These handbook chapters provide walk-through examples and video illustrations of different system designs and their interactive use. Common terms are defined, and information on practical resources is provided (e.g., software tools, data resources) for hands-on project work to develop and evaluate multimodal-multisensor systems. In the final chapter, experts exchange views on a timely and controversial challenge topic, and how they believe multimodal-multisensor interfaces should be designed in the future to most effectively advance human performance.||||Association for Computing Machinery and Morgan &amp; Claypool|9781970001679|2017|The Handbook of Multimodal-Multisensor Interfaces: Foundations, User Modeling, and Common Modality Combinations - Volume 1||book|10.1145/3015783||||||14||||"\"Oviatt, Sharon and Schuller, Bj\"\"{o}rn and Cohen, Philip R. and Sonntag, Daniel and Potamianos, Gerasimos and Kr\\\"\"{u}ger, Antonio\""|||||||||||||||||||42201711|42
||Entropy;Measurement;Correlation;Big data;Data structures;Yttrium;Sparks||1113-1122|2015 IEEE International Conference on Big Data (Big Data)|Data quality is a challenging problem in many real world application domains. While a lot of attention has been given to detect anomalies for data at rest, detecting anomalies for streaming applications still largely remains an open problem. For applications involving several data streams, the challenge of detecting anomalies has become harder over time, as data can dynamically evolve in subtle ways following changes in the underlying infrastructure. In this paper, we describe and empirically evaluate an online anomaly detection pipeline that satisfies two key conditions: generality and scalability. Our technique works on numerical data as well as on categorical data and makes no assumption on the underlying data distributions. We implement two metrics, relative entropy and Pearson correlation, to dynamically detect anomalies. The two metrics we use provide an efficient and effective detection of anomalies over high velocity streams of events. In the following, we describe the design and implementation of our approach in a Big Data scenario using state-of-the-art streaming components. Specifically, we build on Kafka queues and Spark Streaming for realizing our approach while satisfying the generality and scalability requirements given above. We show how a combination of the two metrics we put forward can be applied to detect several types of anomalies - like infrastructure failures, hardware misconfiguration or user-driven anomalies - in large-scale telecommunication networks. We also discuss the merits and limitations of the resulting architecture and empirically evaluate its scalability on a real deployment over live streams capturing events from millions of mobile devices.|10.1109/BigData.2015.7363865|||||2015|Online anomaly detection over Big Data streams|Rettig, Laura and Khayati, Mourad and Cudré-Mauroux, Philippe and Piórkowski, Michal|inproceedings|7363865||Oct|||||||||||||||||||||||||||42921374|42
SEHS '18|Gothenburg, Sweden|conceptual modelling, precision medicine, data quality|4|14–17|Proceedings of the International Workshop on Software Engineering in Healthcare Systems|The continuous improvement in our understanding of the human genome is leading to an increasing viable and effective Precision Medicine. Its intention is to provide a personalized solution to any individual health problem. Nevertheless, three main issues must be considered to make Precision Medicine a reality: i) the understanding of the huge amount of genomic data, spread out in hundreds of genome data sources, with different formats and contents, whose semantic interoperability is a must; ii) the development of information systems intended to guide the search of relevant genomic repositories related with a disease, the identification of significant information for its prevention, diagnosis and/or treatment and its management in an efficient software platform; iii) the high variability in the quality of the publicly available information. This paper presents a conceptual framework for solving these problems by i) using a precise conceptual schema of the human genome, and ii) introducing a method to search, identify, load and adequately interpret the required data, assuring its quality during the entire process.|10.1145/3194696.3194700|https://doi.org/10.1145/3194696.3194700|New York, NY, USA|Association for Computing Machinery|9781450357340|2018|Towards an Effective Medicine of Precision by Using Conceptual Modelling of the Genome: Short Paper|Palacio, Ana Le\'{o}n and L\'{o}pez, \'{O}scar Pastor|inproceedings|10.1145/3194696.3194700|||||||||||||||||||||||||||||44677890|42
||Machine learning, Artificial intelligence, Blockchain technology||521-522|||https://doi.org/10.1016/j.accpm.2018.12.015|https://www.sciencedirect.com/science/article/pii/S2352556818305368||||2019|Data quality and blockchain technology|Valentina Bellini and Alberto Petroni and Giuseppina Palumbo and Elena Bignami|article|BELLINI2019521|||Anaesthesia Critical Care & Pain Medicine|23525568|5|38|||||21100435142|0,942|Q1|38|192|396|4565|590|179|1,48|23,78|Netherlands|Western Europe|2015-2020|Anesthesiology and Pain Medicine (Q1); Critical Care and Intensive Care Medicine (Q1); Medicine (miscellaneous) (Q2)||||48660203|1325666609
Koli Calling '17|Koli, Finland|mechanics, CS education, key concepts, practices, data management, model, core technologies, principles|10|30–39|Proceedings of the 17th Koli Calling International Conference on Computing Education Research|When preparing new topics for teaching, it is important to identify their central aspects. Sets of fundamental ideas, great principles or big ideas have already been described for several parts of computer science. Yet, existing catalogs of ideas, principles and concepts of computer science only consider the field data management marginally. However, we assume that several concepts of data management are fundamental to CS and, despite the significant changes in this field in recent years, have long-term relevance. In order to provide a comprehensive overview of the key concepts of data management and to bring relevant parts of this field to school, we describe and use an empirical approach to determine such central aspects systematically. This results in a model of key concepts of data management. On the basis of examples, we show how the model can be interpreted and used in different contexts and settings.|10.1145/3141880.3141886|https://doi.org/10.1145/3141880.3141886|New York, NY, USA|Association for Computing Machinery|9781450353014|2017|Key Concepts of Data Management: An Empirical Approach|Grillenberger, Andreas and Romeike, Ralf|inproceedings|10.1145/3141880.3141886|||||||||||||||||||||||||||||51487447|42
||Energy big data, Smart energy management, Big data analytics, Smart grid, Demand side management (DSM)||215-225||Large amounts of data are increasingly accumulated in the energy sector with the continuous application of sensors, wireless transmission, network communication, and cloud computing technologies. To fulfill the potential of energy big data and obtain insights to achieve smart energy management, we present a comprehensive study of big data driven smart energy management. We first discuss the sources and characteristics of energy big data. Also, a process model of big data driven smart energy management is proposed. Then taking smart grid as the research background, we provide a systematic review of big data analytics for smart energy management. It is discussed from four major aspects, namely power generation side management, microgrid and renewable energy management, asset management and collaborative operation, as well as demand side management (DSM). Afterwards, the industrial development of big data-driven smart energy management is analyzed and discussed. Finally, we point out the challenges of big data-driven smart energy management in IT infrastructure, data collection and governance, data integration and sharing, processing and analysis, security and privacy, and professionals.|https://doi.org/10.1016/j.rser.2015.11.050|https://www.sciencedirect.com/science/article/pii/S1364032115013179||||2016|Big data driven smart energy management: From big data to big insights|Kaile Zhou and Chao Fu and Shanlin Yang|article|ZHOU2016215|||Renewable and Sustainable Energy Reviews|13640321||56|||||27567|3,522|Q1|295|643|3239|73956|54087|3206|16,30|115,02|United Kingdom|Western Europe|1997-2021|Renewable Energy, Sustainability and the Environment (Q1)||||53424775|1303297312
IDEAS '22|Budapest, Hungary|Architecture Pattern, Category Theory, Data Lakes|9|75–83|Proceedings of the 26th International Database Engineered Applications Symposium|The management of Big Data requires flexible systems to handle the heterogeneity of data models as well as the complexity of analytical workflows. Traditional systems like data warehouses have reached their limits due to their rigid schema-on-write paradigm, that requires well identified and defined use cases to ingest data. Data lakes, with their schema-on-read paradigm, have been proposed as more flexible systems in which raw data are directly stored in their original format associated with metadata, to be accessed and transformed only when users need to process or analyze them. Thus, it is necessary to define and control the different levels of abstraction and the dependencies among functionalities of a data lake to use it efficiently. In this article, we present a formal framework aiming to define a data lake pattern and to unify the interactions among the functionalities. We use the category theory as theoretical foundations to benefit from its high level of abstraction and its compositionality. By relying on different categories and functors, we ensure the navigation among the functionalities and allow the composition of multiples operations, while keeping track of the entire lineage of data. We also show how our framework can be applied on a simple example of data lake.|10.1145/3548785.3548797|https://doi.org/10.1145/3548785.3548797|New York, NY, USA|Association for Computing Machinery|9781450397094|2022|A Formal Framework for Data Lakes Based on Category Theory|Guyot, Alexis and Gillet, Annabelle and Leclercq, Eric and Cullot, Nadine|inproceedings|10.1145/3548785.3548797|||||||||||||||||||||||||||||53915701|42
|Melbourne, Victoria, Australia|||||"\"Welcome to SIGMOD 2015 -- officially, the 2015 ACM SIGMOD International Conference on the Management of Data! This year's conference is being held in the beautiful cultural capital of Australia, Melbourne. During the Gold Rush period of the 19th Century, Melbourne was the richest city in the world, and as a result it is filled with many unique neighborhoods and distinctive buildings. In addition to wonderful neighborhoods to explore, the city has great museums and other cultural attractions, as well as a fine multi-cultural atmosphere. For those who would like to explore the outdoors, popular highlights are the Phillip Island Nature Park (90 minutes away), which features wild penguins who return in a parade each day at sunset, and the Great Ocean Road, one of the world's most scenic coastal drives, including the famous towering 12 Apostles.SIGMOD 2015's exciting technical program reflects not only traditional topics, but the database community's role in broader data science and data analytics. The keynote from Laura Haas, \"\"The Power Behind the Throne: Information Integration in the Age of Data-Driven Discovery\"\" highlights the role of database and data integration techniques in the growing field of data science. Jignesh Patel's talk, \"\"From Data to Insights @ Bare Metal Speed,\"\" explains how hardware and software need to be co-evolved to support the needs of scalable data analytics. Jennifer Widom, winner of the 2015 ACM-W Athena Lecturer Award for fundamental contributions to computer science, will give her award talk, \"\"Three Favorite Results,\"\" on Tuesday. Christopher R\\'{e} will lead a panel on \"\"Machine Learning and Databases: The Sound of Things to Come or a Cacophony of Hype?,\"\" with participants Divyakant Agrawal, Magdalena Balazinska, Michael Cafarella, Michael Jordan, Tim Kraska, and Raghu Ramakrishnan. Of course, there are also 106 research paper presentations, 4 tutorials, 30 demonstrations, and 18 industrial papers. Papers will be presented both as talks during the research sessions, and as part of plenary Poster Sessions.SIGMOD 2015 is preceded by the PhD Workshop, as well as workshops on leading-edge topics like data analytics (DanaC), databases and the Web (WebDB), exploratory search (ExploreDB), managing and mining spatial data (GeoRich), and graph data (GRADES); the New Researcher Symposium will take place on Wednesday. The banquet will be held in a Melbourne landmark, the Town Hall.As in recent years, we had two submission deadlines for SIGMOD this year, one in August and one in November. The review process was journal-style, with multiple rounds of reviews coordinated by the Group Leaders. We accepted 34 of 137 papers from the first deadline and 72 of 278 from the second deadline. The total acceptance rate was about 25.5%, and we believe that the revision processhas improved the quality of the technical program.\""|||New York, NY, USA|Association for Computing Machinery|9781450327589|2015|SIGMOD '15: Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data||proceedings|10.1145/2723372|||||||||||||||||||||||||||||56526169|42
||Data integrity;Transforms;Markov processes;Maintenance engineering;Probabilistic logic;Cleaning;Inference algorithms;Data repairing;data cleaning;data quality;statistical relational learning;DeepDive;factor graph||1-6|2021 International Symposium on Networks, Computers and Communications (ISNCC)|Dirty data is ubiquitous in real-world, and data cleaning is a long-standing problem. The importance of data cleaning is growing in the era of big data. In this paper we propose a novel data repairing approach by leveraging statistical relational learning (SRL). We learn Bayesian networks of attributes from the dirty data, then transform the dependency relationships among attributes into first-order logic formulas. We calculate the weight of each formula based on the mutual information of the attributes involved in the formula and obtain Markov logic network (often abbreviated as MLN) by assigning weight to each first-order logic formula. Then we transform Markov logic networks into inference rules and conduct these inference rules on DeepDive. The inference results are utilized to repair dirty data at last. Experiments on real-world datasets demonstrate that our approach has higher accuracy in terms of different situations and is universal for different kinds of datasets.|10.1109/ISNCC52172.2021.9615868|||||2021|Automatic Data Repairs with Statistical Relational Learning|Li, Ling and Li, Weibang and Zhu, Lidong and Li, Chengjie and Zhang, Zhen|inproceedings|9615868||Oct|||||||||||||||||||||||||||60145467|42
||Deep learning, Data management, Production quality DL models, Challenges, Solutions, Validation||111359||Deep learning (DL) based software systems are difficult to develop and maintain in industrial settings due to several challenges. Data management is one of the most prominent challenges which complicates DL in industrial deployments. DL models are data-hungry and require high-quality data. Therefore, the volume, variety, velocity, and quality of data cannot be compromised. This study aims to explore the data management challenges encountered by practitioners developing systems with DL components, identify the potential solutions from the literature and validate the solutions through a multiple case study. We identified 20 data management challenges experienced by DL practitioners through a multiple interpretive case study. Further, we identified 48 articles through a systematic literature review that discuss the solutions for the data management challenges. With the second round of multiple case study, we show that many of these solutions have limitations and are not used in practice due to a combination of four factors: high cost, lack of skill-set and infrastructure, inability to solve the problem completely, and incompatibility with certain DL use cases. Thus, data management for data-intensive DL models in production is complicated. Although the DL technology has achieved very promising results, there is still a significant need for further research in the field of data management to build high-quality datasets and streams that can be used for building production-ready DL systems. Furthermore, we have classified the data management challenges into four categories based on the availability of the solutions.|https://doi.org/10.1016/j.jss.2022.111359|https://www.sciencedirect.com/science/article/pii/S0164121222000905||||2022|Data management for production quality deep learning models: Challenges and solutions|Aiswarya Raj Munappy and Jan Bosch and Helena Holmström Olsson and Anders Arpteg and Björn Brinne|article|MUNAPPY2022111359|||Journal of Systems and Software|01641212||191|||||19309|0,642|Q1|109|183|619|11845|3058|590|4,94|64,73|United States|Northern America|1979, 1981-2021|Hardware and Architecture (Q1); Information Systems (Q2); Software (Q2)|6,579|2.829|0.00727|60705743|1111852116
||Big data, Internet of things, Data analytics, Sentiment analysis, Social network analysis, Web analytics||293-303||Big data represents a new technology paradigm for data that are generated at high velocity and high volume, and with high variety. Big data is envisioned as a game changer capable of revolutionizing the way businesses operate in many industries. This article introduces an integrated view of big data, traces the evolution of big data over the past 20 years, and discusses data analytics essential for processing various structured and unstructured data. This article illustrates the application of data analytics using merchant review data. The impacts of big data on key business performances are then evaluated. Finally, six technical and managerial challenges are discussed.|https://doi.org/10.1016/j.bushor.2017.01.004|https://www.sciencedirect.com/science/article/pii/S0007681317300046||||2017|Big data: Dimensions, evolution, impacts, and challenges|In Lee|article|LEE2017293|||Business Horizons|00076813|3|60|||||20209|2,174|Q1|87|64|278|2475|2066|227|6,68|38,67|United Kingdom|Western Europe|1957-2020|Business and International Management (Q1); Marketing (Q1)|7,443|6.361|0.00571|62892262|847373672
||Geospatial, Big Data, Spatial technology, Cyberinfrastructure, Data science||74-94||As spatial technology has evolved and become integrated in to archaeology, we face a new set of challenges posed by the sheer size and complexity of data we use and produce. In this paper I discuss the prospects and problems of Geospatial Big Data (GBD) – broadly defined as data sets with locational information that exceed the capacity of widely available hardware, software, and/or human resources. While the datasets we create today remain within available resources, we nonetheless face the same challenges as many other fields that use and create GBD, especially in apprehensions over data quality and privacy. After reviewing the kinds of archaeological geospatial data currently available I discuss the near future of GBD in writing culture histories, making decisions, and visualizing the past. I use a case study from New Zealand to argue for the value of taking a data quantity-in-use approach to GBD and requiring applications of GBD in archaeology be regularly accompanied by a Standalone Quality Report.|https://doi.org/10.1016/j.jas.2017.06.003|https://www.sciencedirect.com/science/article/pii/S0305440317300821||||2017|Geospatial Big Data and archaeology: Prospects and problems too great to ignore|Mark D. McCoy|article|MCCOY201774|||Journal of Archaeological Science|03054403||84||Archaeological GIS Today: Persistent Challenges, Pushing Old Boundaries, and Exploring New Horizons|||31405|1,572|Q1|126|137|365|10414|1239|362|3,04|76,01|United States|Northern America|1974-2020|Archeology (Q1); Archeology (arts and humanities) (Q1); History (Q1)|17,761|3.216|0.01111|63000873|1214954751
||COVID-19;Pandemics;Data integrity;Blockchain;Big Data;Statistics;Testing;Covid-19;Blockchain;Big Data;Data Quality;data governance||84-89|2020 6th IEEE Congress on Information Science and Technology (CiSt)|The effects of COVID-19 have quickly spread around the world, testing the limits of the population and the public health sector. High demand on medical services are offset by disruptions in daily operations as hospitals struggle to function in the face of overcapacity, understaffing and information gaps. Faced with these problems, new technologies are being deployed to fight this pandemic and help medical staff governments to reduce its spread. Among these technologies, we find blockchains and Big Data which have been used in tracking, prediction applications and others. However, despite the help that these new technologies have provided, they remain limited if the data with which they are fed are not of good quality. In this paper, we highlight some benefits of using BIG Data and Blockchain to deal with this pandemic and some data quality issues that still present challenges to decision making. Finally we present a general Blockchain-based framework for data governance that aims to ensure a high level of data trust, security, and privacy.|10.1109/CiSt49399.2021.9357200|||||2020|Technology against COVID-19 A Blockchain-based framework for Data Quality|Ezzine, Imane and Benhlima, Laila|inproceedings|9357200||June||23271884|||||||||||||||||||||||||63973474|2086419430
UbiComp '18|Singapore, Singapore|PM10, sensing, challenges, PM2.5, Air quality, urban air, particulate matter|4|1162–1165|Proceedings of the 2018 ACM International Joint Conference and 2018 International Symposium on Pervasive and Ubiquitous Computing and Wearable Computers|Classic measurement grids with their static and expensive infrastructure are unfit to realize modern air quality monitoring needs, such as source appointment, pollution tracking or the assessment of personal exposure. Fine grained air quality assessment (both in time and space) is the future. Different approaches, ranging from measurement with low-cost sensors over advanced modeling and remote sensing to combinations of these have been proposed. This position paper summarizes our previous contributions in this field and lists what we see as open challenges for future research.|10.1145/3267305.3274762|https://doi.org/10.1145/3267305.3274762|New York, NY, USA|Association for Computing Machinery|9781450359665|2018|Challenges in Capturing and Analyzing High Resolution Urban Air Quality Data|Budde, Matthias and Riedel, Till|inproceedings|10.1145/3267305.3274762|||||||||||||||||||||||||||||64416543|42
Woodhead Publishing Series in Civil and Structural Engineering||Big data, Civil infrastructure sensing, Damage diagnosis, Machine learning, Occupant monitoring, Smart infrastructure, Structural health monitoring||639-677|Sensor Technologies for Civil Infrastructures (Second Edition)|With the growing scale and complexity of city infrastructures, the need for data analysis and machine learning is becoming more and more prominent in the field of civil infrastructure sensing. This coupled with the explosion of available sensing data in smart cities and smart infrastructures has offered new opportunities like never before. Using big data tools at a structure level, we can understand important information about structural properties and damage states, city environmental and operational conditions, as well as an individual user or group patterns. In this chapter, we explore and provide guidance for big data analytics and its application to civil infrastructure problems. Furthermore, we discuss future directions and trends that will enable large-scale monitoring of civil infrastructure and smart cities.|https://doi.org/10.1016/B978-0-08-102706-6.00007-6|https://www.sciencedirect.com/science/article/pii/B9780081027066000076||Woodhead Publishing|978-0-08-102706-6|2022|20 - Big data analysis for civil infrastructure sensing|Hae Young Noh and Jonathon Fagert|incollection|NOH2022639|||||||||Second Edition|Jerome P. Lynch and Hoon Sohn and Ming L. Wang|||||||||||||||||||69271846|42
||Big data;Quality of service;Monitoring;Decision making;Organizations;Data analysis;Big Data;information governance;service science;Service Level Management;service quality||207-212|International Conference on Information Society (i-Society 2014)|The Big Data is a modification of the traditional view of information organization, particularly view of the data warehouses and databases. Nowadays, business organizations must address a mix of structured, unstructured and streaming data that supports queries and reports. Business recognized the wealth of untapped information in open social media data. Therefore, the goal of this paper is to present the procedural approach on how to cope with massive data sets' management. The proposal included in this paper covers service science application.|10.1109/i-Society.2014.7009043|||||2014|Service science facing Big Data|Pankowska, Malgorzata|inproceedings|7009043||Nov|||||||||||||||||||||||||||78720206|42
||Data integration;Power grids;Kalman filters;Monitoring;Big Data;Data models;Distributed databases;smart grid;multi-source data;edge computing;filtering algorithm||97-101|2019 IEEE 5th International Conference on Computer and Communications (ICCC)|Based on the smart grid as the research background, this paper responded to the massive multi-source data processing requirements of the smart grid, and combined with distributed computing to provide the edge of the solution, aiming at the existing data of electric power equipment state monitoring data in noise and redundant data problems. A distributed kalman filter algorithm based on edge of computing was put forward. In this algorithm, event decision strategy was added to the data processing and transmission process of edge computing terminal to control the communication times between nodes and terminals in an event-driven way. Meanwhile, redundant data and data interfered by noise were reduced through the processing of the algorithm, so as to ensure the data quality and improve the fusion efficiency. Finally, the effectiveness of the method was verified by the analysis of compression efficiency and data fusion time.|10.1109/ICCC47050.2019.9064032|||||2019|Research and Analysis Validation of Data Fusion Technology Based on Edge Computing|Gao, Jian and Zhen, Yan and Bai, Huifeng and Huo, Chao and Wang, Dongshan and Zhang, Ganghong|inproceedings|9064032||Dec|||||||||||||||||||||||||||79909732|42
SIGMOD '16|San Francisco, California, USA|data integration, knowledge base construction, dark data, information extraction|13|847–859|Proceedings of the 2016 International Conference on Management of Data|"\"DeepDive is a system for extracting relational databases from dark data: the mass of text, tables, and images that are widely collected and stored but which cannot be exploited by standard relational tools. If the information in dark data --- scientific papers, Web classified ads, customer service notes, and so on --- were instead in a relational database, it would give analysts access to a massive and highly-valuable new set of \"\"big data\"\" to exploit.DeepDive is distinctive when compared to previous information extraction systems in its ability to obtain very high precision and recall at reasonable engineering cost; in a number of applications, we have used DeepDive to create databases with accuracy that meets that of human annotators. To date we have successfully deployed DeepDive to create data-centric applications for insurance, materials science, genomics, paleontologists, law enforcement, and others. The data unlocked by DeepDive represents a massive opportunity for industry, government, and scientific researchers.DeepDive is enabled by an unusual design that combines large-scale probabilistic inference with a novel developer interaction cycle. This design is enabled by several core innovations around probabilistic training and inference.\""|10.1145/2882903.2904442|https://doi.org/10.1145/2882903.2904442|New York, NY, USA|Association for Computing Machinery|9781450335317|2016|Extracting Databases from Dark Data with DeepDive|Zhang, Ce and Shin, Jaeho and R\'{e}, Christopher and Cafarella, Michael and Niu, Feng|inproceedings|10.1145/2882903.2904442|||||||||||||||||||||||||||||81957847|42
||Electricity consumption data, Data quality, Outlier detection, Outlier data, Smart grid||98-105||With the increasing penetration of traditional and emerging information technologies in the electric power industry, together with the rapid development of electricity market reform, the electric power industry has accumulated a large amount of data. Data quality issues have become increasingly prominent, which affect the accuracy and effectiveness of electricity data mining and energy big data analytics. It is also closely related to the safety and reliability of the power system operation and management based on data-driven decision support. In this paper, we study the data quality of electricity consumption data in a smart grid environment. First, we analyze the significance of data quality. Also, the definition and classification of data quality issues are explained. Then we analyze the data quality of electricity consumption data and introduce the characteristics of electricity consumption data in a smart grid environment. The data quality issues of electricity consumption data are divided into three types, namely noise data, incomplete data and outlier data. We make a detailed discussion on these three types of data quality issues. In view of that outlier data is one of the most prominent issues in electricity consumption data, so we mainly focus on the outlier detection of electricity consumption data. This paper introduces the causes of electricity consumption outlier data and illustrates the significance of the electricity consumption outlier data from the negative and positive aspects respectively. Finally, the focus of this paper is to provide a review on the detection methods of electricity consumption outlier data. The methods are mainly divided into two categories, namely the data mining-based and the state estimation-based methods.|https://doi.org/10.1016/j.rser.2016.10.054|https://www.sciencedirect.com/science/article/pii/S1364032116307109||||2017|Data quality of electricity consumption data in a smart grid environment|Wen Chen and Kaile Zhou and Shanlin Yang and Cheng Wu|article|CHEN201798|||Renewable and Sustainable Energy Reviews|13640321||75|||||27567|3,522|Q1|295|643|3239|73956|54087|3206|16,30|115,02|United Kingdom|Western Europe|1997-2021|Renewable Energy, Sustainability and the Environment (Q1)||||82775019|1303297312
||logistics, Robust fuzzy stochastic programming, Sustainable procurement, Big data, Hybrid uncertainty, ε-Constraint||120640||Today, in many organizations, the debate about the difference in core capabilities has become an important factor for market competition. Companies, based on the field of activity, decide to strengthen some of their capabilities, capacities, and expertise. Therefore, the focus of an organization on the strengths and efforts to develop its sustainability will lead to a competitive advantage in the marketplace. Due to changes in environmental factors, organizations have focused on carbon emissions in procurement and transportation that have the highest carbon footprint. This paper proposes a multi-objective, eco-sustainability model for a supply chain. The objectives are to minimize overall costs, maximize the efficiency of transportation vehicles and minimize information fraud in the process of information sharing within supply chain elements. Big data is considered in the amount of information exchanged between customers and other elements of the proposed supply chain; since there are frauds in information sharing then using big data 5Vs the model is adapted to control the cost of information loss leading to customer dissatisfaction. Since uncertainty is inevitable in the real environments, in this research hybrid uncertainty is considered. Because two sources of uncertainty are considered in most of the parameters, thus it is necessary to robustify the decision-making process. The model is a mixed integer nonlinear program including big data for an optimal sustainable procurement and transportation decision. A heuristic method is used to solve the big data problem that makes use of a robust fuzzy stochastic programming approach. The proposed model can prevent disturbances by using a scenario-based stochastic programming approach. An effective hybrid robust fuzzy stochastic method is also employed for controlling uncertainty in parameters and risk taking out of outbound decisions. To solve the multi-objective model, augmented ε-constraint method is utilized. The model performance is investigated in a comprehensive computational study.|https://doi.org/10.1016/j.jclepro.2020.120640|https://www.sciencedirect.com/science/article/pii/S0959652620306879||||2020|A robust fuzzy stochastic programming for sustainable procurement and logistics under hybrid uncertainty using big data|Hadi Gholizadeh and Hamed Fazlollahtabar and Mohammad Khalilzadeh|article|GHOLIZADEH2020120640|||Journal of Cleaner Production|09596526||258|||||19167|1,937|Q1|200|5126|10603|304498|103295|10577|9,56|59,40|United Kingdom|Western Europe|1993-2021|Environmental Science (miscellaneous) (Q1); Industrial and Manufacturing Engineering (Q1); Renewable Energy, Sustainability and the Environment (Q1); Strategy and Management (Q1)|170,352|9.297|0.18299|84081277|1121054297
||Integration methods, Urban functional zone classification, Urban management, Land use||102514||Remote Sensing (RS) has been used in urban mapping for a long time; however, the complexity and diversity of urban functional patterns are difficult to be captured by RS only. Emerging Geospatial Big Data (GBD) are considered as the supplement to RS data, and help to contribute to our understanding of urban lands from physical aspects (i.e., urban land cover) to socioeconomic aspects (i.e., urban land use). Integrating RS and GBD could be an effective way to combine physical and socioeconomic aspects with great potential for high-quality urban land use classification. In this study, we reviewed the existing literature and focused on the state-of-the-art and perspective of the urban land use categorization by integrating RS and GBD. Specifically, the commonly used RS features (e.g., spectral, textural, temporal, and spatial features) and GBD features (e.g., spatial, temporal, semantic, and sequence features) were identified and analyzed in urban land use classification. The integration strategies for RS and GBD features were categorized into feature-level integration (FI) and decision-level integration (DI). To be more specific, the FI method integrates the RS and GBD features and classifies urban land use types using the integrated feature sets; the DI method processes RS and GBD independently and then merges the classification results based on decision rules. We also discussed other critical issues, including analysis unit setting, parcel segmentation, parcel labeling of land use types, and data integration. Our findings provide a retrospect of different features from RS and GBD, strategies of RS and GBD integration, and their pros and cons, which could help to define the framework for future urban land use mapping and better support urban planning, urban environment assessment, urban disaster monitoring and urban traffic analysis.|https://doi.org/10.1016/j.jag.2021.102514|https://www.sciencedirect.com/science/article/pii/S030324342100221X||||2021|Integrating remote sensing and geospatial big data for urban land use mapping: A review|Jiadi Yin and Jinwei Dong and Nicholas A.S. Hamm and Zhichao Li and Jianghao Wang and Hanfa Xing and Ping Fu|article|YIN2021102514|||International Journal of Applied Earth Observation and Geoinformation|15698432||103|||||39563|1,623|Q1|98|16|523|1076|3348|520|6,62|67,25|Netherlands|Western Europe|1998-2020|Computers in Earth Sciences (Q1); Earth-Surface Processes (Q1); Global and Planetary Change (Q1); Management, Monitoring, Policy and Law (Q1)|11,556|5.933|0.01275|85125017|722670997
||cloud computing, IoT, smart manufacturing, industry 4.0, middleware, cyber-physical systems, fog computing|8|29–36||The advantages of the Internet of things (IoT) initiated the vision of Industry 4.0 in Europe and smart manufacturing in USA. Both visions aim to implement the smart factory to achieve similar objectives by utilizing new technologies. These technologies include cloud computing, fog computing, cyber-physical systems (CPS), and data analytics. Together they help automate and autonomize the manufacturing processes and controls to optimize the productivity, reliability, quality, cost-effeteness, and safety of these processes. While both visions are promising, developing and operating Industry 4.0 applications are extremely challenging. This is due to the complexity of the manufacturing processes as well as their management, controls, and integration dynamics. This paper introduces Man4Ware, a service-oriented middleware for Industry 4.0. Man4Ware can help facilitate the development and operations of cloud and fog-integrated smart manufacturing applications. Man4Ware offers many advantages through service level interfaces to enable easy utilization of new technologies and integration of different services to relax many of the challenges facing the development and operations of such applications1.|10.1145/3292384.3292389|https://doi.org/10.1145/3292384.3292389|New York, NY, USA|Association for Computing Machinery||2018|A Service-Oriented Middleware Framework for Manufacturing Industry 4.0|Al-Jaroodi, Jameela and Mohamed, Nader and Jawhar, Imad|article|10.1145/3292384.3292389||nov|SIGBED Rev.||5|15|October 2018||||||||||||||||||||||85833021|42
||Data integrity;Measurement;Internet of Things;Big Data;Data models;Sensor phenomena and characterization;Quality assessment;Big data model;data quality;Internet of Things (IoT);machine learning;trust||19995-20009||Continued development of communication technologies has led to widespread Internet-of-Things (IoT) integration into various domains, including health, manufacturing, automotive, and precision agriculture. This has further led to the increased sharing of data among such domains to foster innovation. Most of these IoT deployments, however, are based on heterogeneous, pervasive sensors, which can lead to quality issues in the recorded data. This can lead to sharing of inaccurate or inconsistent data. There is a significant need to assess the quality of the collected data, should it be shared with multiple application domains, as inconsistencies in the data could have financial or health ramifications. This article builds on the recent research on trust metrics and presents a framework to integrate such metrics into the IoT data cycle for real-time data quality assessment. Critically, this article adopts a mechanism to facilitate end-user parameterization of a trust metric tailoring its use in the framework. Trust is a well-established metric that has been used to determine the validity of a piece or source of data in crowd-sourced or other unreliable data collection techniques such as that in IoT. The article further discusses how the trust-based framework eliminates the requirement for a gold standard and provides visibility into data quality assessment throughout the big data model. To qualify the use of trust as a measure of quality, an experiment is conducted using data collected from an IoT deployment of sensors to measure air quality in which low-cost sensors were colocated with a gold standard reference sensor. The calculated trust metric is compared with two well-understood metrics for data quality, root mean square error (RMSE), and mean absolute error (MAE). A strong correlation between the trust metric and the comparison metrics shows that trust may be used as an indicative quality metric for data quality. The metric incorporates the additional benefit of its ability for use in low context scenarios, as opposed to RMSE and MAE, which require a reference for comparison.|10.1109/JSEN.2022.3203853|||||2022|End-to-End Data Quality Assessment Using Trust for Data Shared IoT Deployments|Byabazaire, John and O’Hare, Gregory M.P. and Delaney, Declan T.|article|9884973||Oct|IEEE Sensors Journal|15581748|20|22|||||||||||||||||||||||87670169|1628430050
||Medical services;Big data;Semantics;Data privacy;Standards;Biomedical imaging;Security;Big Data;technical requirements;data digitalization;semantic annotation;data integration;data privacy and security;data quality||291-296|Proceedings of the 2014 IEEE 15th International Conference on Information Reuse and Integration (IEEE IRI 2014)|Big Data technologies can be used to improve the quality and efficiency of healthcare delivery. The highest impact of Big Data applications is expected when data from various healthcare areas, such as clinical, administrative, financial, or outcome data, can be integrated. However, as of today, the seamless access to the various healthcare data pools is only possible in a very constrained and limited manner. For enabling the seamless access several technical requirements, such as data digitalization, semantic annotation, data sharing, data privacy and security as well as data quality need to be addressed. In this paper, we introduce a detailed analysis of these technical requirements and show how the results of our analysis lead towards a technical roadmap for Big Data in the healthcare domain.|10.1109/IRI.2014.7051902|||||2014|Towards a technology roadmap for big data applications in the healthcare domain|Zillner, Sonja and Oberkampf, Heiner and Bretschneider, Claudia and Zaveri, Amrapali and Faix, Werner and Neururer, Sabrina|inproceedings|7051902||Aug|||||||||||||||||||||||||||87773084|42
|||2|321–322|Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice|||https://doi.org/10.1145/3447404.3447421|New York, NY, USA|Association for Computing Machinery|9781450390293|2021|Ethical Issues in Probabilistic Text Entry|McMenemy, David|inbook|10.1145/3447404.3447421|||||||||1||||||||||||||||||||92330533|42
ICBRA '19|Seoul, Republic of Korea|single-cell sequencing, directional dependency, model-free|8|55–62|Proceedings of the 2019 6th International Conference on Bioinformatics Research and Applications|As severe dropout in single-cell RNA sequencing (scRNA-seq) degrades data quality, current methods for network inference face increased uncertainty from such data. To examine how dropout influences directional dependency inference from scRNA-seq data, we thus studied four methods based on discrete data that are model-free without parametric model assumptions. They include two established methods: conditional entropy and Kruskal-Wallis test, and two recent methods: causal inference by stochastic complexity and function index. We also included three non-directional methods for a contrast. On simulated data, function index performed most favorably at varying dropout rates, sample sizes, and discrete levels. On an scRNA-seq dataset from developing mouse cerebella, function index and Kruskal-Wallis test performed favorably over other methods in detecting expression of developmental genes as a function of time. Overall among the four methods, function index is most resistant to dropout for both directional and dependency inference. The next best choice, Kruskal-Wallis test, carries a directional bias towards a uniformly distributed variable. We conclude that a method robust to marginal distributions with a sufficiently large sample size can reap benefits of single-cell over bulk RNA sequencing in understanding molecular mechanisms at the cellular resolution.|10.1145/3383783.3383793|https://doi.org/10.1145/3383783.3383793|New York, NY, USA|Association for Computing Machinery|9781450372183|2020|Evaluating Model-Free Directional Dependency Methods on Single-Cell RNA Sequencing Data with Severe Dropout|Dvo\v{r}\'{a}kov\'{a}, Eli\v{s}ka and Kumar, Sajal and Kl\'{e}ma, Ji\v{r}\'{\i} and \v{Z}elezn\'{y}, Filip and Drbal, Karel and Song, Mingzhou|inproceedings|10.1145/3383783.3383793|||||||||||||||||||||||||||||93590933|42
||machine learning, classification, scientific applications, graphical data mining, Applied research, estimation, clustering, predictive analytics, domain knowledge|52|||Experimental results are often plotted as 2-dimensional graphical plots (aka graphs) in scientific domains depicting dependent versus independent variables to aid visual analysis of processes. Repeatedly performing laboratory experiments consumes significant time and resources, motivating the need for computational estimation. The goals are to estimate the graph obtained in an experiment given its input conditions, and to estimate the conditions that would lead to a desired graph. Existing estimation approaches often do not meet accuracy and efficiency needs of targeted applications. We develop a computational estimation approach called AutoDomainMine that integrates clustering and classification over complex scientific data in a framework so as to automate classical learning methods of scientists. Knowledge discovered thereby from a database of existing experiments serves as the basis for estimation. Challenges include preserving domain semantics in clustering, finding matching strategies in classification, striking a good balance between elaboration and conciseness while displaying estimation results based on needs of targeted users, and deriving objective measures to capture subjective user interests. These and other challenges are addressed in this work. The AutoDomainMine approach is used to build a computational estimation system, rigorously evaluated with real data in Materials Science. Our evaluation confirms that AutoDomainMine provides desired accuracy and efficiency in computational estimation. It is extendable to other science and engineering domains as proved by adaptation of its sub-processes within fields such as Bioinformatics and Nanotechnology.|10.1145/3502736|https://doi.org/10.1145/3502736|New York, NY, USA|Association for Computing Machinery||2022|Computational Estimation by Scientific Data Mining with Classical Methods to Automate Learning Strategies of Scientists|Varde, Aparna S.|article|10.1145/3502736|86|mar|ACM Trans. Knowl. Discov. Data|15564681|5|16|October 2022||||5800173377|0,728|Q1|59|71|169|3729|816|168|4,54|52,52|United States|Northern America|2007-2020|Computer Science (miscellaneous) (Q1)|1,740|2.713|0.00224|96444584|1302859451
||Ontologies;Diseases;Indexes;Drugs;Semantics;Context;Ontology Based Data Access (OBDA);NoSQL;Document store;SPARQL;Social Application||166-173|2012 Third International Conference on Emerging Intelligent Data and Web Technologies|No SQL stores are emerging as an efficient alternative to relational database management systems in the context of big data. Many actors in this domain consider that to gain a wider adoption, several extensions have to be integrated. Some of them focus on the ways of proposing more schema, supporting adapted declarative query languages and providing integrity constraints in order to control data consistency and enhance data quality. We consider that these issues can be dealt with in the context of Ontology Based Data Access (OBDA). OBDA is a new data management paradigm that exploits the semantic knowledge represented in ontologies when querying data stored in a database. We provide a proof of concept of OBDA's ability to tackle these three issues in a social application related to the medical domain.|10.1109/EIDWT.2012.27|||||2012|On the Potential Integration of an Ontology-Based Data Access Approach in NoSQL Stores|Curé, Olivier and Kerdjoudj, Fadhela and Faye, David and Le Duc, Chan and Lamolle, Myriam|inproceedings|6354737||Sep.|||||||||||||||||||||||||||96998258|42
||Big data, Knowledge teaching evaluation, Performance management||100197||With the widespread use of digital technologies such as big data, cloud computing and artificial intelligence in higher education, how to establish a scientific and systematic evaluation system to turn the traditional classroom with the one-way transmission of knowledge into an interactive space for exchanging ideas and inspiring wisdom has become an essential task for human resource management in universities, and a key to improving teaching quality. However, due to the debate between scientism and humanism in teaching evaluation, studies related to teaching performance have been isolated from human resource management, resulting in the lack of a systematic vision and framework for such studies. Relevant studies are still limited to the evaluation contents of different evaluation subjects. Evaluations also tend to focus only on the teaching process, ignoring the objectives of talent training, making it difficult for evaluations to play a goal-oriented role and hindering the further development of relevant studies. Therefore, this paper draws on human resource management methodologies and analyzes knowledge teaching evaluation system characteristics in colleges and universities in a big data context to construct a “multiple evaluations, trinity and four-step closed-loop” big data-based knowledge teaching evaluation system. “Trinity” represents evaluation from three performance dimensions: teaching effect, teaching behavior and teaching ability. “Multiple evaluations” represents the design of teaching performance indicators based on teaching data, breaking the barriers between different evaluation subjects. “Four-step closed-loop” draws on performance management theory to standardize the teaching performance management process from four aspects: planning, implementation, evaluation, and feedback. This evaluation system provides a systematic methodology for unifying the theory and practice of innovative knowledge teaching evaluation system in universities in a big data context.|https://doi.org/10.1016/j.jik.2022.100197|https://www.sciencedirect.com/science/article/pii/S2444569X22000373||||2022|Review on A big data-based innovative knowledge teaching evaluation system in universities|Xu Xin and Yu Shu-Jiang and Pang Nan and Dou ChenXu and Li Dan|article|XIN2022100197|||Journal of Innovation & Knowledge|2444569X|3|7|||||21100932830|1,720|Q1|20|37|72|2498|691|70|10,83|67,51|Netherlands|Western Europe|2016-2020|Business and International Management (Q1); Economics and Econometrics (Q1); Management of Technology and Innovation (Q1); Marketing (Q1)||||97443202|1367689305
|||6|48–53||When properly secured, anonymized, and optimized for research, administrative data can be put to work to help government programs better serve those in need.|10.1145/3335150|https://doi.org/10.1145/3335150|New York, NY, USA|Association for Computing Machinery||2019|Unlocking Data to Improve Public Policy|Hastings, Justine S. and Howison, Mark and Lawless, Ted and Ucles, John and White, Preston|article|10.1145/3335150||sep|Commun. ACM|00010782|10|62|October 2019||||||||||||||||||||||98849856|647144465
||Data quality;data value;security evaluation;privacy protection||1-8|2020 IEEE Systems Security Symposium (SSS)|Data value (DV) is a novel concept that is introduced as one of the Big Data phenomenon features. While continuing an investigation of the DV ontology and its relationship with the data quality (DQ) on the conceptual level, this paper researches possible applications and use of the DV in the practical design of security and privacy protection systems and tools. We present a novel approach to DV evaluation that maps DQ metrics into DV value. Developed methods allow DV and DQ use in a wide range of application domains. To demonstrate DQ and DV concept employment in real tasks we present two real-life scenarios. The first use case demonstrates the DV use in crowdsensing application design. It shows up how DV can be calculated by integrating various metrics characterizing data application functionality, accuracy, and security. The second one incorporates the privacy consideration into DV calculus by exploring the relationship between privacy, DQ, and DV in the defense against web-site fingerprinting in The Onion Router (TOR) networks. These examples demonstrate how our methods of the DV and DQ evaluation may be employed in the design of real systems with security and privacy consideration.|10.1109/SSS47320.2020.9174457|||||2020|What is the Value of Data Value in Practical Security Applications|Khokhlov, Igor and Reznik, Leon|inproceedings|9174457||July|||||||||||||||||||||||||||101827756|42
||||103795||Human reliability analysis plays an important role in the safety assessment and management of rail operations. This paper discusses how the increasing availability of operational data can be used to develop an understanding of train driver reliability. The paper derives human reliability data for two driving tasks, stopping at red signals and controlling speed on approach to buffer stops. In the first of these cases, a tool has been developed that can estimate the number of times a signal is approached at red by trains on the Great Britain (GB) rail network. The tool has been developed using big data techniques and ideas, recording and analysing millions of pieces of data from live operational feeds to update and summarise statistics from thousands of signal locations in GB on a daily basis. The resulting driver reliability data are compared to similar analyses of other train driving tasks. This shows human reliability approaching the currently accepted limits of human performance. It also shows higher error rates amongst freight train drivers than passenger train drivers for these tasks. The paper highlights the importance of understanding the task specific performance limits if further improvements in human reliability are sought. It also provides a practical example of how big data could play an increasingly important role in system error management, whether from the perspective of understanding normal performance and the limits of performance for specific tasks or as the basis for dynamic safety indicators which, if not leading, could at least become closer to real time.|https://doi.org/10.1016/j.apergo.2022.103795|https://www.sciencedirect.com/science/article/pii/S0003687022001181||||2022|At the limit? Using operational data to estimate train driver human reliability|Chris Harrison and Julian Stow and Xiaocheng Ge and Jonathan Gregory and Huw Gibson and Alice Monk|article|HARRISON2022103795|||Applied Ergonomics|00036870||104|||||||||||||||||||||||106658660|480793108
||Data integrity;Machine learning;Earth;Satellites;Big Data;Process control;Monitoring;Big Data;Machine Learning;Earth Observation Data;Data management;Data Quality;Random Forest||3101-3103|IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium|In the big data era, innovative technologies like cloud computing, artificial intelligence, and machine learning are increasingly utilized in the large-scale data management systems of many industry sectors to make them more scalable and intelligent. Applying them to automate and optimize earth observation data management is a hot topic. To improve data quality control mechanisms, a machine learning method in combination with built-in quality rules is presented in this paper to evolve processes around data quality and enhance management of earth observation data. The rules of quality check are set up to detect the common issues, including data completeness, data latency, bad data, and data duplication, and the machine learning model is trained, tested, and deployed to address these quality issues automatically and reduce manual efforts.|10.1109/IGARSS39084.2020.9323615|||||2020|A Machine Learning Approach for Data Quality Control of Earth Observation Data Management System|Han, Weiguo and Jochum, Matthew|inproceedings|9323615||Sep.||21537003|||||||||||||||||||||||||109266648|1296020740
||Costs;Correlation;Data integrity;Volume measurement;Project management;Big Data;Time measurement;Big Data;Project Management;Sensitive Rule;Quality||1-6|2021 International Conference on Engineering and Emerging Technologies (ICEET)|Big data is term of dataset with characteristic volume, value and veracity that lead to confrontation unable proceed using traditional techniques to extract value, project management perspective is dynamic processing that utilizes the suitable resources of organization in many stages by measuring in four-factor scope, time, cost and quality. In this research aim improve data quality from big data via project management scope consist on high trust which is bring high accuracy from confidence level in volume of data, confidence get with context and value of data which lead to determine accuracy deeply in it and finally select from data depending on veracity of it, the experiment using three main factors time, cost and scope, strongest relation organizing between them start by project scope as strongest one then cost, product and Last one time is weakest between them, in the final when select best quality use two sides mostly from quality degree and be center of quality interval and in particular from closest distance with the strongest factor.|10.1109/ICEET53442.2021.9659660|||||2021|An Improve The Quality Of Data Considering Big Data Aspect Based On Sensitive Of Cost Time|Mohammad, Banan and Alzyadat, Wael and Al-Fayoumi, Mohammad and EL Hawi, Ruba and AyshAlhroob|inproceedings|9659660||Oct||24092983|||||||||||||||||||||||||110625782|475191436
||Logic gates;Big Data;Quality of service;Delays;Real-time systems;Safety;Roads;Big data;gateway;intelligent transportation systems;VANET;vehicle-to-internet||6869-6879||Todays' Intelligent Transportation System (ITS) applications majorly depend on either limited neighbouring traffic data or crowd sourced stale traffic data. Enabling big traffic data analytics in ITS environments is a step closer towards utilizing significant traffic patterns and trends for making more precise and intelligent decisions particularly in connected autonomous vehicular environments. Towards this end, this paper presents a Traffic Aware Data Offloading (TRADING) approach for big traffic data centric ITS applications in connected autonomous vehicular environments. Specifically, TRADING balances offloading data traffic among gateways focusing on vehicular traffic and network status in the vicinity of gateways. In addition, TRADING mitigates the effect of gateway advertisement overhead to liberate the transmission channels for traffic big data transmission. The performance of TRADING is comparatively evaluated in a realistic simulation environment by considering gateway access overhead, load distribution among gateways, data offloading delay, and data offloading success ratio. The comparative performance evaluation results show some significant developments towards enabling big traffic data centric ITS.|10.1109/TVT.2020.2991372|||||2020|TRADING: Traffic Aware Data Offloading for Big Data Enabled Intelligent Transportation System|Darwish, Tasneem S. J. and Bakar, Kamalrulnizam Abu and Kaiwartya, Omprakash and Lloret, Jaime|article|9082104||July|IEEE Transactions on Vehicular Technology|19399359|7|69|||||||||||||||||||||||112295320|1383139408
WWW '16 Companion|Montr\'{e}al, Qu\'{e}bec, Canada|rdf, semantic web, linked data|2|1039–1040|Proceedings of the 25th International Conference Companion on World Wide Web|The ninth workshop on Linked Data (LDOW2016) on the Web is held in Montreal, Quebec, Canada on April 12, 2016 and co-located with the 25rd International World Wide Web Conference (WWW2016). The Web is developing from a medium for publishing textual documents into a medium for sharing structured data. This trend is fueled on the one hand by the adoption of the Linked Data principles by a growing number of data providers. On the other hand, large numbers of websites have started to semantically mark up the content of their HTML pages and thus also contribute to the wealth of structured data available on the Web. The 9th Workshop on Linked Data on the Web aims to stimulate discussion and further research into the challenges of publishing, consuming, and integrating structured data from the Web as well as mining knowledge from the global Web of Data.|10.1145/2872518.2890599|https://doi.org/10.1145/2872518.2890599|Republic and Canton of Geneva, CHE|International World Wide Web Conferences Steering Committee|9781450341448|2016|LDOW2016: 9th Workshop on Linked Data on the Web|"\"Auer, S\"\"{o}ren and Heath, Tom and Bizer, Christian and Berners-Lee, Tim\""|inproceedings|10.1145/2872518.2890599|||||||||||||||||||||||||||||112836741|42
ICSCA 2021|Kuala Lumpur, Malaysia|Business Intelligence, Data Management,, Maintenance Management|7|201–207|2021 10th International Conference on Software and Computer Applications|This paper contributes significantly, which focuses on an intelligent system that lets the government make an integral part of decision-making and can be applied horizontally to solve the problems in maintenance practice through business intelligence. Accordingly, a real-time data management system for maintenance management is proposed in this paper. It looks at a real case study highlighting the need for proper data management in the government sector. Our findings bridge the gap of information technology inserted in government office buildings, with maintenance management being the domain. This paper demonstrates the underlying structure of the developed simulation model.|10.1145/3457784.3457816|https://doi.org/10.1145/3457784.3457816|New York, NY, USA|Association for Computing Machinery|9781450388825|2021|Identification of Business Intelligence in Big Data Maintenance of Government Sector in Putrajaya|Farhana Jamaludin, Ain and Najib Razali, Muhammad and jalil, Rohaya and Othman, Hajar and Adnan, Yasmin|inproceedings|10.1145/3457784.3457816|||||||||||||||||||||||||||||115910174|42
||||||Intelligent Computing for Interactive System Design provides a comprehensive resource on what has become the dominant paradigm in designing novel interaction methods, involving gestures, speech, text, touch and brain-controlled interaction, embedded in innovative and emerging human–computer interfaces. These interfaces support ubiquitous interaction with applications and services running on smartphones, wearables, in-vehicle systems, virtual and augmented reality, robotic systems, the Internet of Things (IoT), and many other domains that are now highly competitive, both in commercial and in research contexts.This book presents the crucial theoretical foundations needed by any student, researcher, or practitioner working on novel interface design, with chapters on statistical methods, digital signal processing (DSP), and machine learning (ML). These foundations are followed by chapters that discuss case studies on smart cities, brain–computer interfaces, probabilistic mobile text entry, secure gestures, personal context from mobile phones, adaptive touch interfaces, and automotive user interfaces. The case studies chapters also highlight an in-depth look at the practical application of DSP and ML methods used for processing of touch, gesture, biometric, or embedded sensor inputs. A common theme throughout the case studies is ubiquitous support for humans in their daily professional or personal activities.In addition, the book provides walk-through examples of different DSP and ML techniques and their use in interactive systems. Common terms are defined, and information on practical resources is provided (e.g., software tools, data resources) for hands-on project work to develop and evaluate multimodal and multi-sensor systems. In a series of short additions to each chapter, an expert on the legal and ethical issues explores the emergent deep concerns of the professional community, on how DSP and ML should be adopted and used in socially appropriate ways, to most effectively advance human performance during ubiquitous interaction with omnipresent computers.This carefully edited collection is written by international experts and pioneers in the fields of DSP and ML. It provides a textbook for students and a reference and technology roadmap for developers and professionals working on interaction design on emerging platforms.|||New York, NY, USA|Association for Computing Machinery|9781450390293|2021|Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice||book|10.1145/3447404||||||34|||1|Eslambolchilar, Parisa and Komninos, Andreas and Dunlop, Mark|||||||||||||||||||116463342|42
||Big data;Quality of service;Public transportation;Vehicles;Industries;Next generation networking;Complexity theory;Great Services;4P QoS||165-172|2016 IEEE International Congress on Big Data (BigData Congress)|Big Data is increasingly adopted by a wide range of service industries to improve the quality and value of their services, e.g., inventory that matches well the supply and demand, and pricing that reflects well the market needs. Customers benefit from higher quality of service enabled by Big Data. Service providers get higher profits from more precise control of costs and accurate knowledge of customer needs. In this paper, we define the next generation high quality services as Great Services, characterized by 4P Quality-of-Service (QoS) dimensions: Panorama, Penetration, Prediction and Personalization, which go much further than current services. The transformation of Big Data into Great Services would be difficult and expensive without methodical techniques and software tools. We call the intermediate step Deep Knowledge, which is generated by Big Data (with the 4V challenges - Volume, Velocity, Variety, and Veracity) and used in the creation of Great Services. Deep Knowledge is distinguished from traditional Big Data by 4C properties (Complexity, Cross-domain, Customization, and Convergence). In order to achieve the 4P QoS dimensions of Great Services, we need Deep Knowledge with 4C properties. In this paper, we describe an informal characterization of Great Services with 4P QoS dimensions with examples, and outline the techniques and tools that facilitate the transformation of Big Data into Deep Knowledge with 4C properties, and then the use of Deep Knowledge in Great Services.|10.1109/BigDataCongress.2016.28|||||2016|From Big Data to Great Services|Yin, Jianwei and Tang, Yan and Lo, Wei and Wu, Zhaohui|inproceedings|7584934||June|||||||||||||||||||||||||||116638380|42
WebDB'15|Melbourne, VIC, Australia||5|1–5|Proceedings of the 18th International Workshop on Web and Databases||10.1145/2767109.2770014|https://doi.org/10.1145/2767109.2770014|New York, NY, USA|Association for Computing Machinery|9781450336277|2015|The Elephant in the Room: Getting Value from Big Data|Abiteboul, Serge and Dong, Luna and Etzioni, Oren and Srivastava, Divesh and Weikum, Gerhard and Stoyanovich, Julia and Suchanek, Fabian M.|inproceedings|10.1145/2767109.2770014|||||||||||||||||||||||||||||119117190|42
NISS2020|Marrakech, Morocco|Credit Scoring, CatBoost, Multi-Agent System, LightGBM, XgBoost, Big Data|7||Proceedings of the 3rd International Conference on Networking, Information Systems &amp; Security|"\"Credit risk is one of the main risks facing banks and credit institutions, with the current progress in machine learning, artificial intelligence and big data. Recent research has proposed several systems for improving credit rating. In this paper, a new scalable credit scoring multi-agent system called \"\"CSMAS\"\" is introduced for the prediction of problems in data mining of credit scoring domain. This engine is built using a seven-layer multi-agent system architecture to generate a data mining process based on the coordination of intelligent agents. CSMAS performance is based on preprocessing and data forecasting. The first layer is designed to retrieve any data from various core banking systems, payment systems, credit Bureaus and external databases and data sources and to store it in big data platform. The second layer is devoted to three different subtasks; feature engineering, pre-processing data and integrating diverse datasets. While the third layer is dedicated to dealing with missing Values and treating outliers. In the fourth layer, the techniques of dimensionality reduction are used to reduce the number of features in the original set of features. The fifth layer is dedicated to build a model using the new generation of Gradient Boosting Algorithms (XGBoost, LightGBM and CatBoost) and make predictions. The sixth layer is designed for the model's evaluation. The seventh layer is made to perform the rating of new credit applicants. The performance of CSMAS is assessed using a large dataset of Home Credit Default Risk from Kaggle Challenge (307511 records) to evaluate the risk of a loan applicant as a major problem for banks. The results show that the CSMAS give relevant results. Therefore, the results indicated that the CSMAS can be further employed as a reliable tool to predict more complicated case in credit scoring.\""|10.1145/3386723.3387851|https://doi.org/10.1145/3386723.3387851|New York, NY, USA|Association for Computing Machinery|9781450376341|2020|CSMAS: Improving Multi-Agent Credit Scoring System by Integrating Big Data and the New Generation of Gradient Boosting Algorithms|Tounsi, Youssef and Anoun, Houda and Hassouni, Larbi|inproceedings|10.1145/3386723.3387851|32||||||||||||||||||||||||||||119612433|42
||Natural language processing, Rapid descriptive methods, Big data, Whisky, Research methodology, Machine learning||103926||As sensory evaluation relies upon humans accurately communicating their sensory experience, the diverse and overlapping vocabulary of flavor descriptors remains a major challenge. The lexicon generation protocols used in methods like Descriptive Analysis are expensive and time-consuming, while the post-facto analyses of natural vocabulary in “quick and dirty” methods like Free Choice or Flash Profiling require considerable subjective decision-making on the part of the analyst. A potential alternative for producing lexicons and analyzing the sensory attributes of products in nonstandardized text can be found in Natural Language Processing (NLP). NLP tools allow for the analysis of larger volumes of free text with fewer subjective decisions. This paper describes the steps necessary to automatically collect, clean, and analyze existing product descriptions from the web. As a case study, online reviews of international whiskies from two prominent websites (2309 reviews from WhiskyCast and 4289 reviews from WhiskyAdvocate) were collected, preprocessed to only retain potentially-descriptive nouns, adjectives, and verbs, and then the final term list was grouped into a flavor wheel using Correspondence Analysis and Agglomerative Hierarchical Clustering. The wheel is compared to an existing Scotch flavor wheel. The ease of collecting nonstandardized descriptions of products and the improved speed of automated methods can facilitate collection of descriptive sensory data for products where no lexicon exists. This has the potential to speed up and standardize many of the bottlenecks in rapid descriptive methods and facilitate the collection and use of very large datasets of product descriptions.|https://doi.org/10.1016/j.foodqual.2020.103926|https://www.sciencedirect.com/science/article/pii/S0950329319308304||||2020|Fast and automated sensory analysis: Using natural language processing for descriptive lexicon development|Leah M. Hamilton and Jacob Lahne|article|HAMILTON2020103926|||Food Quality and Preference|09503293||83|||||23161|1,135|Q1|120|227|558|12290|3000|548|5,43|54,14|United Kingdom|Western Europe|1988-1991, 1993-2021|Food Science (Q1); Nutrition and Dietetics (Q1)|13,058|5.565|0.00829|122681941|400072429
||Cyber security, Ethics, Policy-making, Security, Surveillance||111-124|Smart Cities: Issues and Challenges|Urban space monitoring and surveillance systems are present almost everywhere in various forms of sensing devices such as closed-circuit television, smartphone, and camera. This requires a robust and easy-to-manage information and communication technology (ICT) infrastructure that is generally comprises sensors, protocols, networks, and steps. Smart adoption of such systems could influence, manage, direct, and protect human beings and property. Nevertheless, it may create problems of government support, data quality, privacy, and security. Today's computational world allows implementation of artificial intelligence models for big data analytics to bring cities smart (with intelligence and optimal improvement). This chapter will discuss the applications of urban space monitoring and surveillance systems via ICT. The typical limitations of the current research are discussed in detail.|https://doi.org/10.1016/B978-0-12-816639-0.00007-7|https://www.sciencedirect.com/science/article/pii/B9780128166390000077||Elsevier|978-0-12-816639-0|2019|Chapter 7 - Smart city is a safe city: information and communication technology–enhanced urban space monitoring and surveillance systems: the promise and limitations|Kwok Tai Chui and Pandian Vasant and Ryan Wen Liu|incollection|CHUI2019111||||||||||Anna Visvizi and Miltiadis D. Lytras|||||||||||||||||||124619028|42
C3S2E '16|Porto, Portugal|Data Cleaning, Ontology, Rewriting Process, Schema, Vocabulary|4|85–88|Proceedings of the Ninth International C* Conference on Computer Science &amp; Software Engineering|Dealing with increasing amounts of data creates the need to deal with redundant, inconsistent and/or complementary repositories which may be different in their data models and/or in their schema. Current data cleaning techniques developed to tackle data quality problems are just suitable for scenarios were all repositories share the same model and schema. Recently, an ontology-based methodology was proposed to overcome this limitation. In this paper, this methodology is briefly described and applied to a real scenario in the health domain with data quality problems.|10.1145/2948992.2949007|https://doi.org/10.1145/2948992.2949007|New York, NY, USA|Association for Computing Machinery|9781450340755|2016|Ontology Based Rewriting Data Cleaning Operations|Almeida, Ricardo and Maio, Paulo and Oliveira, Paulo and Barroso, Jo\~{a}o|inproceedings|10.1145/2948992.2949007|||||||||||||||||||||||||||||125395392|42
||Energy efficiency;Performance evaluation;Storage area networks;Big data;Quality of service;Context modeling;Data storage aystems;mass storage;big data;storage management;value of information;repositories;autonomous systems;content analysis;indexing||43-51||Storage system efficiency can be significantly improved by determining the value of data. A key concept is cognitive storage, or optimizing storage systems by better comprehending the relevance of data to user needs and preferences. The Web extra at https://youtu.be/P-ZxlTLwzTI is a video of authors Giovanni Cherubini and Vinodh Venkatesan of IBM Research--Zurich discussing the concepts, applications, and benefits of cognitive storage for big data.|10.1109/MC.2016.117|||||2016|Cognitive Storage for Big Data|Cherubini, Giovanni and Jelitto, Jens and Venkatesan, Vinodh|article|7452294||Apr|Computer|15580814|4|49|||||||||||||||||||||||127199788|43808127
MLMI2018|Ha Noi, Viet Nam|Redundant Features, Feature Selection, High Dimensional Data, Mutual Information|5|3–7|Proceedings of the 2018 International Conference on Machine Learning and Machine Intelligence|As increasing the massive amount of data demands effective and efficient mining strategies, practitioners and researchers are trying to develop scalable mining algorithms, machine learning algorithms and strategies to be successful data mining in turning mountains of data into nuggets. Data of high dimension significantly increases the memory storage requirements and computational costs for data analytics. Therefore, reducing dimension can mainly improve three data mining performance: speed of learning, predictive accuracy and simplicity and comprehensibility of mined result. Feature selection, data preprocessing technique, is effective and efficient in data mining, data analytics and machine learning problems particularly in high dimension reduction. Most feature selection algorithms can eliminate only irrelevant features but redundant features. Not only irrelevant features but also redundant features can degrade learning performance. Mutual information measured feature selection is proposed in this work to remove both irrelevant and redundant features.|10.1145/3278312.3278316|https://doi.org/10.1145/3278312.3278316|New York, NY, USA|Association for Computing Machinery|9781450365567|2018|Mutual Information-Based Feature Selection Approach to Reduce High Dimension of Big Data|Win, Thee Zin and Kham, Nang Saing Moon|inproceedings|10.1145/3278312.3278316|||||||||||||||||||||||||||||127582569|42
||sampling strategy, annotation, multi-label image, active learning, Image classification|35|||Image classification is a key task in image understanding, and multi-label image classification has become a popular topic in recent years. However, the success of multi-label image classification is closely related to the way of constructing a training set. As active learning aims to construct an effective training set through iteratively selecting the most informative examples to query labels from annotators, it was introduced into multi-label image classification. Accordingly, multi-label active learning is becoming an important research direction. In this work, we first review existing multi-label active learning algorithms for image classification. These algorithms can be categorized into two top groups from two aspects respectively: sampling and annotation. The most important component of multi-label active learning is to design an effective sampling strategy that actively selects the examples with the highest informativeness from an unlabeled data pool, according to various information measures. Thus, different informativeness measures are emphasized in this survey. Furthermore, this work also makes a deep investigation on existing challenging issues and future promises in multi-label active learning with a focus on four core aspects: example dimension, label dimension, annotation, and application extension.|10.1145/3379504|https://doi.org/10.1145/3379504|New York, NY, USA|Association for Computing Machinery||2020|Multi-Label Active Learning Algorithms for Image Classification: Overview and Future Promise|Wu, Jian and Sheng, Victor S. and Zhang, Jing and Li, Hua and Dadakova, Tetiana and Swisher, Christine Leon and Cui, Zhiming and Zhao, Pengpeng|article|10.1145/3379504|28|mar|ACM Comput. Surv.|03600300|2|53|March 2021||||23038|2,079|Q1|163|112|365|15859|6978|365|19,16|141,60|United States|Northern America|1969-2020|Computer Science (miscellaneous) (Q1); Theoretical Computer Science (Q1)|10,790|10.282|0.01438|134119172|1517405264
||Big Data, Testing, Verasity, Hadoop||940-948||Big Data, the new buzz word in the industry, is data that exceeds the processing and analytic capacity of conventional database systems within the time necessary to make them useful. With multiple data stores in abundant formats, billions of rows of data with hundreds of millions of data combinations and the urgent need of making best possible decisions, the challenge is big and the solution bigger, Big Data. Comes with it, new advances in computing technology together with its high performance analytics for simpler and faster processing of only relevant data to enable timely and accurate insights using data mining and predictive analytics, text mining, forecasting and optimization on complex data to continuously drive innovation and make the best possible decisions. While Big Data provides solutions to complex business problems like analyzing larger volumes of data than was previously possible to drive more precise answers, analyzing data in motion to capture opportunities that were previously lost, it poses bigger challenges in testing these scenarios. Testing such highly volatile data, which is more often than not unstructured generated from myriad sources such as web logs, radio frequency Id (RFID), sensors embedded in devices, GPS systems etc. and mostly clustered data for its accuracy, high availability, security requires specialization. One of the most challenging things for a tester is to keep pace with changing dynamics of the industry. While on most aspects of testing, the tester need not know the technical details behind the scene however this is where testing Big Data Technology is so different. A tester not only needs to be strong on testing fundamentals but also has to be equally aware of minute details in the architecture of the database designs to analyze several performance bottlenecks and other issues. Like in the example quoted above on In-Memory databases, a tester would need to know how the operating systems allocate and de-allocate memory and understand how much memory is being used at any given time. So, concluding, as the data- analytics Industry evolves further we would see the IT Testing Services getting closely aligned with the Database Engineering and the industry would need more skilled testing professional in this domain to grab the new opportunities.|https://doi.org/10.1016/j.procs.2016.05.285|https://www.sciencedirect.com/science/article/pii/S1877050916306354||||2016|Challenges and Techniques for Testing of Big Data|Naveen Garg and Sanjay Singla and Surender Jangra|article|GARG2016940|||Procedia Computer Science|18770509||85||International Conference on Computational Modelling and Security (CMS 2016)|||19700182801|0,334|-|76|1907|6483|38511|13722|6359|2,09|20,19|Netherlands|Western Europe|2010-2020|Computer Science (miscellaneous)||||134196404|2108686752
ESEC/FSE 2019|Tallinn, Estonia|Anomaly Detection, Data Quality, Log Instability, Deep Learning, Log Analysis|11|807–817|Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering|Logs are widely used by large and complex software-intensive systems for troubleshooting. There have been a lot of studies on log-based anomaly detection. To detect the anomalies, the existing methods mainly construct a detection model using log event data extracted from historical logs. However, we find that the existing methods do not work well in practice. These methods have the close-world assumption, which assumes that the log data is stable over time and the set of distinct log events is known. However, our empirical study shows that in practice, log data often contains previously unseen log events or log sequences. The instability of log data comes from two sources: 1) the evolution of logging statements, and 2) the processing noise in log data. In this paper, we propose a new log-based anomaly detection approach, called LogRobust. LogRobust extracts semantic information of log events and represents them as semantic vectors. It then detects anomalies by utilizing an attention-based Bi-LSTM model, which has the ability to capture the contextual information in the log sequences and automatically learn the importance of different log events. In this way, LogRobust is able to identify and handle unstable log events and sequences. We have evaluated LogRobust using logs collected from the Hadoop system and an actual online service system of Microsoft. The experimental results show that the proposed approach can well address the problem of log instability and achieve accurate and robust results on real-world, ever-changing log data.|10.1145/3338906.3338931|https://doi.org/10.1145/3338906.3338931|New York, NY, USA|Association for Computing Machinery|9781450355728|2019|Robust Log-Based Anomaly Detection on Unstable Log Data|Zhang, Xu and Xu, Yong and Lin, Qingwei and Qiao, Bo and Zhang, Hongyu and Dang, Yingnong and Xie, Chunyu and Yang, Xinsheng and Cheng, Qian and Li, Ze and Chen, Junjie and He, Xiaoting and Yao, Randolph and Lou, Jian-Guang and Chintalapati, Murali and Shen, Furao and Zhang, Dongmei|inproceedings|10.1145/3338906.3338931|||||||||||||||||||||||||||||135883824|42
||Big data architecture, Collaborative networks, Enterprises network, Supply chain network, Flexibility, Robustness||102-111||Nowadays, the world knows a high-speed development and evolution of technologies, vulnerable economic environments, market changes, and personalised consumer trends. The issue and challenge related to enterprises networks design are more and more critical. These networks are often designed for short terms since their strategies must be competitive and better adapted to the environment, social and economical changes. As a solution, to design a flexible and robust network, it is necessary to deal with the trade-off between conflicting qualitative and quantitative criteria such as cost, quality, delivery time, and competition, etc. To this end, using Big Data (BD) as emerging technology will enhance the real performances of these kinds of networks. Moreover, even if the literature is rich with BD models and frameworks developed for a single supply chain network (SCN), there is a real need to scale and extend these BD models to networked supply chains (NSCs). To do so, this paper proposes a BD architecture to drive a mixed-network of SCs that collaborate in serial and parallel fashions. The collaboration is set up by sharing their resources, capabilities, competencies, and information to imitate a unique organisation. The objective is to increase internal value to their shareholders (where value is seen as wealth) and deliver better external value to the end-customer (where value represents customer satisfaction). Within a mixed-network of SCs, both values are formally calculated considering both serial and parallel networks configurations. Besides, some performance factors of the proposed BD architecture such as security, flexibility, robustness and resilience are discussed.|https://doi.org/10.1016/j.comcom.2021.05.008|https://www.sciencedirect.com/science/article/pii/S0140366421001924||||2021|A big data based architecture for collaborative networks: Supply chains mixed-network|Lahcen Tamym and Lyes Benyoucef and Ahmed {Nait Sidi Moh} and Moulay Driss {El Ouadghiri}|article|TAMYM2021102|||Computer Communications|01403664||175|||||13681|0,627|Q1|105|616|599|24961|2424|591|4,08|40,52|Netherlands|Western Europe|1978-2020|Computer Networks and Communications (Q1)|6,725|3.167|0.00513|137068904|550488617
PDC '18|Hasselt and Genk, Belgium|science and technology studies, partnering designer, information management, local collective data management, politics, infrastructuring, matters of care, participatory design, information infrastructure, data care|12||Proceedings of the 15th Participatory Design Conference: Full Papers - Volume 1|"\"In this paper, we think with Puig de la Bellacasa's 'matters of care' about how to support data care and its politics. We use the notion to reflect on participatory design activities in two recent case studies of local collective data management in ecological research. We ask \"\"How to design for data care?\"\" and \"\"How to account for the politics of data care in design?\"\" Articulation of data care together with ethically and politically significant data issues in design, reveals in these cases the invisible labors of care by local data advocates and a 'partnering designer'. With digital data work in the sciences increasing and data infrastructures for research under development at a variety of large scales, the local level is often considered merely a recipient of services rather than an active participant in design of data practices and infrastructures. We identify local collective data management as a 'neglected thing' in infrastructure planning and speculate on how things could be different in the data landscape.\""|10.1145/3210586.3210587|https://doi.org/10.1145/3210586.3210587|New York, NY, USA|Association for Computing Machinery|9781450363716|2018|Data Care and Its Politics: Designing for Local Collective Data Management as a Neglected Thing|Baker, Karen S. and Karasti, Helena|inproceedings|10.1145/3210586.3210587|10||||||||||||||||||||||||||||138508540|42
||GPU, FPGA, MIC, CPU, TPU, Cloud, Big Data, parallel, distributed processing, heterogeneous cloud architecture||112-118||Smart agriculture is one of the most diverse research. In addition, the quantity of data to be stored and the choice of the most efficient algorithms to process are significant elements in this field. The storage of collecting data from Internet of Things (IoT), existing on distributed, local databases and open data need a particular infrastructure to federate all these data to make complex treatments. The storage of this wide range of data that comes at high frequency and variable throughput is particularly difficult. In this paper, we propose the use of distributed databases and high-performance computing architecture in order to exploit multiple re-configurable computing and application specific processing such as CPUs, GPUs, TPUs and FPGAs efficiently. This exploitation allows an accurate training for an application to machine learning, deep learning and unsupervised modeling algorithms. The last ones are used for training supervised algorithms on images when it labels a set of images and unsupervised algorithms on IoT data which are unlabeled with variable qualities. The processing of data is based on Hadoop 3.1 MapReduce to achieve parallel processing and use containerization technologies to distribute treatments on Multi GPU, MIC and FPGA. This architecture allows efficient treatments of data coming from several sources with a cloud high-performance heterogeneous architecture. The proposed 4 layers infrastructure can also implement FPGA and MIC which are now natively supported by recent version of Hadoop. Moreover, with the advent of new technologies like Intel® MovidiusTM; it is now possible to deploy CNN at the Fog level in the IoT network and to make inference with the cloud and therefore limit significantly the network traffic that result in reducing the move of large amounts of data to the cloud.|https://doi.org/10.1016/j.procs.2018.10.156|https://www.sciencedirect.com/science/article/pii/S1877050918318064||||2018|Cloud Platform using Big Data and HPC Technologies for Distributed and Parallels Treatments|Olivier Debauche and Sidi Ahmed Mahmoudi and Saïd Mahmoudi and Pierre Manneback|article|DEBAUCHE2018112|||Procedia Computer Science|18770509||141||The 9th International Conference on Emerging Ubiquitous Systems and Pervasive Networks (EUSPN-2018) / The 8th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare (ICTH-2018) / Affiliated Workshops|||19700182801|0,334|-|76|1907|6483|38511|13722|6359|2,09|20,19|Netherlands|Western Europe|2010-2020|Computer Science (miscellaneous)||||142718713|2108686752
MET '19|Montreal, Quebec, Canada|metamorphic testing, big data, metamorphic data relations, quality assessment, data quality|8|76–83|Proceedings of the 4th International Workshop on Metamorphic Testing|In the era of big data, cloud computing and the Internet of Things, the quality of data has tremendous impact on our everyday life. Moreover, the increasing velocity, volume and variety of data requires new approaches for quality assessment. In this paper, a new approach for quality assessment is presented that applies metamorphic testing to data quality. The exemplary application of the approach on a big data application shows promising results for the suitability of the approach.|10.1109/MET.2019.00019|https://doi.org/10.1109/MET.2019.00019||IEEE Press||2019|Addressing Data Quality Problems with Metamorphic Data Relations|Auer, Florian and Felderer, Michael|inproceedings|10.1109/MET.2019.00019|||||||||||||||||||||||||||||143542009|42
||Upstream business, Heterogeneous and multidimensional data, Data warehousing and mining, Big Data paradigm, Spatial-temporal dimensions||143-158||The emerging Big Data integration imposes diverse challenges, compromising the sustainable business research practice. Heterogeneity, multi-dimensionality, velocity, and massive volumes that challenge Big Data paradigm may preclude the effective data and system integration processes. Business alignments get affected within and across joint ventures as enterprises attempt to adapt to changes in industrial environments rapidly. In the context of the Oil and Gas industry, we design integrated artefacts for a resilient multidimensional warehouse repository. With access to several decades of resource data in upstream companies, we incorporate knowledge-based data models with spatial-temporal dimensions in data schemas to minimize ambiguity in warehouse repository implementation. The design considerations ensure uniqueness and monotonic properties of dimensions, maintaining the connectivity between artefacts and achieving the business alignments. The multidimensional attributes envisage Big Data analysts a scope of business research with valuable new knowledge for decision support systems and adding further business values in geographic scales.|https://doi.org/10.1016/j.jbusres.2018.04.029|https://www.sciencedirect.com/science/article/pii/S0148296318302054||||2018|On big data-guided upstream business research and its knowledge management|Shastri L. Nimmagadda and Torsten Reiners and Lincoln C. Wood|article|NIMMAGADDA2018143|||Journal of Business Research|01482963||89|||||20550|2,049|Q1|195|878|1342|73221|11507|1315|7,38|83,40|United States|Northern America|1973-2021|Marketing (Q1)|46,935|7.550|0.03523|144511496|1502892296
||||405-411||For the first time in history, it is possible to study human behavior on great scale and in fine detail simultaneously. Online services and ubiquitous computational devices, such as smartphones and modern cars, record our everyday activity. The resulting Big Data offers unprecedented opportunities for tracking and analyzing behavior. This paper hypothesizes the applicability and impact of Big Data technologies in the context of psychometrics both for research and clinical applications. It first outlines the state of the art, including the severe shortcomings with respect to quality and quantity of the resulting data. It then presents a technological vision, comprised of (i) numerous data sources such as mobile devices and sensors, (ii) a central data store, and (iii) an analytical platform, employing techniques from data mining and machine learning. To further illustrate the dramatic benefits of the proposed methodologies, the paper then outlines two current projects, logging and analyzing smartphone usage. One such study attempts to thereby quantify severity of major depression dynamically; the other investigates (mobile) Internet Addiction. Finally, the paper addresses some of the ethical issues inherent to Big Data technologies. In summary, the proposed approach is about to induce the single biggest methodological shift since the beginning of psychology or psychiatry. The resulting range of applications will dramatically shape the daily routines of researches and medical practitioners alike. Indeed, transferring techniques from computer science to psychiatry and psychology is about to establish Psycho-Informatics, an entire research direction of its own.|https://doi.org/10.1016/j.mehy.2013.11.030|https://www.sciencedirect.com/science/article/pii/S0306987713005598||||2014|Psycho-Informatics: Big Data shaping modern psychometrics|Alexander Markowetz and Konrad Błaszkiewicz and Christian Montag and Christina Switala and Thomas E. Schlaepfer|article|MARKOWETZ2014405|||Medical Hypotheses|03069877|4|82|||||17833|0,441|Q3|87|816|979|28666|1452|883|1,40|35,13|United States|Northern America|1975-2020|Medicine (miscellaneous) (Q3)|9,727|1.538|0.005|144839438|73153480
||spatial queries, geo-sensory data, spatiotemporal data cleaning, Internet of Things, spatiotemporal dependencies, location refinement, spatial computing, quality management|41|||With the continued deployment of the Internet of Things (IoT), increasing volumes of devices are being deployed that emit massive spatially referenced data. Due in part to the dynamic, decentralized, and heterogeneous architecture of the IoT, the varying and often low quality of spatial IoT data (SID) presents challenges to applications built on top of this data. This survey aims to provide unique insight to practitioners who intend to develop IoT-enabled applications and to researchers who wish to conduct research that relates to data quality in the IoT setting. The survey offers an inventory analysis of major data quality dimensions in SID and covers significant data characteristics and associated quality considerations. The survey summarizes data quality related technologies from both task and technique perspectives. Organizing the technologies from the task perspective, it covers recent progress in SID quality management, encompassing location refinement, uncertainty elimination, outlier removal, fault correction, data integration, and data reduction; and it covers low-quality SID exploitation, encompassing querying, analysis, and decision-making techniques. Finally, the survey covers emerging trends and open issues concerning the quality of SID.|10.1145/3498338|https://doi.org/10.1145/3498338|New York, NY, USA|Association for Computing Machinery||2022|Spatial Data Quality in the Internet of Things: Management, Exploitation, and Prospects|Li, Huan and Lu, Hua and Jensen, Christian S. and Tang, Bo and Cheema, Muhammad Aamir|article|10.1145/3498338|57|feb|ACM Comput. Surv.|03600300|3|55|April 2023||||23038|2,079|Q1|163|112|365|15859|6978|365|19,16|141,60|United States|Northern America|1969-2020|Computer Science (miscellaneous) (Q1); Theoretical Computer Science (Q1)|10,790|10.282|0.01438|147800314|1517405264
||Robust Bayesian network, Data quality feature, Process monitoring, Fault diagnosis||104344||In this paper, a novel robust Bayesian network is proposed for process modeling with low-quality data. Since unreliable data can cause model parameters to deviate from the real distributions and make network structures unable to characterize the true causalities, data quality feature is utilized to improve the process modeling and monitoring performance. With a predetermined trustworthy center, the data quality measurement results can be evaluated through an exponential function with Mahalanobis distances. The conventional Bayesian network learning algorithms including structure learning and parameter learning are modified by the quality feature in a weighting form, intending to extract useful information and make a reasonable model. The effectiveness of the proposed method is demonstrated through TE benchmark process and a real industrial process.|https://doi.org/10.1016/j.conengprac.2020.104344|https://www.sciencedirect.com/science/article/pii/S0967066120300289||||2020|Robust Bayesian networks for low-quality data modeling and process monitoring applications|Guangjie Chen and Zhiqiang Ge|article|CHEN2020104344|||Control Engineering Practice|09670661||97|||||18174|1,175|Q1|119|196|615|8010|2779|611|4,42|40,87|United Kingdom|Western Europe|1993-2020|Applied Mathematics (Q1); Computer Science Applications (Q1); Control and Systems Engineering (Q1); Electrical and Electronic Engineering (Q1)|8,368|3.475|0.00717|149429133|1005708756
|||5|15–19|Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice|||https://doi.org/10.1145/3447404.3447407|New York, NY, USA|Association for Computing Machinery|9781450390293|2021|Ethical Issues in Digital Signal Processing and Machine Learning|McMenemy, David|inbook|10.1145/3447404.3447407|||||||||1||||||||||||||||||||151179735|42
||Data visualization, Visualization languages, Efficient data visualization, Data visualization recommendation|25|93–117||Data visualization is crucial in today’s data-driven business world, which has been widely used for helping decision making that is closely related to major revenues of many industrial companies. However, due to the high demand of data processing w.r.t. the volume, velocity, and veracity of data, there is an emerging need for database experts to help for efficient and effective data visualization. In response to this demand, this article surveys techniques that make data visualization more efficient and effective. (1) Visualization specifications define how the users can specify their requirements for generating visualizations. (2) Efficient approaches for data visualization process the data and a given visualization specification, which then produce visualizations with the primary target to be efficient and scalable at an interactive speed. (3) Data visualization recommendation is to auto-complete an incomplete specification, or to discover more interesting visualizations based on a reference visualization.|10.1007/s00778-019-00588-3|https://doi.org/10.1007/s00778-019-00588-3|Berlin, Heidelberg|Springer-Verlag||2019|Making Data Visualization More Efficient and Effective: A Survey|Qin, Xuedi and Luo, Yuyu and Tang, Nan and Li, Guoliang|article|10.1007/s00778-019-00588-3||nov|The VLDB Journal|10668888|1|29|Jan 2020||||13646|0,653|Q1|90|64|117|4824|544|113|4,46|75,38|United States|Northern America|1992-2020|Hardware and Architecture (Q1); Information Systems (Q1)|2,106|2.868|0.0023|151957684|924310569
||Sleep;Training;Machine learning algorithms;Classification algorithms;Standards;Vegetation;Heart rate;wearable;data quality;sleep;machine learning;Fitbit||768-775|2019 IEEE Intl Conf on Dependable, Autonomic and Secure Computing, Intl Conf on Pervasive Intelligence and Computing, Intl Conf on Cloud and Big Data Computing, Intl Conf on Cyber Science and Technology Congress (DASC/PiCom/CBDCom/CyberSciTech)|Consumer activity wristbands such as Fitbit provide an affordable method for ubiquitous sleep sensing in daily settings. These devices are also increasingly used in scientific studies as measurement tools of sleep outcomes. Nevertheless, the accuracy of Fitbit has raised wide concern. In this paper, we explore the feasibility of applying machine learning to improve the quality of Fitbit sleep data. The problem of interest was formulated into a multiclass imbalanced classification problem. We examined the performance of different combinations of seven machine learning algorithms and three resampling techniques. The preliminary results showed that the accuracy in detecting wakefulness and light sleep was improved by up to 43% and 44% respectively compared to the proprietary algorithm of Fitbit. Our future work will focus on improving the overall accuracy of the classification models in detecting all sleep stages.|10.1109/DASC/PiCom/CBDCom/CyberSciTech.2019.00143|||||2019|Achieving Accurate Ubiquitous Sleep Sensing with Consumer Wearable Activity Wristbands Using Multi-class Imbalanced Classification|Liang, Zilu and Chapa Martell, Mario Alberto|inproceedings|8890371||Aug|||||||||||||||||||||||||||152984168|42
COMPASS '18|Menlo Park and San Jose, CA, USA|community engagement, HCI, care, Data science for social good|9||Proceedings of the 1st ACM SIGCAS Conference on Computing and Sustainable Societies|"\"Data science is an interdisciplinary field that extracts insights from data through a multi-stage process of data collection, analysis and use. When data science is applied for social good, a variety of stakeholders are introduced to the process with an intention to inform policies or programs to improve well-being. Our goal in this paper is to propose an orientation to care in the practice of data science for social good. When applied to data science, a logic of care can improve the data science process and reveal outcomes of \"\"good\"\" throughout. Consideration of care in practice has its origins in Science and Technology Studies (STS) and has recently been applied by Human Computer Interaction (HCI) researchers to understand technology repair and use in under-served environments as well as care in remote health monitoring. We bring care to the practice of data science through a detailed examination of our engaged research with a community group that uses data as a strategy to advocate for permanently affordable housing. We identify opportunities and experiences of care throughout the stages of the data science process. We bring greater detail to the notion of human-centered systems for data science and begin to describe what these look like.\""|10.1145/3209811.3209877|https://doi.org/10.1145/3209811.3209877|New York, NY, USA|Association for Computing Machinery|9781450358163|2018|Care and the Practice of Data Science for Social Good|Zegura, Ellen and DiSalvo, Carl and Meng, Amanda|inproceedings|10.1145/3209811.3209877|34||||||||||||||||||||||||||||153540013|42
CSAE 2019|Sanya, China|Relevance criteria, User relevance, Scientific data retrieval|7||Proceedings of the 3rd International Conference on Computer Science and Application Engineering|Is there a stable cognitive structure of scientific data retrieval process? Based on the theory and method of user relevance research, this study explores the cognitive characteristics of user scientific data query and retrieval. The semi-structured interview method used to collect relevant data, and the content analysis method used to encode and analyze the cognitive process of users' scientific data query and retrieval. The results show that (1) users scientific data relevance judgment not only depend on topicality, but also use accessibility, quality, authority and usefulness. (2) There are 7 combination patterns for the use of user's scientific data relevance criteria, and (3) different patterns correspond to different user relevance types and different user information need states. These 7 criteria usage patterns reveal the cognitive enhancement of user scientific data relevance judgment. The research results have a great inspiration for the development of interactive scientific data retrieval system based on user cognitive enhancement characteristics.|10.1145/3331453.3360954|https://doi.org/10.1145/3331453.3360954|New York, NY, USA|Association for Computing Machinery|9781450362948|2019|The Cognitive Enhancement Process of Scientific Data Retrieval|Liu, Jianping and Wang, Jian and Zhou, Guomin and Zhang, Guilan and Cui, Yunpeng and Liu, Juan|inproceedings|10.1145/3331453.3360954|1||||||||||||||||||||||||||||154262088|42
||Cloud computing;Security;Task analysis;Processor scheduling;Quality of service;Scheduling;Computational modeling;cloud computing;workflow scheduling;security;survey||71-76|2018 Sixth International Conference on Advanced Cloud and Big Data (CBD)|"\"Cloud computing (CC) is a useful tool for executing complex applications. As a result of this, it has become so popular and used in diverse domains such as science, engineering, medicine. etc. CC structure is composed of a number of virtual machines(VMs) provisioned on demand and charged on a \"\"Pay-as-you-go\"\" basis, it is deployed in different form of access levels. Complex applications needed to be executed on clouds are represented as workflows. Workflow scheduling (WS) is one of the most important concepts in cloud computing. WS model contributes to minimizing cost, makespan and energy as well as maximize the quality of service(QoS) of applications in clouds. Despite the security constraints set by each provider, CC has become so critical due to the considerations of applications with sensitive intermediate data, this thereby requires a security level known as Secured workflow Scheduling(SWS). This security is on the level of executing workflows. It indicates that applications with sensitive interdependent data have to be protected during their execution across different cloud VMs. The addition of security in workflow execution generates time overhead, making it complex to meet up with the QoS required by the users. Some research works have proposed algorithms for providing the QoS requirements and security at the same time. In this work, we survey some existing works, by defining the factors needed in securing workflows during execution, clarifying the domains for security, sources of security threats and their solutions as well as cloud computing services that needs security and lastly classify the proposed algorithm depending cloud computing components.\""|10.1109/CBD.2018.00022|||||2018|Exploration of Secured Workflow Scheduling Models in Cloud Environment: A Survey|Francis, Akindipe Olusegun and Emmanuel, Bugingo and Zhang, Defu and Zheng, Wei and Qin, Yingsheng and Zhang, Dongzhan|inproceedings|8530818||Aug|||||||||||||||||||||||||||154603130|42
||Wastewater treatment, Big data, Statistical process control, Process optimization, Monitoring||498-513||Recent advancements in data-driven process control and performance analysis could provide the wastewater treatment industry with an opportunity to reduce costs and improve operations. However, big data in wastewater treatment plants (WWTP) is widely underutilized, due in part to a workforce that lacks background knowledge of data science required to fully analyze the unique characteristics of WWTP. Wastewater treatment processes exhibit nonlinear, nonstationary, autocorrelated, and co-correlated behavior that (i) is very difficult to model using first principals and (ii) must be considered when implementing data-driven methods. This review provides an overview of data-driven methods of achieving fault detection, variable prediction, and advanced control of WWTP. We present how big data has been used in the context of WWTP, and much of the discussion can also be applied to water treatment. Due to the assumptions inherent in different data-driven modeling approaches (e.g., control charts, statistical process control, model predictive control, neural networks, transfer functions, fuzzy logic), not all methods are appropriate for every goal or every dataset. Practical guidance is given for matching a desired goal with a particular methodology along with considerations regarding the assumed data structure. References for further reading are provided, and an overall analysis framework is presented.|https://doi.org/10.1016/j.watres.2019.03.030|https://www.sciencedirect.com/science/article/pii/S0043135419302490||||2019|Data-driven performance analyses of wastewater treatment plants: A review|Kathryn B. Newhart and Ryan W. Holloway and Amanda S. Hering and Tzahi Y. Cath|article|NEWHART2019498|||Water Research|00431354||157|||||18795|3,099|Q1|303|1142|2603|70129|29848|2588|11,32|61,41|United Kingdom|Western Europe|1967-2020|Civil and Structural Engineering (Q1); Ecological Modeling (Q1); Environmental Engineering (Q1); Pollution (Q1); Waste Management and Disposal (Q1); Water Science and Technology (Q1)|120,695|11.236|0.07878|154701721|932773851
KDD '16|San Francisco, California, USA|taxi trajectories, mobile data mining, social influence|10|1285–1294|Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining|With recent advances in mobile and sensor technologies, a large amount of efforts have been made on developing intelligent applications for taxi drivers, which provide beneficial guide and opportunity to improve the profit and work efficiency. However, limited scopes focus on the latent social interaction within cab drivers, and corresponding social propagation scheme to share driving behaviors has been largely ignored. To that end, in this paper, we propose a comprehensive study to reveal how the social propagation affects for better prediction of cab drivers' future behaviors. To be specific, we first investigate the correlation between drivers' skills and their mutual interactions in the latent vehicle-to-vehicle network, which intuitively indicates the effects of social influences. Along this line, by leveraging the classic social influence theory, we develop a two-stage framework for quantitatively revealing the latent driving pattern propagation within taxi drivers. Comprehensive experiments on a real-word data set collected from the New York City clearly validate the effectiveness of our proposed framework on predicting future taxi driving behaviors, which also support the hypothesis that social factors indeed improve the predictability of driving behaviors.|10.1145/2939672.2939799|https://doi.org/10.1145/2939672.2939799|New York, NY, USA|Association for Computing Machinery|9781450342322|2016|Taxi Driving Behavior Analysis in Latent Vehicle-to-Vehicle Networks: A Social Influence Perspective|Xu, Tong and Zhu, Hengshu and Zhao, Xiangyu and Liu, Qi and Zhong, Hao and Chen, Enhong and Xiong, Hui|inproceedings|10.1145/2939672.2939799|||||||||||||||||||||||||||||155546499|42
||Methodological chain, Knowledge bases, Big data, Scales, Visualization, Chronotopologic statistics||57-100|Quantitative Analysis and Modeling of Earth and Environmental Data|Abstract The methodological characteristics of the chronotopologic data analysis chain are discussed. Various kinds of knowledge are considered and properly classified, and several illustrative examples in applied sciences are presented. Big data and data-driven analyses are critically reviewed, and their implementation carefully assessed. Data scale types are classifications considered in property- and attribute-oriented settings. Classical statistics inadequacies are pointed out and the need of a chronotopology-dependent statistics is outlined. The chronotopologic visualization thinking mode and techniques are briefly reviewed.|https://doi.org/10.1016/B978-0-12-816341-2.00010-1|https://www.sciencedirect.com/science/article/pii/B9780128163412000101||Elsevier|978-0-12-816341-2|2022|Chapter 3 - CTDA methodology|Jiaping Wu and Junyu He and George Christakos|incollection|WU202257||||||||||Jiaping Wu and Junyu He and George Christakos|||||||||||||||||||156594537|42
||predictive maintenance, smart maintenance, big data analytics, Sensor Data Analytics||14-21||Predictive analytics methods have become increasingly important in Manufacturing Organization in the context of Smart Maintenance. Standardized process models for data mining already known to search existing data stocks for patterns, trends and correlations. Sensors are progressively implemented in production machines to create a database for data mining processes. But the risk of Big Data, thus the risk of low quality data is probably high. For an economic consideration, the amount of investment in new measurement technology and infrastructure should be assessed. Organizations are confronted with the challenge of how much they have to invest to obtain a meaningful database. For this reason, it is important to research which existing approaches support the development of a sufficient database for predictive maintenance in manufacturing systems and provide a methodical framework.|https://doi.org/10.1016/j.procs.2022.08.002|https://www.sciencedirect.com/science/article/pii/S1877050922007402||||2022|Building a smart database for predictive maintenance in already implemented manufacturing systems|Marina Klees and Safa Evirgen|article|KLEES202214|||Procedia Computer Science|18770509||204||International Conference on Industry Sciences and Computer Science Innovation|||19700182801|0,334|-|76|1907|6483|38511|13722|6359|2,09|20,19|Netherlands|Western Europe|2010-2020|Computer Science (miscellaneous)||||156905741|2108686752
dg.o '13|Quebec, Canada|big data, open government|10|1–10|Proceedings of the 14th Annual International Conference on Digital Government Research|"\"The promises and potential of Big Data in transforming digital government services, governments, and the interaction between governments, citizens, and the business sector, are substantial. From \"\"smart\"\" government to transformational government, Big Data can foster collaboration; create real-time solutions to challenges in agriculture, health, transportation, and more; and usher in a new era of policy- and decision-making. There are, however, a range of policy challenges to address regarding Big Data, including access and dissemination; digital asset management, archiving and preservation; privacy; and security. This paper selectively reviews and analyzes the U.S. policy context regarding Big Data and offers recommendations aimed at facilitating Big Data initiatives.\""|10.1145/2479724.2479730|https://doi.org/10.1145/2479724.2479730|New York, NY, USA|Association for Computing Machinery|9781450320573|2013|Big Data and E-Government: Issues, Policies, and Recommendations|Bertot, John Carlo and Choi, Heeyoon|inproceedings|10.1145/2479724.2479730|||||||||||||||||||||||||||||158202085|42
||Big Data analytics, Semantics, Knowledge extraction||107489||Modern applications of Big Data are transcending from being scalable solutions of data processing and analysis, to now provide advanced functionalities with the ability to exploit and understand the underpinning knowledge. This change is promoting the development of tools in the intersection of data processing, data analysis, knowledge extraction and management. In this paper, we propose TITAN, a software platform for managing all the life cycle of science workflows from deployment to execution in the context of Big Data applications. This platform is characterised by a design and operation mode driven by semantics at different levels: data sources, problem domain and workflow components. The proposed platform is developed upon an ontological framework of meta-data consistently managing processes and models and taking advantage of domain knowledge. TITAN comprises a well-grounded stack of Big Data technologies including Apache Kafka for inter-component communication, Apache Avro for data serialisation and Apache Spark for data analytics. A series of use cases are conducted for validation, which comprises workflow composition and semantic meta-data management in academic and real-world fields of human activity recognition and land use monitoring from satellite images.|https://doi.org/10.1016/j.knosys.2021.107489|https://www.sciencedirect.com/science/article/pii/S0950705121007516||||2021|TITAN: A knowledge-based platform for Big Data workflow management|Antonio Benítez-Hidalgo and Cristóbal Barba-González and José García-Nieto and Pedro Gutiérrez-Moncayo and Manuel Paneque and Antonio J. Nebro and María del Mar Roldán-García and José F. Aldana-Montes and Ismael Navas-Delgado|article|BENITEZHIDALGO2021107489|||Knowledge-Based Systems|09507051||232|||||24772|1,587|Q1|121|716|1187|36777|11094|1181|9,42|51,36|Netherlands|Western Europe|1987-2020|Artificial Intelligence (Q1); Information Systems and Management (Q1); Management Information Systems (Q1); Software (Q1)|22,261|8.038|0.02794|158851496|1341138576
SAC '22|Virtual Event|tamper-proof, GDPR, blockchains, immutable, privacy assessment|10|1218–1227|Proceedings of the 37th ACM/SIGAPP Symposium on Applied Computing|Due to the advancing digitalization, the importance of data is constantly increasing. Application domains such as smart cars, smart cities, or smart healthcare rely on the permanent availability of large amounts of data to all parties involved. As a result, the value of data increases, making it a lucrative target for cyber-attacks. Particularly when human lives depend on the data, additional protection measures are therefore important for data management and provision. Blockchains, i. e., decentralized, immutable, and tamper-proof data stores, are becoming increasingly popular for this purpose. Yet, from a data protection perspective, the immutable and tamper-proof properties of blockchains pose a privacy concern. In this paper, we therefore investigate whether blockchains are in compliance with the General Data Protection Regulation (GDPR) if personal data are involved. To this end, we elaborate which articles of the GDPR are relevant in this regard and present technical solutions for those legal requirements with which blockchains are in conflict. We further identify open research questions that need to be addressed in order to achieve a privacy-by-design blockchain system.|10.1145/3477314.3506986|https://doi.org/10.1145/3477314.3506986|New York, NY, USA|Association for Computing Machinery|9781450387132|2022|Can Blockchains and Data Privacy Laws Be Reconciled? A Fundamental Study of How Privacy-Aware Blockchains Are Feasible|Stach, Christoph and Gritti, Cl\'{e}mentine and Przytarski, Dennis and Mitschang, Bernhard|inproceedings|10.1145/3477314.3506986|||||||||||||||||||||||||||||159555167|42
||Air pollution, Fine particulate matter, Spatiotemporal interpolation, Machine learning, Deep learning||107-134|Spatiotemporal Analysis of Air Pollution and Its Application in Public Health|An accurate understanding of air pollutants in a continuous space-time domain is critical for meaningful assessment of the quantitative relationship between the adverse health effects and the concentrations of air pollutants. Traditional interpolation methods, including various statistic and nonstatistic regression models, typically involve restrictive assumptions regarding independence of observations and distributions of outcomes. Moreover, a set of relationships among variables need to be defined strictly in advance. Machine learning opens a new door to understand the air pollution data based on the exposing data-driven relationships and predicting outcomes without empirical models. In this chapter, the state-of-the-art machine learning methods will be introduced to unlock the full potential of the air pollutant data, that is, to estimate the PM2.5 concentration more accurately in the spatiotemporal domain. The methods can be extended to the other air pollutants.|https://doi.org/10.1016/B978-0-12-815822-7.00005-4|https://www.sciencedirect.com/science/article/pii/B9780128158227000054||Elsevier|978-0-12-815822-7|2020|Chapter 5 - Machine learning for spatiotemporal big data in air pollution|Weitian Tong|incollection|TONG2020107||||||||||Lixin Li and Xiaolu Zhou and Weitian Tong|||||||||||||||||||161210853|42
||Big Data;Meteorology;Portals;Voting;open data;post-truth;fake news;risk identification;risk mitigation;data quality assurance||2588-2594|2017 IEEE International Conference on Big Data (Big Data)|Big Data analysis often relies on open data, integrating it with large private data sets, using it as ground truth information, or providing it as part of the input to large simulations. Data can be released openly by governments to achieve various objectives: transparency, informing citizen engagement, or supporting private enterprise, to name a few. To the latter objective, Big Data analytics algorithms rely on high-quality, timely access to various data sources, including open data. Examples include retail analytics drawing on open demographic data and weather forecast systems drawing on open weather and climate data. In this paper, we describe the rise of post-truth in society, and the risks this poses to the quality, integrity, and authenticity of open data. We also discuss approaches to identifying, assessing, and mitigating these risks, and suggest future steps to manage this data quality concern.|10.1109/BigData.2017.8258218|||||2017|Identifying and mitigating risks to the quality of open data in the post-truth era|Colborne, Adrienne and Smit, Michael|inproceedings|8258218||Dec|||||||||||||||||||||||||||161338422|42
||Dynamic road space allocation, Big data, Smart cities, Intelligent transportation systems, Information and communication technology, Urban planning||100008||Urban planning has focused on reallocating road space from automobile to more sustainable transport modes in many cities worldwide. Mostly in urban areas, road space (from façade to façade) is highly disputed by different urban activities and functions. Nonetheless, there are varying demand periods during the day in which road space is underutilized due to its static design. Underutilized spaces could be used for other mobility or access purposes to improve efficiency. Sensing road space, using big data and transport demand management tools, may characterize different demand patterns, adapt the road space dynamically and, ultimately, promote efficiency in using a scarce resource, such as urban road space. This approach also reinforces short-term flexibility in urban planning, allowing for better responses to unpredictable events. This paper defines the concept of dynamic road space allocation by discussing the previous literature on dynamic allocation of space. We propose a methodological framework and discuss the technological solutions as well as the many challenges of implementing dynamic road space allocation.|https://doi.org/10.1016/j.urbmob.2021.100008|https://www.sciencedirect.com/science/article/pii/S266709172100008X||||2021|Main challenges and opportunities to dynamic road space allocation: From static to dynamic urban designs|Gabriel Valença and Filipe Moura and Ana {Morais de Sá}|article|VALENCA2021100008|||Journal of Urban Mobility|26670917||1|||||||||||||||||||||||162622851|819223842
ARES 21|Vienna, Austria|Server, Identity Management, Security Management, Security|10||Proceedings of the 16th International Conference on Availability, Reliability and Security|In today’s networks, administrative access to Linux servers is commonly managed by Privileged Access Management (PAM). It is not only important to monitor these privileged accounts, but also to control segregation of duty and detect keys as well as accounts that potentially bypass PAM. Unprohibited access can become a business risk. In order to improve the security in a controlled manner, we establish IdMSecMan, a security management process tailored for identity and access management (IAM). Security management processes typically use the Deming Cycle or an adaption for continuous improvements of products, services, or processes within the network infrastructure. We adjust a security management process with visualization for IAM, which also shifts the focus from typical assets to the attacker. With the controlled cycles, the maturity of IAM is measured and can continually advance. This paper presents and applies the work in progress IdMSecMan to a motivating scenario in the field of Linux server. We evaluate our approach in a controlled test environment with first steps to roll it out in our data center. Last but not least, we discuss challenges and future work.|10.1145/3465481.3470055|https://doi.org/10.1145/3465481.3470055|New York, NY, USA|Association for Computing Machinery|9781450390514|2021|Towards Improving Identity and Access Management with the IdMSecMan Process Framework|"\"P\"\"{o}hn, Daniela and Seeber, Sebastian and Hanauer, Tanja and Ziegler, Jule A. and Schmitz, David\""|inproceedings|10.1145/3465481.3470055|89||||||||||||||||||||||||||||164791555|42
||relevance, GAH, data mining, CPR, data quality assessment, Resource allocation|26|||Data mining can hardly solve but always faces a problem that there is little meaningful information within the dataset serving a given requirement. Faced with multiple unknown datasets, to allocate data mining resources to acquire more desired data, it is necessary to establish a data quality assessment framework based on the relevance between the dataset and requirements. This framework can help the user to judge the potential benefits in advance, so as to optimize the resource allocation to those candidates. However, the unstructured data (e.g., image data) often presents dark data states, which makes it tricky for the user to understand the relevance based on content of the dataset in real time. Even if all data have label descriptions, how to measure the relevance between data efficiently under semantic propagation remains an urgent problem. Based on this, we propose a Deep Hash-based Relevance-aware Data Quality Assessment framework, which contains off-line learning and relevance mining parts as well as an on-line assessing part. In the off-line part, we first design a Graph Convolution Network (GCN)-AutoEncoder hash (GAH) algorithm to recognize the data (i.e., lighten the dark data), then construct a graph with restricted Hamming distance, and finally design a Cluster PageRank (CPR) algorithm to calculate the importance score for each node (image) so as to obtain the relevance representation based on semantic propagation. In the on-line part, we first retrieve the importance score by hash codes and then quickly get the assessment conclusion in the importance list. On the one hand, the introduction of GCN and co-occurrence probability in the GAH promotes the perception ability for dark data. On the other hand, the design of CPR utilizes hash collision to reduce the scale of graph and iteration matrix, which greatly decreases the consumption of space and computing resources. We conduct extensive experiments on both single-label and multi-label datasets to assess the relevance between data and requirements as well as test the resources allocation. Experimental results show our framework can gain the most desired data with the same mining resources. Besides, the test results on Tencent1M dataset demonstrate the framework can complete the assessment with a stability for given different requirements.|10.1145/3420038|https://doi.org/10.1145/3420038|New York, NY, USA|Association for Computing Machinery||2021|Deep Hash-Based Relevance-Aware Data Quality Assessment for Image Dark Data|Liu, Yu and Wang, Yangtao and Gao, Lianli and Guo, Chan and Xie, Yanzhao and Xiao, Zhili|article|10.1145/3420038|11|apr|ACM/IMS Trans. Data Sci.|26911922|2|2|May 2021||||||||||||||||||||||166128800|961429963
GEOCROWD '12|Redondo Beach, California|multi-user, dataset, mobility, model, situation, context|6|9–14|Proceedings of the 1st ACM SIGSPATIAL International Workshop on Crowdsourced and Volunteered Geographic Information|Finding the right data source for research is a challenge that many of us face. Although we live in times where 'Open Data' and 'Big Data' have become buzzwords, getting hold of a reasonable size and quality dataset is often hard. When it comes to user data such as mobility data, this becomes even tougher due to privacy-related concerns. This paper briefly explains our research in the area of multi-user context modeling and presents some criteria that we believe are important while selecting a dataset for testing different approaches in this domain. To find the right dataset, some relevant publicly available human mobility datasets are examined using these criteria. The following are the datasets that have been analyzed: Microsoft Research GeoLife Trajectory Dataset, Tracking Delft I Pedestrian Trajectory Dataset, MIT Media Lab Reality Mining Dataset and LifeMap Dataset. Besides these, some other useful data sources for researchers have been cited.|10.1145/2442952.2442955|https://doi.org/10.1145/2442952.2442955|New York, NY, USA|Association for Computing Machinery|9781450316941|2012|Analysis of User Mobility Data Sources for Multi-User Context Modeling|Mehta, Paras and Voisard, Agn\`{e}s|inproceedings|10.1145/2442952.2442955|||||||||||||||||||||||||||||166192896|42
DG.O'21|Omaha, NE, USA|Data quality, Sharing and cooperation|11|142–152|DG.O2021: The 22nd Annual International Conference on Digital Government Research|This paper collects a large number of cases and makes a comparative analysis of the typical application of Chinese and American open government data for public governance. Through comparison, this paper finds the gap between China's open government data and the United States, and then analyzes the reasons. On this basis, through the investigation of advanced experience, this paper puts forward the suggestions of open government data to innovate public governance, including data catalogue compilation, data standard formulating, data quality assessment and open government data sharing cooperation, in order to improve Chinese open government data to innovate the public governance level.|10.1145/3463677.3463687|https://doi.org/10.1145/3463677.3463687|New York, NY, USA|Association for Computing Machinery|9781450384926|2021|Research on Suggestions of Improving Chinese Open Government Data in Innovation of Public Governance|Li, Hongqin and Zhai, Jun|inproceedings|10.1145/3463677.3463687|||||||||||||||||||||||||||||166862013|42
DOLAP '13|San Francisco, California, USA|dbms, big data, sql, mapreduce, parallel algorithms|8|85–92|Proceedings of the Sixteenth International Workshop on Data Warehousing and OLAP|"\"Relational DBMSs remain the main data management technology, despite the big data analytics and no-SQL waves. On the other hand, for data analytics in a broad sense, there are plenty of non-DBMS tools including statistical languages, matrix packages, generic data mining programs and large-scale parallel systems, being the main technology for big data analytics. Such large-scale systems are mostly based on the Hadoop distributed file system and MapReduce. Thus it would seem a DBMS is not a good technology to analyze big data, going beyond SQL queries, acting just as a reliable and fast data repository. In this survey, we argue that is not the case, explaining important research that has enabled analytics on large databases inside a DBMS. However, we also argue DBMSs cannot compete with parallel systems like MapReduce to analyze web-scale text data. Therefore, each technology will keep influencing each other. We conclude with a proposal of long-term research issues, considering the \"\"big data analytics\"\" trend.\""|10.1145/2513190.2513198|https://doi.org/10.1145/2513190.2513198|New York, NY, USA|Association for Computing Machinery|9781450324120|2013|Can We Analyze Big Data inside a DBMS?|Ordonez, Carlos|inproceedings|10.1145/2513190.2513198|||||||||||||||||||||||||||||170971659|42
OpenSym '19|"\"Sk\"\"{o}vde, Sweden\""|impact, open data, educational themes, school pupils, educational resource|10||Proceedings of the 15th International Symposium on Open Collaboration|Private and public institutions are using open and public data to provide better services, which increases the impact of open data on daily life. With the advancement of technology, it becomes also important to equip our younger generation with the essential skills for future challenges. In order to bring up a generation equipped with 21st century skills, open data could facilitate educational processes at school level as an educational resource. Open data could acts as a key resource to enhance the understanding of data through critical thinking and ethical vision among the youth and school pupils. To bring open data into schools, it is important to know the teacher's perspective on open data literacy and its possible impact on pupils. As a research contribution, we answered these questions through a Danish public school teacher's survey where we interviewed 10 Danish public school teachers of grade 5-7th and analyzed their views about the impact of open data on pupils' learning development. After analyzing Copenhagen city's open data, we identified four open data educational themes that could facilitate different subjects, e.g. geography, mathematics, basic science and social science. The survey includes interviews, open discussions, questionnaires and an experiment with the grade 7th pupils, where we test the pupils' understanding with open data. The survey concluded that open data cannot only empower pupils to understand real facts about their local areas, improve civics awareness and develop digital and data skills, but also enable them to come up with the ideas to improve their communities.|10.1145/3306446.3340821|https://doi.org/10.1145/3306446.3340821|New York, NY, USA|Association for Computing Machinery|9781450363198|2019|Bringing Open Data into Danish Schools and Its Potential Impact on School Pupils|Saddiqa, Mubashrah and Rasmussen, Lise and Magnussen, Rikke and Larsen, Birger and Pedersen, Jens Myrup|inproceedings|10.1145/3306446.3340821|9||||||||||||||||||||||||||||172734749|42
||future multi-domain network monitoring, research challenges, next-generation measurement infrastructures|6|29–34||The perfSONAR-based Multi-domain Network Performance Measurement and Monitoring Workshop was held on February 20-21, 2014 in Arlington, VA. The goal of the workshop was to review the state of the perfSONAR effort and catalyze future directions by cross-fertilizing ideas, and distilling common themes among the diverse perfSONAR stakeholders that include: network operators and managers, end-users and network researchers. The timing and organization for the second workshop is significant because there are an increasing number of groups within NSF supported data-intensive computing and networking programs that are dealing with measurement, monitoring and troubleshooting of multi-domain issues. These groups are forming explicit measurement federations using perfSONAR to address a wide range of issues. In addition, the emergence and wide-adoption of new paradigms such as software-defined networking are taking shape to aid in traffic management needs of scientific communities and network operators. Consequently, there are new challenges that need to be addressed for extensible and programmable instrumentation, measurement data analysis, visualization and middleware security features in perfSONAR. This report summarizes the workshop efforts to bring together diverse groups for delivering targeted short/long talks, sharing latest advances, and identifying gaps that exist in the community for solving end-to-end performance problems in an effective, scalable fashion.|10.1145/2805789.2805795|https://doi.org/10.1145/2805789.2805795|New York, NY, USA|Association for Computing Machinery||2015|Research Challenges in Future Multi-Domain Network Performance Measurement and Monitoring|Calyam, Prasad and Swany, Martin|article|10.1145/2805789.2805795||jul|SIGCOMM Comput. Commun. Rev.|01464833|3|45|July 2015||||||||||||||||||||||177950338|121469385
||Analytical models;Adaptation models;Numerical analysis;Data integrity;Computational modeling;Urban areas;Data aggregation;Data Aggregation;Data Analysis;Quality Evaluation;Structural Equation Model||1279-1282|2020 5th International Conference on Mechanical, Control and Computer Engineering (ICMCCE)|In the era of big data, how to evaluate the data quality of multi-source aggregation data is very important. The reason is that uneven data quality will directly lead to inaccurate or ambiguous data in the database, and indirectly lead to the deviation of subsequent data mining and decision-making. In this paper, structural equation model(SEM) is introduced to explore the effectiveness of various data quality evaluation indicators in data aggregation and finding out internal relationship between them. A new quality evaluation method of multi-source aggregation data is proposed, based on the regression's significance analysis and factor loads of each observation index in the SEM model. The case analysis shows that the proposed method is feasible and can be used to evaluate the quality of multi-source aggregation data adaptively for a long time.|10.1109/ICMCCE51767.2020.00280|||||2020|Quality Analysis and Evaluation Method for Multisource Aggregation Data based on Structural Equation Model|Wang, Xiaofeng and Jiang, Yong and Zhan, Gaofeng and Zhao, Tong|inproceedings|9421436||Dec|||||||||||||||||||||||||||178684505|42
ICEGOV '15-16|Montevideo, Uruguay|Crisis Response for Public Safety, Big Data, Information Distribution, Machine Learning, Relevance Assessments|10|266–275|Proceedings of the 9th International Conference on Theory and Practice of Electronic Governance|Crisis response organizations operate in very dynamic environments, in which it is essential for responders to acquire all information critical to their task execution in time. In reality, the responders are often faced with information overload, incomplete information, or a combination of both. This hampers their decision-making process, workflow, situational awareness and, consequently, effective execution of collaborative crisis response. Therefore, getting the right information to the right person at the right time is of crucial importance.The task of processing all data during crisis response situations and determining for whom at a particular moment the information is relevant is not straightforward. When developing an information system to support this task, some important challenges have to be taken into account. These challenges relate to the structure and truthfulness of the used data, the assessment of information relevance, and the dissemination of relevant information in time. While methods and techniques from big data can be used to collect and integrate data, machine learning can be used to build a model for relevance assessments. An example implementation of such a framework of big data is the TAID software system that collects and integrates data communicated between first responders and may send information to crisis responders that were not addressed in the initial communication. As an example of the impact of TAID on crisis response, we show its effect in a simulated crisis response scenario.|10.1145/2910019.2910033|https://doi.org/10.1145/2910019.2910033|New York, NY, USA|Association for Computing Machinery|9781450336406|2016|A Big Data Approach to Support Information Distribution in Crisis Response|Netten, Niels and van den Braak, Susan and Choenni, Sunil and van Someren, Maarten|inproceedings|10.1145/2910019.2910033|||||||||||||||||||||||||||||179545475|42
CSCW '17|Portland, Oregon, USA|facebook, content analysis, dataset, research methods, social media, privacy, machine learning, prediction, mechanical turk, mixed methods|14|567–580|Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing|When social networking sites give users granular control over their privacy settings, the result is that some content across the site is public and some is not. How might this content--or characteristics of users who post publicly versus to a limited audience--be different? If these differences exist, research studies of public content could potentially be introducing systematic bias. Via Mechanical Turk, we asked 1,815 Facebook users to share recent posts. Using qualitative coding and quantitative measures, we characterize and categorize the nature of the content. Using machine learning techniques, we analyze patterns of choices for privacy settings. Contrary to expectations, we find that content type is not a significant predictor of privacy setting; however, some demographics such as gender and age are predictive. Additionally, with consent of participants, we provide a dataset of nearly 9,000 public and non-public Facebook posts.|10.1145/2998181.2998223|https://doi.org/10.1145/2998181.2998223|New York, NY, USA|Association for Computing Machinery|9781450343350|2017|What (or Who) Is Public? Privacy Settings and Social Media Content Sharing|Fiesler, Casey and Dye, Michaelanne and Feuston, Jessica L. and Hiruncharoenvate, Chaya and Hutto, C.J. and Morrison, Shannon and Khanipour Roshan, Parisa and Pavalanathan, Umashanthi and Bruckman, Amy S. and De Choudhury, Munmun and Gilbert, Eric|inproceedings|10.1145/2998181.2998223|||||||||||||||||||||||||||||180908370|42
|||4|3429–3432||Software 2.0 refers to the fundamental shift in software engineering where using machine learning becomes the new norm in software with the availability of big data and computing infrastructure. As a result, many software engineering practices need to be rethought from scratch where data becomes a first-class citizen, on par with code. It is well known that 80--90% of the time for machine learning development is spent on data preparation. Also, even the best machine learning algorithms cannot perform well without good data or at least handling biased and dirty data during model training. In this tutorial, we focus on data collection and quality challenges that frequently occur in deep learning applications. Compared to traditional machine learning, there is less need for feature engineering, but more need for significant amounts of data. We thus go through state-of-the-art data collection techniques for machine learning. Then, we cover data validation and cleaning techniques for improving data quality. Even if the data is still problematic, hope is not lost, and we cover fair and robust training techniques for handling data bias and errors. We believe that the data management community is well poised to lead the research in these directions. The presenters have extensive experience in developing machine learning platforms and publishing papers in top-tier database, data mining, and machine learning venues.|10.14778/3415478.3415562|https://doi.org/10.14778/3415478.3415562||VLDB Endowment||2020|Data Collection and Quality Challenges for Deep Learning|Whang, Steven Euijong and Lee, Jae-Gil|article|10.14778/3415478.3415562||sep|Proc. VLDB Endow.|21508097|12|13|August 2020||||21100199855|0,946|Q1|134|246|598|12401|3759|566|5,50|50,41|United States|Northern America|2008-2020|Computer Science (miscellaneous) (Q1)|7,198|2.047|0.00891|182209330|1216159931
||Internet;Online services;Encyclopedias;Training;Task analysis;Machine translation;Buildings;Dataset;deep learning;natural language processing;Persian;question answering;machine reading comprehension||26045-26057||Developing Question Answering systems (QA) is one of the main goals in Artificial Intelligence. With the advent of Deep Learning (DL) techniques, QA systems have witnessed significant advances. Although DL performs very well on QA, it requires a considerable amount of annotated data for training. Many annotated datasets have been built for the QA task; most of them are exclusively in English. In order to address the need for a high-quality QA dataset in the Persian language, we present PersianQuAD, the native QA dataset for the Persian language. We create PersianQuAD in four steps: 1) Wikipedia article selection, 2) question-answer collection, 3) three-candidates test set preparation, and 4) Data Quality Monitoring. PersianQuAD consists of approximately 20,000 questions and answers made by native annotators on a set of Persian Wikipedia articles. The answer to each question is a segment of the corresponding article text. To better understand PersianQuAD and ensure its representativeness, we analyze PersianQuAD and show it contains questions of varying types and difficulties. We also present three versions of a deep learning-based QA system trained with PersianQuAD. Our best system achieves an F1 score of 82.97% which is comparable to that of QA systems on English SQuAD, made by the Stanford University. This shows that PersianQuAD performs well for training deep-learning-based QA systems. Human performance on PersianQuAD is significantly better (96.49%), demonstrating that PersianQuAD is challenging enough and there is still plenty of room for future improvement. PersianQuAD and all QA models implemented in this paper are freely available.|10.1109/ACCESS.2022.3157289|||||2022|PersianQuAD: The Native Question Answering Dataset for the Persian Language|Kazemi, Arefeh and Mozafari, Jamshid and Nematbakhsh, Mohammad Ali|article|9729745|||IEEE Access|21693536||10|||||21100374601|0,587|Q1|127|18036|24267|771081|116691|24200|4,48|42,75|United States|Northern America|2013-2020|Computer Science (miscellaneous) (Q1); Engineering (miscellaneous) (Q1); Materials Science (miscellaneous) (Q2)|105,968|3.367|0.15396|182962162|1905633267
||Computational modeling;Resource management;Servers;Training;Collaborative work;Particle measurements;Atmospheric measurements;Federated Learning;Incentive Mechanism;Fairness||1-13||Federated Learning (FL) has emerged as a privacy-preserving distributed machine learning paradigm. To motivate data owners to contribute towards FL, research on FL incentive mechanisms is gaining great interest. Existing monetary incentive mechanisms generally share the same FL model with all participants regardless of their contributions. Such an assumption can be unfair towards participants who contributed more and promote undesirable free-riding, especially when the final model is of great utility value to participants. In this paper, we propose a Fairness-Aware Incentive Mechanism for federated learning (FedFAIM) to address such problem. It satisfies two types of fairness notion: 1) aggregation fairness, which determines aggregation results according to data quality; 2) reward fairness, which assigns each participant a unique model with performance reflecting his contribution. Aggregation fairness is achieved through efficient gradient aggregation which examines local gradient quality and aggregates them based on data quality. Reward fairness is achieved through an efficient Shapley value-based contribution assessment method and a novel reward allocation method based on reputation and distribution of local and global gradients. We further prove reward fairness is theoretically guaranteed. Extensive experiments show that FedFAIM provides stronger incentives than similar non-monetary FL incentive mechanisms while achieving a high level of fairness.|10.1109/TBDATA.2022.3183614|||||2022|FedFAIM: A Model Performance-based Fair Incentive Mechanism for Federated Learning|Shi, Zhuan and Zhang, Lan and Yao, Zhenyu and Lyu, Lingjuan and Chen, Cen and Wang, Li and Wang, Junhao and Li, Xiang-Yang|article|9797864|||IEEE Transactions on Big Data|23327790|||||||21101019393|0,959|Q1|6|71|9|998|37|8|4,11|14,06|United States|Northern America|2020|Information Systems (Q1); Information Systems and Management (Q1)|636|3.344|0.00134|186243299|1510992500
||IEEE Standards;Machine learning;Privacy;Modeling;Economics;Collaborative work;Metasearch;computation efficiency;economic viability;federated machine learning (FML);IEEE 3652.1™;incentive mechanism;machine learning;model performance;privacy;privacy regulations;security||1-69||Federated machine learning defines a machine learning framework that allows a collective model to be constructed from data that is distributed across repositories owned by different organizations or devices. A blueprint for data usage and model building across organizations and devices while meeting applicable privacy, security and regulatory requirements is provided in this guide. It defines the architectural framework and application guidelines for federated machine learning, including description and definition of federated machine learning; the categories federated machine learning and the application scenarios to which each category applies; performance evaluation of federated machine learning; and associated regulatory requirements.|10.1109/IEEESTD.2021.9382202|||||2021|IEEE Guide for Architectural Framework and Application of Federated Machine Learning||article|9382202||March|IEEE Std 3652.1-2020||||||||||||||||||||||||||186245948|42
||On-road vehicle emissions, Traffic congestion, Light-duty gasoline vehicles, Real-world, Big data, Emission factors||157581||Light-duty gasoline vehicles (LDGVs) have made up >90 % of vehicle fleets in China since 2019, moreover, with a high annual growth rate (> 10 %) since 2017. Hence, accurate estimates of air pollutant emissions of these fast-changing LDGVs are vital for air quality management, human healthcare, and ecological protection. However, this issue is poorly quantified due to insufficient reserves of timely updated LDGV emission factors, which are dependent on real-world activity levels. Here we constructed a big dataset of explicit emission profiles (e.g., emission factors and accumulated mileages) for 159,051 LDGVs based on an official I/M database by matching real-time traffic dynamics via real-world traffic monitoring (e.g., traffic volumes and speeds). Consequently, we provide robust evidence that the emission factors of these LDGVs follow a clear heavy-tailed distribution. The top 10 % emitters contributed >60 % to the total fleet emissions, while the bottom 50 % contributed <10 %. Such emission factors were effectively reduced by 75.7–86.2 % as official emission standards upgraded gradually (i.e., from China 2 to China 5) within 13 years from 2004 to 2017. Nevertheless, such achievements would be offset once traffic congestion occurred. In the real world, the typical traffic congestions (i.e., vehicle speed <5 km/h) can lead to emissions 5– 9 times higher than those on non-congested roads (i.e., vehicle speed >50 km/h). These empirical analyses enabled us to propose future traffic scenarios that could harmonize emission standards and traffic congestion. Practical approaches on vehicle emission controls under realistic conditions are proposed, which would provide new insights for future urban vehicle emission management.|https://doi.org/10.1016/j.scitotenv.2022.157581|https://www.sciencedirect.com/science/article/pii/S0048969722046794||||2022|Quantifying on-road vehicle emissions during traffic congestion using updated emission factors of light-duty gasoline vehicles and real-world traffic monitoring big data|Xue Chen and Linhui Jiang and Yan Xia and Lu Wang and Jianjie Ye and Tangyan Hou and Yibo Zhang and Mengying Li and Zhen Li and Zhe Song and Jiali Li and Yaping Jiang and Pengfei Li and Xiaoye Zhang and Yang Zhang and Daniel Rosenfeld and John H. Seinfeld and Shaocai Yu|article|CHEN2022157581|||Science of The Total Environment|00489697||847|||||25349|1,795|Q1|244|6929|13278|455970|106837|13125|7,96|65,81|Netherlands|Western Europe|1970, 1972-2021|Environmental Chemistry (Q1); Environmental Engineering (Q1); Pollution (Q1); Waste Management and Disposal (Q1)|210,143|7.963|0.23082|187184842|2019676356
||||235-244||This paper considers the implications of so-called ‘big data’ for the analysis, modelling and planning of transport systems. The primary conceptual focus is on the needs of the practical context of medium-term planning and decision-making, from which perspective the paper seeks to achieve three goals: (i) to try to identify what is truly ‘special’ about big data; (ii) to provoke debate on the future relationship between transport planning and big data; and (iii) to try to identify promising themes for research and application. Differences in the information that can be derived from the data compared to more traditional surveys are discussed, and the respects in which they may impact on the role of models in supporting transport planning and decision-making are identified. It is argued that, over time, changes to the nature of data may lead to significant differences in both modelling approaches and in the expectations placed upon them. Furthermore, it is suggested that the potential widespread availability of data to commercial actors and travellers will affect the performance of the transport systems themselves, which might be expected to have knock-on effects for planning functions. We conclude by proposing a series of research challenges that we believe need to be addressed and warn against adaptations based on minimising change from the status quo.|https://doi.org/10.1016/j.jtrangeo.2017.11.004|https://www.sciencedirect.com/science/article/pii/S0966692317300984||||2019|Big data and understanding change in the context of planning transport systems|Dave Milne and David Watling|article|MILNE2019235|||Journal of Transport Geography|09666923||76|||||29295|1,809|Q1|108|264|515|15444|2931|514|5,21|58,50|United Kingdom|Western Europe|1993-2020|Environmental Science (miscellaneous) (Q1); Geography, Planning and Development (Q1); Transportation (Q1)|11,719|4.986|0.01053|188220679|745814393
||Frequency modulation;Organizations;Big Data;Maintenance engineering;Data mining;Internet of Things;Analytics;Big Data (BD);facilities management (FM);technology implementation||916-929||The recent advances in Internet of Things (IoT), computational analytics, processing power, and assimilation of Big Data (BD) are playing an important role in revolutionizing maintenance and operations regimes within the wider facilities management (FM) sector. The BD offers the potential for the FM to obtain valuable insights from a large amount of heterogeneous data collected through various sources and IoT allows for the integration of sensors. The aim of this article is to extend the exploratory studies conducted on Big Data analytics (BDA) implementation and empirically test and categorize the associated drivers and challenges. Using exploratory factor analysis (EFA), the researchers aim to bridge the current knowledge gap and highlight the principal factors affecting the BDA implementation. Questionnaires detailing 26 variables are sent to the FM organization in the U.K. who are in the process or have already implemented BDA initiatives within their FM operations. Fifty-two valid responses are analyzed by conducting EFA. The findings suggest that driven by market competition and ambitious sustainability goals, the industry is moving to holistically integrate analytics into its decision making. However, data quality, technological barriers, inadequate preparedness, data management, and governance issues and skill gaps are posing to be significant barriers to the fulfillment of expected opportunities. The findings of this study have important implications for FM businesses that are evaluating the potential of the BDA and IoT applications for their operations. Most importantly, it addresses the role of the BD maturity in FM organizations and its implications for perception of drivers.|10.1109/TEM.2019.2959914|||||2022|Drivers and Challenges Associated With the Implementation of Big Data Within U.K. Facilities Management Sector: An Exploratory Factor Analysis Approach|Konanahalli, Ashwini and Marinelli, Marina and Oyedele, Lukumon|article|8962328||Aug|IEEE Transactions on Engineering Management|15580040|4|69|||||||||||||||||||||||188620094|2003339926
||Distributed databases;Database systems;Synchronization;Business;Monitoring;Marine vehicles;Distributed database;advanced replication;materialized view;data conflict||456-459|2019 International Conference on Intelligent Transportation, Big Data & Smart City (ICITBS)|As the core of informatization, data has a huge significance to the development of information-based enterprises. Data replication technology is an important approach to solve the problem of enterprise data sharing based on distributed database system. It plays a crucial role in promoting business integration of enterprises and institutions, improving data quality, enhancing data sharing and improving the application level of back-end big data analysis [1]. It is necessary to do research for making a good data management of the distributed database application system, synchronizing the data to the data, preventing data conflicting and being able to synchronize or asynchronous replication. Combining with the database design model of the ship monitoring and control system, this paper mainly described how to complete the construction of distributed database system using Oracle, based on advanced replication technology named as the combination of multi-agent replication and materialized views hybrid replication technology.|10.1109/ICITBS.2019.00118|||||2019|The Analysis and Design of Ship Monitoring System Based on Hybrid Replication Technology|Jiang, Ying and Zhang, Na and Fang, Ying|inproceedings|8669595||Jan|||||||||||||||||||||||||||190983599|42
ICCCM '19|Bangkok, Thailand|non-profit organization, E-government, management model|5|164–168|Proceedings of the 7th International Conference on Computer and Communications Management|"\"With the development and popularization of Internet technology, our country is increasingly aware of the importance of e-government, and continuously expands the channels and means of e-government development in policy, such as the application of e-government to the management of non-profit organizations. However, in practice, \"\"e-government + NPO (non-profit organization) management\"\" still has problems such as digital divide, information sharing and insufficient disclosure, and information security. Therefore, this paper proposes a more complete non-profit organization management model based on e-government. From the perspectives of optimization services, information sharing, network supervision and information security, it is explained how to effectively realize the efficient management of non-profit organizations based on e-government.\""|10.1145/3348445.3348464|https://doi.org/10.1145/3348445.3348464|New York, NY, USA|Association for Computing Machinery|9781450371957|2019|Government Management Model of Non-Profit Organizations Based on E-Government|Lin, Yuting|inproceedings|10.1145/3348445.3348464|||||||||||||||||||||||||||||192005604|42
LAK '17|Vancouver, British Columbia, Canada|data protection by default, personal information, learning analytics systems design, learning analytics process requirements, data protection, learning analytics, privacy frameworks, privacy by design, data protection by design|10|243–252|Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference|Learning analytics open up a complex landscape of privacy and policy issues, which, in turn, influence how learning analytics systems and practices are designed. Research and development is governed by regulations for data storage and management, and by research ethics. Consequently, when moving solutions out the research labs implementers meet constraints defined in national laws and justified in privacy frameworks. This paper explores how the OECD, APEC and EU privacy frameworks seek to regulate data privacy, with significant implications for the discourse of learning, and ultimately, an impact on the design of tools, architectures and practices that now are on the drawing board. A detailed list of requirements for learning analytics systems is developed, based on the new legal requirements defined in the European General Data Protection Regulation, which from 2018 will be enforced as European law. The paper also gives an initial account of how the privacy discourse in Europe, Japan, South-Korea and China is developing and reflects upon the possible impact of the different privacy frameworks on the design of LA privacy solutions in these countries. This research contributes to knowledge of how concerns about privacy and data protection related to educational data can drive a discourse on new approaches to privacy engineering based on the principles of Privacy by Design. For the LAK community, this study represents the first attempt to conceptualise the issues of privacy and learning analytics in a cross-cultural context. The paper concludes with a plan to follow up this research on privacy policies and learning analytics systems development with a new international study.|10.1145/3027385.3027414|https://doi.org/10.1145/3027385.3027414|New York, NY, USA|Association for Computing Machinery|9781450348706|2017|The Influence of Data Protection and Privacy Frameworks on the Design of Learning Analytics Systems|Hoel, Tore and Griffiths, Dai and Chen, Weiqin|inproceedings|10.1145/3027385.3027414|||||||||||||||||||||||||||||192293392|42
||Big Data Quality Metrics, Big Data Value Chain, Big Data, Big Social Data, Sentiment Analysis, Opinion Mining||803-810||Human beings share their good or bad opinions about subjects, products, and services through internet and social networks. The ability to effectively analyze this kind of information is now seen as a key competitive advantage to better inform decisions. In order to do so, organizations employ Sentiment Analysis (SA) techniques on these data. However, the usage of social media around the world is ever-increasing, which considerably accelerates massive data generation and makes traditional SA systems unable to deliver useful insights. Such volume of data can be efficiently analyzed using the combination of SA techniques and Big Data technologies. In fact, big data is not a luxury but an essential necessary to make valuable predictions. However, there are some challenges associated with big data such as quality that could highly affect the SA systems’ accuracy that use huge volume of data. Thus, the quality aspect should be addressed in order to build reliable and credible systems. For this, the goal of our research work is to consider Big Data Quality Metrics (BDQM) in SA that rely of big data. In this paper, we first highlight the most eloquent BDQM that should be considered throughout the Big Data Value Chain (BDVC) in any big data project. Then, we measure the impact of BDQM on a novel SA method accuracy in a real case study by giving simulation results.|https://doi.org/10.1016/j.procs.2019.11.007|https://www.sciencedirect.com/science/article/pii/S1877050919317077||||2019|The Impact of Big Data Quality on Sentiment Analysis Approaches|Imane El Alaoui and Youssef Gahi|article|ALAOUI2019803|||Procedia Computer Science|18770509||160||The 10th International Conference on Emerging Ubiquitous Systems and Pervasive Networks (EUSPN-2019) / The 9th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare (ICTH-2019) / Affiliated Workshops|||19700182801|0,334|-|76|1907|6483|38511|13722|6359|2,09|20,19|Netherlands|Western Europe|2010-2020|Computer Science (miscellaneous)||||194445953|2108686752
CoDS COMAD 2020|Hyderabad, India|Landsat, temporal prediction, census, satellite imagery, socio-economic development, poverty mapping|9|73–81|Proceedings of the 7th ACM IKDD CoDS and 25th COMAD|Machine learning models based on satellite data have been actively researched to serve as a proxy for the prediction of socio-economic development indicators. Such models have however rarely been tested for transferability over time, i.e. whether models learned on data for a certain year are able to make accurate predictions on data for another year. Using a dataset from the Indian census at two time points, for the years 2001 and 2011, we evaluate the temporal transferability of a simple machine learning model at sub-national scales of districts and propose a generic method to improve its performance. This method can be especially relevant when training datasets are small to train a robust prediction model. Then, we go further to build an aggregate development index at the district-level, on the lines of the Human Development Index (HDI) and demonstrate high accuracy in predicting the index based on satellite data for different years. This can be used to build applications to guide data-driven policy making at fine spatial and temporal scales, without the need to conduct frequent expensive censuses and surveys on the ground.|10.1145/3371158.3371167|https://doi.org/10.1145/3371158.3371167|New York, NY, USA|Association for Computing Machinery|9781450377386|2020|Temporal Prediction of Socio-Economic Indicators Using Satellite Imagery|Bansal, Chahat and Jain, Arpit and Barwaria, Phaneesh and Choudhary, Anuj and Singh, Anupam and Gupta, Ayush and Seth, Aaditeshwar|inproceedings|10.1145/3371158.3371167|||||||||||||||||||||||||||||201825531|42
||Big data, Big data in biomedicine, Data analytics in biomedical research, Multiomics big data, Biomedical data management, Big data in personalized medicine||138-151||Big data is transforming biomedical research by integrating massive amounts of data from laboratory experiments, clinical investigations, healthcare records, and the internet of things. Specifically, the increasing rate at which information is obtained from omics technologies (genomics, epigenomics, transcriptomics, proteomics, metabolomics, and pharmacogenomics) is providing an opportunity for future advances in personalized medicine that are paving the way to improved patient care. The recent advances in omics technologies are profoundly contributing to big data in biomedicine and are anticipated to aid in disease diagnosis and patient care management. Herein, we critically review the major computational techniques, algorithms, and their outcomes that have contributed to recent advances in big data generated from biomedical research in various complex human diseases, such as cancer and infectious diseases. Finally, we discuss trends in the field and the future directions that must be considered to advance the influence of big data on biomedical research and its translation in the healthcare industry.|https://doi.org/10.1016/j.crbiot.2022.02.004|https://www.sciencedirect.com/science/article/pii/S2590262822000090||||2022|Big data: Historic advances and emerging trends in biomedical research|Conor John Cremin and Sabyasachi Dash and Xiaofeng Huang|article|CREMIN2022138|||Current Research in Biotechnology|25902628||4|||||||||||||||||||||||205107382|1056707218
CCIOT 2019|Tokyo, Japan|Twitter, data processing architecture, Apache Spark, social network data|6|1–6|Proceedings of the 2019 4th International Conference on Cloud Computing and Internet of Things|Social media provide continuous data streams that contain information with different level of sensitivity, validity and accuracy. Therefore, this type of information has to be properly filtered, extracted and processed to avoid noisy and inaccurate results. The main goal of this work is to propose architecture and workflow able to process Twitter social network data in near real-time. The primary design of the introduced modern architecture covers all processing aspects from data ingestion and storing to data processing and analysing. This paper presents Apache Spark and Hadoop implementation. The secondary objective is to analyse tweets with the defined topic --- floods. The word frequency method (Word Clouds) is shown as a major tool to analyse the content of the input dataset. The experimental architecture confirmed the usefulness of many well-known functions of Spark and Hadoop in the social data domain. The platforms which were used provided effective tools for optimal data ingesting, storing as well as processing and analysing. Based on the analytical part, it was observed that the word frequency method (n-grams) can effectively reveal the tweets content. According to the results of this study, the tweets proved their high informative potential regarding data quality and quantity.|10.1145/3361821.3361825|https://doi.org/10.1145/3361821.3361825|New York, NY, USA|Association for Computing Machinery|9781450372411|2019|Social Media Data Processing Infrastructure by Using Apache Spark Big Data Platform: Twitter Data Analysis|Podhoranyi, Michal and Vojacek, Lukas|inproceedings|10.1145/3361821.3361825|||||||||||||||||||||||||||||205450013|42
||Data integrity;Rough sets;Feature extraction;Image color analysis;Shape;Packet loss;Data mining;Flight test;data quality;rough set;quality evaluation||1053-1057|2020 13th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)|With the continuous development of flight test technology, the test system is filled with massive, multi-structured, and multi-dimensional data resources. The value of big data has been fully recognized by the society. How to tap the value of data has become an application in various research fields and industries. The most concerned issue of the field. Whether the data is rubbish or treasure, the most important question is whether the data to be analyzed and mined is of high quality. A low-quality data source will not only fail to reflect the value of the data, but may also run counter to the actual situation, which has side effects. In order to effectively evaluate the quality of flight test data, according to the characteristics of test data in flight test, a test data quality evaluation method based on rough set theory is proposed, standard test methods and test indicators for flight test data quality are proposed, and data quality evaluation is given. The method of rule extraction realizes the quality evaluation of flight test data.|10.1109/CISP-BMEI51763.2020.9263667|||||2020|Evaluation of Flight Test Data Quality Based on Rough Set Theory|Xiangwei, Kong|inproceedings|9263667||Oct|||||||||||||||||||||||||||206463876|42
||Quality of life, Point of interest, Happiness||643-653||"\"To explore the construction of a big data indicator system is conducive to a comprehensive, scientific, timely and accurate grasp of the quality of life of our residents and its evolutionary trends. This paper systematically sorts out the performance dimensions of the residents' quality of life, and integrates two types of methods of objective observation and subjective evaluation commercial POI(Point of Interest) data. From the aspects of life, entertainment, transportation, etc., preliminary development has been made including 8 first-level indicators, 16 second-level indicators, and 27 third-level indicators Big data indicator system, and measure the \"\"clogging point\"\" of the improvement of residents' quality of life, with a view to providing a scientific and feasible decision-making reference for \"\"meeting the people's increasing needs for a better life\"\".\""|https://doi.org/10.1016/j.iref.2022.01.004|https://www.sciencedirect.com/science/article/pii/S1059056022000041||||2022|Does city construction improve life quality?-evidence from POI data of China|Yang Wang and Hong Zhang and Libing Liu|article|WANG2022643|||International Review of Economics & Finance|10590560||80|||||22707|0,781|Q2|54|186|529|9509|1462|524|2,45|51,12|United States|Northern America|1992-2021|Economics and Econometrics (Q2); Finance (Q2)||||208222504|1328274359
||Big data;Metadata;Government;ISO Standards;Distributed databases;Open Data Quality;Information Modeling;E-Government;Big Data Knowledge Extraction||210-215|2016 IEEE Second International Conference on Big Data Computing Service and Applications (BigDataService)|Open Data (OD) is one of the most discussed issue of Big Data which raised the joint interest of public institutions, citizens and private companies since 2009. However, the massive amount of freely available data has not yet brought the expected effects: as of today, there is no application that has fully exploited the potential provided by large and distributed information sources in a non-trivial way, nor any service has substantially changed for the better the lives of people. The era of a new generation applications based on OD is far to come. In this context, we observe that OD quality is one of the major threats to achieving the goals of the OD movement. The starting point of this case study is the quality of the OD released by the five Constitutional offices of Italy. Our exploratory case study aims to assess the quality of such releases and the real implementations of OD. The outcome suggests the need of a drastic improvement in OD quality. Finally we highlight some key quality principles for OD, and propose a roadmap for further research.|10.1109/BigDataService.2016.37|||||2016|Big Data Quality: A Roadmap for Open Data|Ciancarini, Paolo and Poggi, Francesco and Russo, Daniel|inproceedings|7474375||March|||||||||||||||||||||||||||210100403|42
DATA '19|Dubai, United Arab Emirates|machin learning, OLAP, data mining, big data, data processing|5||Proceedings of the Second International Conference on Data Science, E-Learning and Information Systems|In this paper, the Big Data challenges and the processing is analyzed, recently great attention has been paid to the challenges for great data, largely due to the wide spread of applications and systems used in real life, such as presentation, modeling, processing and large (often unlimited) data storage. Mass Data Survey, OLAP Mass Data, Mass Data Dissemination and Mass Data Protection. Consequently, we focus on further research trends and, as a default, we will explore a future research challenge research project in this area of research.|10.1145/3368691.3368717|https://doi.org/10.1145/3368691.3368717|New York, NY, USA|Association for Computing Machinery|9781450372848|2019|Big Data Challenges and Achievements: Applications on Smart Cities and Energy Sector|Mohammed, Tareq Abed and Ghareeb, Ahmed and Al-bayaty, Hussein and Aljawarneh, Shadi|inproceedings|10.1145/3368691.3368717|26||||||||||||||||||||||||||||211406538|42
||Conferences;Robots;Intelligent systems;Deep Learning;Healthcare;Biomedical informatics||391-394|2018 International Conference on Robots & Intelligent System (ICRIS)|The expansion of big data in biomedical and health field has driven the need of new effective analysis technology. Deep learning is a powerful machine learning method. With the contribution of rapid computational power improvement, it is becoming a promising technique to generate new knowledge, interpretation and gain insights from high-throughout, heterogeneous and complex biomedical data from different sources, such as medical imaging, clinical genomics, and electronic health records. This paper presents an overview of the application of deep learning approach in the biomedical informatics. First we introduce the development of artificial neural network and deep learning, then mainly focus on the researches applying deep learning in biomedical informatics field. We also discuss the challenges for future improvement, such as data quality and interpretability.|10.1109/ICRIS.2018.00104|||||2018|The Application of Deep Learning in Biomedical Informatics|Wang, Sheng and Fu, Lieyong and Yao, Jianmin and Li, Yun|inproceedings|8410313||May|||||||||||||||||||||||||||212589329|42
||Data integrity;Production;LinkedIn;Big Data;Business;Debugging;Schedules||1579-1590|2020 IEEE 36th International Conference on Data Engineering (ICDE)|Many organizations process big data for important business operations and decisions. Hence, data quality greatly affects their success. Data quality problems continue to be widespread, costing US businesses an estimated $600 billion annually. To date, addressing data quality in production environments still poses many challenges: easily defining properties of high-quality data; validating production-scale data in a timely manner; debugging poor quality data; designing data quality solutions to be easy to use, understand, and operate; and designing data quality solutions to easily integrate with other systems. Current data validation solutions do not comprehensively address these challenges. To address data quality in production environments at LinkedIn, we developed Data Sentinel, a declarative production-scale data validation platform. In a simple and well-structured configuration, users declaratively specify the desired data checks. Then, Data Sentinel performs these data checks and writes the results to an easily understandable report. Furthermore, Data Sentinel provides well-defined schemas for the configuration and report. This makes it easy for other systems to interface or integrate with Data Sentinel. To make Data Sentinel even easier to use, understand, and operate in production environments, we provide Data Sentinel Service (DSS), a complementary system to help specify data checks, schedule, deploy, and tune data validation jobs, and understand data checking results. The contributions of this paper include the following: 1) Data Sentinel, a declarative production-scale data validation platform successfully deployed at LinkedIn 2) A generic design to build and deploy similar systems for production environments 3) Experiences and lessons learned that can benefit practitioners with similar objectives.|10.1109/ICDE48307.2020.00140|||||2020|Data Sentinel: A Declarative Production-Scale Data Validation Platform|Swami, Arun and Vasudevan, Sriram and Huyn, Joojay|inproceedings|9101464||April||2375026X|||||||||||||||||||||||||213978268|986587582
||Big Data, Transfer diagnosis, Automatic Vehicle Location Data||402-409||Since transfers increase the connectivity of routes, they improve the characteristics of transit networks. Designing and managing transfers are well-investigated issues arising at the tactical and operational level. Conversely, the monitoring phase was rarely faced to verify the consistency between well planned and/or delivered transfers. In this paper, we tailor an innovative methodology for measuring the rate of transfers between two routes by using archived Automatic Vehicle Location (AVL) data. This measurement is performed spatially, at shared and unshared (but reasonably quite close) bus stops, and temporally at each time period. The results are represented by easy-to-read control dashboards. This methodology is tested by about 240,000 AVL real records provided by the local bus operator of Cagliari (Italy) and provides valuable insights into the characterization of transfers.|https://doi.org/10.1016/j.trpro.2021.12.052|https://www.sciencedirect.com/science/article/pii/S2352146521009534||||2022|Transfer’s monitoring in bus transit services by Automatic Vehicle Location data|Sara Mozzoni and Massimo Di Francesco and Giulio Maternini and Benedetto Barabino|article|MOZZONI2022402|||Transportation Research Procedia|23521465||60||New scenarios for safe mobility in urban areasProceedings of the XXV International Conference Living and Walking in Cities (LWC 2021), September 9-10, 2021, Brescia, Italy|||||||||||||||||||||214195794|83400274
||Administrative data, Crowdsourcing, Data ethics, Data quality, Data sharing, Representation and bias, Social media, Spatial analysis, Value, Variety, Velocity, Volume, Volunteered geographical information||303-311|International Encyclopedia of Human Geography (Second Edition)|Big Data are deeply impactful for research in providing a diverse and rich array of novel sources for academic enquiry. Geographers are benefitting from varied data types including administrative data, social media, volunteered geographic information, and consumer data. The article presents examples and illustrations associated with each of these data types. An additional benefit of Big Data analytics is that it brings geographers into closer contact with real world partners and policy problems. Big Data also attract challenges including representational bias, variable data quality, difficulties of access and ownership, and ethical and legal restrictions in their use. Hence we conclude that Big Data are ripe for fruitful exploitation, but careful investments will be necessary if opportunities are to be realized to the full.|https://doi.org/10.1016/B978-0-08-102295-5.10616-X|https://www.sciencedirect.com/science/article/pii/B978008102295510616X|Oxford|Elsevier|978-0-08-102296-2|2020|Big Data|Mark Birkin|incollection|BIRKIN2020303|||||||||Second Edition|Audrey Kobayashi|||||||||||||||||||214218281|42
||Predictive models;Correlation;Measurement;Prediction algorithms;Data models;Bipartite graph;Algorithm design and analysis;multilabel classification;ensemble||1241-1246|2013 IEEE 13th International Conference on Data Mining|In the era of big data, a large amount of noisy and incomplete data can be collected from multiple sources for prediction tasks. Combining multiple models or data sources helps to counteract the effects of low data quality and the bias of any single model or data source, and thus can improve the robustness and the performance of predictive models. Out of privacy, storage and bandwidth considerations, in certain circumstances one has to combine the predictions from multiple models or data sources without accessing the raw data. Consensus-based prediction combination algorithms are effective for such situations. However, current research on prediction combination focuses on the single label setting, where an instance can have one and only one label. Nonetheless, data nowadays are usually multilabeled, such that more than one label have to be predicted at the same time. Direct applications of existing prediction combination methods to multilabel settings can lead to degenerated performance. In this paper, we address the challenges of combining predictions from multiple multilabel classifiers and propose two novel algorithms, MLCM-r (MultiLabel Consensus Maximization for ranking) and MLCM-a (MLCM for microAUC). These algorithms can capture label correlations that are common in multilabel classifications, and optimize corresponding performance metrics. Experimental results on popular multilabel classification tasks verify the theoretical analysis and effectiveness of the proposed methods.|10.1109/ICDM.2013.97|||||2013|Multilabel Consensus Classification|Xie, Sihong and Kong, Xiangnan and Gao, Jing and Fan, Wei and Yu, Philip S.|inproceedings|6729628||Dec||23748486|||||||||||||||||||||||||214433853|253568012
||Consortium blockchain, Personal credit reporting, Credit information sharing, Big data crediting||102659||The technical features of blockchain, including decentralization, data transparency, tamper-proofing, traceability, privacy protection and open-sourcing, make it a suitable technology for solving the information asymmetry problem in personal credit reporting transactions. Applying blockchain technology to credit reporting meets the needs of social credit system construction and may become an important technical direction in the future. This paper analyzed the problems faced by China’s personal credit reporting market, designed the framework of personal credit information sharing platform based on blockchain 3.0 architecture, studied the technical details of the platform and the technical advantages, and finally, applied the platform to the credit blacklist sharing transaction and explored the possible implementation approach. The in-depth integration of blockchain technology and personal credit reporting helps to realize the safe sharing of credit data and reduce the cost of credit data collection, thereby helping the technological and efficiency transformation of the personal credit reporting industry and promoting the overall development of the social credit system.|https://doi.org/10.1016/j.jisa.2020.102659|https://www.sciencedirect.com/science/article/pii/S2214212620308139||||2020|Design and application of a personal credit information sharing platform based on consortium blockchain|Jing Zhang and Rong Tan and Chunhua Su and Wen Si|article|ZHANG2020102659|||Journal of Information Security and Applications|22142126||55|||||21100332403|0,610|Q2|40|183|297|8559|1526|292|5,43|46,77|United Kingdom|Western Europe|2013-2020|Computer Networks and Communications (Q2); Safety, Risk, Reliability and Quality (Q2); Software (Q2)|1,526|3.872|0.00209|215231517|68968737
||big data, grid data asset, asset management, data governance||440-447||In the era of Big Data, data assets have become a strategic resource which cannot be overlooked by both society and enterprises. However, data is not equal to the assets. This paper first introduces the necessary conditions of data assetization and discriminates the concepts of data governance, data management and data asset management. Then it focuses on the unique connotation and characteristics of grid data assets. With reference to the mainstream theory of data management, the framework for the grid data asset management is set up in the combination of the characteristics of data assets, business needs and the actual situation in the power supply enterprises. Finally, this paper puts forward higher system requirements and technical requirements for China’s power supply enterprises to conduct data asset management.|https://doi.org/10.1016/j.procs.2018.10.258|https://www.sciencedirect.com/science/article/pii/S1877050918319288||||2018|Research on the Theory and Method of Grid Data Asset Management|Jun Wang and Yun-si Li and Wei Song and Ai-hua Li|article|WANG2018440|||Procedia Computer Science|18770509||139||6th International Conference on Information Technology and Quantitative Management|||19700182801|0,334|-|76|1907|6483|38511|13722|6359|2,09|20,19|Netherlands|Western Europe|2010-2020|Computer Science (miscellaneous)||||216143176|2108686752
||Computer Vision, Real-time Location Systems, healthcare, workflow monitoring|37|||Activities of a clinical staff in healthcare environments must regularly be adapted to new treatment methods, medications, and technologies. This constant evolution requires the monitoring of the workflow, or the sequence of actions from actors involved in a procedure, to ensure quality of medical services. In this context, recent advances in sensing technologies, including Real-time Location Systems and Computer Vision, enable high-precision tracking of actors and equipment. The current state-of-the-art about healthcare workflow monitoring typically focuses on a single technology and does not discuss its integration with others. Such an integration can lead to better solutions to evaluate medical workflows. This study aims to fill the gap regarding the analysis of monitoring technologies with a systematic literature review about sensors for capturing the workflow of healthcare environments. Its main scientific contribution is to identify both current technologies used to track activities in a clinical environment and gaps on their combination to achieve better results. It also proposes a taxonomy to classify work regarding sensing technologies and methods. The literature review does not present proposals that combine data obtained from Real-time Location Systems and Computer Vision sensors. Further analysis shows that a multimodal analysis is more flexible and could yield better results.|10.1145/3177852|https://doi.org/10.1145/3177852|New York, NY, USA|Association for Computing Machinery||2018|A Survey of Sensors in Healthcare Workflow Monitoring|"\"Antunes, Rodolfo S. and Seewald, Lucas A. and Rodrigues, Vinicius F. and Costa, Cristiano A. Da and Jr., Luiz Gonzaga and Righi, Rodrigo R. and Maier, Andreas and Eskofier, Bj\"\"{o}rn and Ollenschl\\\"\"{a}ger, Malte and Naderi, Farzad and Fahrig, Rebecca and Bauer, Sebastian and Klein, Sigrun and Campanatti, Gelson\""|article|10.1145/3177852|42|apr|ACM Comput. Surv.|03600300|2|51|March 2019||||23038|2,079|Q1|163|112|365|15859|6978|365|19,16|141,60|United States|Northern America|1969-2020|Computer Science (miscellaneous) (Q1); Theoretical Computer Science (Q1)|10,790|10.282|0.01438|217084343|1517405264
SEMANTiCS 2016|Leipzig, Germany|Provenance, Reflection, MEX, Metadata, Interoperability, Reproducible Research, Machine Learning Outputs, Annotation|8|17–24|Proceedings of the 12th International Conference on Semantic Systems|"\"Despite recent efforts to achieve a high level of interoperability of Machine Learning (ML) experiments, positively collaborating with the Reproducible Research context, we still run into problems created due to the existence of different ML platforms: each of those have a specific conceptualization or schema for representing data and metadata. This scenario leads to an extra coding-effort to achieve both the desired interoperability and a better provenance level as well as a more automatized environment for obtaining the generated results. Hence, when using ML libraries, it is a common task to re-design specific data models (schemata) and develop wrappers to manage the produced outputs. In this article, we discuss this gap focusing on the solution for the question: \"\"What is the cleanest and lowest-impact solution, i.e., the minimal effort to achieve both higher interoperability and provenance metadata levels in the Integrated Development Environments (IDE) context and how to facilitate the inherent data querying task?\"\". We introduce a novel and low-impact methodology specifically designed for code built in that context, combining Semantic Web concepts and reflection in order to minimize the gap for exporting ML metadata in a structured manner, allowing embedded code annotations that are, in run-time, converted in one of the state-of-the-art ML schemas for the Semantic Web: MEX Vocabulary.\""|10.1145/2993318.2993320|https://doi.org/10.1145/2993318.2993320|New York, NY, USA|Association for Computing Machinery|9781450347525|2016|MEX Interfaces: Automating Machine Learning Metadata Generation|Esteves, Diego and Mendes, Pablo N. and Moussallem, Diego and Duarte, Julio Cesar and Zaveri, Amrapali and Lehmann, Jens|inproceedings|10.1145/2993318.2993320|||||||||||||||||||||||||||||218736455|42
BDE 2019|Hong Kong, Hong Kong|Sentiment analysis, Big data quality metrics, Big data|8|36–43|Proceedings of the 2019 International Conference on Big Data Engineering|In a world increasingly connected, and in which information flows quickly and affects a very large number of people, sentiment analysis has seen a spectacular development over the past ten years. This is due to the fact that the explosion of social networks has allowed anyone with internet access to publicly express his opinion. Moreover, the emergence of big data has brought enormous opportunities and powerful storage and analytics tools to the field of sentiment analysis. However, big data introduces new variables and constraints that could radically affect the traditional models of sentiment analysis. Therefore, new concerns, such as big data quality, have to be addressed to get the most out of big data. To the best of our knowledge, no contributions have been published so far which address big data quality in SA throughout its different processes. In this paper, we first highlight the most important big data quality metrics to consider in any big data project. Then, we show how these metrics could be specifically considered in SA approaches and this for each phase in the big data value chain.|10.1145/3341620.3341629|https://doi.org/10.1145/3341620.3341629|New York, NY, USA|Association for Computing Machinery|9781450360913|2019|Big Data Quality Metrics for Sentiment Analysis Approaches|El Alaoui, Imane and Gahi, Youssef and Messoussi, Rochdi|inproceedings|10.1145/3341620.3341629|||||||||||||||||||||||||||||218992919|42
||artificial intelligence, batch effect, machine learning, RNA sequencing, single cell||1029-1040||Batch effects (BEs) are technical biases that may confound analysis of high-throughput biotechnological data. BEs are complex and effective mitigation is highly context-dependent. In particular, the advent of high-resolution technologies such as single-cell RNA sequencing presents new challenges. We first cover how BE modeling differs between traditional datasets and the new data landscape. We also discuss new approaches for measuring and mitigating BEs, including whether a BE is significant enough to warrant correction. Even with the advent of machine learning and artificial intelligence, the increased complexity of next-generation biotechnological data means increased complexities in BE management. We forecast that BEs will not only remain relevant in the age of big data but will become even more important.|https://doi.org/10.1016/j.tibtech.2022.02.005|https://www.sciencedirect.com/science/article/pii/S0167779922000361||||2022|Are batch effects still relevant in the age of big data?|Wilson Wen Bin Goh and Chern Han Yong and Limsoon Wong|article|GOH20221029|||Trends in Biotechnology|01677799|9|40|||||16146|3,192|Q1|219|167|397|11206|4561|346|11,86|67,10|United Kingdom|Western Europe|1983-2020|Bioengineering (Q1); Biotechnology (Q1)|20,693|19.536|0.01817|219842783|122518092
ASONAM '17|Sydney, Australia||8|1017–1024|Proceedings of the 2017 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining 2017|Completeness as one of the four major dimensions of data quality is a pervasive issue in modern databases. Although data imputation has been studied extensively in the literature, most of the research is focused on inference-based approach. We propose to harness Web tables as an external data source to effectively and efficiently retrieve missing data while taking into account the inherent uncertainty and lack of veracity that they contain.Existing approaches mostly rely on standard retrieval techniques and out-of-the-box matching methods which result in a very low precision, especially when dealing with numerical data. We, therefore, propose a novel data imputation approach by applying numerical context similarity measures which results in a significant increase in the precision of the imputation procedure, by ensuring that the imputed values are of the same domain and magnitude as the local values, thus resulting in an accurate imputation.We use Dresden Web Table Corpus which is comprised of more than 125 million web tables extracted from the Common Crawl as our knowledge source. The comprehensive experimental results demonstrate that the proposed method well outperforms the default out-of-the-box retrieval approach.|10.1145/3110025.3110161|https://doi.org/10.1145/3110025.3110161|New York, NY, USA|Association for Computing Machinery|9781450349932|2017|Context Similarity for Retrieval-Based Imputation|Ahmadov, Ahmad and Thiele, Maik and Lehner, Wolfgang and Wrembel, Robert|inproceedings|10.1145/3110025.3110161|||||||||||||||||||||||||||||219956539|42
||big data, algorithms, cloud computing, tactical environment, analytics|4|86–89||We discuss tactical challenges of the Big Data analytics regarding the underlying data, application space, and com- puting environment, and present a comprehensive solution framework motivated by the relevant tactical use cases. First, we summarize the unique characteristics of the Big Data problem in the Department of Defense (DoD) context and underline the main differences from the commercial Big Data problems. Then, we introduce two use cases, (i) Big Data analytics with multi-intelligence (multi-INT) sensor data and (ii) man-machine crowdsourcing using MapReduce framework. For these two use cases, we introduce Big Data analytics and cloud computing solutions in a coherent frame- work that supports tactical data, application, and computing needs.|10.1145/2627534.2627561|https://doi.org/10.1145/2627534.2627561|New York, NY, USA|Association for Computing Machinery||2014|Tactical Big Data Analytics: Challenges, Use Cases, and Solutions|Savas, Onur and Sagduyu, Yalin and Deng, Julia and Li, Jason|article|10.1145/2627534.2627561||apr|SIGMETRICS Perform. Eval. Rev.|01635999|4|41|March 2014||||26742|0,223|Q3|80|72|323|565|337|301|1,04|7,85|United States|Northern America|1980, 1982, 1984, 1986-1989, 1994, 1996-2020|Computer Networks and Communications (Q3); Software (Q3); Hardware and Architecture (Q4)||||220385160|302815259
ICDEL 2020|Beijing, China|Big data background, foreign exchange management, mode|4|162–165|Proceedings of the 5th International Conference on Distance Education and Learning|The rapid development of Internet technology has promoted the process of economic globalization, and the application of big data technology has injected new vitality into the innovation of foreign exchange management models. Businesses such as foreign exchange deposits and loans and foreign currency exchange have encountered new development opportunities. Under the effect of big data technology, it can quickly process massive amounts of information, respond to various exchange rate changes, and achieve the improvement of foreign exchange business management. Especially under the circumstances of the current RMB marketization, exchange rate reform and the diversification of the international situation, the difficulty of foreign exchange management is gradually increasing. How to better improve the efficiency of foreign exchange management has become a problem that must be solved at present. Therefore, it is of great significance to explore the foreign exchange management model based on the background of big data, build a big data computing mechanism, give full play to its advantages in foreign exchange management, and promote the improvement of foreign exchange management.|10.1145/3402569.3409041|https://doi.org/10.1145/3402569.3409041|New York, NY, USA|Association for Computing Machinery|9781450377546|2020|Research on Foreign Exchange Management Model Based on Big Data|Han, Ping|inproceedings|10.1145/3402569.3409041|||||||||||||||||||||||||||||222353442|42
KDD '14|New York, New York, USA|big data, clustering, missing value|10|651–660|Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining|Solving the missing-value (MV) problem with small estimation errors in big data environments is a notoriously resource-demanding task. As datasets and their user community continuously grow, the problem can only be exacerbated. Assume that it is possible to have a single machine (`Godzilla'), which can store the massive dataset and support an ever-growing community submitting MV imputation requests. Is it possible to replace Godzilla by employing a large number of cohort machines so that imputations can be performed much faster, engaging cohorts in parallel, each of which accesses much smaller partitions of the original dataset? If so, it would be preferable for obvious performance reasons to access only a subset of all cohorts per imputation. In this case, can we decide swiftly which is the desired subset of cohorts to engage per imputation? But efficiency and scalability is just one key concern! Is it possible to do the above while ensuring comparable or even better than Godzilla's imputation estimation errors? In this paper we derive answers to these fundamentals questions and develop principled methods and a framework which offer large performance speed-ups and better, or comparable, errors to that of Godzilla, independently of which missing-value imputation algorithm is used. Our contributions involve Pythia, a framework and algorithms for providing the answers to the above questions and for engaging the appropriate subset of cohorts per MV imputation request. Pythia functionality rests on two pillars: (i) dataset (partition) signatures, one per cohort, and (ii) similarity notions and algorithms, which can identify the appropriate subset of cohorts to engage. Comprehensive experimentation with real and synthetic datasets showcase our efficiency, scalability, and accuracy claims.|10.1145/2623330.2623615|https://doi.org/10.1145/2623330.2623615|New York, NY, USA|Association for Computing Machinery|9781450329569|2014|Scaling out Big Data Missing Value Imputations: Pythia vs. Godzilla|Anagnostopoulos, Christos and Triantafillou, Peter|inproceedings|10.1145/2623330.2623615|||||||||||||||||||||||||||||225382620|42
||Analytics, Clinical decision-making, Big data, Healthcare, Social representation theory||121222||The introduction and use of ‘big data and analytics’ is an on-going issue of discussion in health sectors globally. Healthcare systems of developed countries are trying to create more value and better healthcare through data and use of big data technologies. With an increasing number of articles identifying the value creation of big data and analytics for clinical decision-making, this paper examines how big data is applied, or not applied, in clinical practice. Using social representation theory as a theoretical foundation the paper explores people's perceptions of big data across all levels (policy making, planning, funding, and clinical care) of the New Zealand healthcare sector. The findings show that although adoption of big data technologies is planned for population health and health management, the potential of big data for clinical care has yet to be explored in the New Zealand context. The findings also highlight concern over data quality. The paper provides recommendations for policy and practice particularly around the need for engagement and participation of all levels to discuss data quality as well as big-data-based changes such as precision medicine and technology-assisted clinical decision-making tools. Future avenues of research are suggested.|https://doi.org/10.1016/j.techfore.2021.121222|https://www.sciencedirect.com/science/article/pii/S0040162521006557||||2022|Big data analytics for clinical decision-making: Understanding health sector perceptions of policy and practice|Kasuni Weerasinghe and Shane L. Scahill and David J. Pauleen and Nazim Taskin|article|WEERASINGHE2022121222|||Technological Forecasting and Social Change|00401625||174|||||14704|2,226|Q1|117|448|1099|35581|10127|1061|9,01|79,42|United States|Northern America|1970-2020|Applied Psychology (Q1); Business and International Management (Q1); Management of Technology and Innovation (Q1)|21,116|8.593|0.02416|228059139|1949868303
||Big data, Artificial intelligence, Machine learning, Data mining, Sentiment analytics||257-273||Artificial Intelligence (AI) could be an important foundation of competitive advantage in the market for firms. As such, firms use AI to achieve deep market engagement when the firm's data are employed to make informed decisions. This study examines the role of computer-mediated AI agents in detecting crises related to events in a firm. A crisis threatens organizational performance; therefore, a data-driven strategy will result in an efficient and timely reflection, which increases the success of crisis management. The study extends the situational crisis communication theory (SCCT) and Attribution theory frameworks built on big data and machine learning capabilities for early detection of crises in the market. This research proposes a structural model composed of a statistical and sentimental big data analytics approach. The findings of our empirical research suggest that knowledge extracted from day-to-day data communications such as email communications of a firm can lead to the sensing of critical events related to business activities. To test our model, we use a publicly available dataset containing 517,401 items belonging to 150 users, mostly senior managers of Enron during 1999 through the 2001 crisis. The findings suggest that the model is plausible in the early detection of Enron's critical events, which can support decision making in the market.|https://doi.org/10.1016/j.indmarman.2020.09.015|https://www.sciencedirect.com/science/article/pii/S0019850120308464||||2020|Using artificial intelligence to detect crisis related to events: Decision making in B2B by artificial intelligence|Aydin Farrokhi and Farid Shirazi and Nick Hajli and Mina Tajvidi|article|FARROKHI2020257|||Industrial Marketing Management|00198501||91|||||22792|2,022|Q1|136|314|465|27989|3787|450|6,86|89,14|United States|Northern America|1971-2020|Marketing (Q1)|16,291|6.960|0.00901|228934262|223518748
|||14|1446–1459||Human mobility data have been ubiquitously collected through cellular networks and mobile applications, and publicly released for academic research and commercial purposes for the last decade. Since releasing individual’s mobility records usually gives rise to privacy issues, data sets owners tend to only publish aggregated mobility data, such as the number of users covered by a cellular tower at a specific timestamp, which is believed to be sufficient for preserving users’ privacy. However, in this paper, we argue and prove that even publishing aggregated mobility data could lead to privacy breach in individuals’ trajectories. We develop an attack system that is able to exploit the uniqueness and regularity of human mobility to recover individual’s trajectories from the aggregated mobility data without any prior knowledge. By conducting experiments on two real-world data sets collected from both the mobile application and cellular network, we reveal that the attack system is able to recover users’ trajectories with an accuracy of about 73%~91% at the scale of thousands to ten thousands of mobile users, which indicates severe privacy leakage in such data sets. Our extensive analysis also reveals that by generalization and perturbation, this kind of privacy leakage can only be mitigated. Through the investigation on aggregated mobility data, this paper recognizes a novel privacy problem in publishing statistic data, which appeals for immediate attentions from both the academy and industry.|10.1109/TNET.2018.2829173|https://doi.org/10.1109/TNET.2018.2829173||IEEE Press||2018|A New Privacy Breach: User Trajectory Recovery From Aggregated Mobility Data|Tu, Zhen and Xu, Fengli and Li, Yong and Zhang, Pengyu and Jin, Depeng|article|10.1109/TNET.2018.2829173||jun|IEEE/ACM Trans. Netw.|10636692|3|26|June 2018||||27237|1,022|Q1|174|187|653|6994|3573|652|5,40|37,40|United States|Northern America|1993-2020|Computer Networks and Communications (Q1); Computer Science Applications (Q1); Electrical and Electronic Engineering (Q1); Software (Q1)||||229202047|1453185914
Beowulf '14|Annapolis, MD, USA||10|7–16|Proceedings of the 20 Years of Beowulf Workshop on Honor of Thomas Sterling's 65th Birthday|We study many Big Data applications from a variety of research and commercial areas and suggest a set of characteristic features and possible kernel benchmarks that stress those features for data analytics. We draw conclusions for the hardware and software architectures that are suggested by this analysis.|10.1145/2737909.2737912|https://doi.org/10.1145/2737909.2737912|New York, NY, USA|Association for Computing Machinery|9781450330312|2014|Towards an Understanding of Facets and Exemplars of Big Data Applications|Fox, Geoffrey C. and Jha, Shantenu and Qiu, Judy and Luckow, Andre|inproceedings|10.1145/2737909.2737912|||||||||||||||||||||||||||||229650100|42
SAICSIT '15|Stellenbosch, South Africa|Big Data Analytics, Technology Adoption, South Africa, Big Data|9||Proceedings of the 2015 Annual Research Conference on South African Institute of Computer Scientists and Information Technologists|The purpose of this interpretive study was to explore the challenges to the adoption of Big Data Analytics (BDA) in organisations. The Technology-Organisation-Environment (TOE) model was used to guide the study. Data was collected from a large telecommunication organization in South Africa. Seven participants, from both Information Technology (IT) and business were interviewed to gain a holistic overview of challenges towards the adoption of BDA. An inductive approach was used for analysis. Findings revealed technological challenges to the adoption of BDA as being Data Integration; Data Privacy; Return on Investment; Data Quality; Cost; Data Integrity; and Performance and Scalability. From the organizational perspective, the major challenges were Ownership and Control; Skills Shortages; Business Focus and Prioritisation; Training and Exposure; Silos; and Unclear Processes. From the environmental context there were no major challenges highlighted. Organisational challenges were deemed to be the major inhibitors to adoption of BDA|10.1145/2815782.2815793|https://doi.org/10.1145/2815782.2815793|New York, NY, USA|Association for Computing Machinery|9781450336833|2015|Challenges to the Organisational Adoption of Big Data Analytics: A Case Study in the South African Telecommunications Industry|Malaka, Iman and Brown, Irwin|inproceedings|10.1145/2815782.2815793|27||||||||||||||||||||||||||||231472295|42
SAC '19|Limassol, Cyprus|ACM proceedings, text tagging|9|762–770|Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing|Advancements in the field of healthcare information management have led to the development of a plethora of software, medical devices and standards. As a consequence, the rapid growth in quantity and quality of medical data has compounded the problem of heterogeneity; thereby decreasing the effectiveness and increasing the cost of diagnostics, treatment and follow-up. However, this problem can be resolved by using a semi-structured data storage and processing engine, which can extract semantic value from a large volume of patient data, produced by a variety of data sources, at variable rates and conforming to different abstraction levels. Going beyond the traditional relational model and by re-purposing state-of-the-art tools and technologies, we present, the Ubiquitous Health Profile (UHPr), which enables a semantic solution to the data interoperability problem, in the domain of healthcare1.|10.1145/3297280.3297354|https://doi.org/10.1145/3297280.3297354|New York, NY, USA|Association for Computing Machinery|9781450359337|2019|Resolving Data Interoperability in Ubiquitous Health Profile Using Semi-Structured Storage and Processing|Satti, Fahad Ahmed and Khan, Wajahat Ali and Lee, Ganghun and Khattak, Asad Masood and Lee, Sungyoung|inproceedings|10.1145/3297280.3297354|||||||||||||||||||||||||||||231708487|42
ICIIT '19|Da, Nang, Viet Nam|big data trading platform, regulatory construction, Data assets|6|107–112|Proceedings of the 2019 4th International Conference on Intelligent Information Technology|As a new type of asset, the value of big data resources can only be realized in the transaction circulation. Establishing and improving the big data trading platform market system is a systematic project to transform data from resources into assets. Through the comparative analysis of several typical big data trading platform construction practices in China, this research finds some problems, such as unclear positioning of some platforms leading to overlapping functions, extensive data transaction, lack of unified data pricing methods, unclear data ownership. In addition, there is no difference between the data escrow transaction mode and the aggregate transaction mode, and the rights of the data supply parties cannot be guaranteed. And it also discusses how to promote the construction and improvement of China's big data trading market more systematically, normally and institutionally. Finally, it proposes to build local big data trading platforms according to local conditions, establish a data transaction system based on blockchain, establish a big data transaction pricing index system, and establish a big data standard system.|10.1145/3321454.3321474|https://doi.org/10.1145/3321454.3321474|New York, NY, USA|Association for Computing Machinery|9781450366335|2019|Research on the Construction of Big Data Trading Platform in China|Yu, Bangbo and Zhao, Haijun|inproceedings|10.1145/3321454.3321474|||||||||||||||||||||||||||||234165759|42
ICIT 2021|Guangzhou, China|big data, Analysis, electronic medical records, artificial intelligence, internet of things|7|231–237|2021 The 9th International Conference on Information Technology: IoT and Smart City|Industry 4.0 is the pioneer of the Internet of Things (IoT). The Internet of Things (IoT) are often heard and successful in business revolution from all sectors. The IoT are widely used in numerous sectors including medical services and have become the rise of Internet of Medical Things (IoMT). One of the implementations is the Electronic Health Record (EHR) systems. Previously the health records were used in traditional manner such as print-out health record of a patient and stored to an archive room. With the innovation of EHR, patients’ health records are digitalized which provides advantages from space efficiency and paperless forms. EHR helps medical service management to provide better healthcare services. With the integration of Artificial Intelligence (AI) and Big Data Analysis in EHR, healthcare services provide more accurate and reliable diagnosis.|10.1145/3512576.3512618|https://doi.org/10.1145/3512576.3512618|New York, NY, USA|Association for Computing Machinery|9781450384971|2022|Artificial Intelligence and Big Data Analysis Implementation in Electronic Medical Records|Sardjono, Wahyu and Retnowardhani, Astari and Emil Kaburuan, Robert and Rahmasari, Aninda|inproceedings|10.1145/3512576.3512618|||||||||||||||||||||||||||||234725271|42
||Big data, Smart Product-Service System, Sales predict, Economic batch quantity, production plan||814-818||The era of big data has brought new challenges to chemical enterprises. In order to maximize the benefits, enterprises are considering to implement intelligent service technology into traditional production systems to improve the level of intelligence in business. This paper proposes a service framework based on big data driven prediction, which includes information perception layer, information application layer and big data service layer. In this paper, the composition of big data service layer is described in detail, and a sales predicting method based on neural network is introduced. The salability of products is divided, and the qualitative economic production volume mechanism is finally given. Based on the framework, an intelligent service system for enterprises with the characteristics of mass production is implemented. Experimental results show that the big data service framework can support chemical enterprises to make decisions to reduce costs, and provides an effective method for Smart Product Service System (PSS).|https://doi.org/10.1016/j.procir.2019.05.023|https://www.sciencedirect.com/science/article/pii/S2212827119310169||||2019|Big data driven decision-making for batch-based production systems|Yongheng Zhang and Rui Zhang and Yizhong Wang and Hongfei Guo and Ray Y Zhong and Ting Qu and Zhiwu Li|article|ZHANG2019814|||Procedia CIRP|22128271||83||11th CIRP Conference on Industrial Product-Service Systems|||21100243809|0,683|-|65|1456|3191|30451|8225|3145|2,40|20,91|Netherlands|Western Europe|2012-2020|Control and Systems Engineering; Industrial and Manufacturing Engineering||||235094863|2127027836
CHI EA '20|Honolulu, HI, USA|transportation management and planning, decision-maker, user study, persona, decision-making|7|1–7|Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems|Transportation decision-makers from government agencies play an important role in addressing the traffic network conditions, which in turn, have a major impact on the well-being of citizens. The practices, challenges, and needs of this group of practitioners are less represented in the HCI literature. We address this gap through an interview study with 19 practitioners from Transports Qu\'{e}bec, a government agency responsible for transportation infrastructures in Qu\'{e}bec, Canada. We found that this group of decision-makers can most benefit from research about data analysis tools and platforms that (1) provide information to support data quality awareness, (2) are interoperable with other tools in the complex workflow of the practitioners, and (3) support intuitive and customizable visual analytics. These implications can also be informative to the design of tools supporting other decision-making tasks and domains.|10.1145/3334480.3382864|https://doi.org/10.1145/3334480.3382864|New York, NY, USA|Association for Computing Machinery|9781450368193|2020|Capturing the Practices, Challenges, and Needs of Transportation Decision-Makers|Sharbatdar, Nasim and Lamine, Yassine and Milord, Brigitte and Morency, Catherine and Cheng, Jinghui|inproceedings|10.1145/3334480.3382864|||||||||||||||||||||||||||||240690886|42
WI-IAT '14||privacy preserving data mining, algorithm, data publishing|8|495–502|Proceedings of the 2014 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT) - Volume 02|In the era of big data, privacy becomes a challenging issue which already attracts a good number of research efforts. In the literature, most of existing privacy preserving algorithms focus on protecting users' privacy from being disclosed by making the set of designated semi-id features indiscriminate. However, how to automatically determine the appropriate semi-id features from high-dimensional correlated data is seldom studied. Therefore, in this paper we first theoretically study the problem and propose the IPFS algorithm to find all possible features forming the candidate semi-id feature set which can infer users' privacy. Then, the KIPFS algorithm is proposed to find the key features from the candidate semi-id feature set. By anonymizing the key feature set, called as key inferring privacy features (KIPFS), users' privacy is protected. To evaluate the effectiveness and the efficacy of the proposed approach, two state-of-the-art algorithms, i.e., K-anonymity and t-closeness, applied on the designated semi-id feature set are chose as the baseline algorithms and their revised versions are applied on the KIPFS for the performance comparison. The promising results showed that by anonymizing the identified KIPFS, both aforementioned algorithms can achieve better performance than the original ones in terms of efficiency and data quality.|10.1109/WI-IAT.2014.139|https://doi.org/10.1109/WI-IAT.2014.139|USA|IEEE Computer Society|9781479941438|2014|Protecting Data Privacy from Being Inferred from High Dimensional Correlated Data|Ba, Huafeng and Gao, Xiaoming and Zhang, Xiaofeng and He, Zhenyu|inproceedings|10.1109/WI-IAT.2014.139|||||||||||||||||||||||||||||240938172|42
SIGMOD '13|New York, New York, USA|big data, hadoop, data pipeline, machine learning, offline processing, data mining|10|1125–1134|Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data|The use of large-scale data mining and machine learning has proliferated through the adoption of technologies such as Hadoop, with its simple programming semantics and rich and active ecosystem. This paper presents LinkedIn's Hadoop-based analytics stack, which allows data scientists and machine learning researchers to extract insights and build product features from massive amounts of data. In particular, we present our solutions to the ``last mile'' issues in providing a rich developer ecosystem. This includes easy ingress from and egress to online systems, and managing workflows as production processes. A key characteristic of our solution is that these distributed system concerns are completely abstracted away from researchers. For example, deploying data back into the online system is simply a 1-line Pig command that a data scientist can add to the end of their script. We also present case studies on how this ecosystem is used to solve problems ranging from recommendations to news feed updates to email digesting to descriptive analytical dashboards for our members.|10.1145/2463676.2463707|https://doi.org/10.1145/2463676.2463707|New York, NY, USA|Association for Computing Machinery|9781450320375|2013|The Big Data Ecosystem at LinkedIn|Sumbaly, Roshan and Kreps, Jay and Shah, Sam|inproceedings|10.1145/2463676.2463707|||||||||||||||||||||||||||||242859047|42
||Analytics, Machine learning, Prediction, Clustering, Classification, Regression, Time series, Text analysis, Image analysis||209-270|Process Safety and Big Data|The chapter discusses the basics of big data analytics and the features of using analytical models in the field of process safety and risk management. The definition and basic principles of data analytics are necessary to understand the analytical techniques. The requirements for input data and the properties of analytical models are important for effective analytics. The concept, basic components, and varieties of machine learning are discussed. We consider such basic machine learning algorithms as clustering, classification, and regression. As advanced methods of data analytics, time series analysis methods, text analysis, and image analysis are proposed. Examples of the application of data analytics for risk management in the framework of process safety are considered.|https://doi.org/10.1016/B978-0-12-822066-5.00001-7|https://www.sciencedirect.com/science/article/pii/B9780128220665000017||Elsevier|978-0-12-822066-5|2021|Chapter 6 - Big data analytics and process safety|Sagit Valeev and Natalya Kondratyeva|incollection|VALEEV2021209||||||||||Sagit Valeev and Natalya Kondratyeva|||||||||||||||||||243786054|42
||Digital transformation, Digital certificates, Systemic metrology, Big data, Industry 4.0, Open science, FAIR||100232||Based on digital technologies, big data, artificial intelligence and machine-readable information, the digital transformation rapidly changes society, industries, and economies. Metrology as a central element of international trade, for confidence in measurements and part of the quality infrastructure is facing several challenges and opportunities in these developments. In this contribution we discuss some of the key challenges and a potential future role of metrology in the digital age. We address metrological principles for confidence in data and Algorithms, cyber-physical systems, FAIR data and metrology, and the role of metrology in the digital transformation in the quality infrastructure.|https://doi.org/10.1016/j.measen.2021.100232|https://www.sciencedirect.com/science/article/pii/S2665917421001951||||2021|Metrology for the digital age|Sascha Eichstädt and Anke Keidel and Julia Tesch|article|EICHSTADT2021100232|||Measurement: Sensors|26659174||18|||||||||||||||||||||||246213793|1962629531
ArabWIC 2019|Rabat, Morocco|Knowledge-intensive, Data-intensive, Process mining challenges, Adaptive Case Management, Process Mining, Business Process Management|6||Proceedings of the ArabWIC 6th Annual International Conference Research Track|Process mining has emerged as a research field that focuses on the analysis of processes using event data. Process mining is a current topic that reveals several challenges, the most important of which have defined in the Process Mining Manifesto [1]. However, none of the published works have mentioned the progress of process challenges from data-intensive system to knowledge-intensive system related to process mining field. Therefore, the objective of this paper is to provide researchers with the recent challenges emerged during the passage from data-intensive system to knowledge-intensive system.|10.1145/3333165.3333168|https://doi.org/10.1145/3333165.3333168|New York, NY, USA|Association for Computing Machinery|9781450360890|2019|Passage Challenges from Data-Intensive System to Knowledge-Intensive System Related to Process Mining Field|Lamghari, Zineb and Radgui, Maryam and Saidi, Rajaa and Rahmani, Moulay Driss|inproceedings|10.1145/3333165.3333168|3||||||||||||||||||||||||||||247131284|42
||Big Data;Quality Measurement;Quality Model;Quality Assurance||1-3|2019 International Conference on Big Data and Computational Intelligence (ICBDCI)|Big Data, is a growing technique these days. There are many uses of Big Data; Artificial Intelligence, Health Care, Business, and many more. For that reason, it becomes necessary to deal with this massive volume of data with caution and care in a term to make sure that the data used and produced is in high quality. Therefore, the Big Data quality is must, and its rules have to be satisfied. In this paper, the main Big Data Quality Factors, which need to be measured, is presented in the perspective of the data itself, the data management, data processing, and data users. This research highlights the quality factors that may be used later to create different Big Data quality models.|10.1109/ICBDCI.2019.8686099|||||2019|Big Data Quality Challenges|Abdallah, Mohammad|inproceedings|8686099||Feb|||||||||||||||||||||||||||250170634|42
||Smart Maintenance, Predictive Maintenance, Machine Learning, Long Short-Term Memory (LSTM), CRISP-DM, Industrial Robots, Manufacturing||115-120||Maintaining equipment is critical for increasing production capacity and decreasing production time. With the advent of digitalization, industries are able to access massive amounts of data that can be used to ensure their long-term viability and competitive advantage by implementing predictive maintenance. Therefore, this study aims to demonstrate a predictive maintenance application for a robot cell using real-world manufacturing big data coming from a company in the automotive industry. A hyperparameter tuned Long Short-Term Memory (LSTM) model is developed, and the results show that this model is capable of predicting the day of failure with good accuracy. The difficulties inherent in conducting real-world industrial initiatives are analyzed, and recommendations for improvement are presented.|https://doi.org/10.1016/j.ifacol.2022.09.193|https://www.sciencedirect.com/science/article/pii/S2405896322014082||||2022|A Predictive Maintenance Application for A Robot Cell using LSTM Model|Doyel Joseph and Tilani Gallege and Ebru Turanoglu Bekar and Catarina Dudas and Anders Skoogh|article|JOSEPH2022115|||IFAC-PapersOnLine|24058963|19|55||5th IFAC Workshop on Advanced Maintenance Engineering, Services and Technologies AMEST 2022|||21100456158|0,308|Q3|72|115|8407|1913|9863|8351|1,13|16,63|Austria|Western Europe|2002-2019|Control and Systems Engineering (Q3)||||250539672|676980763
||ethics, cyber security, law, network measurement, trust|4|76–79||"\"The inaugural Cyber-security Research Ethics Dialogue &amp; Strategy Workshop was held on May 23, 2013, in conjunction with the IEEE Security Privacy Symposium in San Francisco, California. CREDS embraced the theme of \"\"ethics-by-design\"\" in the context of cyber security research, and aimed to: Educate participants about underlying ethics principles and applications;Discuss ethical frameworks and how they are applied across the various stakeholders and respective communities who are involved;Impart recommendations about how ethical frameworks can be used to inform policymakers in evaluating the ethical underpinning of critical policy decisions;Explore cyber security research ethics techniques,tools,standards and practices so researchers can apply ethical principles within their research methodologies; andDiscuss specific case vignettes and explore the ethical implications of common research acts and omissions.\""|10.1145/2602204.2602217|https://doi.org/10.1145/2602204.2602217|New York, NY, USA|Association for Computing Machinery||2014|Cyber-Security Research Ethics Dialogue &amp; Strategy Workshop|Kenneally, Erin and Bailey, Michael|article|10.1145/2602204.2602217||apr|SIGCOMM Comput. Commun. Rev.|01464833|2|44|April 2014||||||||||||||||||||||251602388|121469385
AICCC '18|Tokyo, Japan|Data Pond, Power Grid, Data Lake, Monitoring And Diagnostic|7|192–198|Proceedings of the 2018 Artificial Intelligence and Cloud Computing Conference|In this paper, a data lake architecture is proposed for a class of monitoring and diagnostic systems applied to power grid. The differences between data lake and data warehouse is studied to make an informed decision on how to manage a huge amount of data. To adapt to the characteristics and performances of historical data and real-time data of power grid equipment, a monitoring and diagnosis system based on data lake storage architecture is designed. The application of the framework indicates the applicability and effectiveness of data lake architecture.|10.1145/3299819.3299850|https://doi.org/10.1145/3299819.3299850|New York, NY, USA|Association for Computing Machinery|9781450366236|2018|A Data Lake Architecture for Monitoring and Diagnosis System of Power Grid|Li, Ying and Zhang, AiMin and Zhang, Xinman and Wu, Zhihui|inproceedings|10.1145/3299819.3299850|||||||||||||||||||||||||||||252947951|42
||knowledge bases, data quality, graph-based repairing, Data flows|39|||In the big data era, data integration is becoming increasingly important. It is usually handled by data flows processes that extract, transform, and clean data from several sources, and populate the data integration system (DIS). Designing data flows is facing several challenges. In this article, we deal with data quality issues such as (1) specifying a set of quality rules, (2) enforcing them on the data flow pipeline to detect violations, and (3) producing accurate repairs for the detected violations. We propose QDflows, a system for designing quality-aware data flows that considers the following as input: (1) a high-quality knowledge base (KB) as the global schema of integration, (2) a set of data sources and a set of validated users’ requirements, (3) a set of defined mappings between data sources and the KB, and (4) a set of quality rules specified by users. QDflows uses an ontology to design the DIS schema. It offers the ability to define the DIS ontology as a module of the knowledge base, based on validated users’ requirements. The DIS ontology model is then extended with multiple types of quality rules specified by users. QDflows extracts and transforms data from sources to populate the DIS. It detects violations of quality rules enforced on the data flows, constructs repair patterns, searches for horizontal and vertical matches in the knowledge base, and performs an automatic repair when possible or generates possible repairs. It interactively involves users to validate the repair process before loading the clean data into the DIS. Using real-life and synthetic datasets, the DBpedia and Yago knowledge bases, we experimentally evaluate the generality, effectiveness, and efficiency of QDflows. We also showcase an interactive tool implementing our system.|10.1145/3064173|https://doi.org/10.1145/3064173|New York, NY, USA|Association for Computing Machinery||2017|QDflows: A System Driven by Knowledge Bases for Designing Quality-Aware Data Flows|Abdellaoui, Sabrina and Nader, Fahima and Chalal, Rachid|article|10.1145/3064173|14|jun|J. Data and Information Quality|19361955|3–4|8|July 2017||||||||||||||||||||||254506184|833754770
||Big Data analytics, Cognitive computing, Framework, Model, Systematic literature review, Value creation||617-632||Big Data represents a promising area for value creation and frontier research. The potential to extract actionable insights from Big Data has gained increasing attention of both academics and practitioners operating in several industries. Marketing domain has become from the start a field for experiments with Big Data approaches, even if the adoption of Big Data solutions does not always generate effective value for the adopters. Therefore, the gap existing between the potential of value creation embedded in the Big Data paradigm and the current limited exploitation of this value represents an area of investigation that this paper aims to explore. In particular, by following a systematic literature review, this study aims at presenting a framework that outlines the multiple value directions that the Big Data paradigm can generate for the adopting organizations. Eleven distinct value directions have been identified and then grouped in five dimensions (Informational, Transactional, Transformational, Strategic, Infrastructural Value), which constitute the pillars of the proposed framework. Finally, the framework has been also preliminarily applied in three case studies conducted within three Italian based companies operating in different industries (e-commerce, fast-moving consumer goods, and banking) in the final aim to see its applicability in real business scenarios.|https://doi.org/10.1016/j.indmarman.2020.03.015|https://www.sciencedirect.com/science/article/pii/S0019850120302212||||2020|A multi-dimension framework for value creation through Big Data|Gianluca Elia and Gloria Polimeno and Gianluca Solazzo and Giuseppina Passiante|article|ELIA2020617|||Industrial Marketing Management|00198501||90|||||22792|2,022|Q1|136|314|465|27989|3787|450|6,86|89,14|United States|Northern America|1971-2020|Marketing (Q1)|16,291|6.960|0.00901|255506805|223518748
||data quality, fact checking, data fusion, information extraction, Truth discovery|3||||10.1145/2935753|https://doi.org/10.1145/2935753|New York, NY, USA|Association for Computing Machinery||2016|Veracity of Big Data: Challenges of Cross-Modal Truth Discovery|Berti-Equille, Laure and Ba, Mouhamadou Lamine|article|10.1145/2935753|12|aug|J. Data and Information Quality|19361955|3|7|September 2016||||||||||||||||||||||257346982|833754770
||software-defined IoT, software-defined network, network function virtualization, network softwarization, Internet of Things|40|||Internet of Things (IoT) and Network Softwarization are fast becoming core technologies of information systems and network management for the next-generation Internet. The deployment and applications of IoT range from smart cities to urban computing and from ubiquitous healthcare to tactile Internet. For this reason, the physical infrastructure of heterogeneous network systems has become more complicated and thus requires efficient and dynamic solutions for management, configuration, and flow scheduling. Network softwarization in the form of Software Defined Networks and Network Function Virtualization has been extensively researched for IoT in the recent past. In this article, we present a systematic and comprehensive review of virtualization techniques explicitly designed for IoT networks. We have classified the literature into software-defined networks designed for IoT, function virtualization for IoT networks, and software-defined IoT networks. These categories are further divided into works that present architectural, security, and management solutions. Besides, the article highlights several short-term and long-term research challenges and open issues related to the adoption of software-defined Internet of Things.|10.1145/3379444|https://doi.org/10.1145/3379444|New York, NY, USA|Association for Computing Machinery||2020|A Survey of Network Virtualization Techniques for Internet of Things Using SDN and NFV|Alam, Iqbal and Sharif, Kashif and Li, Fan and Latif, Zohaib and Karim, M. M. and Biswas, Sujit and Nour, Boubakr and Wang, Yu|article|10.1145/3379444|35|apr|ACM Comput. Surv.|03600300|2|53|March 2021||||23038|2,079|Q1|163|112|365|15859|6978|365|19,16|141,60|United States|Northern America|1969-2020|Computer Science (miscellaneous) (Q1); Theoretical Computer Science (Q1)|10,790|10.282|0.01438|258161699|1517405264
|||11|209–219||Understanding complex biological phenomena involves answering complex biomedical questions on multiple biomolecular information simultaneously, which are expressed through multiple genomic and proteomic semantic annotations scattered in many distributed and heterogeneous data sources; such heterogeneity and dispersion hamper the biologists’ ability of asking global queries and performing global evaluations. To overcome this problem, we developed a software architecture to create and maintain a Genomic and Proteomic Knowledge Base (GPKB), which integrates several of the most relevant sources of such dispersed information (including Entrez Gene, UniProt, IntAct, Expasy Enzyme, GO, GOA, BioCyc, KEGG, Reactome, and OMIM). Our solution is general, as it uses a flexible, modular, and multilevel global data schema based on abstraction and generalization of integrated data features, and a set of automatic procedures for easing data integration and maintenance, also when the integrated data sources evolve in data content, structure, and number. These procedures also assure consistency, quality, and provenance tracking of all integrated data, and perform the semantic closure of the hierarchical relationships of the integrated biomedical ontologies. At http://www.bioinformatics.deib.polimi.it/GPKB/, a Web interface allows graphical easy composition of queries, although complex, on the knowledge base, supporting also semantic query expansion and comprehensive explorative search of the integrated data to better sustain biomedical knowledge extraction.|10.1109/TCBB.2015.2453944|https://doi.org/10.1109/TCBB.2015.2453944|Washington, DC, USA|IEEE Computer Society Press||2016|Integration and Querying of Genomic and Proteomic Semantic Annotations for Biomedical Knowledge Extraction|Masseroli, Marco and Canakoglu, Arif and Ceri, Stefano|article|10.1109/TCBB.2015.2453944||mar|IEEE/ACM Trans. Comput. Biol. Bioinformatics|15455963|2|13|March 2016||||||||||||||||||||||258293995|1878427007
WSDM '18|Marina Del Rey, CA, USA|emergency management, social media, disaster response|2|791–792|Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining|"\"During large-scale emergencies such as natural and man-made disasters, a massive amount of information is posted by the public in social media. Collecting, aggregating, and presenting this information to stakeholders can be extremely challenging, particularly if an understanding of the \"\"big picture»» is sought. This international workshop, the fifth in the series, is a key venue for researchers and practitioners to discuss research challenges and technical issues around the usage of social media in disaster management. Workshop»s website: https://sites.google.com/site/swdm2018/\""|10.1145/3159652.3160594|https://doi.org/10.1145/3159652.3160594|New York, NY, USA|Association for Computing Machinery|9781450355810|2018|The 5th International Workshop on Social Web for Disaster Management(SWDM'18): Collective Sensing, Trust, and Resilience in Global Crises|Lin, Yu-Ru and Castillo, Carlos and Yin, Jie|inproceedings|10.1145/3159652.3160594|||||||||||||||||||||||||||||258305965|42
DEBS '12|Berlin, Germany|artificial intelligence, pattern matching, uncertainty, event processing, event recognition|12|32–43|Proceedings of the 6th ACM International Conference on Distributed Event-Based Systems|"\"Big data is recognized as one of the three technology trends at the leading edge a CEO cannot afford to overlook in 2012. Big data is characterized by volume, velocity, variety and veracity (\"\"data in doubt\"\"). As big data applications, many of the emerging event processing applications must process events that arrive from sources such as sensors and social media, which have inherent uncertainties associated with them. Consider, for example, the possibility of incomplete data streams and streams including inaccurate data. In this tutorial we classify the different types of uncertainty found in event processing applications and discuss the implications on event representation and reasoning. An area of research in which uncertainty has been studied is Artificial Intelligence. We discuss, therefore, the main Artificial Intelligence-based event processing systems that support probabilistic reasoning. The presented approaches are illustrated using an example concerning crime detection.\""|10.1145/2335484.2335488|https://doi.org/10.1145/2335484.2335488|New York, NY, USA|Association for Computing Machinery|9781450313155|2012|Event Processing under Uncertainty|Artikis, Alexander and Etzion, Opher and Feldman, Zohar and Fournier, Fabiana|inproceedings|10.1145/2335484.2335488|||||||||||||||||||||||||||||258954351|42
||Big data, Smart data, Volume, Velocity, Variety, Automotive distribution||100406||This research examines for the first time the relationship between Big data and Smart data among French automotive distributors. Many low-tech firms engage in these data policies to improve their decisions and performance through the predictive capacities of their data. A discussion emerges in the literature according to which an effective policy lies in the conversion of a mass of raw data into so-called intelligent data. In order to understand better this digital transition, we question the transformation of data policies practiced in low-tech firms through the founding model of 3Vs (Volume, Variety and Velocity of data). First of all, this empirical study of 112 French automotive distributors develops the existing literature by proposing an original and detailed typology of the data policies practiced (Low data, Big data and Smart data). Secondly, after specifying the elements of the differences between the quantitative nature of Big data and the qualitative nature of Smart data, our results reveal and analyse for the first time the existence of their synergistic relationship. Companies transform their Big data approach into Smart data when they move from massive exploitation to intelligent exploitation of their data. The phenomenon is part of a high-end loop data exploitation. Initially, the exploitation of intelligent data can only be done by extracting a sample from a large raw data pool previously made by a Big data policy. Secondly, the organization's raw data pool is in turn enriched by the repayment of contributions made by the Smart data approach. Thus, this study develops three important ways. First off, we identify, detail and compare the current data policies of a traditional industry. Secondly, we reveal and explain the evolution of digital practices within organizations that now combine both quantitative and qualitative data exploitation. Finally, our results guide decision-makers towards the synergistic and the legitimate association of different forms of data management for better performance.|https://doi.org/10.1016/j.hitech.2021.100406|https://www.sciencedirect.com/science/article/pii/S1047831021000031||||2021|Big data and Smart data: two interdependent and synergistic digital policies within a virtuous data exploitation loop|Jean-Sébastien Lacam and David Salvetat|article|LACAM2021100406|||The Journal of High Technology Management Research|10478310|1|32|||||19713|0,684|Q2|46|18|56|1219|243|56|4,76|67,72|United Kingdom|Western Europe|1990-2020|Computer Science Applications (Q2); Information Systems and Management (Q2); Management of Technology and Innovation (Q2); Marketing (Q2); Strategy and Management (Q2)||||259861377|116475591
||data generation, mechanical turk, hybrid intelligence, behavioral experiments, model evaluation, crowdsourcing, incentives|46|7026–7071||This survey provides a comprehensive overview of the landscape of crowdsourcing research, targeted at the machine learning community. We begin with an overview of the ways in which crowdsourcing can be used to advance machine learning research, focusing on four application areas: 1) data generation, 2) evaluation and debugging of models, 3) hybrid intelligence systems that leverage the complementary strengths of humans and machines to expand the capabilities of AI, and 4) crowdsourced behavioral experiments that improve our understanding of how humans interact with machine learning systems and technology more broadly. We next review the extensive literature on the behavior of crowdworkers themselves. This research, which explores the prevalence of dishonesty among crowdworkers, how workers respond to both monetary incentives and intrinsic forms of motivation, and how crowdworkers interact with each other, has immediate implications that we distill into best practices that researchers should follow when using crowdsourcing in their own research. We conclude with a discussion of additional tips and best practices that are crucial to the success of any project that uses crowdsourcing, but rarely mentioned in the literature.||||JMLR.org||2017|Making Better Use of the Crowd: How Crowdsourcing Can Advance Machine Learning Research|Vaughan, Jennifer Wortman|article|10.5555/3122009.3242050||jan|J. Mach. Learn. Res.|15324435|1|18|January 2017||||||||||||||||||||||261754799|1642452964
|||14|362–375||Incentivized by the enormous economic profits, the data marketplace platform has been proliferated recently. In this paper, we consider the data marketplace setting where a data shopper would like to buy data instances from the data marketplace for correlation analysis of certain attributes. We assume that the data in the marketplace is dirty and not free. The goal is to find the data instances from a large number of datasets in the marketplace whose join result not only is of high-quality and rich join informativeness, but also delivers the best correlation between the requested attributes. To achieve this goal, we design DANCE, a middleware that provides the desired data acquisition service. DANCE consists of two phases: (1) In the off-line phase, it constructs a two-layer join graph from samples. The join graph includes the information of the datasets in the marketplace at both schema and instance levels; (2) In the online phase, it searches for the data instances that satisfy the constraints of data quality, budget, and join informativeness, while maximizing the correlation of source and target attribute sets. We prove that the complexity of the search problem is NP-hard, and design a heuristic algorithm based on Markov chain Monte Carlo (MCMC). Experiment results on two benchmark and one real datasets demonstrate the efficiency and effectiveness of our heuristic data acquisition algorithm.|10.14778/3297753.3297757|https://doi.org/10.14778/3297753.3297757||VLDB Endowment||2018|Cost-Efficient Data Acquisition on Online Data Marketplaces for Correlation Analysis|Li, Yanying and Sun, Haipei and Dong, Boxiang and Wang, Hui (Wendy)|article|10.14778/3297753.3297757||dec|Proc. VLDB Endow.|21508097|4|12|December 2018||||21100199855|0,946|Q1|134|246|598|12401|3759|566|5,50|50,41|United States|Northern America|2008-2020|Computer Science (miscellaneous) (Q1)|7,198|2.047|0.00891|263389959|1216159931
CSAE 2020|Sanya, China|Data product, Pricing model, Pricing strategy, Value factor|5||Proceedings of the 4th International Conference on Computer Science and Application Engineering|The article focuses on the analysis of data product value influencing factors and establishes a data product pricing model based on value factors. The research reviews the existing research on the value evaluation of data assets, and summarizes the characteristics of data products in the data product trading system based on the alliance chain [1], and then obtains factors for data product value evaluation. Combined with the dynamics and personalized requirements of data products, a value-based three-stage dynamic pricing model for data products is proposed.|10.1145/3424978.3425146|https://doi.org/10.1145/3424978.3425146|New York, NY, USA|Association for Computing Machinery|9781450377720|2020|Research on Factors Influencing the Value of Data Products and Pricing Models|Si, Yaqing and Qin, Siyao and Su, Jing and Wang, Mingyue|inproceedings|10.1145/3424978.3425146|161||||||||||||||||||||||||||||263828146|42
KDD '14|New York, New York, USA|glitch explanations, data quality, crossover subsampling, quantitative data cleaning|10|572–581|Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining|Data glitches are unusual observations that do not conform to data quality expectations, be they logical, semantic or statistical. By applying data integrity constraints, potentially large sections of data could be flagged as being noncompliant. Ignoring or repairing significant sections of the data could fundamentally bias the results and conclusions drawn from analyses. In the context of Big Data where large numbers and volumes of feeds from disparate sources are integrated, it is likely that significant portions of seemingly noncompliant data are actually legitimate usable data.In this paper, we introduce the notion of Empirical Glitch Explanations - concise, multi-dimensional descriptions of subsets of potentially dirty data - and propose a scalable method for empirically generating such explanatory characterizations. The explanations could serve two valuable functions: (1) Provide a way of identifying legitimate data and releasing it back into the pool of clean data. In doing so, we reduce cleaning-related statistical distortion of the data; (2) Used to refine existing data quality constraints and generate and formalize domain knowledge.We conduct experiments using real and simulated data to demonstrate the scalability of our method and the robustness of explanations. In addition, we use two real world examples to demonstrate the utility of the explanations where we reclaim over 99% of the suspicious data, keeping data repair related statistical distortion close to 0.|10.1145/2623330.2623716|https://doi.org/10.1145/2623330.2623716|New York, NY, USA|Association for Computing Machinery|9781450329569|2014|Empirical Glitch Explanations|Dasu, Tamraparni and Loh, Ji Meng and Srivastava, Divesh|inproceedings|10.1145/2623330.2623716|||||||||||||||||||||||||||||264170304|42
||Data warehouse, Distributed, Data complexity, Data quality, Quality assurance, Optimization, Machine learning||233-244||The significance of distributed data warehouses is to initiate the proliferation of various analytical applications. However, with the increase of ubiquitous devices, it is likely that massive volumes of data will be generated, which poses further problems based on the degradation of data quality. The practical reasons for the degradation of data quality in distributed warehouses are identified as heterogeneous data, uncertain inferior data which further affect predictions. The proposed system presents an integrated optimization model to address all the quality degradation problems and to provide a better computational model which effectively incorporates a higher degree of quality assurance. An analytical methodology is adopted in order to develop the proposed quality assurance model for distributed data warehouses.|https://doi.org/10.1016/j.compeleceng.2019.02.003|https://www.sciencedirect.com/science/article/pii/S0045790618318470||||2019|Robust analysis and optimization of a novel efficient quality assurance model in data warehousing|P. Amuthabala and R. Santhosh|article|AMUTHABALA2019233|||Computers & Electrical Engineering|00457906||74|||||18159|0,630|Q1|64|298|1040|7464|4626|979|4,79|25,05|United Kingdom|Western Europe|1973-1984, 1986-2020|Computer Science (miscellaneous) (Q1); Control and Systems Engineering (Q2); Electrical and Electronic Engineering (Q2)||||264545974|1285201041
ICCDE 2020|Sanya, China|combat test, Big data mining, combat effectiveness evaluation|5|131–135|Proceedings of 2020 the 6th International Conference on Computing and Data Engineering|In the evaluation of equipment combat effectiveness, it is necessary to comprehensively analyze the data of outfield test and infield test, including a variety of audio-visual, image and other combat test data. These data can be classified, extracted, stored and managed by building data model through big data mining technology. The evaluation method of equipment combat effectiveness based on big data mining is based on massive data, through machine learning, statistical analysis, neural network, database and other methods to analyze and process the data, mining the correlation between test data, evaluation index and evaluation conclusion, and extracting useful information and finding new knowledge from it to realize the evaluation of the combat effectiveness of the tested system.|10.1145/3379247.3379282|https://doi.org/10.1145/3379247.3379282|New York, NY, USA|Association for Computing Machinery|9781450376730|2020|Evaluation Method of Equipment Combat Effectiveness Based On Big Data Mining|Liyao, Zhou and Xiaofang, Liu and Chunyu, Hu|inproceedings|10.1145/3379247.3379282|||||||||||||||||||||||||||||267046577|42
Next Gen Tech Driven Personalized Med&Smart Healthcare||||xvii-xxvii|Artificial Intelligence and Big Data Analytics for Smart Healthcare||https://doi.org/10.1016/B978-0-12-822060-3.00018-8|https://www.sciencedirect.com/science/article/pii/B9780128220603000188||Academic Press|978-0-12-822060-3|2021|Preface: artificial intelligence and big data analytics for smart healthcare: a digital transformation of healthcare primer|Miltiadis D. Lytras and Anna Visvizi and Akila Sarirete and Kwok Tai Chui|incollection|LYTRAS2021xvii||||||||||Miltiadis D. Lytras and Akila Sarirete and Anna Visvizi and Kwok Tai Chui|||||||||||||||||||271752888|42
IMMS 2019|Chengdu, China|Big data technology, mining analysis, business and data fusion|5|49–53|Proceedings of the 2019 2nd International Conference on Information Management and Management Sciences|With the development of power enterprise informationization after more than ten years of development, Achieve full coverage of the company's business, effectively supporting the full-business operation of the power grid, and the accumulated business data has exploded. However, there are still problems such as low data quality, insufficient integration of multi-source business and data fusion, which makes it difficult to monitor and analyze the full-business of the power grid. This paper will combine the big data technology to study how to conduct monitoring and analysis of power grid full-business operation based on multi-source business and data fusion, and realize the three-layer architecture of business and data combination layer, business and data integration layer and business and data aggregation layer. Different levels of analysis and application, such as indicator monitoring analysis, subject monitoring analysis, and special monitoring analysis, effectively support enterprise management analysis and analytical decision.|10.1145/3357292.3357306|https://doi.org/10.1145/3357292.3357306|New York, NY, USA|Association for Computing Machinery|9781450371445|2019|Application Research of Power Grid Full-Business Monitoring and Analysis Based on Multi-Source Business and Data Fusion|Min, Han Xue and Feng, Zheng Gao and Xi, Liu Peng and Dong, Wang Xu and Ping, Xu Zhong|inproceedings|10.1145/3357292.3357306|||||||||||||||||||||||||||||272743636|42
||Commercial space, Retail, Retail geography, Symbolic capital, Big data, Foursquare, Madrid||102859||While commerce is one of the key activities in cities, its spatial description still requires further attention, especially by considering the different dimensions of commercial space: physical, economic and socio-symbolic. The latter is becoming more and more important in an era where consumption is at the centre of social relations. Further, although data availability has been an enduring obstacle in commercial research, we are witnessing the advent of new data sources, and social-network big data is an opportunity to unveil the places to which consumers attribute prestige or symbolic capital, at the extent of entire metropolitan areas. This paper compares the physical, economic and socio-symbolic dimensions of commercial spaces through the analysis of three different commercial data sources: cadastral micro-data, business register and social-network big data. For the case of Madrid Metropolitan Area, the three databases are compared with correlation analysis and density maps, coming out as partly redundant and partly complementary. Getis-Ord's hotspot statistics integrated into a cluster analysis enable a comprehensive understanding of commercial environments, enriching previous spatial hierarchies. The spatial distribution of symbolic capital unveils a relation with socio-spatial segregation and paves the way to new reflections on the spatiality of consumption as a social practice.|https://doi.org/10.1016/j.cities.2020.102859|https://www.sciencedirect.com/science/article/pii/S0264275120312075||||2020|Consumption and symbolic capital in the metropolitan space: Integrating ‘old’ retail data sources with social big data|José Carpio-Pinedo and Javier Gutiérrez|article|CARPIOPINEDO2020102859|||Cities|02642751||106|||||16956|1,771|Q1|90|419|732|28409|4866|723|6,19|67,80|United Kingdom|Western Europe|1983-2020|Development (Q1); Sociology and Political Science (Q1); Tourism, Leisure and Hospitality Management (Q1); Urban Studies (Q1)|11,076|5.835|0.01251|273811939|2090576639
ARES 2018|Hamburg, Germany|Cyber-security, Cyber-crime, Privacy, Data Security|5||Proceedings of the 13th International Conference on Availability, Reliability and Security|The way of live in the modern society has changed radically over the past few decades. In particular, thanks to the strong use of information technology, many activities have moved from the real world to the digital world. This has obviously introduced advantages in terms of data management and communication efficiency. Nevertheless, it has given also to the criminals the possibility to move into cybernetic space and, as a consequence, to exploit all the technological advantages available for carrying out their activities. In this context the paper provide an overview on the cybercrime and organized crime by focusing on the concept of crime as a service as well as the main issues related to big data by highlighting the social aspects.|10.1145/3230833.3233288|https://doi.org/10.1145/3230833.3233288|New York, NY, USA|Association for Computing Machinery|9781450364485|2018|Cybercrime and Organized Crime|"\"Jirovsk\\'{y}, V\\'{a}clav and Pastorek, Andrej and M\"\"{u}hlh\\\"\"{a}user, Max and Tundis, Andrea\""|inproceedings|10.1145/3230833.3233288|61||||||||||||||||||||||||||||275619724|42
||ALCOA, Blockchain, Data anaytics, Data quality, Intelligent agents, Smart contracts||100172||Production lines in pharmaceutical manufacturing generate numerous heterogeneous data sets from various embedded systems which control the multiple processes of medicine production. Such data sets should arguably ensure end-to-end traceability and data integrity in order to release a medicine batch, which is uniquely identified and tracked by its batch number/code. Consequently, auditable computerised systems are crucial on pharmaceutical production lines, since the industry is becoming increasingly regulated for product quality and patient health purposes. This paper describes the EU-funded SPuMoNI project, which aims to ensure the quality of large amounts of data produced by computerised production systems in representative pharmaceutical environments. Our initial results include significant progress in: (i) end-to-end verification taking advantage of blockchain properties and smart contracts to ensure data authenticity, transparency, and immutability; (ii) data quality assessment models to identify data behavioural patterns that can violate industry practices and/or international regulations; and (iii) intelligent agents to collect and manipulate data as well as perform smart decisions. By analysing multiple sensors in medicine production lines, manufacturing work centres, and quality control laboratories, our approach has been initially evaluated using representative industry-grade pharmaceutical manufacturing data sets generated at an IT environment with regulated processes inspected by regulatory and government agencies.|https://doi.org/10.1016/j.bdr.2020.100172|https://www.sciencedirect.com/science/article/pii/S221457962030040X||||2021|Smart Pharmaceutical Manufacturing: Ensuring End-to-End Traceability and Data Integrity in Medicine Production|Fátima Leal and Adriana E. Chis and Simon Caton and Horacio González–Vélez and Juan M. García–Gómez and Marta Durá and Angel Sánchez–García and Carlos Sáez and Anthony Karageorgos and Vassilis C. Gerogiannis and Apostolos Xenakis and Efthymios Lallas and Theodoros Ntounas and Eleni Vasileiou and Georgios Mountzouris and Barbara Otti and Penelope Pucci and Rossano Papini and David Cerrai and Mariola Mier|article|LEAL2021100172|||Big Data Research|22145796||24|||||21100356018|0,565|Q2|25|11|76|547|324|69|4,06|49,73|United States|Northern America|2014-2020|Computer Science Applications (Q2); Information Systems (Q2); Information Systems and Management (Q2); Management Information Systems (Q2)|586|3.578|0.00126|277735324|1627174784
||user interface design, visual analytics, Personal health records, reasoning about belief and knowledge, neural networks|20|||Clinical decision support systems are widely used to assist with medical decision making. However, clinical decision support systems typically require manually curated rules and other data that are difficult to maintain and keep up to date. Recent systems leverage advanced deep learning techniques and electronic health records to provide a more timely and precise result. Many of these techniques have been developed with a common focus on predicting upcoming medical events. However, although the prediction results from these approaches are promising, their value is limited by their lack of interpretability. To address this challenge, we introduce CarePre, an intelligent clinical decision assistance system. The system extends a state-of-the-art deep learning model to predict upcoming diagnosis events for a focal patient based on his or her historical medical records. The system includes an interactive framework together with intuitive visualizations designed to support diagnosis, treatment outcome analysis, and the interpretation of the analysis results. We demonstrate the effectiveness and usefulness of the CarePre&nbsp;system by reporting results from a quantities evaluation of the prediction algorithm, two case studies, and interviews with senior physicians and pulmonologists.|10.1145/3344258|https://doi.org/10.1145/3344258|New York, NY, USA|Association for Computing Machinery||2020|CarePre: An Intelligent Clinical Decision Assistance System|Jin, Zhuochen and Cui, Shuyuan and Guo, Shunan and Gotz, David and Sun, Jimeng and Cao, Nan|article|10.1145/3344258|6|mar|ACM Trans. Comput. Healthcare|26911957|1|1|January 2020||||||||||||||||||||||278478731|1983512862
SITA'20|Rabat, Morocco|Data Quality evaluation, Data Quality, Quality Models, Big Data|6||Proceedings of the 13th International Conference on Intelligent Systems: Theories and Applications|In recent years, as more and more data sources have become available and the volumes of data potentially accessible have increased, the assessment of data quality has taken a central role whether at the academic, professional or any other sector. Given that users are often concerned with the need to filter a large amount of data to better satisfy their requirements and needs, and that data analysis can be based on inaccurate, incomplete, ambiguous, duplicated and of poor quality, it makes everyone wonder what the results of these analyses will really be like. However, there is a very complex process involved in the identification of new, valid, potentially useful and meaningful data from a large data collection and various information systems, and is critically dependent on a number of measures to be developed to ensure data quality. To this end, the main objective of this paper is to introduce a general study on data quality related with big data, by providing what other researchers came up with on that subject. The paper will be finalized by a comparative study between the different existing data quality models.|10.1145/3419604.3419803|https://doi.org/10.1145/3419604.3419803|New York, NY, USA|Association for Computing Machinery|9781450377331|2020|Towards a Data Quality Assessment in Big Data|Reda, Oumaima and Sassi, Imad and Zellou, Ahmed and Anter, Samir|inproceedings|10.1145/3419604.3419803|16||||||||||||||||||||||||||||279584119|42
||Big data, Medical big data, Healthcare data, Medical big data analytics, Healthcare data analytics, Data analytics, Pharmacology data analytics||189-212|Internet of Things in Biomedical Engineering|The idea of big data is mainly reflected in its dimensions, which are popularly known as the Big Vs, which stands for Volume, Variety, Velocity, and Veracity. However, the concept goes beyond the Big Vs and testing of hypotheses, to focus on data analysis, hypothesis generation, and ascertaining the progressive strength of association. Preliminary study reveals that big data analytics adopts many data mining methods, such as descriptive, diagnostic, predictive, and prescriptive analytics. This evolving technology has tremendous application in healthcare, such as surveillance of safety or disease, predictive modeling, public health, pharma data analytics, clinical data analytics, healthcare analytics, and research. Moreover, the journey of big data in the medical domain is proving to be one of the important research thrusts of recent times. Study reveals that medical data is very specific and heterogeneous due to varied data sources such as scanned images, CT scan reports, doctor prescriptions, electronic health records (EHRs), etc. Medical data analytics faces some bottlenecks due to missing data, high dimensions, bias, and limitations of the study of patients through observation. Therefore, special big data techniques are required to handle them. Besides, many ethical, legal, social, clinical, and utility challenges are also a part of the data-handling process, which makes the role of big data in the medical field very challenging. Nevertheless, big data analytics is a fuel to the healthcare system that will provide a healthier life to patients; the issues and bottlenecks when removed from the system will be a boon for the entire human race. The chapter focuses on understanding the big data characteristics in medical big data, medical big data analytics, and its various applications in the interest of society.|https://doi.org/10.1016/B978-0-12-817356-5.00010-3|https://www.sciencedirect.com/science/article/pii/B9780128173565000103||Academic Press|978-0-12-817356-5|2019|Chapter 8 - Why Big Data and What Is It? Basic to Advanced Big Data Journey for the Medical Industry|Neha Sharma and Malini M. Patil and Madhavi Shamkuwar|incollection|SHARMA2019189||||||||||Valentina E. Balas and Le Hoang Son and Sudan Jha and Manju Khari and Raghvendra Kumar|||||||||||||||||||282061095|42
||Data integrity;Decision making;Process control;Quality control;Organizations;Big Data;Data models;data quality control;data quality assessment;unsupervised learning;anomaly detection;automated data quality control||2327-2336|2021 IEEE International Conference on Big Data (Big Data)|Data is one of the most valuable assets of an organization and has a tremendous impact on its long-term success and decision-making processes. Typically, organizational data error and outlier detection processes perform manually and reactively, making them time-consuming and prone to human errors. Additionally, rich data types, unlabeled data, and increased volume have made such data more complex. Accordingly, an automated anomaly detection approach is required to improve data management and quality control processes. This study introduces an unsupervised anomaly detection approach based on models comparison, consensus learning, and a combination of rules of thumb with iterative hyper-parameter tuning to increase data quality. Furthermore, a domain expert is considered a human in the loop to evaluate and check the data quality and to judge the output of the unsupervised model. An experiment has been conducted to assess the proposed approach in the context of a case study. The experiment results confirm that the proposed approach can improve the quality of organizational data and facilitate anomaly detection processes.|10.1109/BigData52589.2021.9671672|||||2021|Unsupervised Anomaly Detection in Data Quality Control|Poon, Lex and Farshidi, Siamak and Li, Na and Zhao, Zhiming|inproceedings|9671672||Dec|||||||||||||||||||||||||||283026921|42
||||90-93||Big data and deep learning will profoundly change various areas of professions and research in the future. This will also happen in medicine and medical imaging in particular. As medical physicists, we should pursue beyond the concept of technical quality to extend our methodology and competence towards measuring and optimising the diagnostic value in terms of how it is connected to care outcome. Functional implementation of such methodology requires data processing utilities starting from data collection and management and culminating in the data analysis methods. Data quality control and validation are prerequisites for the deep learning application in order to provide reliable further analysis, classification, interpretation, probabilistic and predictive modelling from the vast heterogeneous big data. Challenges in practical data analytics relate to both horizontal and longitudinal analysis aspects. Quantitative aspects of data validation, quality control, physically meaningful measures, parameter connections and system modelling for the future artificial intelligence (AI) methods are positioned firmly in the field of Medical Physics profession. It is our interest to ensure that our professional education, continuous training and competence will follow this significant global development.|https://doi.org/10.1016/j.ejmp.2018.11.005|https://www.sciencedirect.com/science/article/pii/S1120179718313152||||2018|The European Federation of Organisations for Medical Physics (EFOMP) White Paper: Big data and deep learning in medical imaging and in relation to medical physics profession|Mika Kortesniemi and Virginia Tsapaki and Annalisa Trianni and Paolo Russo and Ad Maas and Hans-Erik Källman and Marco Brambilla and John Damilakis|article|KORTESNIEMI201890|||Physica Medica|11201797||56|||||17037|0,883|Q1|44|307|792|11838|2126|779|2,56|38,56|Italy|Western Europe|1989-2020|Physics and Astronomy (miscellaneous) (Q1); Radiology, Nuclear Medicine and Imaging (Q1); Biophysics (Q2); Medicine (miscellaneous) (Q2)||||283074365|1073114539
||Smart farming, Internet of things, Wireless sensor network, Farm management information system, Big data, Machine learning||60-84||The Internet of Things is allowing agriculture, here specifically arable farming, to become data-driven, leading to more timely and cost-effective production and management of farms, and at the same time reducing their environmental impact. This review is addressing an analytical survey of the current and potential application of Internet of Things in arable farming, where spatial data, highly varying environments, task diversity and mobile devices pose unique challenges to be overcome compared to other agricultural systems. The review contributes an overview of the state of the art of technologies deployed. It provides an outline of the current and potential applications, and discusses the challenges and possible solutions and implementations. Lastly, it presents some future directions for the Internet of Things in arable farming. Current issues such as smart phones, intelligent management of Wireless Sensor Networks, middleware platforms, integrated Farm Management Information Systems across the supply chain, or autonomous vehicles and robotics stand out because of their potential to lead arable farming to smart arable farming. During the implementation, different challenges are encountered, and here interoperability is a key major hurdle throughout all the layers in the architecture of an Internet of Things system, which can be addressed by shared standards and protocols. Challenges such as affordability, device power consumption, network latency, Big Data analysis, data privacy and security, among others, have been identified by the articles reviewed and are discussed in detail. Different solutions to all identified challenges are presented addressing technologies such as machine learning, middleware platforms, or intelligent data management.|https://doi.org/10.1016/j.biosystemseng.2019.12.013|https://www.sciencedirect.com/science/article/pii/S1537511020300039||||2020|Internet of Things in arable farming: Implementation, applications, challenges and potential|Andrés Villa-Henriksen and Gareth T.C. Edwards and Liisa A. Pesonen and Ole Green and Claus Aage Grøn Sørensen|article|VILLAHENRIKSEN202060|||Biosystems Engineering|15375110||191|||||||||||||||||||||||287177857|806142298
SC '20|Atlanta, Georgia|performance evaluation, data compression, multi-layer neural network|15||Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis|As the computation power of supercomputers increases, so does simulation size, which in turn produces orders-of-magnitude more data. Because generated data often exceed the simulation's disk quota, many simulations would stand to benefit from data-reduction techniques to reduce storage requirements. Such techniques include autoencoders, data compression algorithms, and sampling. Lossy compression techniques can significantly reduce data size, but such techniques come at the expense of losing information that could result in incorrect post hoc analysis results. To help scientists determine the best compression they can get while keeping their analyses accurate, we have developed Foresight, an analysis framework that enables users to evaluate how different data-reduction techniques will impact their analyses. We use particle data from a cosmology simulation, turbulence data from Direct Numerical Simulation, and asteroid impact data from xRage to demonstrate how Foresight can help scientists determine the best data-reduction technique for their simulations.||||IEEE Press|9781728199986|2020|Foresight: Analysis That Matters for Data Reduction|Grosset, Pascal and Biwer, Christopher M. and Pulido, Jesus and Mohan, Arvind T. and Biswas, Ayan and Patchett, John and Turton, Terece L. and Rogers, David H. and Livescu, Daniel and Ahrens, James|inproceedings|10.5555/3433701.3433811|83||||||||||||||||||||||||||||289353713|42
||Electrical engineering;Data integrity;Big Data;Cleaning;Data mining;Artificial intelligence;Research and development;instance-level data cleaning;duplicate records cleaning;attribute cleaning||238-242|2021 International Conference on Artificial Intelligence, Big Data and Algorithms (CAIBDA)|Effectivedata analysis and data mining are based on data availability and data quality. Data cleaning is a commonly used technique to improve data quality. Instance-level data cleaning is an important part of data cleaning. The focus is on the comparison and analysis of the detection and cleaning methods of attributes and recorded values in the instance-level data cleaning technology, and the experimental analysis of the repeated record cleaning methods. This paper introduces the application field of data cleaning technology represented by the electrical engineering field combined with the application situation, and provides valuable selection suggestions for the characteristics of different data sets and the applicable instancelevel data cleaning technology. Summarizing and analyzing the existing detection and cleaning technology methods, it is concluded that instance-level data cleaning has a lot of research and development space in long text, unstructured data and specific fields. Finally, the challenges and development directions of the instance-level data cleaning technology are prospected.|10.1109/CAIBDA53561.2021.00057|||||2021|Research on instance-level data cleaning technology|Ying, KangHui and Hu, WenYu and Chen, Jin Bo and Li, Guo Nong|inproceedings|9545944||May|||||||||||||||||||||||||||291846530|42
FOSE 2014|Hyderabad, India|Data Quality, Statistics, Operational Data, Data Engineering, Data Science, Analytics, Game Theory|15|85–99|Future of Software Engineering Proceedings|Structured and unstructured data in operational support tools have long been prevalent in software engineering. Similar data is now becoming widely available in other domains. Software systems that utilize such operational data (OD) to help with software design and maintenance activities are increasingly being built despite the difficulties of drawing valid conclusions from disparate and low-quality data and the continuing evolution of operational support tools. This paper proposes systematizing approaches to the engineering of OD-based systems. To prioritize and structure research areas we consider historic developments, such as big data hype; synthesize defining features of OD, such as confounded measures and unobserved context; and discuss emerging new applications, such as diverse and large OD collections and extremely short development intervals. To sustain the credibility of OD-based systems more research will be needed to investigate effective existing approaches and to synthesize novel, OD-specific engineering principles.|10.1145/2593882.2593889|https://doi.org/10.1145/2593882.2593889|New York, NY, USA|Association for Computing Machinery|9781450328654|2014|Engineering Big Data Solutions|Mockus, Audris|inproceedings|10.1145/2593882.2593889|||||||||||||||||||||||||||||292389926|42
||non-monetary incentive, incentive mechanism, blockchain, monetary incentive||||In a blockchain-based system, the lack of centralized control requires active participation and cooperative behaviors of system entities to ensure system security and sustainability. However, dynamic environments and unpredictable entity behaviors challenge the performances of such systems in practice. Therefore, designing a feasible incentive mechanism to regulate entity behaviors becomes essential to improve blockchain system performance. The prosperous characteristics of blockchain can also contribute to an effective incentive mechanism. Unfortunately, current literature still lacks a thorough survey on incentive mechanisms related to the blockchain to understand how incentive mechanisms and blockchain make each other better. To this end, we propose evaluation requirements in terms of the properties and costs of incentive mechanisms. On one hand, we provide a taxonomy of the incentive mechanisms of blockchain systems according to blockchain versions, incentive forms and incentive goals. On the other hand, we categorize blockchain-based incentive mechanisms according to application scenarios and incentive goals. During the review, we discuss the advantages and disadvantages of state-of-art incentive mechanisms based on the proposed evaluation requirements. Through careful review, we present how incentive mechanisms and blockchain benefit with each other, discover a number of unresolved issues, and point out corresponding potential directions for future research.|10.1145/3539604|https://doi.org/10.1145/3539604|New York, NY, USA|Association for Computing Machinery||2022|How Can Incentive Mechanisms and Blockchain Benefit with Each Other? A Survey|Han, Rong and Yan, Zheng and Liang, Xueqin and Yang, Laurence T.|article|10.1145/3539604||jun|ACM Comput. Surv.|03600300||||Just Accepted|||23038|2,079|Q1|163|112|365|15859|6978|365|19,16|141,60|United States|Northern America|1969-2020|Computer Science (miscellaneous) (Q1); Theoretical Computer Science (Q1)|10,790|10.282|0.01438|293913656|1517405264
||Data quality, Information quality, Multi-criteria decision making, Analytic hierarchy process, Decision support systems, Maintenance||145-164||Today’s largest and fastest growing companies’ assets are no longer physical, but rather digital (software, algorithms...). This is all the more true in the manufacturing, and particularly in the maintenance sector where quality of enterprise maintenance services are closely linked to the quality of maintenance data reporting procedures. If quality of the reported data is too low, it can results in wrong decision-making and loss of money. Furthermore, various maintenance experts are involved and directly concerned about the quality of enterprises’ daily maintenance data reporting (e.g., maintenance planners, plant managers...), each one having specific needs and responsibilities. To address this Multi-Criteria Decision Making (MCDM) problem, and since data quality is hardly considered in existing expert maintenance systems, this paper develops a maintenance reporting quality assessment (MRQA) dashboard that enables any company stakeholder to easily – and in real-time – assess/rank company branch offices in terms of maintenance reporting quality. From a theoretical standpoint, AHP is used to integrate various data quality dimensions as well as expert preferences. A use case describes how the proposed MRQA dashboard is being used by a Finnish multinational equipment manufacturer to assess and enhance reporting practices in a specific or a group of branch offices.|https://doi.org/10.1016/j.eswa.2016.06.043|https://www.sciencedirect.com/science/article/pii/S095741741630330X||||2016|Data quality assessment of maintenance reporting procedures|Manik Madhikermi and Sylvain Kubler and Jérémy Robert and Andrea Buda and Kary Främling|article|MADHIKERMI2016145|||Expert Systems with Applications|09574174||63|||||24201|1,368|Q1|207|770|1945|42314|17345|1943|8,67|54,95|United Kingdom|Western Europe|1990-2021|Artificial Intelligence (Q1); Computer Science Applications (Q1); Engineering (miscellaneous) (Q1)|55,444|6.954|0.04053|294849622|1377770283
|||28|379–406|Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice|||https://doi.org/10.1145/3447404.3447426|New York, NY, USA|Association for Computing Machinery|9781450390293|2021|Building Adaptive Touch Interfaces—Case Study 6|Buschek, Daniel and Alt, Florian|inbook|10.1145/3447404.3447426|||||||||1||||||||||||||||||||300250107|42
||Machine learning;Privacy;Computed tomography;Medical diagnostic imaging;COVID-19;deep learning;EfficientNet;X-Ray;radiology imaging;PATE;differential privacy||1-6|2020 International Conference on INnovations in Intelligent SysTems and Applications (INISTA)|Medical sciences are an important application area of artificial intelligence. Healthcare requires meticulousness in the whole process from collecting data to processing. It should also be handled in terms of data quality, data size, and data privacy. Various data are used within the scope of the COVID-19 outbreak struggle. Medical and location data collected from mobile phones and wearable devices are used to prevent the spread of the epidemic. In addition to this, artificial intelligence approaches are presented by using medical images in order to identify COVID-19 infected people. However, studies should be carried out by taking care not to endanger the security of the data, people, and countries needed for these useful applications. Therefore, differential privacy (DP) application, which was an interesting research subject, has been included in this study. CXR images have been collected from COVID-19 infected 139 and a total of 373 public data sources were used for a diagnostic concept. It has been trained with EfficientNet- B0, a recent and robust deep learning model, and proposal the possibility of infected with an accuracy of 94.7%. Other evaluation parameters were also discussed in detail. Despite the data constraint, this performance showed that it can be improved by augmenting the dataset. The most important aspect of the study was the proposal of differential privacy practice for such applications to be reliable in real-life use cases. With this view, experiments were repeated with DP applied images and the results obtained were presented. Here, Private Aggregation of Teacher Ensembles (PATE) approach was used to ensure privacy assurance.|10.1109/INISTA49547.2020.9194651|||||2020|Differential Privacy Practice on Diagnosis of COVID-19 Radiology Imaging Using EfficientNet|Müftüoğlu, Zümrüt and Kizrak, M. Ayyüce and Yildlnm, Tülay|inproceedings|9194651||Aug|||||||||||||||||||||||||||300702813|42
ARIC '20|Seattle, Washington|digital city, Deck.GL, infrastructure 3D tiles, Mapbox GL JS|9|1–9|Proceedings of the 3rd ACM SIGSPATIAL International Workshop on Advances in Resilient and Intelligent Cities|In this study, we develop a platform that can display approximately 20 types of data via a web browser to realize a digital twin of a wider area, including a detailed reading display of block units and individual three-dimensional point cloud data (point cloud) of a city. Using actual data, we examine if the data model and visualization design correspond with the zoom level. Owing to the comparative examination of the wide-area display performance and the map representation design in a JavaScript-based open-source library, we were able to develop a platform with light architecture and an easily customizable display. Furthermore, prototyping, based on Mapbox GL JS and Deck.GL, and the display of spatiotemporal flow layers, such as background maps, point cloud data in many places, dozens of layer display types, and the General Transit Feed Specification (GTFS) allowed for the seamless transition from the local government to the wide-area display in the prefecture unit in approximately 10-20 s.It is recommended that this digital smart city platform should be standardized by other local governments, especially in areas where higher-order data visualization is yet to advance. To display this digital city in a lightweight environment, we consider the digital data situation of local governments in Japan. It is necessary to define the visualized design for each zoom level according to the characteristics of the data. We then arranged the display model of each zoom level for 20 types of urban infrastructure data related to the digital smart city by referring to the style schema of the tile form. Through these tasks, we organized the commonality and optimization of data models and formats.|10.1145/3423455.3430316|https://doi.org/10.1145/3423455.3430316|New York, NY, USA|Association for Computing Machinery|9781450381659|2020|Constructing a Digital City on a Web-3D Platform: Simultaneous and Consistent Generation of Metadata and Tile Data from a Multi-Source Raw Dataset|Seto, Toshikazu and Sekimoto, Yoshihide and Asahi, Kosuke and Endo, Takahiro|inproceedings|10.1145/3423455.3430316|||||||||||||||||||||||||||||304738919|42
CIKM '20|Virtual Event, Ireland|artificial intelligence, human life, privacy, sensors, deep learning, knowledge, big data, security|2|3539–3540|Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management|The present scenario of Covid-19 pandemic has disrupted the human life to a larger extent. In such context, human-centric applications and systems that endeavor to positively impact the human quality of life is of utmost importance. Knowledge-driven analytics that help to build such intelligent systems play important role to construct the required eco-system on the macro-scale. It is worth mentioning that Knowledge-Driven Analytics and Systems Impacting Human Quality of Life (KDAH) workshop in ACM International Conference on Information and Knowledge Management (CIKM), attempts to bring out the intricate research direction for enabling a sustainable human society through the positive co-existence of human beings and intelligent systems.|10.1145/3340531.3414077|https://doi.org/10.1145/3340531.3414077|New York, NY, USA|Association for Computing Machinery|9781450368599|2020|On the Knowledge-Driven Analytics and Systems Impacting Human Quality of Life|Ukil, Arijit and Marin, Leandro and Jara, Antonio and Farserotu, John|inproceedings|10.1145/3340531.3414077|||||||||||||||||||||||||||||305998508|42
||Data quality improvement, record completion, patenting, gender name mapping|18|||Mappings of first name to gender have been widely recognized as a critical tool for the completion, study, and validation of data records in a range of areas. In this study, we investigate how organizations with large databases of existing entities can create their own mappings between first names and gender and how these mappings can be improved and utilized. Therefore, we first explore a dataset with demographic information on more than 4 million people, which was provided by a car insurance company. Then, we study how naming conventions have changed over time and how they differ by nationality. Next, we build a probabilistic first-name-to-gender mapping and augment the mapping by adding nationality and decade of birth to improve the mapping's performance. We test our mapping in two-label and three-label settings and further validate our mapping by categorizing patent filings by gender of the inventor. We compare the results with previous studies’ outcomes and find that our mapping produces high-precision results. We validate that the additional information of nationality and year of birth improve the precision scores of name-to-gender mappings. Therefore, the proposed approach constitutes an efficient process for improving the data quality of organizations’ records, if the gender attribute is missing or unreliable.|10.1145/3297720|https://doi.org/10.1145/3297720|New York, NY, USA|Association for Computing Machinery||2019|Augmenting Data Quality through High-Precision Gender Categorization|"\"M\"\"{u}ller, Daniel and Jain, Pratiksha and Te, Yieh-Funk\""|article|10.1145/3297720|8|may|J. Data and Information Quality|19361955|2|11|June 2019||||||||||||||||||||||309647723|833754770
||dataset collection, deep learning, Code clone detection|25|||In recent years, applying deep learning to detect semantic code clones has received substantial attention from the research community. Accordingly, various evaluation benchmark datasets, with the most popular one as BigCloneBench, are constructed and selected as benchmarks to assess and compare different deep learning models for detecting semantic clones. However, there is no study to investigate whether an evaluation benchmark dataset such as BigCloneBench is properly used to evaluate models for detecting semantic code clones. In this article, we present an experimental study to show that BigCloneBench typically includes semantic clone pairs that use the same identifier names, which however are not used in non-semantic-clone pairs. Subsequently, we propose an undesirable-by-design Linear-Model that considers only which identifiers appear in a code fragment; this model can achieve high effectiveness for detecting semantic clones when evaluated on BigCloneBench, even comparable to state-of-the-art deep learning models recently proposed for detecting semantic clones. To alleviate these issues, we abstract a subset of the identifier names (including type, variable, and method names) in BigCloneBench to result in AbsBigCloneBench and use AbsBigCloneBench to better assess the effectiveness of deep learning models on the task of detecting semantic clones.|10.1145/3502852|https://doi.org/10.1145/3502852|New York, NY, USA|Association for Computing Machinery||2022|Assessing and Improving an Evaluation Dataset for Detecting Semantic Code Clones via Deep Learning|Yu, Hao and Hu, Xing and Li, Ge and Li, Ying and Wang, Qianxiang and Xie, Tao|article|10.1145/3502852|62|jul|ACM Trans. Softw. Eng. Methodol.|1049331X|4|31|October 2022||||||||||||||||||||||310175717|1429302734
||Tourism research, Big data, Literature review, Tourism management, Tourist behavior||301-323||Even at an early stage, diverse big data have been applied to tourism research and made an amazing improvement. This paper might be the first attempt to present a comprehensive literature review on different types of big data in tourism research. By data sources, the tourism-related big data fall into three primary categories: UGC data (generated by users), including online textual data and online photo data; device data (by devices), including GPS data, mobile roaming data, Bluetooth data, etc.; transaction data (by operations), including web search data, webpage visiting data, online booking data, etc. Carrying different information, different data types address different tourism issues. For each type, a systematical analysis is conducted from the perspectives of research focuses, data characteristics, analytic techniques, major challenges and further directions. This survey facilitates a thorough understanding of this sunrise research and offers valuable insights into its future prospects.|https://doi.org/10.1016/j.tourman.2018.03.009|https://www.sciencedirect.com/science/article/pii/S0261517718300591||||2018|Big data in tourism research: A literature review|Jingjing Li and Lizhi Xu and Ling Tang and Shouyang Wang and Ling Li|article|LI2018301|||Tourism Management|02615177||68|||||16547|3,328|Q1|199|159|690|11910|8354|687|11,06|74,91|United Kingdom|Western Europe|1982-2021|Development (Q1); Strategy and Management (Q1); Tourism, Leisure and Hospitality Management (Q1); Transportation (Q1)|37,117|10.967|0.02226|310292500|514576676
CHI '20|Honolulu, HI, USA|privacy, data use, cambridge analytica, data monetization, linkedin, facebook, social media attitudes, ownership|12|1–12|Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems|Has widespread news of abuse changed the public's perceptions of how user-contributed content from social networking sites like Facebook and LinkedIn can be used? We collected two datasets that reflect participants' attitudes about content ownership, privacy, and control, one in April 2018, while Cambridge Analytica was still in the news, and another in February 2019, after the event had faded from the headlines, and aggregated the data according to participants' awareness of the story, contrasting the attitudes of those who reported the greatest awareness with those who reported the least. Participants with the greatest awareness of the news story's details have more polarized attitudes about reuse, especially the reuse of content as data. They express a heightened desire for data mobility, greater concern about networked privacy rights, increased skepticism of algorithmically targeted advertising and news, and more willingness for social media platforms to demand corrections of inaccurate or deceptive content.|10.1145/3313831.3376662|https://doi.org/10.1145/3313831.3376662|New York, NY, USA|Association for Computing Machinery|9781450367080|2020|Ownership, Privacy, and Control in the Wake of Cambridge Analytica: The Relationship between Attitudes and Awareness|Shipman, Frank M. and Marshall, Catherine C.|inproceedings|10.1145/3313831.3376662|||||||||||||||||||||||||||||312641069|42
||Safety science, Information science, Safety information, Safety informatics, Safety 4.0||119852||Safety is a central dimension in contemporary debates on human health, loss prevention, environmental protection, sustainability, and cleaner production. In the information age, especially in the era of big data, safety information is an essential strategy for safety, and safety informatics has become a major research interest and a popular issue in the field of safety science. In recent years, safety informatics—a new area of safety science—has received increasing attention, developing greatly with successful research on the subject. The three key purposes of this paper are: (i) to analyze the historical development of safety informatics, (ii) to review the research progress of safety informatics, and (iii) to review limitations and propose future directions in the field of safety informatics. First, the development process of safety informatics is divided into four typical stages: (i) the embryonic stage (1940–1980), (ii) the initial stage (1980–1990), (iii) the formation stage (1990–2010), and (iv) the deepening stage (2010–present). Then, a review of safety informatics research is provided from seven aspects, including: (i) the discipline construction of safety informatics, (ii) theoretical safety information model, (iii) accident causation model from a safety information perspective, (iv) safety management based on safety information, (v) safety big data, (vi) safety intelligence, and (vii) safety information technology. Finally, limitations and future research directions in the safety informatics area are briefly discussed.|https://doi.org/10.1016/j.jclepro.2019.119852|https://www.sciencedirect.com/science/article/pii/S0959652619347225||||2020|Safety informatics as a new, promising and sustainable area of safety science in the information age|Bing Wang and Chao Wu|article|WANG2020119852|||Journal of Cleaner Production|09596526||252|||||19167|1,937|Q1|200|5126|10603|304498|103295|10577|9,56|59,40|United Kingdom|Western Europe|1993-2021|Environmental Science (miscellaneous) (Q1); Industrial and Manufacturing Engineering (Q1); Renewable Energy, Sustainability and the Environment (Q1); Strategy and Management (Q1)|170,352|9.297|0.18299|313277149|1121054297
||Soft sensors;Redundancy;Relational databases;Big Data;Data science;Database systems;Task analysis||3147-3154|2021 IEEE International Conference on Big Data (Big Data)|In a typical Data Science project, the analyst uses many programming languages to explore and analyze big data coming from diverse data sources. A major challenge is managing and pre-processing so much data, with potentially inconsistent content, significant redundancy, in diverse formats, with varying data quality. Database systems research has tackled such problems for a long time, but mostly on relational databases. With such motivation in mind, this paper compares strengths and weaknesses of popular languages used nowadays from a database pespective: Python, R and SQL. We discuss the entire analytic pipeline, going from data integration, cleaning and pre-processing to model application and tuning. From a database systems perspective, we present a comprehensive survey of storage mechanisms, data processing algorithms, external algorithms, run-time memory management, consistency, optimizations and parallel processing. From a programming languages angle, we consider elegance, expressiveness, abstraction, composability, interactive behavior and automatic code optimization. We present a short experimental evaluation comparing the performance of the three languages on typical data exploration and pre-processing tasks. Our conclusion: there is no winner.|10.1109/BigData52589.2021.9672007|||||2021|Programming Languages in Data Science: a Comparison from a Database Angle|Zhou, Xiantian and Ordonez, Carlos|inproceedings|9672007||Dec|||||||||||||||||||||||||||313471314|42
||Crowdsourcing;Big Data;Reliability theory;Sensors;Computational modeling;Measurement;Big data;crowdsourcing;task assignment||1-6|2017 IEEE International Conference on Communications (ICC)|With the ubiquitous deployment of the mobile devices with increasingly better communication and computation capabilities, an emerging model called spatial crowdsourcing is proposed to solve the problem of unstructured big data by publishing location-based tasks to participating workers. However, massive spatial data generated by spatial crowdsourcing entails a critical challenge that the system has to guarantee quality control of crowdsourcing. This paper first studies a practical problem of task assignment, namely reliability aware spatial crowdsourcing (RA-SC), which takes the constrained tasks and numerous dynamic workers into consideration. Specifically, the worker confidence is introduced to reflect the completion reliability of the assigned task. Our RA-SC problem is to perform task assignments such that the reliability under budget constraints is maximized. Then, we reveal the typical property of the proposed problem, and design an effective strategy to achieve a high reliability of the task assignment. Besides the theoretical analysis, extensive experimental results also demonstrate that the proposed strategy is stable and effective for spatial crowdsourcing.|10.1109/ICC.2017.7996546|||||2017|A reliable task assignment strategy for spatial crowdsourcing in big data environment|Gu, Liqiu and Wang, Kun and Liu, Xiulong and Guo, Song and Liu, Bo|inproceedings|7996546||May||19381883|||||||||||||||||||||||||315931330|276554849
|Macau|||||The SIGGRAPH Asia Courses program will feature a variety of instructional sessions catered to the different levels of expertise of our attendees. Sessions from introductory to advanced topics in computer graphics and interactive techniques will be conducted by speakers from renowned organizations and academic research institutions from over the world.The program has been the premier source for practitioners, developers, researchers, artists, and students who want to learn about the state-of-the-art technologies in computer graphics and their related topics.|||New York, NY, USA|Association for Computing Machinery|9781450345385|2016|SA '16: SIGGRAPH ASIA 2016 Courses||proceedings|10.1145/2988458|||||||||||||||||||||||||||||320049131|42
||Recognition of automated standards, mining, decision trees||893-898||External cause injuries are defined as intentionally or unintentionally harm or injury to a person, which may be caused by trauma, poisoning, assault, accidents, etc., being fatal (fatal injury) or not leading to death (non-fatal injury). External injuries have been considered a global health problem for two decades. This work aims to determine criminal patterns using data mining techniques to a sample of patients from Mumbai city in India.|https://doi.org/10.1016/j.procs.2020.03.114|https://www.sciencedirect.com/science/article/pii/S1877050920305524||||2020|Identification of Patterns of Fatal Injuries in Humans through Big Data|Jesus Silva and Jack Zilberman and Ligia Romero and Omar Bonerge Pineda and Yaneth Herazo-Beltran|article|SILVA2020893|||Procedia Computer Science|18770509||170||The 11th International Conference on Ambient Systems, Networks and Technologies (ANT) / The 3rd International Conference on Emerging Data and Industry 4.0 (EDI40) / Affiliated Workshops|||19700182801|0,334|-|76|1907|6483|38511|13722|6359|2,09|20,19|Netherlands|Western Europe|2010-2020|Computer Science (miscellaneous)||||320790698|2108686752
||Databases;Data integrity;Big Data;Data warehouses;Data mining;Information technology;Standards;classification;marketing;association rules;Big Data;k-mean;prediction;preprocess||149-155|2021 International Conference on Information Technology (ICIT)|Big Data filed is an unsettled standard comparing with a traditional database, data mining, or data warehouse. Stability measure aims to acquire the quality dataset which encourages to use of preprocessing data method to handle instability that miniaturization missing data. Therefore, to increase the data quality in order to achieve an accurate prediction, significant rules are used to provide value and meaningful data. Through, three measures by support, confidence, and the lift to acquire frequently rules. These rules are used to conduct the objective extracting pattern, to estimate each browsing customer's likelihood of making a purchase, and to choose meaningful patterns from the discovered association rules.|10.1109/ICIT52682.2021.9491733|||||2021|Big Data, Classification, Clustering and Generate Rules: An inevitably intertwined for Prediction|Alzyadat, Wael and AlHroob, Aysh and Almukahel, Ikhlas Hassan and Muhairat, Mohammad and Abdallah, Mohammad and Althunibat, Ahmad|inproceedings|9491733||July|||||||||||||||||||||||||||321276359|42
||Fuzzy sets;Geospatial analysis;Big Data;Data models;NoSQL databases;Spatial databases;Fuzzy set theory;Spatial databases;data storage representations;schema and subschema;fuzzy set;imprecision;consistency;big data;NoSQL systems;JSON||468-482||In this era of big data, as relational databases are inefficient, NoSQL databases are a workable solution for data storage. In this context, one of the key issues is the veracity and therefore the data quality. Indeed, as with classic data, geospatial big data are generally fuzzy even though they are stored as crisp data (perfect data). Hence, if data are geospatial and fuzzy, additional complexities appear because of the complex syntax and semantic features of such data. The NoSQL databases do not offer strict data consistency. Therefore, new challenges are needed to be overcome to develop efficient methods that simultaneously ensure the performance and the consistency in storing fuzzy geospatial big data. This paper presents a new methodology that tackles the storage issues and validates the fuzzy spatial entities' consistency in a document-based NoSQL system. Consequently, first, to better express the structure of fuzzy geospatial data in such a system, we present a logical model called Fuzzy GeoJSON schema. Second, for consistent storage, we implement a schema-driven pipeline based on the Fuzzy GeoJSON schema and semantic constraints.|10.1109/TBDATA.2017.2725904|||||2021|A New Methodology for Storing Consistent Fuzzy Geospatial Data in Big Data Environment|Khalfi, Besma and de Runz, Cyril and Faiz, Sami and Akdag, Herman|article|7974765||June|IEEE Transactions on Big Data|23327790|2|7|||||21101019393|0,959|Q1|6|71|9|998|37|8|4,11|14,06|United States|Northern America|2020|Information Systems (Q1); Information Systems and Management (Q1)|636|3.344|0.00134|322472072|1510992500
||Cloud computing;Databases;Semantics;Decision making;Production;Ontologies;Big Data;Ontology;Agriculture;Digital Database;Cloud Computing;Big Data;Decision Making||386-392|2021 IEEE 15th International Conference on Semantic Computing (ICSC)|This paper presents an ontology for the structuring of digital databases with the objective of acting in a cloud environment and meeting big data sources in the agricultural context of grain production. Its conception is structured in three stages: the first stage presents an ontological architecture aimed at public and private cloud environments, the second stage deals with a semantic model at process level, and a pseudocode for ontological application is elaborated in the third stage, considering the technologies applied to the cloud. This work combines advanced features to support decision making from Data Lake storage solutions, semantic treatment of big data, as well as the presentation of strategies based on machine learning and data quality analysis to obtain data and metadata organized for application in a decision model. The configuration of the ontology presented meets the diversity of big data projects in the grain production context, the characteristics of which are based on interoperability in the use of heterogeneous data and its integration, elasticity of computational resources, and high availability of cloud access.|10.1109/ICSC50631.2021.00071|||||2021|Ontology for Structuring a Digital Databases for Decision Making in Grain Production|Neves, Ricardo A. and Cruvinel, Paulo E.|inproceedings|9364425||Jan||23256516|||||||||||||||||||||||||328432373|112473334
IPSN '14|Berlin, Germany|crowdsourcing, indoor localization, navigation|2|331–332|Proceedings of the 13th International Symposium on Information Processing in Sensor Networks|In this demonstration paper, we present the Anyplace system that relies on the abundance of sensory data on smartphones (e.g., WiFi signal strength and inertial measurements) to deliver reliable indoor geolocation information. Our system features two highly desirable properties, namely crowdsourcing and scalability. Anyplace implements a set of crowdsourcing-supportive mechanisms to handle the enormous amount of crowdsensed data, filter incorrect user contributions and exploit WiFi data from heterogeneous mobile devices. Moreover, Anyplace follows a big-data architecture for efficient and scalable storage and retrieval of localization and mapping data.||||IEEE Press|9781479931460|2014|Demonstration Abstract: Crowdsourced Indoor Localization and Navigation with Anyplace|Petrou, Lambros and Larkou, George and Laoudias, Christos and Zeinalipour-Yazti, Demetrios and Panayiotou, Christos G.|inproceedings|10.5555/2602339.2602400|||||||||||||||||||||||||||||332805565|42
||Big Data;Data integrity;Quality management;Data visualization;Databases;Sensors;Social network services;Big Data, Data Quality, Quality Management framework, Quality of Big Data||166-173|2018 IEEE International Congress on Big Data (BigData Congress)|With the advances in communication technologies and the high amount of data generated, collected, and stored, it becomes crucial to manage the quality of this data deluge in an efficient and cost-effective way. The storage, processing, privacy and analytics are the main keys challenging aspects of Big Data that require quality evaluation and monitoring. Quality has been recognized by the Big Data community as an essential facet of its maturity. Yet, it is a crucial practice that should be implemented at the earlier stages of its lifecycle and progressively applied across the other key processes. The earlier we incorporate quality the full benefit we can get from insights. In this paper, we first identify the key challenges that necessitates quality evaluation. We then survey, classify and discuss the most recent work on Big Data management. Consequently, we propose an across-the-board quality management framework describing the key quality evaluation practices to be conducted through the different Big Data stages. The framework can be used to leverage the quality management and to provide a roadmap for Data scientists to better understand quality practices and highlight the importance of managing the quality. We finally, conclude the paper and point to some future research directions on quality of Big Data.|10.1109/BigDataCongress.2018.00029|||||2018|Big Data Quality: A Survey|Taleb, Ikbal and Serhani, Mohamed Adel and Dssouli, Rachida|inproceedings|8457745||July|||||||||||||||||||||||||||333054913|42
DEBS '19|Darmstadt, Germany|Event-processing, broker network, stream-processing, publish/subscribe, big data, infrastructure, quality of information, financial data|2|254–255|Proceedings of the 13th ACM International Conference on Distributed and Event-Based Systems|Financial markets are event- and data-driven to an extremely high degree. For making decisions and triggering actions stakeholders require notifications about significant events and reliable background information that meet their individual requirements in terms of timeliness, accuracy, and completeness. As one of Europe's leading providers of financial data and regulatory solutions vwd: processes an average of 18 billion event notifications from 500+ data sources for 30 million symbols per day. Our large-scale distributed event-based systems handle daily peak rates of 1+ million event notifications per second and additional load generated by singular pivotal events with global impact.In this poster we give practical insights into our IT systems. We outline the infrastructure we operate and the event-driven architecture we apply at vwd. In particular we showcase the (geo)distributed publish/subscribe broker network we operate across locations and countries to provide market data to our customers with varying quality of information (QoI) properties.|10.1145/3328905.3332513|https://doi.org/10.1145/3328905.3332513|New York, NY, USA|Association for Computing Machinery|9781450367943|2019|A Real-World Distributed Infrastructure for Processing Financial Data at Scale|Frischbier, Sebastian and Paic, Mario and Echler, Alexander and Roth, Christian|inproceedings|10.1145/3328905.3332513|||||||||||||||||||||||||||||334614195|42
||Urban computing, Big data, Data fusion, Deep learning||123-133||Urban big data fusion creates huge values for urban computing in solving urban problems. In recent years, various models and algorithms based on deep learning have been proposed to unlock the power of knowledge from urban big data. To clarify the methodologies of urban big data fusion based on deep learning (DL), this paper classifies them into three categories: DL-output-based fusion, DL-input-based fusion and DL-double-stage-based fusion. These methods use deep learning to learn feature representation from multi-source big data. Then each category of fusion methods is introduced and some examples are shown. The difficulties and ideas of dealing with urban big data will also be discussed.|https://doi.org/10.1016/j.inffus.2019.06.016|https://www.sciencedirect.com/science/article/pii/S1566253519301393||||2020|Urban big data fusion based on deep learning: An overview|Jia Liu and Tianrui Li and Peng Xie and Shengdong Du and Fei Teng and Xin Yang|article|LIU2020123|||Information Fusion|15662535||53|||||||||||||||||||||||335864220|1192976563
||Filtering;Databases;Data integrity;Redundancy;Data preprocessing;Feature extraction;Stability analysis;data mining;data preprocessing;feature selection||248-252|2020 16th International Conference on Computational Intelligence and Security (CIS)|Data mining is the focus of big data applications in various fields. Data pre-processing is a crucial step in the data mining process. With the development of the information society and the application of databases, the educational data has seen explosive growth, and the data on poor students has become informative. However, the actual student financial aid management system collects the data on poor students which generally has problems such as missing values, attributes redundancy, and noise. To solve this problem, we proposed a novel method called DPBP to preprocess data. The proposed DPBP approach consists of four stages: the preparation of data, the scoping of characteristics, the combination of characteristics, and the filtering of missing number. Firstly, we prepare the dataset by extracting data. Next, the characteristic range is limited by choosing experimental results of feature selection algorithm. Then, third stage performs feature combination to obtain the feature decomposition sets. Finally, based on accuracy and missing number, we gain the optimal dataset. Series of experiments result show that our proposed method significantly improves the data quality and stability.|10.1109/CIS52066.2020.00060|||||2020|Data Preprocessing Method For The Analysis Of Incomplete Data On Students In Poverty|Huang, Haiyan and Wei, Bizhong and Dai, Jian and Ke, Wenlong|inproceedings|9407474||Nov|||||||||||||||||||||||||||336880831|42
||Information management;Data handling;Data storage systems;Government policies;Databases;Business;Legal aspects;Data privacy;policy;privacy;data;social impact;big data;private data commons||46-53||"\"Businesses and governments exploit big data without regard for issues of legality, data quality, disparate data meanings, and process quality. This often results in poor decisions, with individuals bearing the greatest risk. The threats harbored by big data extend far beyond the individual, however, and call for new legal structures, business processes, and concepts such as a Private Data Commons. The Web extra at http://youtu.be/TvXoQhrrGzg is a video in which author Marcus Wigan expands on his article \"\"Big Data's Big Unintended Consequences\"\" and discusses how businesses and governments exploit big data without regard for issues of legality, data quality, disparate data meanings, and process quality. This often results in poor decisions, with individuals bearing the greatest risk. The threats harbored by big data extend far beyond the individual, however, and call for new legal structures, business processes, and concepts such as a Private Data Commons.\""|10.1109/MC.2013.195|||||2013|Big Data's Big Unintended Consequences|Wigan, Marcus R. and Clarke, Roger|article|6527249||June|Computer|15580814|6|46|||||||||||||||||||||||338158531|43808127
||Containers;LinkedIn;Data models;Big data;Java;Measurement;Real-time systems;Apache Samza;capacity model;data filtering;performance||2600-2605|2015 IEEE International Conference on Big Data (Big Data)|Data quality is essential in big data paradigm as poor data can have serious consequences when dealing with large volumes of data. While it is trivial to spot poor data for small-scale and offline use cases, it is challenging to detect and fix data inconsistency in large-scale and online (real-time or near-real time) big data context. An example of such scenario is spotting and fixing poor data using Apache Samza, a stream processing framework that has been increasingly adopted to process near-real-time data at LinkedIn. To optimize the deployment of Samza processing and reduce business cost, in this work we propose a memory capacity model for Apache Samza to allow denser deployments of high performing data-filtering applications built on Samza. The model can be used to provision just-enough memory resource to applications by tightening the bounds on the memory allocations. We apply our memory capacity model on Linkedln's real use cases in production, which significantly increases the deployment density and saves business costs. We will share key learning in this paper.|10.1109/BigData.2015.7364058|||||2015|A memory capacity model for high performing data-filtering applications in Samza framework|Feng, Tao and Zhuang, Zhenyun and Pan, Yi and Ramachandra, Haricharan|inproceedings|7364058||Oct|||||||||||||||||||||||||||338498770|42
||Big data, Analytics, Government, Public administration, Policy-making, Decision-making, Science-policy interface, Network governance||101383||Despite great potential, high hopes and big promises, the actual impact of big data on the public sector is not always as transformative as the literature would suggest. In this paper, we ascribe this predicament to an overly strong emphasis the current literature places on technical-rational factors at the expense of political decision-making factors. We express these two different emphases as two archetypical narratives and use those to illustrate that some political decision-making factors should be taken seriously by critiquing some of the core ‘techno-optimist’ tenets from a more ‘policy-pessimist’ angle. In the conclusion we have these two narratives meet ‘eye-to-eye’, facilitating a more systematized interrogation of big data promises and shortcomings in further research, paying appropriate attention to both technical-rational and political decision-making factors. We finish by offering a realist rejoinder of these two narratives, allowing for more context-specific scrutiny and balancing both technical-rational and political decision-making concerns, resulting in more realistic expectations about using big data for policymaking in practice.|https://doi.org/10.1016/j.giq.2019.05.010|https://www.sciencedirect.com/science/article/pii/S0740624X18302326||||2019|Techno-optimism and policy-pessimism in the public sector big data debate|Simon Vydra and Bram Klievink|article|VYDRA2019101383|||Government Information Quarterly|0740624X|4|36|||||14735|2,121|Q1|103|76|205|5894|1946|193|8,39|77,55|United Kingdom|Western Europe|1984-2020|E-learning (Q1); Law (Q1); Library and Information Sciences (Q1); Sociology and Political Science (Q1)|5,379|7.279|0.00502|340337481|1582933551
|Yokohama, Japan||||||||New York, NY, USA|Association for Computing Machinery|9781450380959|2021|CHI EA '21: Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems||proceedings|10.1145/3411763|||||||||||||||||||||||||||||340925436|42
||Precision agriculture, Precision livestock farming, High performance computing, Big data analytics||107035||According to McKinsey & Company, about a third of food produced is lost or wasted every year, amounting to a $940 billion economic hit. Inefficiencies in planting, harvesting, water use, reduced animal contributions, as well as uncertainty about weather, pests, consumer demand and other intangibles contribute to the loss. Precision Agriculture (PA) and Precision Livestock Farming (PLF) come to assist in optimizing agricultural and livestock production and minimizing the wastes and costs aforementioned. PA is a technology-enabled, data-driven approach to farming management that observes, measures, and analyzes the needs of individual fields and crops. PLF is also a technology-enabled, data-driven approach to livestock production management, which exploits technology to quantitatively measure the behavior, health and performance of animals. Big data delivered by a plethora of data sources related to these domains, has a multitude of payoffs including precision monitoring of fertilizer and fungicide levels to optimize crop yields, risk mitigation that results from monitoring when temperature and humidity levels reach dangerous levels for crops, increasing livestock production while minimizing the environmental footprint of livestock farming, ensuring high levels of welfare and health for animals, and more. By adding analytics to these sensor and image data, opportunities also exist to further optimize PA and PLF by having continuous data on how a field or the livestock is responding to a protocol. For these domains, two main challenges exist: 1) to exploit this multitude of data facilitating dedicated improvements in performance, and 2) to make available advanced infrastructure so as to harness the power of this information in order to benefit from the new insights, practices and products, efficiently time-wise, lowering responsiveness down to seconds so as to cater for time-critical decisions. The current paper aims to introduce CYBELE, a platform aspiring to safeguard that the stakeholders involved in the agri-food value chain (research community, SMEs, entrepreneurs, etc.) have integrated, unmediated access to a vast amount of very large scale datasets of diverse types and coming from a variety of sources, and that they are capable of actually generating value and extracting insights out of these data, by providing secure and unmediated access to large-scale High Performance Computing (HPC) infrastructures supporting advanced data discovery, processing, combination and visualization services, solving computationally-intensive challenges modelled as mathematical algorithms requiring very high computing power and capability.|https://doi.org/10.1016/j.comnet.2019.107035|https://www.sciencedirect.com/science/article/pii/S1389128619305353||||2020|CYBELE – Fostering precision agriculture & livestock farming through secure access to large-scale HPC enabled virtual industrial experimentation environments fostering scalable big data analytics|Konstantinos Perakis and Fenareti Lampathaki and Konstantinos Nikas and Yiannis Georgiou and Oskar Marko and Jarissa Maselyne|article|PERAKIS2020107035|||Computer Networks|13891286||168|||||26811|0,798|Q1|135|383|896|19762|4981|877|5,93|51,60|Netherlands|Western Europe|1977-1984, 1989-1990, 1996-2020|Computer Networks and Communications (Q1)|11,644|4.474|0.00957|341643305|424954694
||Big data, Data mining, Health sector, Intelligent medical diagnosis systems||95-109|Big Data Analytics for Healthcare|Big data has been used in the health sector to improve the quality of life, predict epidemics, cure diseases, and avoid preventable deaths, beyond increasing profits or reducing the burden of excess labor. Data sources in healthcare have become quite diversified and accessible to individuals, such as wearable and implantable devices, smartphones, and real-time sensors. When combined with existing health data, daily (even instantaneous) data from these devices can be used to predict future health conditions of individuals and to identify necessary intervention points. This chapter discusses a number of recent studies that introduces methods for using big data to create intelligent systems for patient diagnosis, triage, predicting lab results, and even detecting tumors. These studies open ways for researchers in the healthcare sector to improve the quality of services provided to the patients as well as reducing costs for the healthcare institutions.|https://doi.org/10.1016/B978-0-323-91907-4.00006-6|https://www.sciencedirect.com/science/article/pii/B9780323919074000066||Academic Press|978-0-323-91907-4|2022|Chapter 9 - Recent applications of data mining in medical diagnosis and prediction|Ozge Doguc and Zehra Nur Canbolat and Gokhan Silahtaroglu|incollection|DOGUC202295||||||||||Pantea Keikhosrokiani|||||||||||||||||||346962622|42
||Forrester Research, V-list, cloud computing, Big Data, data mining||119-128|Joe Celko’s Complete Guide to NoSQL|Big Data is largely a buzzword in IT right now. It was coined by Forrester Research to put a wrapper around existing database mining, data management, and other extensions of existing technology to the current hardware. The goal is to use mixed tools with larger volumes of several different forms of data being brought together under one roof. Along with this approach to data, we are also concerned with cloud computing, which is a public or private Internet network that replaces the tradition hardwired landlines within a company.|https://doi.org/10.1016/B978-0-12-407192-6.00009-1|https://www.sciencedirect.com/science/article/pii/B9780124071926000091|Boston|Morgan Kaufmann|978-0-12-407192-6|2014|Chapter 9 - Big Data and Cloud Computing|Joe Celko|incollection|CELKO2014119||||||||||Joe Celko|||||||||||||||||||353462750|42
SIGMOD '13|New York, New York, USA|integer linear programming, data pricing|12|613–624|Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data|We develop a new pricing system, QueryMarket, for flexible query pricing in a data market based on an earlier theoretical framework (Koutris et al., PODS 2012). To build such a system, we show how to use an Integer Linear Programming formulation of the pricing problem for a large class of queries, even when pricing is computationally hard. Further, we leverage query history to avoid double charging when queries purchased over time have overlapping information, or when the database is updated. We then present a technique that fairly shares revenue when multiple sellers are involved. Finally, we implement our approach in a prototype and evaluate its performance on several query workloads.|10.1145/2463676.2465335|https://doi.org/10.1145/2463676.2465335|New York, NY, USA|Association for Computing Machinery|9781450320375|2013|Toward Practical Query Pricing with QueryMarket|Koutris, Paraschos and Upadhyaya, Prasang and Balazinska, Magdalena and Howe, Bill and Suciu, Dan|inproceedings|10.1145/2463676.2465335|||||||||||||||||||||||||||||356351698|42
||Machine learning, Deep learning, Smart grid, Distribution grid, Smart energy service||111459||Artificial intelligence techniques lead to data-driven energy services in distribution power systems by extracting value from the data generated by the deployed metering and sensing devices. This paper performs a holistic analysis of artificial intelligence applications to distribution networks, ranging from operation, monitoring and maintenance to planning. The potential artificial intelligence techniques for power system applications and needed data sources are identified and classified. The following data-driven services for distribution networks are analyzed: topology estimation, observability, fraud detection, predictive maintenance, non-technical losses detection, forecasting, energy management systems, aggregated flexibility services and trading. A review of the artificial intelligence methods implemented in each of these services is conducted. Their interdependencies are mapped, proving that multiple services can be offered as a single clustered service to different stakeholders. Furthermore, the dependencies between the AI techniques with each energy service are identified. In recent years there has been a significant rise of deep learning applications for time series prediction tasks. Another finding is that unsupervised learning methods are mainly being applied to customer segmentation, buildings efficiency clustering and consumption profile grouping for non-technical losses detection. Reinforcement learning is being widely applied to energy management systems design, although more testing in real environments is needed. Distribution network sensorization should be enhanced and increased in order to obtain larger amounts of valuable data, enabling better service outcomes. Finally, the future opportunities and challenges for applying artificial intelligence in distribution grids are discussed.|https://doi.org/10.1016/j.rser.2021.111459|https://www.sciencedirect.com/science/article/pii/S1364032121007413||||2021|Artificial intelligence techniques for enabling Big Data services in distribution networks: A review|Sara Barja-Martinez and Mònica Aragüés-Peñalba and Íngrid Munné-Collado and Pau Lloret-Gallego and Eduard Bullich-Massagué and Roberto Villafafila-Robles|article|BARJAMARTINEZ2021111459|||Renewable and Sustainable Energy Reviews|13640321||150|||||27567|3,522|Q1|295|643|3239|73956|54087|3206|16,30|115,02|United Kingdom|Western Europe|1997-2021|Renewable Energy, Sustainability and the Environment (Q1)||||358910535|1303297312
ICAAI 2021|Virtual Event, United Kingdom|deep learning, deep neural network, Classification, Support Vector Machine, flight delays|10|1–10|2021 The 5th International Conference on Advances in Artificial Intelligence (ICAAI)|Flight delays have negatively impacted the socio-economics state of passengers, airlines and airports, resulting in huge economic losses. Hence, it has become necessary to correctly predict their occurrences in decision-making because it is important for the effective management of the aviation industry. Developing accurate flight delays classification models depends mostly on the air transportation system complexity and the infrastructure available in airports, which may be a region-specific issue. However, no specific prediction or classification model can handle the individual characteristics of all airlines and airports at the same time. Hence, the need to further develop and compare predictive models for the aviation decision system of the future cannot be over-emphasised. In this research, flight on-time data records from the United State Bureau of Transportation Statistics was employed to evaluate the performances of Deep Feedforward Neural Network, Neural Network, and Support Vector Machine models on a binary classification problem. The research revealed that the models achieved different accuracies of flight delay classifications. The Support Vector Machine had the worst average accuracy than Neural Network and Deep Feedforward Neural Network in the initial experiment. The Deep Feedforward Neural Network outperformed Support Vector Machines and Neural Network with the best average percentage accuracies. Going further to investigate the Deep Feedforward Neural Network architecture on different parameters against itself suggest that training a Deep Feedforward Neural Network algorithm, regardless of data training size, the classification accuracy peaks. We examine which number of epochs works best in our flight delay classification settings for the Deep Feedforward Neural Network. Our experiment results demonstrate that having many epochs affects the convergence rate of the model; unlike when hidden layers are increased, it does not ensure better or higher accuracy in a binary classification of flight delays. Finally, we recommended further studies on the applicability of the Deep Feedforward Neural Network in flight delays prediction with specific case studies of either airlines or airports to check the impact on the model's performance.|10.1145/3505711.3505712|https://doi.org/10.1145/3505711.3505712|New York, NY, USA|Association for Computing Machinery|9781450390699|2022|A Deep Feedforward Neural Network and Shallow Architectures Effectiveness Comparison: Flight Delays Classification Perspective|Bala Bisandu, Desmond and Salih Homaid, Mohammed and Moulitsas, irene and Filippone, Salvatore|inproceedings|10.1145/3505711.3505712|||||||||||||||||||||||||||||360314356|42
|||2|141–142|Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice|||https://doi.org/10.1145/3447404.3447413|New York, NY, USA|Association for Computing Machinery|9781450390293|2021|Ethical Issues of Digital Signal Processing|McMenemy, David|inbook|10.1145/3447404.3447413|||||||||1||||||||||||||||||||362879224|42
||quality metrics, statistical significance analysis, context-based evaluation, Dimensionality reduction|40|||Dimensionality reduction is a commonly used technique in data analytics. Reducing the dimensionality of datasets helps not only with managing their analytical complexity but also with removing redundancy. Over the years, several such algorithms have been proposed with their aims ranging from generating simple linear projections to complex non-linear transformations of the input data. Subsequently, researchers have defined several quality metrics in order to evaluate the performances of different algorithms. Hence, given a plethora of dimensionality reduction algorithms and metrics for their quality analysis, there is a long-existing need for guidelines on how to select the most appropriate algorithm in a given scenario. In order to bridge this gap, in this article, we have compiled 12 state-of-the-art quality metrics and categorized them into 5 identified analytical contexts. Furthermore, we assessed 15 most popular dimensionality reduction algorithms on the chosen quality metrics using a large-scale and systematic experimental study. Later, using a set of robust non-parametric statistical tests, we assessed the generalizability of our evaluation on 40 real-world datasets. Finally, based on our results, we present practitioners’ guidelines for the selection of an appropriate dimensionally reduction algorithm in the present analytical contexts.|10.1145/3428077|https://doi.org/10.1145/3428077|New York, NY, USA|Association for Computing Machinery||2021|Context-Based Evaluation of Dimensionality Reduction Algorithms—Experiments and Statistical Significance Analysis|Ghosh, Aindrila and Nashaat, Mona and Miller, James and Quader, Shaikh|article|10.1145/3428077|24|jan|ACM Trans. Knowl. Discov. Data|15564681|2|15|April 2021||||5800173377|0,728|Q1|59|71|169|3729|816|168|4,54|52,52|United States|Northern America|2007-2020|Computer Science (miscellaneous) (Q1)|1,740|2.713|0.00224|364053424|1302859451
||Ambient Assisted Living (AAL), Big data analytics, Internet of Medical Things (IoMT), Machine learning techniques, Physical activities, Wearable sensors||136-151||In the era of pervasive computing, human living has become smarter by the latest advancements in IoMT (Internet of Medical Things), wearable sensors and telecommunication technologies in order to deliver smart healthcare services. IoMT has the potential to revolutionize the healthcare industry. IoMT interconnects wearable sensors, patients, healthcare providers and caregivers via software and ICT (Information and Communication Technology). AAL (Ambient Assisted Living) enables integration of new technologies to be part of our daily life activities. In this paper, we have provided a novel smart healthcare framework for AAL to monitor the physical activities of elderly people using IoMT and intelligent machine learning algorithms for faster analysis, decision making and better treatment recommendations. Data is collected from multiple wearable sensors placed on subject’s left ankle, right arm, and chest, is transmitted through IoMT devices to the integrated cloud and data analytics layer. To process huge amounts of data in parallel, Hadoop MapReduce techniques are used. Multinomial Naïve Bayes classifier, which fits into the MapReduce paradigm, is utilized to recognize the motion experienced by different body parts and provides higher scalability and better performance with parallel processing when compared to serial processor. Our proposed framework predicts 12 physical activities with an overall accuracy of 97.1%. This can be considered as an optimal solution for recognizing physical activities to remotely monitor health conditions of elderly people.|https://doi.org/10.1016/j.future.2019.06.004|https://www.sciencedirect.com/science/article/pii/S0167739X18321071||||2019|Smart healthcare framework for ambient assisted living using IoMT and big data analytics techniques|Liyakathunisa Syed and Saima Jabeen and Manimala S. and Abdullah Alsaeedi|article|SYED2019136|||Future Generation Computer Systems|0167739X||101|||||12264|1,262|Q1|119|798|1935|36054|16877|1868|9,11|45,18|Netherlands|Western Europe|1984-2021|Computer Networks and Communications (Q1); Hardware and Architecture (Q1); Software (Q1)||||364320355|562237118
ACM TURC'20|Hefei, China|Nash equilibrium, Data watermarking, Particle swarm optimization algorithm, Majority voting strategy, Copyright protection, Constrained optimization, Big data|5|21–25|Proceedings of the ACM Turing Celebration Conference - China|Data watermarking technology is an effective means to protect the copyright of big data. In order to embed robust and highly available data watermarks, firstly, based on the game theory, a Nash equilibrium model between watermark robustness and data quality is established to solve the optimal number of data partitioning. Then, the mapping relationship between data partitioning and watermark bit is established by using secure hash algorithm. Finally, under the constraint of data usability, the improved particle swarm optimization algorithm is used to calculate the optimal solution of data change for each data partitioning, and then the data is changed accordingly to complete the embedding of watermark bit. In order to verify the copyright ownership of big data, this paper also gives the corresponding watermark extraction method. Watermark extraction is the inverse process of watermark embedding. First, traverse all partitions and extract the possible embedded bit values in each data partitioning. Then, the actual embedded watermark bit is finally determined by majority voting strategies. The experimental results show that our proposed method can not only detect watermarks under different attack conditions, ensure the robustness of big data watermarks, but also achieve better data quality, and the comprehensive effect of data watermarks is better than the existing methods.|10.1145/3393527.3393532|https://doi.org/10.1145/3393527.3393532|New York, NY, USA|Association for Computing Machinery|9781450375344|2020|Research on Copyright Protection Method of Big Data Based on Nash Equilibrium and Constraint Optimization|Shi, Bin and YabinXu|inproceedings|10.1145/3393527.3393532|||||||||||||||||||||||||||||365472693|42
ASE '20|Virtual Event, Australia|exception handling, deep learning, code generation, neural network|13|29–41|Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering|Exception handling is an important built-in feature of many modern programming languages such as Java. It allows developers to deal with abnormal or unexpected conditions that may occur at runtime in advance by using try-catch blocks. Missing or improper implementation of exception handling can cause catastrophic consequences such as system crash. However, previous studies reveal that developers are unwilling or feel it hard to adopt exception handling mechanism, and tend to ignore it until a system failure forces them to do so. To help developers with exception handling, existing work produces recommendations such as code examples and exception types, which still requires developers to localize the try blocks and modify the catch block code to fit the context. In this paper, we propose a novel neural approach to automated exception handling, which can predict locations of try blocks and automatically generate the complete catch blocks. We collect a large number of Java methods from GitHub and conduct experiments to evaluate our approach. The evaluation results, including quantitative measurement and human evaluation, show that our approach is highly effective and outperforms all baselines. Our work makes one step further towards automated exception handling.|10.1145/3324884.3416568|https://doi.org/10.1145/3324884.3416568|New York, NY, USA|Association for Computing Machinery|9781450367684|2021|Learning to Handle Exceptions|Zhang, Jian and Wang, Xu and Zhang, Hongyu and Sun, Hailong and Pu, Yanjun and Liu, Xudong|inproceedings|10.1145/3324884.3416568|||||||||||||||||||||||||||||367067092|42
IMMS '18|Chengdu, China|feature analysis, visualization, management information system, dynamic complexity|10|206–215|Proceedings of the 2018 International Conference on Information Management &amp; Management Science|The volume and complexity of the data in the information system continues to increase during the operation of the information system. The phenomenon leads to the fact that decision-makers are faced with data-rich and information-deficient situations. How to intuitively grasp the dynamic complexity of information system data has become a concern of scholars. Therefore, this paper collected and organized the system log data of two shipping companies, adopted multiple visualization forms, and combined the dynamic complexity measurement methods of information systems, focusing on the overall distribution status and development trend of the data dynamic complexity and internal information system and the influence of complexity of subsystems within the information system on overall data dynamic operation complexity, and then analyzed the overall feature of the dynamic complexity of information system data, and summarized the feature regulars of the complexity of the visualization method. The results show that the visual feature analysis of the dynamic complexity of information systems can improve the ability of enterprises to analyze and make decisions, and provide a basis for enterprise managers to develop corresponding management measures.|10.1145/3277139.3277179|https://doi.org/10.1145/3277139.3277179|New York, NY, USA|Association for Computing Machinery|9781450364867|2018|Analysis of Dynamic Complexity Feature of Information System Data Based on Visualization|Lu, Yi and Yin, Jun and Ge, Shilun|inproceedings|10.1145/3277139.3277179|||||||||||||||||||||||||||||369398062|42
||Influencers, Market mavens, Big data, Social media, Twitter||102246||The increased availability of social media big data has created a unique challenge for marketing decision-makers; turning this data into useful information. One of the significant areas of opportunity in digital marketing is influencer marketing, but identifying these influencers from big data sets is a continual challenge. This research illustrates how one type of influencer, the market maven, can be identified using big data. Using a mixed-method combination of both self-report survey data and publicly accessible big data, we gathered 556,150 tweets from 370 active Twitter users. We then proposed and tested a range of social-media-based metrics to identify market mavens. Findings show that market mavens (when compared to non-mavens) have more followers, post more often, have less readable posts, use more uppercase letters, use less distinct words, and use hashtags more often. These metrics are openly available from public Twitter accounts and could integrate into a broad-scale decision support system for marketing and information systems managers. These findings have the potential to improve influencer identification effectiveness and efficiency, and thus improve influencer marketing.|https://doi.org/10.1016/j.ijinfomgt.2020.102246|https://www.sciencedirect.com/science/article/pii/S0268401220314456||||2021|Identifying influencers on social media|Paul Harrigan and Timothy M. Daly and Kristof Coussement and Julie A. Lee and Geoffrey N. Soutar and Uwana Evers|article|HARRIGAN2021102246|||International Journal of Information Management|02684012||56|||||15631|2,770|Q1|114|224|382|20318|5853|373|16,16|90,71|United Kingdom|Western Europe|1970, 1986-2021|Artificial Intelligence (Q1); Computer Networks and Communications (Q1); Information Systems (Q1); Information Systems and Management (Q1); Library and Information Sciences (Q1); Management Information Systems (Q1); Marketing (Q1)|12,245|14.098|0.01167|373650569|747927863
ITiCSE-WGR '20|Trondheim, Norway|contemporary computing education, high-performance computing curricula, iticse working group, computer science education, high performance computing, hpc education|24|51–74|Proceedings of the Working Group Reports on Innovation and Technology in Computer Science Education|High Performance Computing (HPC) is the ability to process data and perform complex calculations at extremely high speeds. Current HPC platforms can achieve calculations on the order of quadrillions of calculations per second, with quintillions on the horizon. The past three decades witnessed a vast increase in the use of HPC across different scientific, engineering, and business communities on problems such as sequencing the genome, predicting climate changes, designing modern aerodynamics, or establishing customer preferences. Although HPC has been well incorporated into science curricula such as bioinformatics, the same cannot be said for most computing programs. Computing educators are only now beginning to recognize the need for HPC Education (HPCEd). Building on earlier work, this working group explored how HPCEd can make inroads into computing education, focusing on the undergraduate level. This paper presents the background of HPC and HPCEd, identifies several of the needed core HPC competencies for students, identifies the support needed by educators for HPCEd, and explores the symbiosis between HPCEd and computing education in contemporary areas such as artificial intelligence and data science, as well as how HPCEd can be applied to benefit diverse non-computing domains such as atmospheric science, biological sciences and critical infrastructure protection. Finally, the report makes several recommendations to improve and facilitate HPC education in the future.|10.1145/3437800.3439203|https://doi.org/10.1145/3437800.3439203|New York, NY, USA|Association for Computing Machinery|9781450382939|2020|High Performance Computing Education: Current Challenges and Future Directions|Raj, Rajendra K. and Romanowski, Carol J. and Impagliazzo, John and Aly, Sherif G. and Becker, Brett A. and Chen, Juan and Ghafoor, Sheikh and Giacaman, Nasser and Gordon, Steven I. and Izu, Cruz and Rahimi, Shahram and Robson, Michael P. and Thota, Neena|inproceedings|10.1145/3437800.3439203|||||||||||||||||||||||||||||374167428|42
||Irrigation;Quality assurance;Data integrity;Signal processing;Feature extraction;Robustness;Object recognition;Data quality;internet of things;smart applications;precision irrigation||1-6|2019 IEEE Latin-American Conference on Communications (LATINCOM)|Most current scientific and industrial efforts in IoT are geared towards building integrated platforms to finally realize its potential in commercial scale applications. The IoT and Big Data contemporary context brings a number of challenges, such as providing quality assurance (defined by availability and veracity) for sensor data. Traditional signal processing approaches are no longer sufficient, requiring combined approaches in both architectural and analytical layers. This paper proposes a discussion on the adequate foundations of a new general approach aimed at increasing robustness and antifragility of IoT-based smart applications. In addition, it shows results of preliminary experiments with real data in the context of precision irrigation using multivariate methods to identify relevant situations, such as sensor failures and the mismatch of contextual sensor information due to different spatial granularities capture. Our results provide initial indications of the adequacy of the proposed framework.|10.1109/LATINCOM48065.2019.8937930|||||2019|Foundations of Data Quality Assurance for IoT-based Smart Applications|Togneri, Rodrigo and Camponogara, Glauber and Soininen, Juha-Pekka and Kamienski, Carlos|inproceedings|8937930||Nov||2330989X|||||||||||||||||||||||||374911518|1121894868
|||3|15–17||This section is compiled from reports of recent events sponsored or run in cooperation with ACM SIGAI. In general these reports were written and submitted by the conference organisers.|10.1145/3511322.3511326|https://doi.org/10.1145/3511322.3511326|New York, NY, USA|Association for Computing Machinery||2022|Conference Reports|Dennis, Louise A.|article|10.1145/3511322.3511326||jan|AI Matters||3|7|September 2021||||||||||||||||||||||375604121|42
||Safety, Road, Transport, Digital technology, Information||106543||Each year, 1.35 million people are killed on the world’s roads and another 20–50 million are seriously injured. Morbidity or serious injury from road traffic collisions is estimated to increase to 265 million people between 2015 and 2030. Current road safety management systems rely heavily on manual data collection, visual inspection and subjective expert judgment for their effectiveness, which is costly, time-consuming, and sometimes ineffective due to under-reporting and the poor quality of the data. A range of innovations offers the potential to provide more comprehensive and effective data collection and analysis to improve road safety. However, there has been no systematic analysis of this evidence base. To this end, this paper provides a systematic review of the state of the art. It identifies that digital technologies - Artificial Intelligence (AI), Machine-Learning, Image-Processing, Internet-of-Things (IoT), Smartphone applications, Geographic Information System (GIS), Global Positioning System (GPS), Drones, Social Media, Virtual-reality, Simulator, Radar, Sensor, Big Data – provide useful means for identifying and providing information on road safety factors including road user behaviour, road characteristics and operational environment. Moreover, the results show that digital technologies such as AI, Image processing and IoT have been widely applied to enhance road safety, due to their ability to automatically capture and analyse data while preventing the possibility of human error. However, a key gap in the literature remains their effectiveness in real-world environments. This limits their potential to be utilised by policymakers and practitioners.|https://doi.org/10.1016/j.aap.2021.106543|https://www.sciencedirect.com/science/article/pii/S0001457521005741||||2022|Understanding the potential of emerging digital technologies for improving road safety|Mehran {Eskandari Torbaghan} and Manu Sasidharan and Louise Reardon and Leila C.W. Muchanga-Hvelplund|article|ESKANDARITORBAGHAN2022106543|||Accident Analysis & Prevention|00014575||166|||||||||||||||||||||||376685585|381026298
||Big data marketing, Differential game, Closed-loop supply chain, Internet service platform||1180-1193||In the age of “Internet+”, many Internet service platforms (ISPs) in China have been widely introduced to the closed-loop supply chain (CLSC). To further study the role of the Internet service platform, this paper considers a CLSC composed of a manufacturer, a retailer and an Internet service platform who invests in research and development (R&D), advertising and Big Data marketing, and develops the goodwill dynamic model based on the differential game theory. The construction of a goodwill dynamic model has two purposes, namely, to increase sales and the return rate. The optimal decisions for 3 players under two different cooperative scenarios are obtained, namely, the retailer payment scenario (scenario D) and the manufacturer cost-sharing scenario (scenario S). The supply chain members gain more profit or achieve a higher level of goodwill for products under certain conditions, i.e., a high residual value from remanufacturing, a high sharing rate of residual value from the retailer's recycled products, and a low recycling cost. Interestingly, the wholesale price increases with the residual value of recycled products when goodwill effectiveness is low, while the price declines when goodwill effectiveness is high. After comparing two cooperative scenarios, the result shows that an Internet service platform will invest more in Big Data marketing under the manufacturer cost-sharing scenario, and cooperation between the manufacturer and the Internet service platform can help improve the goodwill of enterprises or products. Moreover, the manufacturer cost-sharing scenario is payoff-Pareto-improving in most cases through the coordination of a cost-sharing rate, and the effectiveness of Big Data marketing exerts a positive effect on goodwill and the development of the industry. In addition, the retailer has “free rider” tendencies in the manufacturer cost-sharing scenario. The results encourage more enterprises to enhance the value of goodwill through cooperation with Internet service platforms because Internet service platforms conveniently utilize Big Data marketing to increase the sales of products and the collecting rate of used products, which in turn helps environmental sustainability.|https://doi.org/10.1016/j.jclepro.2019.01.310|https://www.sciencedirect.com/science/article/pii/S0959652619303373||||2019|Dynamic cooperation strategies of the closed-loop supply chain involving the internet service platform|Zehua Xiang and Minli Xu|article|XIANG20191180|||Journal of Cleaner Production|09596526||220|||||19167|1,937|Q1|200|5126|10603|304498|103295|10577|9,56|59,40|United Kingdom|Western Europe|1993-2021|Environmental Science (miscellaneous) (Q1); Industrial and Manufacturing Engineering (Q1); Renewable Energy, Sustainability and the Environment (Q1); Strategy and Management (Q1)|170,352|9.297|0.18299|376733311|1121054297
|||14|1339–1352||The attacks, faults, and severe communication/system conditions in Mobile Crowd Sensing (MCS) make false data detection a critical problem. Observing the intrinsic low dimensionality of general monitoring data and the sparsity of false data, false data detection can be performed based on the separation of normal data and anomalies. Although the existing separation algorithm based on Direct Robust Matrix Factorization (DRMF) is proven to be effective, requiring iteratively performing Singular Value Decomposition (SVD) for low-rank matrix approximation would result in a prohibitively high accumulated computation cost when the data matrix is large. In this work, we observe the quick false data location feature from our empirical study of DRMF, based on which we propose an intelligent Light weight Low Rank and False Matrix Separation algorithm (LightLRFMS) that can reuse the previous result of the matrix decomposition to deduce the one for the current iteration step. Depending on the type of data corruption, random or successive/mass, we design two versions of LightLRFMS. From a theoretical perspective, we validate that LightLRFMS only requires one round of SVD computation and thus has very low computation cost. We have done extensive experiments using a PM 2.5 air condition trace and a road traffic trace. Our results demonstrate that LightLRFMS can achieve very good false data detection performance with the same highest detection accuracy as DRMF but with up to 20 times faster speed thanks to its lower computation cost.|10.1109/TNET.2020.2982685|https://doi.org/10.1109/TNET.2020.2982685||IEEE Press||2020|Quick and Accurate False Data Detection in Mobile Crowd Sensing|Li, Xiaocan and Xie, Kun and Wang, Xin and Xie, Gaogang and Xie, Dongliang and Li, Zhenyu and Wen, Jigang and Diao, Zulong and Wang, Tian|article|10.1109/TNET.2020.2982685||jun|IEEE/ACM Trans. Netw.|10636692|3|28|June 2020||||27237|1,022|Q1|174|187|653|6994|3573|652|5,40|37,40|United States|Northern America|1993-2020|Computer Networks and Communications (Q1); Computer Science Applications (Q1); Electrical and Electronic Engineering (Q1); Software (Q1)||||377351903|1453185914
||Big data;Biomedical imaging;Personnel;Complexity theory;Bioinformatics;Process control;Instruments;Big Data;Data Quality;Returns to Scale||2644-2653|2015 IEEE International Conference on Big Data (Big Data)|"\"A USAF sponsored MITRE research team undertook four separate, domain-specific case studies about Big Data applications. Those case studies were initial investigations into the question of whether or not data quality issues encountered in Big Data collections are substantially different in cause, manifestation, or detection than those data quality issues encountered in more traditionally sized data collections. The study addresses several factors affecting Big Data Quality at multiple levels, including collection, processing, and storage. Though not unexpected, the key findings of this study reinforce that the primary factors affecting Big Data reside in the limitations and complexities involved with handling Big Data while maintaining its integrity. These concerns are of a higher magnitude than the provenance of the data, the processing, and the tools used to prepare, manipulate, and store the data. Data quality is extremely important for all data analytics problems. From the study's findings, the \"\"truth about Big Data\"\" is there are no fundamentally new DQ issues in Big Data analytics projects. Some DQ issues exhibit return-s-to-scale effects, and become more or less pronounced in Big Data analytics, though. Big Data Quality varies from one type of Big Data to another and from one Big Data technology to another.\""|10.1109/BigData.2015.7364064|||||2015|Big data, big data quality problem|Becker, David and King, Trish Dunn and McMullen, Bill|inproceedings|7364064||Oct|||||||||||||||||||||||||||381209288|42
|||3||||10.1145/2629605|https://doi.org/10.1145/2629605|New York, NY, USA|Association for Computing Machinery||2014|Discovering Product Counterfeits in Online Shops: A Big Data Integration Challenge|Rahm, Erhard|article|10.1145/2629605|3|sep|J. Data and Information Quality|19361955|1–2|5|August 2014||||||||||||||||||||||385464927|833754770
||Redundancy;Heuristic algorithms;Indexing;Approximation algorithms;Measurement;Runtime;Similarity search;Stream search;Retention policy;Locality sensitive hashing;Dynamic popularity||964-969|2017 IEEE International Conference on Big Data (Big Data)|Similarity search is the task of retrieving data items that are similar to a given query. In this paper, we introduce the time-sensitive notion of similarity search over endless data-streams (SSDS), which takes into account data quality and temporal characteristics in addition to similarity. SSDS is challenging as it needs to process unbounded data, while computation resources are bounded. We propose Stream-LSH, a randomized SSDS algorithm that bounds the index size by retaining items according to their freshness, quality, and dynamic popularity attributes. We show that Stream-LSH increases recall when searching for similar items compared to alternative approaches using the same space capacity.|10.1109/BigData.2017.8258016|||||2017|Fishing in the stream: Similarity search over endless data|Kraus, Naama and Carmel, David and Keidar, Idit|inproceedings|8258016||Dec|||||||||||||||||||||||||||386843286|42
BCB '22|Northbrook, Illinois|health informatics, model interpretation, bronchoalveolar lavage fluid, gene markers, single cell RNA sequencing, COVID-19|5||Proceedings of the 13th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics|Bio-marker identification for COVID-19 remains a vital research area to improve current and future pandemic responses. Innovative artificial intelligence and machine learning-based systems may leverage the large quantity and complexity of single cell sequencing data to quickly identify disease with high sensitivity. In this study, we developed a novel approach to classify patient COVID-19 infection severity using single-cell sequencing data derived from patient BronchoAlveolar Lavage Fluid (BALF) samples. We also identified key genetic biomarkers associated with COVID-19 infection severity. Feature importance scores from high performing COVID-19 classifiers were used to identify a set of novel genetic biomarkers that are predictive of COVID-19 infection severity. Treatment development and pandemic reaction may be greatly improved using our novel big-data approach. Our implementation is available on https://github.com/aekanshgoel/COVID-19_scRNAseq.|10.1145/3535508.3545519|https://doi.org/10.1145/3535508.3545519|New York, NY, USA|Association for Computing Machinery|9781450393867|2022|Identification of COVID-19 Severity and Associated Genetic Biomarkers Based on ScRNA-Seq Data|Goel, Aekansh and Mudge, Zachary and Bi, Sarah and Brenner, Charles and Huffman, Nicholas and Giuste, Felipe and Marteau, Benoit and Shi, Wenqi and Wang, May D.|inproceedings|10.1145/3535508.3545519|45||||||||||||||||||||||||||||387513572|42
SBSI'19|Aracaju, Brazil|heterogeneous provenance data integration, polystore, Workflows interoperability|8||Proceedings of the XV Brazilian Symposium on Information Systems|In the last decade the (big) data-driven science paradigm became a wide-spread reality. However, this approach has some limitations such as a performance dependency on the quality of the data and the lack of reproducibility of the results. In order to enable this reproducibility, many tools such as Workflow Management Systems were developed to formalize process pipelines and capture execution traces. However, interoperating data generated by these solutions became a problem, since most systems adopted proprietary data models. To support interoperability across heterogeneous provenance data, we propose a Service Oriented Architecture with a polystore storage design in which provenance is conceptually represented utilizing the ProvONE model. A wrapper layer is responsible for transforming data described by heterogeneous formats into ProvONE-compliant. Moreover, we propose a query layer that provides location and access transparency to users. Furthermore, we conduct two feasibility studies, showcasing real usecase scenarios. Firstly, we illustrate how two research groups can compare their processes and results. Secondly, we show how our architecture can be used as a queriable provenance repository. We show Polyflow's viability for both scenarios using the Goal-Question-Metric methodology. Finally, we show our solution usability and extensibility appeal by comparing it to similar approaches.|10.1145/3330204.3330259|https://doi.org/10.1145/3330204.3330259|New York, NY, USA|Association for Computing Machinery|9781450372374|2019|Polyflow: A SOA for Analyzing Workflow Heterogeneous Provenance Data in Distributed Environments|"\"Mendes, Yan and Braga, Regina and Str\"\"{o}ele, Victor and de Oliveira, Daniel\""|inproceedings|10.1145/3330204.3330259|49||||||||||||||||||||||||||||387992227|42
||Big Data;Medical services;Data integrity;Biomedical monitoring;Business;Big Data;Analytics;healthcare quality;entity reconciliation||302-305|2020 IEEE 20th Mediterranean Electrotechnical Conference ( MELECON)|Health information technology is showing an impressive growing interest towards Big Data. Big Data Analytics is expected to bring important achievements for building sophisticated models, methods and tools that are expected to improve healthcare services and citizen health and wellbeing. In spite of these expectations data quality and analytics methods are not getting the attention they deserve. In this short paper, we aimed to highlight the issues of data quality in the context of Big Data Healthcare Analytics. The common sources of errors, the consequence of these errors, and potential solutions that should be considered to mitigate errors and pitfalls are discussed in the healthcare context.|10.1109/MELECON48756.2020.9140534|||||2020|The quality concerns in health care Big Data|Molinari, Andrea and Nollo, Giandomenico|inproceedings|9140534||June||21588481|||||||||||||||||||||||||392217396|1970985088
||Smart manufacturing;US Government;Science - general;Maintenance engineering;Systematics;Software;Sensor systems||xxiii-xxiii|2021 IEEE World Congress on Services (SERVICES)|Summary form only given, as follows. The complete presentation was not made available for publication as part of the conference proceedings. Industrial AI, Big Data Analytics, Machine Learning, and Cyber Physical Systems are changing the way we design product, manufacturing, and service systems. It is clear that as more sensors and smart analytics software are integrated in the networked industrial products and manufacturing systems, predictive technologies can further learn and autonomously optimize service productivity and performance. This presentation will address the trends of Industrial AI for smart service realization. First, Industrial AI systematic approach will be introduced. Case studies on advanced predictive analytics technologies for different maintenance and service operations will be demonstrated. In addition, issues on data quality for high performance and real-time data analytics in future digital service will be discussed.|10.1109/SERVICES51467.2021.00055|||||2021|Transformation: Case Studies and Lessons Learned : Keynote 2|Lee, Jay|inproceedings|9604414||Sep.||2642939X|||||||||||||||||||||||||393001787|1883754313
BlockSys'18|Shenzhen, China|Blockchain, non linear control, WSN, IoT, edge computing, data quality false data detection|6|19–24|Proceedings of the 1st Workshop on Blockchain-Enabled Networked Sensor Systems|Smart home presents a challenge in control and monitoring of its wireless sensors networks (WSN) and the internet of things (IoT) devices which form it. The current IoT architectures are centralized, complex, with poor security in its communications and with upstream communication channels mainly. As a result, there are problems with data reliability. These problems include data missing, malicious data inserted, communications network overload, and overload of computing power at the central node. In this paper a new architecture is presented. This architecture based in blockchain introduce the edge computing layer and a new algorithm to improve data quality and false data detection.|10.1145/3282278.3282282|https://doi.org/10.1145/3282278.3282282|New York, NY, USA|Association for Computing Machinery|9781450360500|2018|Blockchain Framework for IoT Data Quality via Edge Computing|Casado-Vara, Roberto and de la Prieta, Fernando and Prieto, Javier and Corchado, Juan M.|inproceedings|10.1145/3282278.3282282|||||||||||||||||||||||||||||393368443|42
AIAM2021|Manchester, United Kingdom||9|1352–1360|2021 3rd International Conference on Artificial Intelligence and Advanced Manufacture|Based on the characteristics of online customers, such as user characteristics, interaction behavior, frequency of visits and business queries, this paper uses big data analysis mining algorithm to conduct exploratory analysis on each business data, and builds a weight division model (Entropy Value Method) to achieve online customer ranking.|10.1145/3495018.3495398|https://doi.org/10.1145/3495018.3495398|New York, NY, USA|Association for Computing Machinery|9781450385046|2022|Exploring and Analyzing Data Mining Algorithm Technology in Internet Customer Ranking|Zhang, Mingjie and Sheng, Yan and Tian, Nuo and Liu, Wei and Wang, Hui and Zhu, Longzhu and Xu, Qing|inproceedings|10.1145/3495018.3495398|||||||||||||||||||||||||||||397307334|42
||Cloud computing;Computational modeling;Unified modeling language;Analytical models;Computer architecture;Software;Object oriented modeling;cloud;CPS;big data;AADL;specification||26-29|2018 17th International Symposium on Distributed Computing and Applications for Business Engineering and Science (DCABES)|In cyber physical systems(CPS), the physical world and the information world are merged to form a new structure that combines both hardware and software, and become the core technology system that supports and leads the transformation of a new generation of industries. With the rapid development of network technology, the data generated has also increased rapidly, which means that today's information society has entered the era of big data, and the technology of adapting to the cloud platform has gradually matured. The cloud computing platform provides flexible and relatively inexpensive storage space and computing resources for the development of big data technology. This also provides basic support for the development of big data driven CPS based on the cloud platform. In this paper, we specify and model cloud cyber physical systems based on AADL, which can specify, model, and analyze cloud cyber physical systems, finally implement cyber physical systems on cloud platforms, provide availability analysis, reliability analysis. data quality analysis, real-time performance analysis, security analysis and resource consumption analysis.|10.1109/DCABES.2018.00017|||||2018|Specifying and Modeling Cloud Cyber Physical Systems Based on AADL|Zhang, Lichen|inproceedings|8572515||Oct||24733636|||||||||||||||||||||||||397560339|1915637180
||Analytics, Big data, Managerial decision making, Managerial work, Digital universe||673-688||With the explosion of the digital universe, it is becoming increasingly important to understand how organizational decision making (i.e., the business-oriented perspective) is intertwined with an understanding of enterprise data assets (i.e., the data-oriented perspective). This article first compares the business- and data-oriented perspectives to describe how the two views mesh with each other. It then presents three elements in the data-oriented perspective that are collectively referred to as the data triad: (1) use, (2) design and storage, and (3) processes and people. In describing the data triad, this article highlights practices, architectural techniques, and example tools that are used to manage, access, analyze, and deliver data. By presenting different elements of the data-oriented perspective, this article broadly and concretely describes the data triad and how it can play a role in the redefined scope of work for data-driven business managers.|https://doi.org/10.1016/j.bushor.2016.06.001|https://www.sciencedirect.com/science/article/pii/S0007681316300519||||2016|Managerial work in the realm of the digital universe: The role of the data triad|Vijay Khatri|article|KHATRI2016673|||Business Horizons|00076813|6|59||CYBERSECURITY IN 2016: PEOPLE, TECHNOLOGY, AND PROCESSES|||20209|2,174|Q1|87|64|278|2475|2066|227|6,68|38,67|United Kingdom|Western Europe|1957-2020|Business and International Management (Q1); Marketing (Q1)|7,443|6.361|0.00571|397887245|847373672
||Artificial intelligence, Big Data, database, deep learning, registries, repository||e97-e103||Modern artificial intelligence techniques have solved some previously intractable problems and produced impressive results in selected medical domains. One of their drawbacks is that they often need very large amounts of data. Pre-existing datasets in the form of national cancer registries, image/genetic depositories and clinical datasets already exist and have been used for research. In theory, the combination of healthcare Big Data with modern, data-hungry artificial intelligence techniques should offer significant opportunities for artificial intelligence development, but this has not yet happened. Here we discuss some of the structural reasons for this, barriers preventing artificial intelligence from making full use of existing datasets, and make suggestions as to enable progress. To do this, we use the framework of the 6Vs of Big Data and the FAIR criteria for data sharing and availability (Findability, Accessibility, Interoperability, and Reuse). We share our experience in navigating these barriers through The Brain Tumour Data Accelerator, a Brain Tumour Charity-supported initiative to integrate fragmented patient data into an enriched dataset. We conclude with some comments as to the limits of such approaches.|https://doi.org/10.1016/j.clon.2021.11.040|https://www.sciencedirect.com/science/article/pii/S0936655521004593||||2022|Registries, Databases and Repositories for Developing Artificial Intelligence in Cancer Care|J.W. Wang and M. Williams|article|WANG2022e97|||Clinical Oncology|09366555|2|34||Artificial Intelligence in Radiation Therapy|||||||||||||||||||||400106011|902190112
||Trajectory;Sequences;Time series analysis;Education;Employment;joint sequence analysis;optimal matching;missing values;time series clustering;data quality||2620-2627|2017 IEEE International Conference on Big Data (Big Data)|The goal of this paper is to investigate the impact of missing values in categorical time series sequences on common data analysis tasks. Being able to more effectively identify patterns in socio-demographic longitudinal data is an important component in a number of social science settings. However, performing fundamental analytical operations, such as clustering for grouping these data based on similarity patterns, is challenging due to the categorical and multi-dimensional nature of the data, and their corruption by missing and inconsistent values. To study these data quality issues, we employ longitudinal sequence data representations, a similarity measure designed for categorical and longitudinal data, together with state-of-the art clustering methodologies reliant on hierarchical algorithms. The key to quantifying the similarity and difference among data records is a distance metric. Given the categorical nature of our data, we employ an “edit” type distance using Optimal Matching (OM). Because each data record has multiple variables of different types, we investigate the impact of mixing these variables in a single similarity measure. Between variables with binary values and those with multiple nominal values, we find that the ability to overcome missing data problems is harder in the nominal domain versus the binary domain. Additionally, artificial clusters introduced by the alignment of leading missing values can be resolved by tuning the missing value substitution cost parameter.|10.1109/BigData.2017.8258222|||||2017|Data quality challenges with missing values and mixed types in joint sequence analysis|Lazar, Alina and Jin, Ling and Spurlock, C. Anna and Wu, Kesheng and Sim, Alex|inproceedings|8258222||Dec|||||||||||||||||||||||||||400845929|42
||Sparks;Web services;Clustering algorithms;Big Data;Quality of service;Computer science;Parallel processing;Skyline;big data;Spark;Hadoop;parallelization||1-5|2019 IEEE 9th International Conference on Electronics Information and Emergency Communication (ICEIEC)|With the continuous development of the Internet, there are many web services with the same functional attributes but different functional attributes. It is urgent to find a web service that can satisfy itself quickly and efficiently from the massive web service data. This paper improves the traditional Skyline algorithm, divides the web service data set into regions, greatly reduces the data points without dominance, and saves memory usage. The improved Skyline algorithm can significantly improve the speed of Web service selection. However, the improved Skyline algorithm will still have insufficient computing resources when processing massive Web service data, resulting in a significant decrease in computing speed and even computer jam. In view of the above situation, this paper will parallelize the improved Skyline algorithm and parallelize the improved Skyline algorithm through the Spark platform. Experiments show that the parallelized Skyline algorithm can better handle massive Web service data.|10.1109/ICEIEC.2019.8784671|||||2019|Research on Web Service Selection Based on Parallel Skyline Algorithm|Xinmei, Liang and Luqin|inproceedings|8784671||July||2377844X|||||||||||||||||||||||||402498951|967917689
||Big data, Hydroinformatics||184-191||Big data is an increasingly hot concept in the past five years in the area of computer science, e-commence, and bioinformatics, because more and more data has been collected by the internet, remote sensing network, wearable devices and the Internet of Things. The big data technology provides techniques and analytical tools to handle large datasets, so that creative ideas and new values can be extracted from them. However, the hydroinformatics research community are not so familiar with big data. This paper provides readers who are embracing the data-rich era with a timely review on big data and its relevant technology, and then points out the relevance with hydroinformatics in three aspects.|https://doi.org/10.1016/j.proeng.2016.07.443|https://www.sciencedirect.com/science/article/pii/S187770581631832X||||2016|On Big Data and Hydroinformatics|Yiheng Chen and Dawei Han|article|CHEN2016184|||Procedia Engineering|18777058||154||12th International Conference on Hydroinformatics (HIC 2016) - Smart Water for the Future|||18700156717|0,320|-|74|0|5873|0|9870|5804|1,88|0,00|Netherlands|Western Europe|2009-2019|Engineering (miscellaneous)||||405478861|506091674
||Convenience store, Data mining, Machine learning, Energy consumption characteristics, Energy consumption affecting factor||120-136||This study applies big data mining, machine learning analysis technique and uses the Waikato Environment for Knowledge Analysis (WEKA) as a tool to discuss the convenience stores energy consumption performance in Taiwan which consists of (a). Influential factors of architectural space environment and geographical conditions; (b). Influential factors of management type; (c). Influential factors of business equipment; (d). Influential factors of local climatic conditions; (e). Influential factors of service area socioeconomic conditions. The survey data of 1,052 chain convenience stores belong to 7-Eleven, Family Mart and Hi-Life groups by Taiwan Architecture and Building Center (TABC) in 2014. The implicit knowledge will be explored in order to improve the traditional analysis technique which is unlikely to build a model for complex, inexact and uncertain dynamic energy consumption system for convenience stores. The analysis process comprises of (a). Problem definition and objective setting; (b). Data source selection; (c). Data collection; (d). Data preprocessing/preparation; (e). Data attributes selection; (f). Data mining and model construction; (g). Results analysis and evaluation; (h). Knowledge discovery and dissemination. The key factors influencing the convenience stores energy consumption and the influence intensity order can be explored by data attributes selection. The numerical prediction model for energy consumption is built by applying regression analysis and classification techniques. The optimization thresholds of various influential factors are obtained. The different cluster data are compared by using clustering analysis to verify the correlation between the factors influencing the convenience stores energy consumption characteristic. The implicit knowledge of energy consumption characteristic obtained by the aforesaid analysis can be used to (a). Provide the owners with accurate predicted energy consumption performance to optimize architectural space, business equipment and operations management mode; (b). The design planners can obtain the optimum design proposal of Cost Performance Ratio (C/P) by planning the thresholds of various key factors and the validation of prediction model; (c). Provide decision support for government energy and environment departments, to make energy saving and carbon emission reduction policies, in order to estimate and set the energy consumption scenarios of convenience store industry.|https://doi.org/10.1016/j.enbuild.2018.03.021|https://www.sciencedirect.com/science/article/pii/S0378778817334345||||2018|Analyze the energy consumption characteristics and affecting factors of Taiwan's convenience stores-using the big data mining approach|Chung-Feng {Jeffrey Kuo} and Chieh-Hung Lin and Ming-Hao Lee|article|JEFFREYKUO2018120|||Energy and Buildings|03787788||168|||||29359|1,737|Q1|184|705|2402|39338|15832|2394|6,33|55,80|Netherlands|Western Europe|1970, 1977-2020|Building and Construction (Q1); Civil and Structural Engineering (Q1); Electrical and Electronic Engineering (Q1); Mechanical Engineering (Q1)|51,254|5.879|0.04387|409417607|1347637955
||Performance evaluation;Energy consumption;Costs;Uncertainty;Distributed databases;Programming;Internet of Things||1-3|2022 IEEE Symposium on Computers and Communications (ISCC)|With the proliferation of raw Internet of Things (IoTs) data, Fog Computing is emerging as a computing paradigm for delay-sensitive streaming analytics with operators deploying big data distributed engines on Fog resources [1]. Nevertheless, the current (Cloud-based) distributed analytics solutions are unaware of the unique characteristics of Fog realms. For instance, task placement algorithms consider homogeneous underlying resources without considering the Fog nodes' heterogeneity and the non-uniform network connections, resulting in sub-optimal processing performance. Moreover, data quality can play an important role, where corrupted data, and network uncertainty may lead to less useful results. In turn, energy consumption can critically impact the overall cost and liveness of the underlying processing infrastructure. Specifically, scheduling tasks on nodes with energy-hungry profiles or battery-powered devices may temporarily be beneficial for the performance, but it may increase the overall cost, or/and the battery-powered devices may not be available when needed. A Fog-enabled analytics stack must allow users to optimize Fog-specific indicators or trade-offs among them. For instance, users may sacrifice a portion of the execution performance to minimize energy consumption or vice versa. Except for the performance issues raised by Fog, the state-of-the-art distributed processing engines offer only low-level procedural programming interfaces with operators facing a steep learning curve to master them. So, query abstractions are crucial for minimizing the deployment time, errors, and debugging.|10.1109/ISCC55528.2022.9913026|||||2022|Demo: The RAINBOW Analytics Stack for the Fog Continuum|Symeonides, Moysis and Trihinas, Demetris and Georgiou, Joanna and Kasioulis, Michalis and Pallis, George and Dikaiakos, Marios D. and Toliopoulos, Theodoros and Michailidou, Anna-Valentini and Gounaris, Anastasios|inproceedings|9913026||June||26427389|||||||||||||||||||||||||410133250|1380367770
||Sensors;Task analysis;Analytic hierarchy process;Training;Technological innovation;Data integrity;Simulation;Mobile crowdsensing;incentive mechanism;analytic hierarchy process;multi-attribute user selection;participation intention analysis||65384-65396||In the user selection phase of mobile crowdsensing, most existing incentive mechanisms focus on either single-attribute selection or random selection, which possibly lead to serious consequences such as low user enthusiasm, decreased task completion rate, and increased cost of platform consumption. To tackle these issues, in this paper, we propose a novel incentive mechanism MAIM, which is based on multi-attribute user selection and participation intention analysis function in mobile crowdsensing. In this mechanism, the sensing platform employs the analytic hierarchy process to determine the weights of three attributes: participation threshold, cost, and reputation. The weight calculation results of each sensing user with respect to each attribute are then integrated to obtain the sorted weight of each user, with which the sensing platform will then obtain the optimal user set. From the users' perspective, they can autonomously decide whether to accept task processing requests, as enabled by the participation intention analysis function, thereby voiding the absolute authority and control of the sensing platform over users and achieving a two-way selection between the sensing platform and the sensing users. Furthermore, the sensing platform establishes a score-based reputation reward to inspire active performers and utilizes a punishment mechanism to overawe malicious vandals, which substantially helps activize enthusiasm of user participation and improve sensing data quality. Simulation results indicate that the proposed MAIM has significantly improved the sensing task completion ratio and the budget surplus ratio compared with the existing incentive mechanisms in mobile crowdsensing.|10.1109/ACCESS.2018.2878761|||||2018|MAIM: A Novel Incentive Mechanism Based on Multi-Attribute User Selection in Mobile Crowdsensing|Xiong, Jinbo and Chen, Xiuhua and Tian, Youliang and Ma, Rong and Chen, Lei and Yao, Zhiqiang|article|8528409|||IEEE Access|21693536||6|||||21100374601|0,587|Q1|127|18036|24267|771081|116691|24200|4,48|42,75|United States|Northern America|2013-2020|Computer Science (miscellaneous) (Q1); Engineering (miscellaneous) (Q1); Materials Science (miscellaneous) (Q2)|105,968|3.367|0.15396|413652542|1905633267
||Mobile telephones, Call detail record (CDR) data, Traffic semantic analysis, Traffic zone division, Traffic zone attribute index, Travel patterns||278-291||Call detail record (CDR) data from mobile communication carriers offer an emerging and promising source of information for analysis of traffic problems. To date, research on insights and information to be gleaned from CDR data for transportation analysis has been slow, and there has been little progress on development of specific applications. This paper proposes the traffic semantic concept to extract traffic commuters’ origins and destinations information from the mobile phone CDR data and then use the extracted data for traffic zone division. A K-means clustering method was used to classify a cell-area (the area covered by a base stations) and tag a certain land use category or traffic semantic attribute (such as working, residential, or urban road) based on four feature data (including real-time user volume, inflow, outflow, and incremental flow) extracted from the CDR data. By combining the geographic information of mobile phone base stations, the roadway network within Beijing’s Sixth Ring Road was divided into a total of 73 traffic zones using another K-means clustering algorithm. Additionally, we proposed a traffic zone attribute-index to measure tendency of traffic zones to be residential or working. The calculated attribute-index values of 73 traffic zones in Beijing were consistent with the actual traffic and land-use data. The case study demonstrates that effective traffic and travel data can be obtained from mobile phones as portable sensors and base stations as fixed sensors, providing an opportunity to improve the analysis of complex travel patterns and behaviors for travel demand modeling and transportation planning.|https://doi.org/10.1016/j.trc.2015.06.007|https://www.sciencedirect.com/science/article/pii/S0968090X15002223||||2015|Traffic zone division based on big data from mobile phone base stations|Honghui Dong and Mingchao Wu and Xiaoqing Ding and Lianyu Chu and Limin Jia and Yong Qin and Xuesong Zhou|article|DONG2015278|||Transportation Research Part C: Emerging Technologies|0968090X||58||Big Data in Transportation and Traffic Engineering|||20893|3,185|Q1|133|327|834|17795|8956|824|10,15|54,42|United Kingdom|Western Europe|1993-2020|Automotive Engineering (Q1); Civil and Structural Engineering (Q1); Computer Science Applications (Q1); Management Science and Operations Research (Q1); Transportation (Q1)||||414507044|307592163
||Cloud systems, Quality of service, Service oriented architectures, Semantics, Ontologies, Linked data, Sensor data, Big data||307-323||Cloud Computing and Service Oriented Architectures have seen a dramatic increase of the amount of applications, services, management platforms, data, etc. gaining momentum for the necessity of new complex methods and techniques to deal with the vast heterogeneity of data sources or services. In this sense Quality of Service (QoS) seeks for providing an intelligent environment of self-management components based on domain knowledge in which cloud components can be optimized easing the transition to an advanced governance environment. On the other hand, semantics and ontologies have emerged to afford a common and standard data model that eases the interoperability, integration and monitoring of knowledge-based systems. Taking into account the necessity of an interoperable and intelligent system to manage QoS in cloud-based systems and the emerging application of semantics in different domains, this paper reviews the main approaches for semantic-based QoS management as well as the principal methods, techniques and standards for processing and exploiting diverse data providing advanced real-time monitoring services. A semantic-based framework for QoS management is also outlined taking advantage of semantic technologies and distributed datastream processing techniques. Finally a discussion of existing efforts and challenges is also provided to suggest future directions.|https://doi.org/10.1016/j.future.2013.10.015|https://www.sciencedirect.com/science/article/pii/S0167739X1300232X||||2014|Semantic-based QoS management in cloud systems: Current status and future challenges|Dimitrios Kourtesis and Jose María Alvarez-Rodríguez and Iraklis Paraskakis|article|KOURTESIS2014307|||Future Generation Computer Systems|0167739X||32||Special Section: The Management of Cloud Systems, Special Section: Cyber-Physical Society and Special Section: Special Issue on Exploiting Semantic Technologies with Particularization on Linked Data over Grid and Cloud Architectures|||12264|1,262|Q1|119|798|1935|36054|16877|1868|9,11|45,18|Netherlands|Western Europe|1984-2021|Computer Networks and Communications (Q1); Hardware and Architecture (Q1); Software (Q1)||||415283139|562237118
||quality assessment, Biba’s model, data quality, quality dimension, information flow, information trustworthiness, information integrity, noninterference, Integrity, Clark-Wilson model, information quality, security requirements, information security|35|||The understanding and promotion of integrity in information security has traditionally been underemphasized or even ignored. From implantable medical devices and electronic voting to vehicle control, the critical importance of information integrity to our well-being has compelled review of its treatment in the literature. Through formal information flow models, the data modification view, and the relationship to data quality, information integrity will be surveyed. Illustrations are given for databases and information trustworthiness. Integrity protection is advancing but lacks standardization in terminology and application. Integrity must be better understood, and pursued, to achieve devices and systems that are beneficial and safe for the future.|10.1145/3436817|https://doi.org/10.1145/3436817|New York, NY, USA|Association for Computing Machinery||2021|Information Integrity: Are We There Yet?|Harley, Kelsey and Cooper, Rodney|article|10.1145/3436817|33|feb|ACM Comput. Surv.|03600300|2|54|March 2022||||23038|2,079|Q1|163|112|365|15859|6978|365|19,16|141,60|United States|Northern America|1969-2020|Computer Science (miscellaneous) (Q1); Theoretical Computer Science (Q1)|10,790|10.282|0.01438|420702141|1517405264
||Government;Data visualization;Production;Big Data;Agriculture;Mobile handsets;Servers;GIS;Big-Data;Geospatial;Agriculture;map-reduce||1-7|2021 International Conference on Electronic Engineering (ICEEM)|In the past few decades, the use of geographic information systems (GIS) was efficient with servers that could handle the amount of data used. However, as geographical big data grows in size and complexity, storing, managing, processing, analyzing, visualizing, and confirming data quality becomes more difficult. Academia, industry, government, and other institutions are increasingly interested in this information. It’s known as Big Data. Since that kind of data recently became massive, there was a need to develop methods to deal with big data and analyze it to keep pace with development. In this paper, we review the previous studies that involve both Big Data and GIS in different applications. Moreover, we focus on the field of agriculture, which is considered one of the most important sources of the economy. Produced results in this research area help decision-makers to make sound executive steps to reach better production.|10.1109/ICEEM52022.2021.9480634|||||2021|Empowering GIS with Big Data: A review of recent advances|Elsahlamy, Ebtsam and Eshra, Abeer and Eshra, Nadia and El-Fishawy, Nawal|inproceedings|9480634||July|||||||||||||||||||||||||||424308863|42
||Big data analytics, Customer agility, Effective use of data, New product success||135-143||The complexity that characterises the dynamic nature of the various environmental factors makes it very compelling for firms to be capable of addressing the changing customers' needs. The current study examines the role of big data in new product success. We develop a qualitative research with case study approach to look at this. Specifically, we look at multiple cases to get in-depth understanding of customer agility for new product success with big data analytics. The findings of the study provide insight into the role of customer agility in new product success. This study unpacks the interconnectedness of the effective use of data aggregation tools, the effectiveness of data analysis tools and customer agility. It also explores the link between all of these factors and new product success. The study is reasonably telling in that it shows that the effective use of data aggregation and data analysis tools results in customer agility which in itself explains how an organisation senses and responds speedily to opportunities for innovation in the competitive marketing environment. The current study provides significant theoretical contributions by providing evidence for the role of big data analytics, big data aggregation tools, customer agility, organisational slack and environmental turbulence in new product success.|https://doi.org/10.1016/j.indmarman.2019.09.010|https://www.sciencedirect.com/science/article/pii/S0019850118304735||||2020|Understanding market agility for new product success with big data analytics|Nick Hajli and Mina Tajvidi and Ayantunji Gbadamosi and Waqar Nadeem|article|HAJLI2020135|||Industrial Marketing Management|00198501||86|||||22792|2,022|Q1|136|314|465|27989|3787|450|6,86|89,14|United States|Northern America|1971-2020|Marketing (Q1)|16,291|6.960|0.00901|429407451|223518748
||Analytical models;Renewable energy sources;Codes;Databases;Computational modeling;Data integrity;Metadata;Metadata Management Model;Cloud Computing;Physical Business Intelligence Platform;Big Data||1656-1659|2022 International Conference on Electronics and Renewable Systems (ICEARS)|Through the method of metadata management development, it can give full play to its advantages and make up for its disadvantages. In order to fully grasp the composition, conversion, analysis and processing process of data in the platform, from metadata sources, metadata scope, metadata classification, metadata users, metadata integration project development methods, metadata models and metadata standards, metadata management the implementation of the system and other aspects expounded the metadata management strategy in the business intelligence system. Effective metadata management has increased the usability of the platform by 5.6% and the data quality of the platform by 7.8%.|10.1109/ICEARS53579.2022.9752191|||||2022|Analysis on the Design and Implementation of the Metadata Management Model in the Cloud Computing Business Intelligence Platform|Zhao, Lixia and Jin, Wei|inproceedings|9752191||March|||||||||||||||||||||||||||433165986|42
||Visualization;Data security;Process control;Collaboration;Big Data;Power grids;Resource management;Electric power data;Data management;Data sharing;Data security||98-102|2021 IEEE 2nd International Conference on Information Technology, Big Data and Artificial Intelligence (ICIBA)|With the intensified application of power information systems and the advent of the “big data” era, higher requirements are put forward for power data resource management and power data security. Electric power companies have carried out research on key technologies for data management, established a three-level management system at the provincial, prefectural and county levels, built a panoramic view of data resources, a data operation management platform, a data negative list sharing mechanism, and a data security protection mechanism, which were applied to all aspects of data management and data governance. Through the support of business processes, data standards, data quality, etc., it has effectively improved the management efficiency of power data, improved the company's data management level, promoted business collaboration and efficiency.|10.1109/ICIBA52610.2021.9687865|||||2021|Research and application of power data management key technology|Yan, Peipei and Li, Feng and Xiang, Zhiwei and Li, Mingxuan and Fan, Shuming|inproceedings|9687865||Dec||||2|||||||||||||||||||||||433285986|42
ICCAI '22|Tianjin, China|component microservice, knowledge generalization, Enterprise knowledge graph, intelligent visualization|6|259–264|Proceedings of the 8th International Conference on Computing and Artificial Intelligence|Knowledge Graph is widely used in artificial intelligence fields such as intelligent search, intelligent recommendation and intelligent question answering, and EKG (Enterprise Knowledge Graph) is an important foundation for enterprises to build intelligent platforms. This paper proposes the design idea of constructing EKG based on the five dimensions of human, financial, material, time and information. First, the EKG framework is built by applying component microservices, pre-construction and business orchestration; Mining and analyzing prediction methods, and finally designing an intelligent visual EKG with auxiliary decision-making functions.|10.1145/3532213.3532252|https://doi.org/10.1145/3532213.3532252|New York, NY, USA|Association for Computing Machinery|9781450396110|2022|Design and Generalization of Enterprise Knowledge Graph|Ming Tang, Chun and Tao, Peng and Li, Yan|inproceedings|10.1145/3532213.3532252|||||||||||||||||||||||||||||434033308|42
||mHealth frameworks, wearable sensing, mobile sensing frameworks, mobile sensing, mHealth sensing|28|||With the widespread use of smartphones and wearable health sensors, a plethora of mobile health (mHealth) applications to track well-being, run human behavioral studies, and clinical trials have emerged in recent years. However, the design, development, and deployment of mHealth applications is challenging in many ways. To address these challenges, several generic mobile sensing frameworks have been researched in the past decade. Such frameworks assist developers and researchers in reducing the complexity, time, and cost required to build and deploy health-sensing applications. The main goal of this article is to provide the reader with an overview of the state-of-the-art of health-focused generic mobile and wearable sensing frameworks. This review gives a detailed analysis of functional and non-functional features of existing frameworks, the health studies they were used in, and the stakeholders they support. Additionally, we also analyze the historical evolution, uptake, and maintenance after the initial release. Based on this analysis, we suggest new features and opportunities for future generic mHealth sensing frameworks.|10.1145/3422158|https://doi.org/10.1145/3422158|New York, NY, USA|Association for Computing Machinery||2021|Mobile and Wearable Sensing Frameworks for MHealth Studies and Applications: A Systematic Review|Kumar, Devender and Jeuris, Steven and Bardram, Jakob E. and Dragoni, Nicola|article|10.1145/3422158|8|dec|ACM Trans. Comput. Healthcare|26911957|1|2|January 2021||||||||||||||||||||||436981045|1983512862
||Citizen Science, Crowdsourcing, Machine learning, Volunteer engagement||185-219|Space Science and Public Engagement|The past two decades has seen a tremendous rise in citizen science and crowdsourcing techniques as a means to carry out ground-breaking research while at the same time engage the general public in the wonders of space science. This article reviews some of the recent advances made in this realm as well as lessons learned from the unique perspective of the author’s role as a cofounder of the Zooniverse citizen science platform and practicing astrophysics researcher. I briefly describe the factors that led to the recent rise of citizen science including the formation of governance bodies at national and international levels, and the adoption by Federal Agencies within the United States government. I address concerns raised by research colleagues on the validity of citizen science as a research methodology, and then describe several key metrics for the success of citizen science including the link between data quality and publications, and the critical role that motivation and engagement of volunteer participants play in project success. I use the Green Pea galaxies discovered by Galaxy Zoo volunteers and an aurora-like phenomenon known as STEVE discovered by Aurorasaurus volunteers as examples of how, with the right tools and support, non-professional volunteers can make key contributions to space science. I then describe the role that machine learning can play when judiciously teamed with citizen scientists to tackle the ever-growing challenge of big data and close with some reflections on what it takes to support and manage a large platform like the Zooniverse.|https://doi.org/10.1016/B978-0-12-817390-9.00009-9|https://www.sciencedirect.com/science/article/pii/B9780128173909000099||Elsevier|978-0-12-817390-9|2021|Chapter 10 - From Green Peas to STEVE: Citizen Science Engagement in Space Science|Lucy Fortson|incollection|FORTSON2021185||||||||||Amy Paige Kaminski|||||||||||||||||||439451666|42
||Capabilities, Entanglement view, Big data analytics, Hierarchical modeling||113-131||The recent interest in big data has led many companies to develop big data analytics capability (BDAC) in order to enhance firm performance (FPER). However, BDAC pays off for some companies but not for others. It appears that very few have achieved a big impact through big data. To address this challenge, this study proposes a BDAC model drawing on the resource-based theory (RBT) and the entanglement view of sociomaterialism. The findings show BDAC as a hierarchical model, which consists of three primary dimensions (i.e., management, technology, and talent capability) and 11 subdimensions (i.e., planning, investment, coordination, control, connectivity, compatibility, modularity, technology management knowledge, technical knowledge, business knowledge and relational knowledge). The findings from two Delphi studies and 152 online surveys of business analysts in the U.S. confirm the value of the entanglement conceptualization of the higher-order BDAC model and its impact on FPER. The results also illuminate the significant moderating impact of analytics capability–business strategy alignment on the BDAC–FPER relationship.|https://doi.org/10.1016/j.ijpe.2016.08.018|https://www.sciencedirect.com/science/article/pii/S0925527316302110||||2016|How to improve firm performance using big data analytics capability and business strategy alignment?|Shahriar Akter and Samuel Fosso Wamba and Angappa Gunasekaran and Rameshwar Dubey and Stephen J. Childe|article|AKTER2016113|||International Journal of Production Economics|09255273||182|||||19165|2,406|Q1|185|327|891|21712|8124|881|8,31|66,40|Netherlands|Western Europe|1991-2021|Business, Management and Accounting (miscellaneous) (Q1); Economics and Econometrics (Q1); Industrial and Manufacturing Engineering (Q1); Management Science and Operations Research (Q1)|32,606|7.885|0.0228|439877332|850534974
ESEC/FSE 2021|Athens, Greece|Apache Spark, Logging, Monitoring|11|516–526|Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering|To analyze large-scale data efficiently, developers have created various big data processing frameworks (e.g., Apache Spark). These big data processing frameworks provide abstractions to developers so that they can focus on implementing the data analysis logic. In traditional software systems, developers leverage logging to monitor applications and record intermediate states to assist workload understanding and issue diagnosis. However, due to the abstraction and the peculiarity of big data frameworks, there is currently no effective monitoring approach for big data applications. In this paper, we first manually study 1,000 randomly sampled Spark-related questions on Stack Overflow to study their root causes and the type of information, if recorded, that can assist developers with motioning and diagnosis. Then, we design an approach, DPLOG, which assists developers with monitoring Spark applications. DPLOG leverages statistical sampling to minimize performance overhead and provides intermediate information and hint/warning messages for each data processing step of a chained method pipeline. We evaluate DPLOG on six benchmarking programs and find that DPLOG has a relatively small overhead (i.e., less than 10% increase in response time when processing 5GB data) compared to without using DPLOG, and reduce the overhead by over 500% compared to the baseline. Our user study with 20 developers shows that DPLOG can reduce the needed time to debug big data applications by 63% and the participants give DPLOG an average of 4.85/5 for its usefulness. The idea of DPLOG may be applied to other big data processing frameworks, and our study sheds light on future research opportunities in assisting developers with monitoring big data applications.|10.1145/3468264.3468613|https://doi.org/10.1145/3468264.3468613|New York, NY, USA|Association for Computing Machinery|9781450385626|2021|Would You like a Quick Peek? Providing Logging Support to Monitor Data Processing in Big Data Applications|Wang, Zehao and Zhang, Haoxiang and Chen, Tse-Hsun (Peter) and Wang, Shaowei|inproceedings|10.1145/3468264.3468613|||||||||||||||||||||||||||||441320615|42
||Industries;Rails;Data models;Rail transportation;Systematics;Decision making;data quality;rail;quality by design;data quality schema||3792-3799|2017 IEEE International Conference on Big Data (Big Data)|The railways worldwide are increasingly looking to the integration of their data resources coupled with advanced analytics to enhance traffic management, to provide new insights on the health of infrastructure assets, to provide soft linkages to other transport modes, and ultimately to enable them to better serve their customers. As in many industrial sectors, over the past decade the rail industry has been investing heavily in sensing technologies that record every aspect of the operation of the railway network. However, as any data scientist knows, it does not matter how good an algorithm is, if you put rubbish in, you get rubbish out; and as the traditional industry model of working with data only within the system that it was collected by becomes increasingly fragile, the industry is discovering that it knows less than it thought about the data it is gathering. When coupled with legacy data resources of unknown accuracy, such as design diagrams for assets that in many cases are decades old, the rail industry now faces a crisis in which its data may become essentially worthless due to a poor understanding of the quality of its data. This paper reports the findings of the first phase of a three-phase systematic review of literature about how data quality can be managed and evaluated in the rail domain. It begins by discussing why data quality matters in a rail context, before going on to define the quality, introduce and expand the concept of a data quality schema.|10.1109/BigData.2017.8258380|||||2017|Understanding data quality: Ensuring data quality by design in the rail industry|Fu, Qian and Easton, John M.|inproceedings|8258380||Dec|||||||||||||||||||||||||||444158159|42
ICBDC '22|Shenzhen, China|Xie_Beni index, Cluster analysis, Water resources allocation, Fuzzy data mining, Correlation analysis|8|6–13|Proceedings of the 7th International Conference on Big Data and Computing|To focus on the problem of water shortage and the contradictory relationship between water supply and demand in China, the water supply case sets of each administrative region in Hubei Province were studied in order to provide reference for the policy formulation of regional water allocation. Based on the construction of a multi-dimensional data index system, this paper applied the fuzzy hierarchical clustering method to cluster and characterize the water distribution cases in administrative regions, and used the Xie_beni index to test the validity of the clustering results. The results show that water allocation in different administrative regions of Hubei province by industry is divided into four categories, showing different industry-oriented water allocation characteristics. By Xie_Beni clustering validity index test, the minimum value of Xie_Beni index is 12.26, and the relative error of irrigation area water demand prediction is less than 15%, which confirms the validity of the method. Xie_Beni index can better test the validity of clustering results. However, there are still some drawbacks. In the case of increasing the number of clusters, the Xie_beni index will gradually lose its judgment ability. Besides, the method requires high data quality of the research object, and requires each index to be quantifiable and rich historical data.|10.1145/3545801.3545803|https://doi.org/10.1145/3545801.3545803|New York, NY, USA|Association for Computing Machinery|9781450396097|2022|Regional Water Resources Allocation Based on Fuzzy Data Mining|Liu, Junhong and Ren, Tianqi and Huang, Ziyue and Tu, Yan|inproceedings|10.1145/3545801.3545803|||||||||||||||||||||||||||||445470787|42
||Smart manufacturing, Big data, Cloud computing, Cloud manufacturing, Internet of things, NoSQL||101861||Advanced manufacturing is one of the core national strategies in the US (AMP), Germany (Industry 4.0) and China (Made-in China 2025). The emergence of the concept of Cyber Physical System (CPS) and big data imperatively enable manufacturing to become smarter and more competitive among nations. Many researchers have proposed new solutions with big data enabling tools for manufacturing applications in three directions: product, production and business. Big data has been a fast-changing research area with many new opportunities for applications in manufacturing. This paper presents a systematic literature review of the state-of-the-art of big data in manufacturing. Six key drivers of big data applications in manufacturing have been identified. The key drivers are system integration, data, prediction, sustainability, resource sharing and hardware. Based on the requirements of manufacturing, nine essential components of big data ecosystem are captured. They are data ingestion, storage, computing, analytics, visualization, management, workflow, infrastructure and security. Several research domains are identified that are driven by available capabilities of big data ecosystem. Five future directions of big data applications in manufacturing are presented from modelling and simulation to real-time big data analytics and cybersecurity.|https://doi.org/10.1016/j.rcim.2019.101861|https://www.sciencedirect.com/science/article/pii/S0736584519300559||||2020|Manufacturing big data ecosystem: A systematic literature review|Yesheng Cui and Sami Kara and Ka C. Chan|article|CUI2020101861|||Robotics and Computer-Integrated Manufacturing|07365845||62|||||18080|1,561|Q1|93|139|404|6448|2949|400|7,35|46,39|United Kingdom|Western Europe|1984-1994, 1996-2021|Computer Science Applications (Q1); Control and Systems Engineering (Q1); Industrial and Manufacturing Engineering (Q1); Mathematics (miscellaneous) (Q1); Software (Q1)|6,215|5.666|0.0057|447403041|1967447291
||Virtual documents (VD), Source document, Hadoop file System (HDFS), DW Ranking algorithm, Top  algorithm.||371-379||The term big data has come into use in recent years. It is used to refer to the ever-increasing amount of data that organizations are storing, processing and analyzing. An Interesting fact with bigdata is that it differ in Volume, Variety, Velocity characteristics which makes it difficult to process using the conventional Database Management System. Hence there is a need of schema less Management Systems even this will never be complete solution to bigdata analysis since the processing has no focus on the semantic information as they consider only the structural information. Content Management System like Wikipedia stores and links huge amount of documents and files. There is lack of semantic linking and analysis in such systems even though this kind of CMS uses clusters and distributed framework for storing big data. The retrieved references for a particular article are random and enormous. In order to reduce the number of references for a selected content there is a need for semantic matching. In this paper we propose framework which make use of the distributed parallel processing capability of Hadoop Distributed File System (HDFS) to perform semantic analysis over the volume of documents (bigdata) to find the best matched source document from the collection source documents for the same virtual document.|https://doi.org/10.1016/j.procs.2015.06.043|https://www.sciencedirect.com/science/article/pii/S1877050915013678||||2015|Semantic Retrieval of Relevant Sources for Large Scale Virtual Documents|R. Priyadarshini and Latha Tamilselvan and T. Khuthbudin and S. Saravanan and S. Satish|article|PRIYADARSHINI2015371|||Procedia Computer Science|18770509||54||Eleventh International Conference on Communication Networks, ICCN 2015, August 21-23, 2015, Bangalore, India Eleventh International Conference on Data Mining and Warehousing, ICDMW 2015, August 21-23, 2015, Bangalore, India Eleventh International Conference on Image and Signal Processing, ICISP 2015, August 21-23, 2015, Bangalore, India|||19700182801|0,334|-|76|1907|6483|38511|13722|6359|2,09|20,19|Netherlands|Western Europe|2010-2020|Computer Science (miscellaneous)||||449480896|2108686752
|||6|33–38|||10.1145/2430456.2430466|https://doi.org/10.1145/2430456.2430466|New York, NY, USA|Association for Computing Machinery||2013|The Data Analytics Group at the Qatar Computing Research Institute|Beskales, George and Das, Gautam and Elmagarmid, Ahmed K. and Ilyas, Ihab F. and Naumann, Felix and Ouzzani, Mourad and Papotti, Paolo and Quiane-Ruiz, Jorge and Tang, Nan|article|10.1145/2430456.2430466||jan|SIGMOD Rec.|01635808|4|41|December 2012||||13622|0,372|Q2|142|39|81|808|127|57|1,67|20,72|United States|Northern America|1969, 1973-1978, 1981-2020|Information Systems (Q2); Software (Q2)|1,819|0.775|7.5E-4|449727680|962972343
Advances in Clinical Chemistry||Translational biomarkers, Omics, Big data, Artificial intelligence, Clinical trials||191-232||In this chapter we discuss the past, present and future of clinical biomarker development. We explore the advent of new technologies, paving the way in which health, medicine and disease is understood. This review includes the identification of physicochemical assays, current regulations, the development and reproducibility of clinical trials, as well as, the revolution of omics technologies and state-of-the-art integration and analysis approaches.|https://doi.org/10.1016/bs.acc.2020.08.002|https://www.sciencedirect.com/science/article/pii/S0065242320300913||Elsevier||2021|Chapter Four - Translational biomarkers in the era of precision medicine|Laura Bravo-Merodio and Animesh Acharjee and Dominic Russ and Vartika Bisht and John A. Williams and Loukia G. Tsaprouni and Georgios V. Gkoutos|incollection|BRAVOMERODIO2021191||||00652423||102||||Gregory S. Makowski|16759|1,330|Q1|46|59|118|10307|495|18|4,45|174,69|United States|Northern America|1958-1973, 1975-1978, 1980-1981, 1983, 1985-1987, 1989-1990, 1992-1994, 1996, 1998-2001, 2003-2020|Chemistry (miscellaneous) (Q1); Clinical Biochemistry (Q1)|1,701|5.394|0.00219|449771965|27680084
||Big data, Internet of vehicle, Electric vehicles, Data cleaning, Battery management system, Battery state estimation||119292||Battery is one of the most important and costly devices in electric vehicles (EVs). Developing an efficient battery management method is of great significance to enhancing vehicle safety and economy. Recently developed big-data and cloud platform computing technologies bring a bright perspective for efficient utilization and protection of vehicle batteries. However, a reliable data transmission network and a high-quality cloud battery dataset are indispensable to enable this benefit. This paper makes the first effort to systematically solve data quality problems in cloud-based vehicle battery monitoring and management by developing a novel integrated battery data cleaning framework. In the first stage, the outlier samples are detected by analyzing the temporal features in the battery data time series. The outlier data in the dataset can be accurately detected to avoid their impacts on battery monitoring and management. Then, the abnormal samples, including the noise polluted data and missing value, are restored by a novel future fusion data restoring model. The real electric bus operation data collected by a cloud-based battery monitoring and management platform are used to verify the performance of the developed data cleaning method. More than 93.3% of outlier samples can be detected, and the data restoring error can be limited to 2.11%, which validates the effectiveness of the developed methods. The proposed data cleaning method provides an effective data quality assessment tool in cloud-based vehicle battery management, which can further boost the practical application of the vehicle big data platform and Internet of vehicle.|https://doi.org/10.1016/j.apenergy.2022.119292|https://www.sciencedirect.com/science/article/pii/S030626192200647X||||2022|Data cleaning and restoring method for vehicle battery big data platform|Shuangqi Li and Hongwen He and Pengfei Zhao and Shuang Cheng|article|LI2022119292|||Applied Energy|03062619||320|||||28801|3,035|Q1|212|1729|5329|100144|56804|5304|10,59|57,92|United Kingdom|Western Europe|1975-2020|Building and Construction (Q1); Energy (miscellaneous) (Q1); Management, Monitoring, Policy and Law (Q1); Mechanical Engineering (Q1)|122,712|9.746|0.15329|451088076|892883729
||Big Data;Phase frequency detectors;Lakes;Picture archiving and communication systems;Databases;Task analysis;Proposals;Integrity constraints;data dependencies||4717-4736||Besides the conventional schema-oriented tasks, data dependencies are recently revisited for data quality applications, such as violation detection, data repairing and record matching. To address the variety and veracity issues of big data, data dependencies have been extended as data quality rules to adapt to various data types, ranging from (1) categorical data with equality relationships to (2) heterogeneous data with similarity relationships, and (3) numerical data with order relationships. In this survey, we briefly review the recent proposals on data dependencies categorized into the aforesaid types of data. In addition to (a) the concepts of these data dependency notations, we investigate (b) the extension relationships between data dependencies, e.g., conditional functional dependencies (CFDs) extend the conventional functional dependencies (FDs). It forms a family tree of extensions, mostly rooted in FDs, helping us understand the expressive power of various data dependencies. Moreover, we summarize (c) the discovery of dependencies from data, since data dependencies are often unlikely to be manually specified in a traditional way, given the huge volume and high variety of big data. We further outline (d) the applications of the extended data dependencies, in particular in data quality practice. It guides users to select proper data dependencies with sufficient expressive power and reasonable discovery cost. Finally, we conclude with several directions of future studies on the emerging data.|10.1109/TKDE.2020.3046443|||||2022|Data Dependencies Extended for Variety and Veracity: A Family Tree|Song, Shaoxu and Gao, Fei and Huang, Ruihong and Wang, Chaokun|article|9302878||Oct|IEEE Transactions on Knowledge and Data Engineering|15582191|10|34|||||||||||||||||||||||453269524|1598944404
||Data-driven modelling; car-following; physics-guided machine learning; online learning; just-in-time simulation; real-time simulation; symbiotic simulation; digital twin||||Symbiotic simulation systems that incorporate data-driven methods (such as machine/deep learning) are effective and efficient tools for just-in-time (JIT) operational decision making. With the growing interest on Digital Twin City, such systems are ideal for real-time microscopic traffic simulation. However, learning-based models are heavily biased towards the training data and could produce physically inconsistent outputs. In terms of microscopic traffic simulation, this could lead to unsafe driving behaviours causing vehicle collisions in the simulation. As for symbiotic simulation, this could severely affect the performance of real-time base simulation model resulting in inaccurate or unrealistic forecasts, which could, in turn, mislead JIT what-if analysis. To overcome this issue, a physics-guided data-driven modelling paradigm should be adopted so that the resulting model could capture both accurate and safe driving behaviours. However, very few works exist in the development of such a car-following model that can balance between simulation accuracy and physical consistency. Therefore, in this paper, a new “jointly-trained physics-guided Long Short-term Memory (JTPG-LSTM)” neural network, is proposed and integrated to a dynamic data-driven simulation system to capture dynamic car-following behaviours. An extensive set of experiments was conducted to demonstrate the advantages of the proposed model from both modelling and simulation perspectives.|10.1145/3558555|https://doi.org/10.1145/3558555|New York, NY, USA|Association for Computing Machinery||2022|Dynamic Data-Driven Microscopic Traffic Simulation Using Jointly Trained Physics-Guided Long Short-Term Memory|Naing, Htet and Cai, Wentong and Nan, Hu and Tiantian, Wu and Liang, Yu|article|10.1145/3558555||sep|ACM Trans. Model. Comput. Simul.|10493301||||Just Accepted|||||||||||||||||||||454352478|992473651
||Computational chemistry, Consensus prediction, Database, Nuclear receptors, Toxicology||133422||According to Eurostat, the EU production of chemicals hazardous to health reached 211 million tonnes in 2019. Thus, the possibility that some of these chemical compounds interact negatively with the human endocrine system has received, especially in the last decade, considerable attention from the scientific community. It is obvious that given the large number of chemical compounds it is impossible to use in vitro/in vivo tests for identifying all the possible toxic interactions of these chemicals and their metabolites. In addition, the poor availability of highly curated databases from which to retrieve and download the chemical, structure, and regulative information about all food contact chemicals has delayed the application of in silico methods. To overcome these problems, in this study we use robust computational approaches, based on a combination of highly curated databases and molecular docking, in order to screen all food contact chemicals against the nuclear receptor family in a cost and time-effective manner.|https://doi.org/10.1016/j.chemosphere.2021.133422|https://www.sciencedirect.com/science/article/pii/S0045653521038960||||2022|Computational methods on food contact chemicals: Big data and in silico screening on nuclear receptors family|Pietro Cozzini and Francesca Cavaliere and Giulia Spaggiari and Gianluca Morelli and Marco Riani|article|COZZINI2022133422|||Chemosphere|00456535||292|||||24657|1,632|Q1|248|3039|6460|173496|46391|6428|7,04|57,09|United Kingdom|Western Europe|1972-2021|Chemistry (miscellaneous) (Q1); Environmental Chemistry (Q1); Environmental Engineering (Q1); Health, Toxicology and Mutagenesis (Q1); Medicine (miscellaneous) (Q1); Pollution (Q1); Public Health, Environmental and Occupational Health (Q1)|127,067|7.086|0.0979|455188028|1400777600
IDEAS '15|Yokohama, Japan|Real-Time Spatial Big Data, Feedback Control Scheduling, Quality of Service, Transaction, Heterogeneous Real-Time Geospatial Data, Geographic Information System|6|174–179|Proceedings of the 19th International Database Engineering &amp; Applications Symposium|Geographic Information System (GIS) is a computer system designed to capture, store, manipulate, analyze, manage, and present all types of spatial data. Spatial data, whether captured through remote sensors or large scale simulations becomes more and big and heterogenous. As a result, structured data and unstructured content are simultaneously accessed via an integrated user interface. The issue of real-time and heterogeneity is extremely important for taking effective decision. Thus, heterogeneous real-time spatial data management is a very active research domain nowadays. Existing research are interested in querying of real-time spatial data and their updates without taking into account the heterogeneity of real-time geospatial data. In this paper, we propose the use of the real-time Spatial Big Data and we define a new architecture called FCSA-RTSBD (Feedback Control Scheduling Architecture for Real-Time Spatial Big Data). The main objectives of this architecture are the following: take in account the heterogeneity of data, guarantee the data freshness, enhance the deadline miss ratio even in the presence of conflicts and finally satisfy the requirements of users by the improving of the quality of service (QoS).|10.1145/2790755.2790774|https://doi.org/10.1145/2790755.2790774|New York, NY, USA|Association for Computing Machinery|9781450334143|2015|A New QoS Management Approach in Real-Time GIS with Heterogeneous Real-Time Geospatial Data Using a Feedback Control Scheduling|Hamdi, Sana and Bouazizi, Emna and Faiz, Sami|inproceedings|10.1145/2790755.2790774|||||||||||||||||||||||||||||457034711|42
||Integrated modelling, Environmental sensors, Population health, Environmental health, Big data||238-246||Sensors are becoming ubiquitous in everyday life, generating data at an unprecedented rate and scale. However, models that assess impacts of human activities on environmental and human health, have typically been developed in contexts where data scarcity is the norm. Models are essential tools to understand processes, identify relationships, associations and causality, formalize stakeholder mental models, and to quantify the effects of prevention and interventions. They can help to explain data, as well as inform the deployment and location of sensors by identifying hotspots and areas of interest where data collection may achieve the best results. We identify a paradigm shift in how the integration of models and sensors can contribute to harnessing ‘Big Data’ and, more importantly, make the vital step from ‘Big Data’ to ‘Big Information’. In this paper, we illustrate current developments and identify key research needs using human and environmental health challenges as an example.|https://doi.org/10.1016/j.envsoft.2015.06.003|https://www.sciencedirect.com/science/article/pii/S136481521500167X||||2015|Integrating modelling and smart sensors for environmental and human health|Stefan Reis and Edmund Seto and Amanda Northcross and Nigel W.T. Quinn and Matteo Convertino and Rod L. Jones and Holger R. Maier and Uwe Schlink and Susanne Steinle and Massimo Vieno and Michael C. Wimberly|article|REIS2015238|||Environmental Modelling & Software|13648152||74|||||23295|1,828|Q1|136|223|717|14218|4373|711|5,47|63,76|Netherlands|Western Europe|1997-2020|Ecological Modeling (Q1); Environmental Engineering (Q1); Software (Q1)||||460338184|665084965
|||3|273–275|Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice|||https://doi.org/10.1145/3447404.3447419|New York, NY, USA|Association for Computing Machinery|9781450390293|2021|Ethical Issues in Brain–Computer Interfaces|McMenemy, David|inbook|10.1145/3447404.3447419|||||||||1||||||||||||||||||||462602896|42
||Data integration;Systems architecture;Distribution networks;Production;Big Data applications;Reliability engineering;Business;distribution network;application requirements;data application architecture;design and realization||2354-2359|2020 IEEE Sustainable Power and Energy Conference (iSPEC)|In recent years, the rapid growth of all kinds of data and information in power grid has brought great challenges to the safe and stable operation and data analysis of the system. This paper constructs the data application architecture oriented to the requirements of distribution network based on the data requirements of reliability and economy evaluation, operation state evaluation and weak link identification, asset operation efficiency evaluation and lean management. It can realize the functions of data automatic classification storage, data processing, data quality monitoring and evaluation, multi-source heterogeneous data fusion and hierarchical classification database construction, etc. It supports the lean management of production business in distribution network comprehensively.|10.1109/iSPEC50848.2020.9351123|||||2020|Design and Realization of Data Application Architecture Oriented to the Requirements of Distribution Network|Ouyang, Jianna and Liang, Shuo and Chen, Shaonan and Li, Shan and Zhou, Yangjun and Liwen, QIN|inproceedings|9351123||Nov|||||||||||||||||||||||||||467625151|42
||Internet of things in agriculture, Big data, High-throughput phenotype, Data mining||123651||With continuous collaborative research in sensor technology, communication technology, plant science, computer science and engineering science, Internet of Things (IoT) in agriculture has made a qualitative leap through environmental sensor networks, non-destructive imaging, spectral analysis, robotics, machine vision and laser radar technology. Physical and chemical analysis can continuously obtain environmental data, experimental metadata (including text, image and spectral, 3D point cloud and real-time growth data) through integrated automation platform equipment and technical means. Based on data on multi-scale, multi-environmental and multi-mode plant traits that constitute big data on plant phenotypes, genotype–phenotype–envirotype relationship in the omics system can be explored deeply. Detailed information on the formation mechanism of specific biological traits can promote the process of functional genomics, plant molecular breeding and efficient cultivation. This study summarises the development background, research process and characteristics of high-throughput plant phenotypes. A systematic review of the research progress of IoT in agriculture and plant high-throughput phenotypes is conducted, including the acquisition and analysis of plant phenotype big data, phenotypic trait prediction and multi-recombination analysis based on plant phenomics. This study proposes key techniques for current plant phenotypes, and looks forward to the research on plant phenotype detection technology in the field environment, fusion and data mining of plant phenotype multivariate data, simultaneous observation of multi-scale phenotype platform and promotion of a comprehensive high-throughput phenotype technology.|https://doi.org/10.1016/j.jclepro.2020.123651|https://www.sciencedirect.com/science/article/pii/S0959652620336969||||2021|The future of Internet of Things in agriculture: Plant high-throughput phenotypic platform|Jiangchuan Fan and Ying Zhang and Weiliang Wen and Shenghao Gu and Xianju Lu and Xinyu Guo|article|FAN2021123651|||Journal of Cleaner Production|09596526||280|||||19167|1,937|Q1|200|5126|10603|304498|103295|10577|9,56|59,40|United Kingdom|Western Europe|1993-2021|Environmental Science (miscellaneous) (Q1); Industrial and Manufacturing Engineering (Q1); Renewable Energy, Sustainability and the Environment (Q1); Strategy and Management (Q1)|170,352|9.297|0.18299|468507466|1121054297
||Reproducibility, Robustness, Relevance, Quality assurance, Neuroscience, Pre-clinical||1803-1807||Current limitations impeding on data reproducibility are often poor statistical design, underpowered studies, lack of robust data, lack of methodological detail, biased reporting and lack of open data sharing, coupled with wrong research incentives. To improve data reproducibility, robustness and quality for brain disease research, a Preclinical Data Forum Network was formed under the umbrella of the European College of Neuropsychopharmacology (ECNP). The goal of this network, members of which met for the first time in October 2014, is to establish a forum to collaborate in precompetitive space, to exchange and develop best practices, and to bring together the members from academia, pharmaceutical industry, publishers, journal editors, funding organizations, public/private partnerships and non-profit advocacy organizations. To address the most pertinent issues identified by the Network, it was decided to establish a data sharing platform that allows open exchange of information in the area of preclinical neuroscience and to develop an educational scientific program. It is also planned to reach out to other organizations to align initiatives to enhance efficiency, and to initiate activities to improve the clinical relevance of preclinical data. Those Network activities should contribute to scientific rigor and lead to robust and relevant translational data. Here we provide a synopsis of the proceedings from the inaugural meeting.|https://doi.org/10.1016/j.euroneuro.2015.05.011|https://www.sciencedirect.com/science/article/pii/S0924977X15001674||||2015|The preclinical data forum network: A new ECNP initiative to improve data quality and robustness for (preclinical) neuroscience|Thomas Steckler and Katja Brose and Magali Haas and Martien J. Kas and Elena Koustova and Anton Bespalov|article|STECKLER20151803|||European Neuropsychopharmacology|0924977X|10|25|||||15576|1,603|Q1|112|154|383|9717|1610|371|3,85|63,10|Netherlands|Western Europe|1990-2020|Neurology (Q1); Neurology (clinical) (Q1); Pharmacology (Q1); Pharmacology (medical) (Q1); Psychiatry and Mental Health (Q1); Biological Psychiatry (Q2)|8,999|4.600|0.01119|468725740|426205101
||Algorithm design and analysis;Noise measurement;Resource management;Data models;Robustness;Computational modeling;Proteins;Near-duplicate detection;shingling algorithm;n-gram;entity conflation||2606-2611|2015 IEEE International Conference on Big Data (Big Data)|In modern web-scale applications that collect data from different sources, entity conflation is a challenging task due to various data quality issues. In this paper, we propose a robust and distributed framework to perform conflation on noisy data in the Microsoft Academic Service dataset. Our framework contains two major components. In the offline component, we train a GBDT model to determine whether two papers from different sources should be conflated to the same paper entity. In the online component, we propose a scalable shingling algorithm that can apply our offline model to over 100 million instances. The result shows that our algorithm can conflate noisy data robustly and efficiently.|10.1109/BigData.2015.7364059|||||2015|Robust and distributed web-scale near-dup document conflation in microsoft academic service|Wu, Chieh-Han and Song, Yang|inproceedings|7364059||Oct|||||||||||||||||||||||||||468731604|42
|||12|1766–1777||As the Internet-of-Vehicles (IoV) technology becomes an increasingly important trend for future transportation, designing large-scale IoV systems has become a critical task that aims to process big data uploaded by fleet vehicles and to provide data-driven services. The IoV data, especially high-frequency vehicle statuses (e.g., location, engine parameters), are characterized as large volume with a low density of value and low data quality. Such characteristics pose challenges for developing real-time applications based on such data. In this paper, we address the challenges in designing a scalable IoV system by describing CarStream, an industrial system of big data processing for chauffeured car services. Connected with over 30,000 vehicles, CarStream collects and processes multiple types of driving data including vehicle status, driver activity, and passenger-trip information. Multiple services are provided based on the collected data. CarStream has been deployed and maintained for three years in industrial usage, collecting over 40 terabytes of driving data. This paper shares our experiences on designing CarStream based on large-scale driving-data streams, and the lessons learned from the process of addressing the challenges in designing and maintaining CarStream.|10.14778/3137765.3137781|https://doi.org/10.14778/3137765.3137781||VLDB Endowment||2017|CarStream: An Industrial System of Big Data Processing for Internet-of-Vehicles|Zhang, Mingming and Wo, Tianyu and Xie, Tao and Lin, Xuelian and Liu, Yaxiao|article|10.14778/3137765.3137781||aug|Proc. VLDB Endow.|21508097|12|10|August 2017||||21100199855|0,946|Q1|134|246|598|12401|3759|566|5,50|50,41|United States|Northern America|2008-2020|Computer Science (miscellaneous) (Q1)|7,198|2.047|0.00891|468884751|1216159931
||Decision support systems;Handheld computers;Conferences;Hadoop Streaming;Cassandra;Mongodb;MapReduce||273-276|2016 3rd International Conference on Computing for Sustainable Global Development (INDIACom)|Now a days Bulk of data generating on the system. This data is very important for user and today's user accessing, searching and sorting the data from database is very difficult. To overcome this problem, data is distributed in different node using Hadoop technology. A system is proposed in which the collected data is to be distributed using map reduce technique for sorting the data is very easily on Hadoop environment. In this case used Cassandra and mongodb tools to storing large amount of data on Hadoop Framework. NoSQL data is stores in unstructured data format which is a key focus area for “Big Data” research. The quantity and quality of unstructured data growing high. The Hadoop Framework used to large amount of data on a different nodes in a cluster data. NoSQL databases using different structure and unstructured data of high scalability for getting high performance of system. To present the approaches solving Problem of NoSQL data to stores with MapReduce process to under in non-Java application. A Cassandra is to provide the platform for the fast and efficient data queries. In this paper presents the tools of the Cassandra and the mongodb using NoSQL database for connecting different node with the Hadoop MapReduce engine.||||||2016|A generic tool to process mongodb or Cassandra dataset using Hadoop streaming|Gopal, R. Chandangole and Bharat, A. Tidke|inproceedings|7724270||March|||||||||||||||||||||||||||469736787|42
||Big-data, Analytics, Data centers, Distributed systems||2561-2573||One of the major applications of future generation parallel and distributed systems is in big-data analytics. Data repositories for such applications currently exceed exabytes and are rapidly increasing in size. Beyond their sheer magnitude, these datasets and associated applications’ considerations pose significant challenges for method and software development. Datasets are often distributed and their size and privacy considerations warrant distributed techniques. Data often resides on platforms with widely varying computational and network capabilities. Considerations of fault-tolerance, security, and access control are critical in many applications (Dean and Ghemawat, 2004; Apache hadoop). Analysis tasks often have hard deadlines, and data quality is a major concern in yet other applications. For most emerging applications, data-driven models and methods, capable of operating at scale, are as-yet unknown. Even when known methods can be scaled, validation of results is a major issue. Characteristics of hardware platforms and the software stack fundamentally impact data analytics. In this article, we provide an overview of the state-of-the-art and focus on emerging trends to highlight the hardware, software, and application landscape of big-data analytics.|https://doi.org/10.1016/j.jpdc.2014.01.003|https://www.sciencedirect.com/science/article/pii/S0743731514000057||||2014|Trends in big data analytics|Karthik Kambatla and Giorgos Kollias and Vipin Kumar and Ananth Grama|article|KAMBATLA20142561|||Journal of Parallel and Distributed Computing|07437315|7|74||Special Issue on Perspectives on Parallel and Distributed Processing|||25621|0,638|Q1|87|169|592|7166|2473|568|4,72|42,40|United States|Northern America|1984-2021|Computer Networks and Communications (Q1); Hardware and Architecture (Q1); Artificial Intelligence (Q2); Software (Q2); Theoretical Computer Science (Q2)|4,371|3.734|0.00477|470259913|1083163244
||Big data, Firm performance, Manufacturing companies, DEMATEL, ANFIS||199-210||Big Data is one of the recent technological advances with the strong applicability in almost every industry, including manufacturing. However, despite business opportunities offered by this technology, its adoption is still in early stage in many industries. Thus, this study aimed to identify and rank the significant factors influencing adoption of big data and in turn to predict the influence of big data adoption on manufacturing companies' performance using a hybrid approach of decision-making trial and evaluation laboratory (DEMATEL)- adaptive neuro-fuzzy inference systems (ANFIS). This study identified the critical adoption factors from literature review and categorized them into technological, organizational and environmental dimensions. Data was collected from 234 industrial managers who were involved in the decision-making process regarding IT procurement in Malaysian manufacturing companies. Research results showed that technological factors (perceived benefits, complexity, technology resources, big data quality and integration) have the highest influence on the big data adoption and firms' performance. This study is one of the pioneers in using DEMATEL-ANFIS approach in the big data adoption context. In addition to the academic contribution, findings of this study can hopefully assist manufacturing industries, big data service providers, and governments to precisely focus on vital factors found in this study in order to improve firm performance by adopting big data.|https://doi.org/10.1016/j.techfore.2018.07.043|https://www.sciencedirect.com/science/article/pii/S0040162518304141||||2018|Influence of big data adoption on manufacturing companies' performance: An integrated DEMATEL-ANFIS approach|Elaheh Yadegaridehkordi and Mehdi Hourmand and Mehrbakhsh Nilashi and Liyana Shuib and Ali Ahani and Othman Ibrahim|article|YADEGARIDEHKORDI2018199|||Technological Forecasting and Social Change|00401625||137|||||14704|2,226|Q1|117|448|1099|35581|10127|1061|9,01|79,42|United States|Northern America|1970-2020|Applied Psychology (Q1); Business and International Management (Q1); Management of Technology and Innovation (Q1)|21,116|8.593|0.02416|470454119|1949868303
||Air Pollution Monitoring, Health Management, Artificial Intelligence, Big Data, PM, Personalization, Smart Behavioural Intervention, Health and Well-being Improvement||441-450||All people in the world are entitled to enjoy a clean environment and a good quality of life. With big data and artificial intelligence technologies, it is possible to estimate personalized air pollution exposure and synchronize it with activity, health, quality of life and behavioural data, and provide real-time, personalized and interactive alert and advice to improve the health and well-being of individual citizens. In this paper, we propose an overarching framework outlining five major challenges to personalized air pollution monitoring and health management, and respective methodologies in an integrated interdisciplinary manner. First, urban air quality data is sparse, rendering it difficult to provide timely personalized alert and advice. Second, collected data, especially those involving human inputs such as health perception, are often missing and erroneous. Third, the data collected are heterogeneous, and highly complex, not easily comprehensible to facilitate individual and collective decision-making. Fourth, the causal relationships between personal air pollutants exposure (specifically, PM2.5 and PM1.0 and NO2) and personal health conditions, and health-related quality of life perception, of young asthmatics and young healthy citizens in Hong Kong (HK), are yet to be established. Fifth, whether personalized and smart information and advice provided can induce behavioural change and improve health and quality of life are yet to be determined. To overcome these challenges, our first novelty is to develop an AI and big data framework to estimate and forecast air quality in high temporal-spatial resolution and real-time. Our second novelty includes the deployment of mobile pollution sensor platforms to substantially improve the accuracy of estimated and forecasted air quality data, and the collection of activity, health condition and perception data. Our third novelty is the development of visualization tools and comprehensible indexes, by correlating personal exposure with four types of personal data, to provide timely, personalized pollution, health and travel alerts and advice. Our fourth novelty is determining causal relationship, if any, between personal pollutants, PM1.0 and PM2.5, NO2 exposure and personal health condition, and personal health perception, based on a clinical experiment of 150 young asthmatics and 150 young healthy citizens in HK. Our fifth novelty is an intervention study to determine if smart information, presented via our proposed visualized platform, will induce personal behavioural change. Our novel big data AI-driven approach, when integrated with other analytical approaches, provides an integrated interdisciplinary framework for personalized air pollution monitoring and health management, easily transferrable to and applicable in other domains and countries.|https://doi.org/10.1016/j.envsci.2021.06.011|https://www.sciencedirect.com/science/article/pii/S1462901121001714||||2021|A Big Data and Artificial Intelligence Framework for Smart and Personalized Air Pollution Monitoring and Health Management in Hong Kong|Victor O.K. Li and Jacqueline C.K. Lam and Yang Han and Kenyon Chow|article|LI2021441|||Environmental Science & Policy|14629011||124|||||||||||||||||||||||473838178|1729298893
WWW '11|Hyderabad, India|web scale data analytics, distributed data analytics, temporal web analytics|2|307–308|Proceedings of the 20th International Conference Companion on World Wide Web|The objective of the 1st Temporal Web Analytics Workshop (TWAW) is to provide a venue for researchers of all domains (IE/IR, Web mining etc.) where the temporal dimension opens up an entirely new range of challenges and possibilities. The workshop's ambition is to help shaping a community of interest on the research challenges and possibilities resulting from the introduction of the time dimension in Web analysis. The maturity of the Web, the emergence of large scale repositories of Web material, makes this very timely and a growing sets of research and services (recorded future1, truthy2 launched just in the last months) are emerging that have this focus in common. Having a dedicated workshop will help, we believe, to take a rich and cross-domain approach to this new research challenge with a strong focus on the temporal dimension.|10.1145/1963192.1963325|https://doi.org/10.1145/1963192.1963325|New York, NY, USA|Association for Computing Machinery|9781450306379|2011|The 1st Temporal Web Analytics Workshop (TWAW)|Baeza-Yates, Ricardo and Masan\`{e}s, Julien and Spaniol, Marc|inproceedings|10.1145/1963192.1963325|||||||||||||||||||||||||||||473951042|42
|||3|76–78||This is a forum for perspectives on designing for marginalized communities worldwide. Articles will discuss design methods, theoretical/conceptual contributions, and participatory interventions with underserved communities. --- Nithya Sambasivan, Editor|10.1145/3466160|https://doi.org/10.1145/3466160|New York, NY, USA|Association for Computing Machinery||2021|Seeing like a Dataset from the Global South|Sambasivan, Nithya|article|10.1145/3466160||jun|Interactions|10725520|4|28|July - August 2021||||4000148705|0,247|Q3|46|125|292|742|503|292|1,82|5,94|United States|Northern America|1994-1995, 1997, 2006-2020|Human-Computer Interaction (Q3)||||475628462|2035703392
|||13|1097–1109||Efficiently estimating the inclusion coefficient - the fraction of values of one column that are contained in another column - is useful for tasks such as data profiling and foreign-key detection. We present a new estimator, BML, for inclusion coefficient based on Hyperloglog sketches that results in significantly lower error compared to the state-of-the art approach that uses Bottom-k sketches. We evaluate the error of the BML estimator using experiments on industry benchmarks such as TPC-H and TPC-DS, and several real-world databases. As an independent contribution, we show how Hyperloglog sketches can be maintained incrementally with data deletions using only a constant amount of additional memory.|10.14778/3231751.3231759|https://doi.org/10.14778/3231751.3231759||VLDB Endowment||2018|Efficient Estimation of Inclusion Coefficient Using Hyperloglog Sketches|Nazi, Azade and Ding, Bolin and Narasayya, Vivek and Chaudhuri, Surajit|article|10.14778/3231751.3231759||jun|Proc. VLDB Endow.|21508097|10|11|June 2018||||21100199855|0,946|Q1|134|246|598|12401|3759|566|5,50|50,41|United States|Northern America|2008-2020|Computer Science (miscellaneous) (Q1)|7,198|2.047|0.00891|476660340|1216159931
SAC '19|Limassol, Cyprus|cloud, healthcare, fog, internet-of-things, coordination|8|2008–2015|Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing|"\"With the increasing popularity of the Internet-of-Things (IoT), organizations are revisiting their practices as well as adopting new ones so they can deal with an ever-growing amount of sensed and actuated data that IoT-compliant things generate. Some of these practices are about the use of cloud and/or fog computing. The former promotes \"\"anything-as-a-service\"\" and the latter promotes \"\"process data next to where it is located\"\". Generally presented as competing models, this paper discusses how cloud and fog could work hand-in-hand through a seamless coordination of their respective \"\"duties\"\". This coordination stresses out the importance of defining where the data of things should be sent (either cloud, fog, or cloud&amp;fog concurrently) and in what order (either cloud then fog, fog then cloud, or fog&amp;cloud concurrently). Applications' concerns with data such as latency, sensitivity, and freshness dictate both the appropriate recipients and the appropriate orders. For validation purposes, a healthcare-driven IoT application along with an in-house testbed, that features real sensors and fog and cloud platforms, have permitted to carry out different experiments that demonstrate the technical feasibility of the coordination model.\""|10.1145/3297280.3297477|https://doi.org/10.1145/3297280.3297477|New York, NY, USA|Association for Computing Machinery|9781450359337|2019|Towards a Seamless Coordination of Cloud and Fog: Illustration through the Internet-of-Things|Maamar, Zakaria and Baker, Thar and Faci, Noura and Ugljanin, Emir and Khafajiy, Mohammed Al and Bur\'{e}gio, Vanilson|inproceedings|10.1145/3297280.3297477|||||||||||||||||||||||||||||477813236|42
||User Study; Information Seeking Systems; Clarifying Questions||||The use of clarifying questions (CQs) is a fairly new and useful technique to aid systems in recognizing the intent, context, and preferences behind user queries. Yet, understanding the extent of the effect of CQs on user behavior and the ability to identify relevant information remains relatively unexplored. In this work, we conduct a large user study to understand the interaction of users with CQs in various quality categories, and the effect of CQ quality on user search performance in terms of finding relevant information, search behavior, and user satisfaction. Analysis of implicit interaction data and explicit user feedback demonstrates that high-quality CQs improve user performance and satisfaction. By contrast, low- and mid-quality CQs are harmful, and thus allowing the users to complete their tasks without CQ support may be preferred in this case. We also observe that user engagement, and therefore the need for CQ support, is affected by several factors, such as search result quality or perceived task difficulty. The findings of this study can help researchers and system designers realize why, when, and how users interact with CQs, leading to a better understanding and design of search clarification systems.|10.1145/3524110|https://doi.org/10.1145/3524110|New York, NY, USA|Association for Computing Machinery||2022|Users Meet Clarifying Questions: Toward a Better Understanding of User Interactions for Search Clarification|Zou, Jie and Aliannejadi, Mohammad and Kanoulas, Evangelos and Pera, Maria Soledad and Liu, Yiqun|article|10.1145/3524110||apr|ACM Trans. Inf. Syst.|10468188||||Just Accepted|||18997|0,672|Q1|83|44|126|3081|850|126|7,10|70,02|United States|Northern America|1983-2020|Business, Management and Accounting (miscellaneous) (Q1); Information Systems (Q1); Computer Science Applications (Q2)|2,193|4.797|0.00183|480166460|2120551289
BIGDSE '16|Austin, Texas|ecosystem, big data, innovation, architecture landscape, energy industry, value engineering, value discovery|7|44–50|Proceedings of the 2nd International Workshop on BIG Data Software Engineering|"\"This article articulates the requirements for an effective big data value engineering method. It then presents a value discovery method, called Eco-ARCH (Eco-ARCHitecture), tightly integrated with the BDD (Big Data Design) method for addressing these requirements, filling a methodological void. Eco-ARCH promotes a fundamental shift in design thinking for big data system design -- from \"\"bounded rationality\"\" for problem solving to \"\"expandable rationality\"\" for design for innovation. The Eco-ARCH approach is most suitable for big data value engineering when system boundaries are fluid, requirements are ill-defined, many stakeholders are unknown and design goals are not provided, where no architecture pre-exists, where system behavior is non-deterministic and continuously evolving, and where co-creation with consumers and prosumers is essential to achieving innovation goals. The method was augmented and empirically validated in collaboration with an IT service company in the energy industry to generate a new business model that we call \"\"eBay in the Grid\"\".\""|10.1145/2896825.2896837|https://doi.org/10.1145/2896825.2896837|New York, NY, USA|Association for Computing Machinery|9781450341523|2016|Toward Big Data Value Engineering for Innovation|Chen, Hong-Mei and Kazman, Rick and Garbajosa, Juan and Gonzalez, Eloy|inproceedings|10.1145/2896825.2896837|||||||||||||||||||||||||||||481789619|42
||Data analytics platforms, Agriculture, Systematic literature review, Big Data||106813||With the rapid developments in ICT, the current agriculture businesses have become increasingly data-driven and are supported by advanced data analytics techniques. In this context, several studies have investigated the adopted data analytics platforms in the agricultural sector. However, the main characteristics and overall findings on these platforms are scattered over the various studies, and to the best of our knowledge, there has been no attempt yet to systematically synthesize the features and obstacles of the adopted data analytics platforms. This article presents the results of an in-depth systematic literature review (SLR) that has explicitly focused on the domains of the platforms, the stakeholders, the objectives, the adopted technologies, the data properties and the obstacles. According to the year-wise analysis, it is found that no relevant primary study between 2010 and 2013 was found. This implies that the research of data analytics in agricultural sectors is a popular topic from recent years, so the results from before 2010 are likely less relevant. In total, 535 papers published from 2010 to 2020 were retrieved using both automatic and manual search strategies, among which 45 journal articles were selected for further analysis. From these primary studies, 33 features and 34 different obstacles were identified. The identified features and obstacles help characterize the different data analytics platforms and pave the way for further research.|https://doi.org/10.1016/j.compag.2022.106813|https://www.sciencedirect.com/science/article/pii/S0168169922001302||||2022|Data analytics platforms for agricultural systems: A systematic literature review|Ngakan {Nyoman Kutha Krisnawijaya} and Bedir Tekinerdogan and Cagatay Catal and Rik van der Tol|article|NYOMANKUTHAKRISNAWIJAYA2022106813|||Computers and Electronics in Agriculture|01681699||195|||||30441|1,208|Q1|115|648|1300|28725|9479|1298|7,27|44,33|Netherlands|Western Europe|1985-2020|Agronomy and Crop Science (Q1); Animal Science and Zoology (Q1); Computer Science Applications (Q1); Forestry (Q1); Horticulture (Q1)|17,657|5.565|0.01646|483625466|1531073408
||Data quality standards, assessing data quality, data quality monitoring, data quality reporting, data issue management, quality improvement methodology||187-228|Meeting the Challenges of Data Quality Management|This chapter describes the functions required to build organizational capacity to manage data for quality over time. They include: data quality standards, data quality assessment, data quality monitoring, data quality reporting, data quality issue management, and data quality improvement. These activities are likely to be executed more consistently and with greater impact if there is a data quality team specifically responsible for defining them and facilitating their adoption within the organization.|https://doi.org/10.1016/B978-0-12-821737-5.00009-2|https://www.sciencedirect.com/science/article/pii/B9780128217375000092||Academic Press|978-0-12-821737-5|2022|Chapter 9 - Core Data Quality Management Capabilities|Laura Sebastian-Coleman|incollection|SEBASTIANCOLEMAN2022187||||||||||Laura Sebastian-Coleman|||||||||||||||||||488357325|42
ICBDR 2018|Weihai, China|power grid planning, data management, mechanism design, data fusion|6|21–26|Proceedings of the 2nd International Conference on Big Data Research|In order to deal with diversity of massive data structures and the variety of information formats, a novel mechanism is designed for unified management of power grid planning data. By integrating many business systems including production management system (PMS), geographic information system (GIS), energy management system (EMS), distribution network information acquisition system in the power supply company's system and adopting various technical means such as data warehouse technology (e.g. ETL, Extract-Transform-Load) and incremental capture, data structure that supports the whole process management of power grid business is designed, data correlation analysis and integration &amp; migration are carried out, and efficient access and deep fusion of massive relational data, file-type data, distributed data and spatial data are realized. Besides, through computing modes such as diagnostic analysis, load analysis and spatial analysis, the integrated database for power grid planning that integrates data fusion, storage, mining, modeling, computing, analysis and intelligent perception is finally constructed based on the designed data management mechanism, which could provide the comprehensive model and data support for power grid development. Field application shows the engineering benefit of the designed data management mechanism.|10.1145/3291801.3291826|https://doi.org/10.1145/3291801.3291826|New York, NY, USA|Association for Computing Machinery|9781450364768|2018|Mechanism Design for Unified Management of Power Grid Planning Data|Sun, Donglei and Zeng, Jun and Zhu, Yi and Cao, Xiangyang and Wang, Yiqun and Yang, Bo and Yang, Bin and Wang, Nan and Bo, Qibin and Fu, Yimu and Wei, Jia and Liu, Dong|inproceedings|10.1145/3291801.3291826|||||||||||||||||||||||||||||488411075|42
||urban/community dynamics, cross-space sensing and mining, human-machine systems, Mobile phone sensing, crowd intelligence|31|||With the surging of smartphone sensing, wireless networking, and mobile social networking techniques, Mobile Crowd Sensing and Computing (MCSC) has become a promising paradigm for cross-space and large-scale sensing. MCSC extends the vision of participatory sensing by leveraging both participatory sensory data from mobile devices (offline) and user-contributed data from mobile social networking services (online). Further, it explores the complementary roles and presents the fusion/collaboration of machine and human intelligence in the crowd sensing and computing processes. This article characterizes the unique features and novel application areas of MCSC and proposes a reference framework for building human-in-the-loop MCSC systems. We further clarify the complementary nature of human and machine intelligence and envision the potential of deep-fused human--machine systems. We conclude by discussing the limitations, open issues, and research opportunities of MCSC.|10.1145/2794400|https://doi.org/10.1145/2794400|New York, NY, USA|Association for Computing Machinery||2015|Mobile Crowd Sensing and Computing: The Review of an Emerging Human-Powered Sensing Paradigm|Guo, Bin and Wang, Zhu and Yu, Zhiwen and Wang, Yu and Yen, Neil Y. and Huang, Runhe and Zhou, Xingshe|article|10.1145/2794400|7|aug|ACM Comput. Surv.|03600300|1|48|September 2015||||23038|2,079|Q1|163|112|365|15859|6978|365|19,16|141,60|United States|Northern America|1969-2020|Computer Science (miscellaneous) (Q1); Theoretical Computer Science (Q1)|10,790|10.282|0.01438|489003122|1517405264
HT '14|Santiago, Chile|marketing intelligence, social media, sentiment identification, mapreduce, network analysis|10|190–199|Proceedings of the 25th ACM Conference on Hypertext and Social Media|This paper investigates characteristics of implicit brand networks extracted from a large dataset of user historical activities on a social media platform. To our knowledge, this is one of the first studies to comprehensively examine brands by incorporating user-generated social content and information about user interactions. This paper makes several important contributions. We build and normalize a weighted, undirected network representing interactions among users and brands. We then explore the structure of this network using modified network measures to understand its characteristics and implications. As a part of this exploration, we address three important research questions: (1) What is the structure of a brand-brand network? (2) Does an influential brand have a large number of fans? (3) Does an influential brand receive more positive or more negative comments from social users? Experiments conducted with Facebook data show that the influence of a brand has (a) high positive correlation with the size of a brand, meaning that an influential brand can attract more fans, and, (b) low negative correlation with the sentiment of comments made by users on that brand, which means that negative comments have a more powerful ability to generate awareness of a brand than positive comments. To process the large-scale datasets and networks, we implement MapReduce-based algorithms.|10.1145/2631775.2631806|https://doi.org/10.1145/2631775.2631806|New York, NY, USA|Association for Computing Machinery|9781450329545|2014|Empirical Analysis of Implicit Brand Networks on Social Media|Zhang, Kunpeng and Bhattacharyya, Siddhartha and Ram, Sudha|inproceedings|10.1145/2631775.2631806|||||||||||||||||||||||||||||489572767|42
Handbook of Statistics||Big Data, Hadoop, MapReduce, Image search, Multimedia retrieval, Smart deployment, SIFT, HDFS, Hadoop deployment, Hadoop configuration, Hadoop performance, Map waves||279-301|Big Data Analytics|While the past decade has witnessed an unprecedented growth of data generated and collected all over the world, existing data management approaches lack the ability to address the challenges of Big Data. One of the most promising tools for Big Data processing is the MapReduce paradigm. Although it has its limitations, the MapReduce programming model has laid the foundations for answering some of the Big Data challenges. In this chapter, we focus on Hadoop, the open-source implementation of the MapReduce paradigm. Using as case study a Hadoop-based application, i.e., image similarity search, we present our experiences with the Hadoop framework when processing terabytes of data. The scale of the data and the application workload allowed us to test the limits of Hadoop and the efficiency of the tools it provides. We present a wide collection of experiments and the practical lessons we have drawn from our experience with the Hadoop environment. Our findings can be shared as best practices and recommendations to the Big Data researchers and practitioners.|https://doi.org/10.1016/B978-0-444-63492-4.00012-5|https://www.sciencedirect.com/science/article/pii/B9780444634924000125||Elsevier||2015|Chapter 12 - Terabyte-Scale Image Similarity Search|Diana Moise and Denis Shestakov|incollection|MOISE2015279||||01697161||33||||Venu Govindaraju and Vijay V. Raghavan and C.R. Rao|17700156445|0,125|Q4|42|19|89|1023|82|1|1,12|53,84|Netherlands|Western Europe|1980, 1982-1985, 1988, 1991, 1993-1994, 1996-1998, 2000-2001, 2003, 2005-2007, 2009, 2012-2020|Applied Mathematics (Q4); Modeling and Simulation (Q4); Statistics and Probability (Q4)||||496071913|2062690730
ICBDR 2017|Osaka, Japan|minimal attribute set, ETL, Metadata, PROV, data provenance|5|57–61|Proceedings of the 2017 International Conference on Big Data Research|For the ETL process, this paper designs a provenance tool based on inversible transformation, and describes the meta-information of ETL and data provenance process in two ways: one is to take the database two-dimensional table to describe the relevant information in logical level, easy to record; the other is the use of PROV model information on the xml description, and shows the ETL and the provenance process in the directed acyclic graph, easy to understand.|10.1145/3152723.3152730|https://doi.org/10.1145/3152723.3152730|New York, NY, USA|Association for Computing Machinery|9781450353564|2017|Design of ETL Provenance Tool Based on Minimal Attribute Set|Chaofan, Dai and Ran, Zhang and Pei, Li and Wenqian, Wang|inproceedings|10.1145/3152723.3152730|||||||||||||||||||||||||||||498104581|42
||Personal big data, Data privacy, Privacy protection, Differential privacy, Positive pricing, Reverse pricing, Privacy budget, Privacy compensation||102529||Personal big data can greatly promote social management, business applications, and personal services, and bring certain economic benefits to users. The difficulty with personal big data security and privacy protection lies in realizing the maximization of the value of personal big data and in striking a balance between data privacy protection and sharing on the premise of satisfying personal big data security and privacy protection. Thus, in this paper, we propose a personal big data pricing method based on differential privacy (PMDP). We design two different mechanisms of positive and reverse pricing to reasonbly price personal big data. We perform aggregate statistics on an open dataset and extensively evaluated its performance. The experimental results show that PMDP can provide reasonable pricing for personal big data and fair compensation to data owners, ensuring an arbitrage-free condition and finding a balance between privacy protection and data utility.|https://doi.org/10.1016/j.cose.2021.102529|https://www.sciencedirect.com/science/article/pii/S0167404821003539||||2022|Personal big data pricing method based on differential privacy|Yuncheng Shen and Bing Guo and Yan Shen and Xuliang Duan and Xiangqian Dong and Hong Zhang and Chuanwu Zhang and Yuming Jiang|article|SHEN2022102529|||Computers & Security|01674048||113|||||28898|0,861|Q1|92|321|559|17204|3843|550|6,75|53,60|United Kingdom|Western Europe|1982-2020|Computer Science (miscellaneous) (Q1); Law (Q1)||||503256275|1687137310
||Energy consumption;Temperature distribution;Water storage;Data integrity;Water heating;Solar energy;Big Data;energy management;energy big data;energy information collection||1450-1452|2021 International Conference on Information and Communication Technology Convergence (ICTC)|Although the solar energy industry is becoming widespread, it is necessary to manage the charging and generating scheduling of solar power generation according to the ever-changing climate environment. In order to do this, a judgment criterion that can give timely charge / discharge instructions is needed and it needs to be actively performed. In this paper, we define a big-data platform for residential heat energy consumption. As a technology to secure thermal energy data of apartment houses, collect thermal energy data by dividing it into supply/equipment/usage. In order to secure standardized thermal energy data from the calorimeter installed. Equipped with data classification and processing, LP storage and management, data quality measurement and analysis functions. Develop a data adapter, from several multiunit dwellings with different calorimeter types. We will collect thermal energy data with an integrated big data system.|10.1109/ICTC52510.2021.9620761|||||2021|Mechanism of a big-data platform for residential heat energy consumption|Ku, Tai-Yeon and Park, Wan-Ki and Choi, Hoon|inproceedings|9620761||Oct||21621233|||||||||||||||||||||||||503894301|966235993
ICSE '16|Austin, Texas|telemetry, practices, developer tools, logging, collaboration, boundary object|10|92–101|Proceedings of the 38th International Conference on Software Engineering Companion|Large software organizations are transitioning to event data platforms as they culturally shift to better support data-driven decision making. This paper offers a case study at Microsoft during such a transition. Through qualitative interviews of 28 participants, and a quantitative survey of 1,823 respondents, we catalog a diverse set of activities that leverage event data sources, identify challenges in conducting these activities, and describe tensions that emerge in data-driven cultures as event data flow through these activities within the organization. We find that the use of event data span every job role in our interviews and survey, that different perspectives on event data create tensions between roles or teams, and that professionals report social and technical challenges across activities.|10.1145/2889160.2889231|https://doi.org/10.1145/2889160.2889231|New York, NY, USA|Association for Computing Machinery|9781450342056|2016|The Bones of the System: A Case Study of Logging and Telemetry at Microsoft|Barik, Titus and DeLine, Robert and Drucker, Steven and Fisher, Danyel|inproceedings|10.1145/2889160.2889231|||||||||||||||||||||||||||||504106223|42
SIGMOD '22|Philadelphia, PA, USA|quality management, geo-sensory data, internet of things|9|2474–2482|Proceedings of the 2022 International Conference on Management of Data|Within the rapidly expanding Internet of Things (IoT), growing amounts of spatially referenced data are being generated. Due to the dynamic, decentralized, and heterogeneous nature of the IoT, spatial IoT data (SID) quality has attracted considerable attention in academia and industry. How to invent and use technologies for managing spatial data quality and exploiting low-quality spatial data are key challenges in the IoT. In this tutorial, we highlight the SID consumption requirements in applications and offer an overview of spatial data quality in the IoT setting. In addition, we review pertinent technologies for quality management and low-quality data exploitation, and we identify trends and future directions for quality-aware SID management and utilization. The tutorial aims to not only help researchers and practitioners to better comprehend SID quality challenges and solutions, but also offer insights that may enable innovative research and applications.|10.1145/3514221.3522568|https://doi.org/10.1145/3514221.3522568|New York, NY, USA|Association for Computing Machinery|9781450392495|2022|Spatial Data Quality in the IoT Era: Management and Exploitation|Li, Huan and Tang, Bo and Lu, Hua and Cheema, Muhammad Aamir and Jensen, Christian S.|inproceedings|10.1145/3514221.3522568|||||||||||||||||||||||||||||504516376|42
||||61-63||Nowadays, there are a huge number of autonomous and diverse information sources providing heterogeneous data. Sensors, social media data, data on the Web, open data, just to name a few, resulting in a major confluence of Big Data. In this survey, we discuss these diverse data sources and detail the way in which data are acquired, stored, processed and analysed. Although some of the opportunities in this new state are mentioned, the main objective of this analysis is to present the challenges for Big Data. To accomplish this goal, we examine the new proposals and approaches presented in this special issue with the aim of establishing new models for improving the management of the volume, velocity, and variety, of Big Data. Some of these schemes establish the use of Ontologies, Semantic Processing, Cloud Computing and Data Management and could be seen as intelligent services integrated as context-aware services.|https://doi.org/10.1016/j.csi.2017.03.006|https://www.sciencedirect.com/science/article/pii/S0920548917301022||||2017|Big Data. New approaches of modelling and management|David Gil and Il-Yeol Song and José F. Aldana and Juan Trujillo|article|GIL201761|||Computer Standards & Interfaces|09205489||54||SI: New modeling in Big Data|||24303|0,556|Q1|63|36|246|2302|1013|243|3,71|63,94|Netherlands|Western Europe|1985-2020|Law (Q1); Hardware and Architecture (Q2); Software (Q2)||||507041687|827980402
||Smart buildings, IoT, Big data analytics, Reactif systems, Complex event processing||161-168||The processes of digital transformation have involved a variety of socio-technical activities, with the objective of increasing productivity, safety and quality of execution, sustainable development, collaborative working and solutions for the sustainable smart city. The major digital trends, changing the building sector and revealing new trends of understanding information technologies to integrate in this sector. Current smart building management systems incorporate a variety of sensors, actuators and dedicated networks. Their objectives are to observe the condition of specific areas and apply appropriate rules to preserve or improve comfort while saving energy. In this paper, we propose a review of related works to IoT, Big Data Analytics in smart buildings.|https://doi.org/10.1016/j.procs.2020.03.021|https://www.sciencedirect.com/science/article/pii/S1877050920304506||||2020|IoT and Big Data Analytics for Smart Buildings: A Survey|Abdellah Daissaoui and Azedine Boulmakoul and Lamia Karim and Ahmed Lbath|article|DAISSAOUI2020161|||Procedia Computer Science|18770509||170||The 11th International Conference on Ambient Systems, Networks and Technologies (ANT) / The 3rd International Conference on Emerging Data and Industry 4.0 (EDI40) / Affiliated Workshops|||19700182801|0,334|-|76|1907|6483|38511|13722|6359|2,09|20,19|Netherlands|Western Europe|2010-2020|Computer Science (miscellaneous)||||508267799|2108686752
||evaluation, automation, crowdsourcing, data mining, Artificial intelligence|4||||10.1145/2935752|https://doi.org/10.1145/2935752|New York, NY, USA|Association for Computing Machinery||2016|Replacing Mechanical Turkers? Challenges in the Evaluation of Models with Semantic Properties|Morstatter, Fred and Liu, Huan|article|10.1145/2935752|15|oct|J. Data and Information Quality|19361955|4|7|October 2016||||||||||||||||||||||509938500|833754770
WI '17|Leipzig, Germany|personalization, decision support, data purchase, computational intelligence|7|396–402|Proceedings of the International Conference on Web Intelligence|"\"The Big Data era is affording a paradigm change on decision-making approaches. More and more, companies as well as individuals are relying on data rather than on the so called \"\"gut feeling\"\" to make decisions. However, searching the Web for carrying out purchases is not completely satisfactory yet, given the arduousness of finding suitable quality data. This has contributed to the emergence of data marketplaces as an alternative to traditional data commerce, as they provide appropriate online environments for data offering and purchasing. Nevertheless, as the number of available datasets to purchase increases, the task of buying appropriate offers is, very often, challenging. In this sense, we propose an intelligent decision support system to help buyers in purchasing data offers based on a multiple-criteria decision analysis. Experimental results show that our approach provides an interactive way that addresses buyers' needs, allowing them to state and easily refine their preferences, without any specific order, via a series of dataset recommendations.\""|10.1145/3106426.3106434|https://doi.org/10.1145/3106426.3106434|New York, NY, USA|Association for Computing Machinery|9781450349512|2017|Intelligent Decision Support for Data Purchase|Martins, Denis Mayr Lima and Vossen, Gottfried and de Lima Neto, Fernando Buarque|inproceedings|10.1145/3106426.3106434|||||||||||||||||||||||||||||510483893|42
KDD '16|San Francisco, California, USA|predictive modeling, machine learning, machine learning platforms, business analytics|2|2139–2140|Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining|Predictive modeling is the art of building statistical models that forecast probabilities and trends of future events. It has broad applications in industry across different domains. Some popular examples include user intention predictions, lead scoring, churn analysis, etc. In this tutorial, we will focus on the best practice of predictive modeling in the big data era and its applications in industry, with motivating examples across a range of business tasks and relevance products. We will start with an overview of how predictive modeling helps power and drive various key business use cases. We will introduce the essential concepts and state of the art in building end-to-end predictive modeling solutions, and discuss the challenges, key technologies, and lessons learned from our practice, including case studies of LinkedIn feed relevance and a platform for email response prediction. Moreover, we will discuss some practical solutions of building predictive modeling platform to scale the modeling efforts for data scientists and analysts, along with an overview of popular tools and platforms used across the industry.|10.1145/2939672.2945388|https://doi.org/10.1145/2939672.2945388|New York, NY, USA|Association for Computing Machinery|9781450342322|2016|Business Applications of Predictive Modeling at Scale|Zhu, Qiang and Guo, Songtao and Ogilvie, Paul and Liu, Yan|inproceedings|10.1145/2939672.2945388|||||||||||||||||||||||||||||512781591|42
|||7|16–22||Recently, both academia and industry have shown increasing interest in unlocking the potential applications of digital twin. As an emerging technology, digital twin builds a virtual representation of physical objects and makes predictive strategies. As compared with conventional simulation and modeling technologies, digital twin can ensure the high fidelity of the virtual model through continuous updating and self-learning. The emerging standardization of digital twin facilitates the development of digital twin, and will eventually realize the interconnection of data, models and services between different enterprises or areas. This article overviews the recent progress of digital twin standards, the progress of digital twin network, and the challenges for successfully deploying digital twins.|10.1145/3568113.3568119|https://doi.org/10.1145/3568113.3568119|New York, NY, USA|Association for Computing Machinery||2022|An Introduction to Digital Twin Standards|Sun, Wen and Ma, Wenqiang and Zhou, Yu and Zhang, Yan|article|10.1145/3568113.3568119||oct|GetMobile: Mobile Comp. and Comm.|23750529|3|26|September 2022||||||||||||||||||||||513049713|1320313121
||Intrinsic motivation learning, Curiosity loop, Reinforcement learning, Big data, Data science, Feature selection||42-54||In state-of-the-art big-data applications, the process of building machine learning models can be very challenging due to continuous changes in data structures and the need for human interaction to tune the variables and models over time. Hence, expedited learning in rapidly changing environments is required. In this work, we address this challenge by implementing concepts from the field of intrinsically motivated computational learning, also known as artificial curiosity (AC). In AC, an autonomous agent acts to optimize its learning about itself and its environment by receiving internal rewards based on prediction errors. We present a novel method of intrinsically motivated learning, based on the curiosity loop, to learn the data structures in large and varied datasets. An autonomous agent learns to select a subset of relevant features in the data, i.e., feature selection, to be used later for model construction. The agent optimizes its learning about the data structure over time without requiring external supervision. We show that our method, called the Curious Feature Selection (CFS) algorithm, positively impacts the accuracy of learning models on three public datasets.|https://doi.org/10.1016/j.ins.2019.02.009|https://www.sciencedirect.com/science/article/pii/S0020025519301100||||2019|Curious Feature Selection|Michal Moran and Goren Gordon|article|MORAN201942|||Information Sciences|00200255||485|||||15134|1,524|Q1|184|928|2184|40007|17554|2168|7,89|43,11|United States|Northern America|1968-2021|Artificial Intelligence (Q1); Computer Science Applications (Q1); Control and Systems Engineering (Q1); Information Systems and Management (Q1); Software (Q1); Theoretical Computer Science (Q1)|44,038|6.795|0.04908|514784958|1633962588
dg.o 2019|Dubai, United Arab Emirates|Data Analytics, DMBOK, Data Management, Policy Analysis, Artificial Intelligence|6|171–176|Proceedings of the 20th Annual International Conference on Digital Government Research|Technical and organizational innovations such as Open Data, Internet of Things and Big Data have fueled renewed interest in policy analytics in the public sector. This revamped version of policy analysis continues the long-standing tradition of applying statistical modeling to better understand policy effects and decision making, but also incorporates other computational approaches such as artificial intelligence (AI) and computer simulation. Although much attention has been given to the development of capabilities for data analysis, there is much less attention to understanding the role of data management in a context of AI in government. In this paper, we argue that data management capabilities are foundational to data analysis of any kind, but even more important in the present AI context. This is so because without proper data management, simply acquiring data or systems will not produce desired outcomes. We also argue that realizing the potential of AI for social good relies on investments specifically focused on this social outcome, investments in the processes of building trust in government data, and ensuring the data are ready and suitable for use, for both immediate and future uses.|10.1145/3325112.3325245|https://doi.org/10.1145/3325112.3325245|New York, NY, USA|Association for Computing Machinery|9781450372046|2019|The Data Firehose and AI in Government: Why Data Management is a Key to Value and Ethics|Harrison, Teresa and F. Luna-Reyes, Luis and Pardo, Theresa and De Paula, Nic and Najafabadi, Mahdi and Palmer, Jillian|inproceedings|10.1145/3325112.3325245|||||||||||||||||||||||||||||515728370|42
||Machine learning algorithms;Smart cities;Big Data;Prediction algorithms;Data mining;Traffic congestion;Regression tree analysis;Data Mining;Big Data;Machine learning;Smart Cities;Prediction;Classification;Traffic Congestion||1-8|2020 11th International Conference on Information, Intelligence, Systems and Applications (IISA|This paper provides an analysis and proposes a methodology for predicting traffic congestion. Several machine learning algorithms and approaches are compared to select the most appropriate one. The methodology was implemented using Data Mining and Big Data techniques along with Python, SQL, and GIS technologies and was tested on data originating from one of the most problematic, regarding traffic congestion, streets in Thessaloniki, the 2nd most populated city in Greece. Evaluation and results have shown that data quality and size were the most critical factors towards algorithmic accuracy. Result comparison showed that Decision Trees were more accurate than Logistic Regression.|10.1109/IISA50023.2020.9284399|||||2020|Big Data Mining for Smart Cities: Predicting Traffic Congestion using Classification|Mystakidis, Aristeidis and Tjortjis, Christos|inproceedings|9284399||July|||||||||||||||||||||||||||518065148|42
||Social networking (online);Soft sensors;Transforms;Data warehouses;Big Data applications;Data models;Security;data integration;data warehouse;data lake;big data;extract transform load;data processing workflow;data processing pipeline;data quality;ETL optimization;data source evolution;metadata||01-03|2021 Eighth International Conference on Social Network Analysis, Management and Security (SNAMS)|During recent years, we observe a widespread of new data sources, especially all types of social media and IoT devices, which produce huge data volumes, whose content ranges from fully structured to totally unstructured. All these types of data are commonly referred to as big data. They are typically described by the three most important characteristics, called 3V [1], namely: an extremely large volume, a variety of data models and structures (data representations), as well as a high velocity at which data are generated. We argue that out of these three Vs, the most challenging is variety [2]. Such data need to be integrated and transformed into a common representation, which is suitable for analysis, in a similar manner as traditional (mainly table-like) data.|10.1109/SNAMS53716.2021.9732098|||||2021|Still Open Problems in Data Warehouse and Data Lake Research: extended abstract|Wrembel, Robert|inproceedings|9732098||Dec|||||||||||||||||||||||||||523365897|42
JCDL '17|Toronto, Ontario, Canada|web archiving, internet archive|2|352–353|Proceedings of the 17th ACM/IEEE Joint Conference on Digital Libraries|This workshop will explore integration of Web archiving and digital libraries, so the complete life cycle involved is covered: creation/authoring, uploading/publishing in the Web (2.0), (focused) crawling, indexing, exploration (searching, browsing), archiving (of events), etc. It will include particular coverage of current topics of interest, like: big data, mobile web archiving, and systems (e.g., Memento, SiteStory, Hadoop processing).||||IEEE Press|9781538638613|2017|Web Archiving and Digital Libraries (WADL)|Fox, Edward A. and Xie, Zhiwu and Klein, Martin|inproceedings|10.5555/3200334.3200410|||||||||||||||||||||||||||||523849870|42
||Energy efficiency management, ISO 19030, Hull and propeller maintenance||108953||This study analyzes the energy efficiency of ships based on ISO 19030, which is a standard for the measurement of changes in hull and propeller performance. The goal is to provide energy efficiency management with digital indicators that have not been easily provided. The ship navigation information platform (SNIP) is developed to determine the dynamic information of each ship, including the fuel consumption, ship speed, horsepower of the engine, rotation speed of the engine, wind direction, and wind speed. In addition, model test data and computational fluid dynamics (CFD) calculation data are applied to calculate the energy efficiency performance indicators. Finally, the relationship of the speed through water and the speed over ground enables us to modify the effects of the ocean currents. The results verify that these indicators can be used as a reference for performance monitoring and maintenance prediction of international maritime affairs.|https://doi.org/10.1016/j.oceaneng.2021.108953|https://www.sciencedirect.com/science/article/pii/S0029801821003887||||2021|Marine big data analysis of ships for the energy efficiency changes of the hull and maintenance evaluation based on the ISO 19030 standard|Heiu-Jou Shaw and Cheng-Kuan Lin|article|SHAW2021108953|||Ocean Engineering|00298018||232|||||28339|1,321|Q1|100|1216|2475|51177|10979|2469|4,31|42,09|United Kingdom|Western Europe|1968-2020|Environmental Engineering (Q1); Ocean Engineering (Q1)|23,463|3.795|0.0247|524046757|1406779183
EGOSE '16|St. Petersburg, Russia|municipal operations center, smart cities governance, multiple cases study, smart cities|12|19–30|Proceedings of the International Conference on Electronic Governance and Open Society: Challenges in Eurasia|Cities around the world have been facing complex challenges from the growing urbanization. The increase of urban problems is a consequence of this phenomenon, added to the lack of policies focusing in citizens' well-being and safety. Municipal operations centers have played an important role in response of social events and natural disasters as a way to address the urgency and dynamism of urban problems. This research aims at analyzing the main dimensions and factors for implementing municipal operations centers as smart city initiatives. In order to explore this phenomenon it was conducted an exploratory study, based on multiple case studies. The empirical setting of this research is determined by municipal operations centers in Rio de Janeiro, Porto Alegre and Belo Horizonte. The research findings evidenced that the implementation of the centers comprises technological, organizational and managerial factors, in addition to political and institutional factors. Increasing smart cities governance is the main result from the initiatives.|10.1145/3014087.3014110|https://doi.org/10.1145/3014087.3014110|New York, NY, USA|Association for Computing Machinery|9781450348591|2016|Building Understanding of Municipal Operations Centers as Smart City' Initiatives: Insights from a Cross-Case Analysis|Pereira, Gabriela Viale and Testa, Maur\'{\i}cio Gregianin and Macadar, Marie Anne and Parycek, Peter and de Azambuja, Luiza Schuch|inproceedings|10.1145/3014087.3014110|||||||||||||||||||||||||||||525325453|42
||Data integrity;Quality assessment;Writing;Gaussian distribution;Big Data;Sampling methods;Time-frequency analysis;Data Quality, Quality Assessment, Sampling||1306-1311|2019 International Conference on Computational Science and Computational Intelligence (CSCI)|Data is useful to the extent that it can be quickly analyzed to reveal valuable information. With high-quality data, we can increase revenue, reduce cost, and reduce risk. On the other hand, the consequences of poor-quality data can be severe. It has been estimated that poor quality customer data costs U.S. businesses $611 billion annually in postage, printing, and staff overhead. These issues make data quality assessment a necessary and critical step in any data-related systems. Big data brings new challenges to data quality assessment due to the scale of data, streaming data, and different forms of data. Therefore, we proposed a sampling-based approximate quality assessment model on large data. Sampling large datasets can make all quality assessment processes cheaper and more feasible because of data reduction. The protocol of this work: First, the sample size is determined for estimating a large dataset. Next, sampling techniques are applied to collect samples. Then, these samples are used to estimate the quality of the large dataset. The objective of quality assessment in this work is to evaluate the completeness, accuracy, and timeliness of data and to return fast and approximate scores. Using different sample sizes and different sampling methods, we obtained 72 sets of data and compared them. These results show that the proposed approach is efficient and provides some insight into the quality assessment with samples.|10.1109/CSCI49370.2019.00244|||||2019|Approximate Quality Assessment with Sampling Approaches|Liu, Hong and Sang, Zhenhua and Karali, Sameer|inproceedings|9071249||Dec|||||||||||||||||||||||||||525924230|42
CFI '16|Nanjing, China|cloud computing, User behavior analysis, SQL-on-Hadoop, Internet TV|7|36–42|Proceedings of the 11th International Conference on Future Internet Technologies|The characteristic of Internet TV user behavior is quite essential for designers to optimize resource schedule and improve user experience. With the rapid development of Internet, both Internet TV users and STB (set top boxes) models are booming. This brings a large amount of behavior data which requires matching computing and storage resource to process. Therefore, scalable Internet TV user behavior analysis becomes more difficult. As a solution, cloud computing framework such as Hive is emerged. But limited by performance, it's not an appropriate choice for interactive analysis or real-time data exploration. In this paper, we present a real-time Internet TV user behavior analysis system with advantages of high concurrency, low latency and good transportability. Firstly, we design an event capture scheme, consisted of agents embedded in STBs and capture server clusters, to capture every manipulation performed by users. Secondly, we develop a SQL-on-Hadoop engine with distributed transactional management to decrease the response time. The engine has excellent query performance and ability to interactively query various data sources in different Hadoop formats. Lastly, we evaluate RBAS in a commercial Internet TV platform of 16 million registered users. The results show that, with a 32-node cluster, the system can effectively process 10.2 TB of behavior data every day, which is about 40x faster than original Hive-based system.|10.1145/2935663.2935664|https://doi.org/10.1145/2935663.2935664|New York, NY, USA|Association for Computing Machinery|9781450341813|2016|RBAS: A Real-Time User Behavior Analysis System for Internet TV in Cloud Computing|Zhu, Chengang and Cheng, Guang and Guo, Xiaojun and Wang, Yuxiang|inproceedings|10.1145/2935663.2935664|||||||||||||||||||||||||||||527682322|42
https://doi.org/10.1016/j.lungcan.2020.05.033|https://www.sciencedirect.com/science/article/pii/S0169500220304670||||2020|Ten-year patient journey of stage III non-small cell lung cancer patients: A single-center, observational, retrospective study in Korea (Realtime autOmatically updated data warehOuse in healTh care; UNIVERSE-ROOT study)|Hyun Ae Jung and Jong-Mu Sun and Se-Hoon Lee and Jin Seok Ahn and Myung-Ju Ahn and Keunchil Park|article|JUNG2020112|||Lung Cancer|01695002||146||||||||||||||||||||||||||||||528894861|42
||Data governance, accuracy, completeness, consistency, currency, data requirements, consumer expectations, data quality dimensions, metadata, reference data, repurposing, enrichment, enhancement||39-48|Big Data Analytics|In this chapter we look at the need for oversight and governance for the data, especially when those developing big data applications often bypass traditional IT and data management channels. Some of the key issues involve the fact that for big data applications that consume massive amounts of data streamed from external sources, there is little or no control that can be exerted to ensure data quality and usability. We consider five key concepts, namely managing data consumer expectations, defining critical data quality dimensions, monitoring consistency of metadata, data repurposing and reinterpretation, and data enrichment when possible.|https://doi.org/10.1016/B978-0-12-417319-4.00005-3|https://www.sciencedirect.com/science/article/pii/B9780124173194000053|Boston|Morgan Kaufmann|978-0-12-417319-4|2013|Chapter 5 - Data Governance for Big Data Analytics: Considerations for Data Policies and Processes|David Loshin|incollection|LOSHIN201339||||||||||David Loshin|||||||||||||||||||529281786|42
||Participatory sensing, probabilistic database, trust management|30|||Participatory sensing has become a new data collection paradigm that leverages the wisdom of the crowd for big data applications without spending cost to buy dedicated sensors. It collects data from human sensors by using their own devices such as cell phone accelerometers, cameras, and GPS devices. This benefit comes with a drawback: human sensors are arbitrary and inherently uncertain due to the lack of quality guarantee. Moreover, participatory sensing data are time series that exhibit not only highly irregular dependencies on time but also high variance between sensors. To overcome these limitations, we formulate the problem of validating uncertain time series collected by participatory sensors. In this article, we approach the problem by an iterative validation process on top of a probabilistic time series model. First, we generate a series of probability distributions from raw data by tailoring a state-of-the-art dynamical model, namely <u>G</u>eneralised <u>A</u>uto <u>R</u>egressive <u>C</u>onditional <u>H</u>eteroskedasticity (GARCH), for our joint time series setting. Second, we design a feedback process that consists of an adaptive aggregation model to unify the joint probabilistic time series and an efficient user guidance model to validate aggregated data with minimal effort. Through extensive experimentation, we demonstrate the efficiency and effectiveness of our approach on both real data and synthetic data. Highlights from our experiences include the fast running time of a probabilistic model, the robustness of an aggregation model to outliers, and the significant effort saving of a guidance model.|10.1145/3326164|https://doi.org/10.1145/3326164|New York, NY, USA|Association for Computing Machinery||2019|Efficient User Guidance for Validating Participatory Sensing Data|Cong, Phan Thanh and Tam, Nguyen Thanh and Yin, Hongzhi and Zheng, Bolong and Stantic, Bela and Hung, Nguyen Quoc Viet|article|10.1145/3326164|37|jul|ACM Trans. Intell. Syst. Technol.|21576904|4|10|July 2019||||||||||||||||||||||531862585|273436860
||Cloud computing;Task analysis;Processor scheduling;Dynamic scheduling;Big Data;Job shop scheduling;Servers;Big Data;Quality of Big Data;Scheduling;Cloud scheduling;Dynamic cloud scheduling||210-219|2018 11th International Conference on the Quality of Information and Communications Technology (QUATIC)|The quality of services in Cloud Computing (CC) depends on the scheduling strategies selected for processing of the complex workloads in the physical cloud clusters. Using the scheduler of the single type does not guarantee of the optimal mapping of jobs onto cloud resources, especially in the case of the processing of the big data workloads. In this paper, we compare the performances of the cloud schedulers for various combinations of the cloud workloads with different characteristics. We define several scenarios where the proper types of schedulers can be selected from a list of scheduling models implemented in the system, and used to schedule the concrete workloads based on the workloads' parameters and the feedback on the efficiency of the schedulers. The presented work is the first step in the development and implementation of an automatic intelligent scheduler selection system. In our simple experimental analysis, we confirm the usefulness of such a system in today's data-intensive cloud computing.|10.1109/QUATIC.2018.00039|||||2018|Quality of Cloud Services Determined by the Dynamic Management of Scheduling Models for Complex Heterogeneous Workloads|Fernández-Cerero, Damian and Fernández-Montes, Alejandro and Kolodziej, Joanna and Lefèvre, Laurent|inproceedings|8590192||Sep.|||||||||||||||||||||||||||536193741|42
dg.o 2019|Dubai, United Arab Emirates||9|100–108|Proceedings of the 20th Annual International Conference on Digital Government Research|The adoption of artificial intelligence (AI) is becoming increasingly popular in the public sector, but there is a severe lack of relevant theoretical research. The government of China also has high expectations for AI innovation. This paper proposes a four-stage model for AI development in public sectors to help public administrators think about the impact of AI on their organizations. We empirically investigate a case of AI adoption for delivering public services in local government in China. The findings improve our understanding of not only the status of AI innovation but also the factors motivating and challenging public sectors that are intending to adopt AI. Given that AI application in public sectors is still in its infancy, this study provides us with an opportunity to conduct longitudinal tracking of AI innovation in local government in China. This could help public administrators to think more comprehensively about the changes and transformations that AI may bring to the public sector.|10.1145/3325112.3325243|https://doi.org/10.1145/3325112.3325243|New York, NY, USA|Association for Computing Machinery|9781450372046|2019|AI Innovation for Advancing Public Service: The Case of China's First Administrative Approval Bureau|Chen, Tao and Ran, Longya and Gao, Xian|inproceedings|10.1145/3325112.3325243|||||||||||||||||||||||||||||539753768|42
||Accuracy;Diseases;Big data;Robustness;Educational institutions;Bioinformatics;Malignant tumors;Microarray;integrated analytics;biomarkers||1-7|2014 IEEE Symposium on Computational Intelligence in Big Data (CIBD)|The advance of high throughput biotechnology enables the generation of large amount of biomedical data. The microarray is increasingly a popular approach for the detection of genome-wide gene expression. Microarray data have thus increased significantly in public accessible database repositories, which provide valuable big data for scientific research. To deal with the challenge of microarray big data collected in different research labs using different experimental set-ups and on different bio-samples, this paper presents a primary study to evaluate the impact of two important factors (the origin of bio-samples and the quality of microarray data) on the integrated analytics of multiple microarray data. The aim is to enable the extraction of reliable and robust gene biomarkers from microarray big data. Our work showed that in order to enhance biomarker discovery from microarray big data (i) it is necessary to treat the microarray data differently in terms of their quality, (ii) it is recommended to stratifying (i.e., sub-group) the data according to the origin of bio-samples in the analytics.|10.1109/CIBD.2014.7011535|||||2014|Integrated analytics of microarray big data reveals robust gene signature|Liu, Wanting and Peng, Yonghong and Tobin, Desmond J|inproceedings|7011535||Dec|||||||||||||||||||||||||||545788161|42
||Big Data, public health, cloud computing, medical applications||175-197||The concept of Big Data is popular in a variety of domains. The purpose of this review was to summarize the features, applications, analysis approaches, and challenges of Big Data in health care. Big Data in health care has its own features, such as heterogeneity, incompleteness, timeliness and longevity, privacy, and ownership. These features bring a series of challenges for data storage, mining, and sharing to promote health-related research. To deal with these challenges, analysis approaches focusing on Big Data in health care need to be developed and laws and regulations for making use of Big Data in health care need to be enacted. From a patient perspective, application of Big Data analysis could bring about improved treatment and lower costs. In addition to patients, government, hospitals, and research institutions could also benefit from the Big Data in health care.|https://doi.org/10.2478/dim-2018-0014|https://www.sciencedirect.com/science/article/pii/S2543925122000791||||2018|Big Data in Health Care: Applications and Challenges|Liang Hong and Mengqi Luo and Ruixue Wang and Peixin Lu and Wei Lu and Long Lu|article|HONG2018175|||Data and Information Management|25439251|3|2|||||||||||||||||||||||546675834|529068339
||Business intelligence, big data, data lake, BI architecture||516-524||The data lake approach has emerged as a promising way to handle large volumes of structured and unstructured data. This big data technology enables enterprises to profoundly improve their Business Intelligence. However, there is a lack of empirical studies on the use of the data lake approach in enterprises. This paper provides the results of an exploratory study designed to improve the understanding of the use of the data lake approach in enterprises. I interviewed 12 experts who had implemented this approach in various enterprises and identified three important purposes of implementing data lakes: (1) as staging areas or sources for data warehouses, (2) as a platform for experimentation for data scientists and analysts, and (3) as a direct source for self-service business intelligence. The study also identifies several perceived benefits and challenges of the data lake approach. The results may be beneficial for both academics and practitioners. Further, suggestions for future research is presented.|https://doi.org/10.1016/j.procs.2018.10.071|https://www.sciencedirect.com/science/article/pii/S1877050918317046||||2018|Data lakes in business intelligence: reporting from the trenches|Marilex Rea Llave|article|LLAVE2018516|||Procedia Computer Science|18770509||138||CENTERIS 2018 - International Conference on ENTERprise Information Systems / ProjMAN 2018 - International Conference on Project MANagement / HCist 2018 - International Conference on Health and Social Care Information Systems and Technologies, CENTERIS/ProjMAN/HCist 2018|||19700182801|0,334|-|76|1907|6483|38511|13722|6359|2,09|20,19|Netherlands|Western Europe|2010-2020|Computer Science (miscellaneous)||||548914271|2108686752
||Medical multimedia system, gastrointestinal tract, evaluation|26|||Holistic medical multimedia systems covering end-to-end functionality from data collection to aided diagnosis are highly needed, but rare. In many hospitals, the potential value of multimedia data collected through routine examinations is not recognized. Moreover, the availability of the data is limited, as the health care personnel may not have direct access to stored data. However, medical specialists interact with multimedia content daily through their everyday work and have an increasing interest in finding ways to use it to facilitate their work processes. In this article, we present a novel, holistic multimedia system aiming to tackle automatic analysis of video from gastrointestinal (GI) endoscopy. The proposed system comprises the whole pipeline, including data collection, processing, analysis, and visualization. It combines filters using machine learning, image recognition, and extraction of global and local image features. The novelty is primarily in this holistic approach and its real-time performance, where we automate a complete algorithmic GI screening process. We built the system in a modular way to make it easily extendable to analyze various abnormalities, and we made it efficient in order to run in real time. The conducted experimental evaluation proves that the detection and localization accuracy are comparable or even better than existing systems, but it is by far leading in terms of real-time performance and efficient resource consumption.|10.1145/3079765|https://doi.org/10.1145/3079765|New York, NY, USA|Association for Computing Machinery||2017|From Annotation to Computer-Aided Diagnosis: Detailed Evaluation of a Medical Multimedia System|Riegler, Michael and Pogorelov, Konstantin and Eskeland, Sigrun Losada and Schmidt, Peter Thelin and Albisser, Zeno and Johansen, Dag and Griwodz, Carsten and Halvorsen, P\r{a}l and Lange, Thomas De|article|10.1145/3079765|26|may|ACM Trans. Multimedia Comput. Commun. Appl.|15516857|3|13|August 2017||||||||||||||||||||||549604735|343492898
||Data integrity;Tools;Companies;Cleaning;Task analysis;Machine learning;Regulation;E-business;Big data;data quality;dirty data;machine learning||209-213|2020 11th IEEE Control and System Graduate Research Colloquium (ICSGRC)|The reliability, efficiency, and accuracy of e-business depend on the quality of data that is associated with a buyer, seller, brokers, e-business portals, admins, managers, decision-makers and so on. However, maintaining the quality of data in e-business is very challenging. It is because e-business data typically comes from different communication channels and sources. Integrating and managing the data quality of different sources is generally much troublesome than dealing with traditional business data. Even though there are several data cleaning methods and tools exist those methods and tools have some constraints. None of them directly working, particularly on e-business data that motivates to do research to highlight the aspects of big data quality related to e-business. Therefore, this research demonstrates the problems related to data quality related to online business, discusses the existing literature of data quality, the current tools and techniques that are being used for data quality and provides a research finding highlighting the weaknesses of current tools to address the problem of online business.|10.1109/ICSGRC49013.2020.9232648|||||2020|A Study on the Aspects of Quality of Big Data on Online Business and Recent Tools and Trends Towards Cleaning Dirty Data|Hossen, Md Ismail and Goh, Michael and Hossen, Abid and Rahman, Md. Armanur|inproceedings|9232648||Aug|||||||||||||||||||||||||||549692264|42
|||36|21–56|Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice|||https://doi.org/10.1145/3447404.3447408|New York, NY, USA|Association for Computing Machinery|9781450390293|2021|Internet of Everything|Chatzigiannakis, Ioannis and Tselios, Christos|inbook|10.1145/3447404.3447408|||||||||1||||||||||||||||||||550143401|42
||Audit, Data quality, Blockchain, Secure aggregation, Federated learning||||The development of data-driven artificial intelligence technology has given birth to a variety of big data applications. Data has become an essential factor to improve these applications. Federated learning, a privacy-preserving machine learning method, is proposed to leverage data from different data owners. It is typically used in conjunction with cryptographic methods, in which data owners train the global model by sharing encrypted model updates. However, data encryption makes it difficult to identify the quality of these model updates. Malicious data owners may launch attacks such as data poisoning and free-riding. To defend against such attacks, it is necessary to find an approach to audit encrypted model updates. In this paper, we propose a blockchain-based audit approach for encrypted gradients. It uses a behavior chain to record the encrypted gradients from data owners, and an audit chain to evaluate the gradients’ quality. Specifically, we propose a privacy-preserving homomorphic noise mechanism in which the noise of each gradient sums to zero after aggregation, ensuring the availability of aggregated gradient. In addition, we design a joint audit algorithm that can locate malicious data owners without decrypting individual gradients. Through security analysis and experimental evaluation, we demonstrate that our approach can defend against malicious gradient attacks in federated learning.|https://doi.org/10.1016/j.dcan.2022.05.006|https://www.sciencedirect.com/science/article/pii/S2352864822000979||||2022|A blockchain-based audit approach for encrypted data in federated learning|Zhe Sun and Junping Wan and Lihua Yin and Zhiqiang Cao and Tianjie Luo and Bin Wang|article|SUN2022|||Digital Communications and Networks|23528648|||||||21100823476|1,082|Q1|26|77|105|3226|881|96|8,81|41,90|China|Asiatic Region|2015-2020|Communication (Q1); Computer Networks and Communications (Q1); Hardware and Architecture (Q1)|823|6.797|0.00138|550832189|493706778
||data anonymization, multi-dimensional data, Spark, resilient distributed dataset (RDD), Mondrian|25|||Multi-dimensional data anonymization approaches (e.g., Mondrian) ensure more fine-grained data privacy by providing a different anonymization strategy applied for each attribute. Many variations of multi-dimensional anonymization have been implemented on different distributed processing platforms (e.g., MapReduce, Spark) to take advantage of their scalability and parallelism supports. According to our critical analysis on overheads, either existing iteration-based or recursion-based approaches do not provide effective mechanisms for creating the optimal number of and relative size of resilient distributed datasets (RDDs), thus heavily suffer from performance overheads. To solve this issue, we propose a novel hybrid approach for effectively implementing a multi-dimensional data anonymization strategy (e.g., Mondrian) that is scalable and provides high-performance. Our hybrid approach provides a mechanism to create far fewer RDDs and smaller size partitions attached to each RDD than existing approaches. This optimal RDD creation and operations approach is critical for many multi-dimensional data anonymization applications that create tremendous execution complexity. The new mechanism in our proposed hybrid approach can dramatically reduce the critical overheads involved in re-computation cost, shuffle operations, message exchange, and cache management.|10.1145/3484945|https://doi.org/10.1145/3484945|New York, NY, USA|Association for Computing Machinery||2021|A Novel Hybrid Approach for Multi-Dimensional Data Anonymization for Apache Spark|Bazai, Sibghat Ullah and Jang-Jaccard, Julian and Alavizadeh, Hooman|article|10.1145/3484945|5|nov|ACM Trans. Priv. Secur.|24712566|1|25|February 2022||||21100832567|0,743|Q1|14|28|59|1607|275|59|4,80|57,39|United States|Northern America|2016-2020|Computer Science (miscellaneous) (Q1); Safety, Risk, Reliability and Quality (Q1)|138|1.909|4.1E-4|551859747|129075232
||Big data, Machine learning, Smart meters, Electricity consumption, Clustering, Questionnaire analytics||106902||The consumption data from smart meters and complex questionnaires reveals the electricity consumers’ willingness to adapt their lifestyle to reduce or change their behaviour in electricity usage to flatten the peak in electricity consumption and release the stress in the power grid. Thus, the electricity consumption can support the enforcement of tariff and demand response strategies. Although the plethora of complex, unstructured and heterogeneous data is collected from various devices connected to the Internet, smart meters, plugs, sensors and complex questionnaires, there is an undoubted challenge to handle the data flow that does not provide much information as it remains unprocessed. Therefore, in this paper, we propose an innovative methodology that organizes and extracts valuable information from the increasing volume of data, such as data about the electricity consumption measured and recorded at 30 min intervals, as well as data collected from complex questionnaires.|https://doi.org/10.1016/j.compeleceng.2020.106902|https://www.sciencedirect.com/science/article/pii/S0045790620307540||||2021|Insights into demand-side management with big data analytics in electricity consumers’ behaviour|Simona-Vasilica Oprea and Adela Bâra and Bogdan George Tudorică and Maria Irène Călinoiu and Mihai Alexandru Botezatu|article|OPREA2021106902|||Computers & Electrical Engineering|00457906||89|||||18159|0,630|Q1|64|298|1040|7464|4626|979|4,79|25,05|United Kingdom|Western Europe|1973-1984, 1986-2020|Computer Science (miscellaneous) (Q1); Control and Systems Engineering (Q2); Electrical and Electronic Engineering (Q2)||||554316558|1285201041
||debt cost, big data, quality of accounting information, corporate governance, LASSO method||532-541||Under the background of big data era today, once been widely used method – multiple linear regressions can not satisfy people's need to handle big data any more because of its bad characteristics such as multicollinearity, instability, subjectivity in model chosen etc. Contrary to MLR, LASSO method has many good natures. it is stable and can handle multicollinearity and successfully select the best model and do estimation in the same time. LASSO method is an effective improvement of multiple linear regressions. It is a natural change and innovation to introduce LASSO method into the accounting field and use it to deal with the debt costs problems. It helps us join the statistic field and accounting field together step by step. What's more, in order to proof the applicability of LASSO method in dealing with debt costs problems, we take 2301 companies’ data from Shanghai and Shenzhen A-share market in 2012 as samples, and chose 18 indexes to verify that the results of LASSO method is scientific, reasonable and accurate. In the end, we compare LASSO method with traditional multiple linear regressions and ridge regression, finding out that LASSO method can not only offer the most accurate prediction but also simplify the model.|https://doi.org/10.1016/j.procs.2014.05.299|https://www.sciencedirect.com/science/article/pii/S1877050914004761||||2014|A New Idea of Study on the Influence Factors of Companies’ Debt Costs in the Big Data Era|Li Lin and Wang Shuang and Liu Yifang and Wang Shouyang|article|LIN2014532|||Procedia Computer Science|18770509||31||2nd International Conference on Information Technology and Quantitative Management, ITQM 2014|||19700182801|0,334|-|76|1907|6483|38511|13722|6359|2,09|20,19|Netherlands|Western Europe|2010-2020|Computer Science (miscellaneous)||||554426008|2108686752
||Big data, Electronic health record (EHR), Neonatology, Cost-savings, Clinical decision making, Healthcare analytics||481-497|||https://doi.org/10.1016/j.cnc.2018.07.005|https://www.sciencedirect.com/science/article/pii/S0899588518309754||||2018|Big Data in Neonatal Health Care: Big Reach, Big Reward?|Lynn E. Bayne|article|BAYNE2018481|||Critical Care Nursing Clinics of North America|08995885|4|30||Neonatal Nursing|||27632|0,320|Q3|29|51|164|1964|188|140|1,04|38,51|United Kingdom|Western Europe|1989-2020|Critical Care Nursing (Q3)|592|1.326|6.1E-4|554923990|395947824
Chandos Information Professional Series||Big data, libraries, security, privacy, infrastructure, Hadoop||95-110|Emerging Library Technologies|As experts at searching, retrieving, analyzing, and managing information, librarians are uniquely suited to work with big data. This chapter provides an overview of the popular big data technology. We examine what big data is, challenges and opportunities, and how it is currently being used in many industries and libraries. The chapter concludes with additional resources, some technologies for managing big data, big data terminology, and questions for further discussion.|https://doi.org/10.1016/B978-0-08-102253-5.00005-8|https://www.sciencedirect.com/science/article/pii/B9780081022535000058||Chandos Publishing|978-0-08-102253-5|2018|Chapter 5 - Information Seeking With Big Data: Not Just the Facts|Ida Arlene Joiner|incollection|JOINER201895||||||||||Ida Arlene Joiner|||||||||||||||||||559677131|42
AIEE 2022|Bangkok, Thailand|Power Distribution Network Virtual Production Command Engine, Dispatch Professional Decision, Power Knowledge Graph, Power Distribution Network Regulations, AI|5|33–37|2022 The 3rd International Conference on Artificial Intelligence in Electronics Engineering|Abstract: As the distribution network business hub, the distribution network production command center faces the need to improve the efficiency of the distribution network production command business. This article draws on international mainstream artificial intelligence (such as Google AlphaGo) and other independent learning models to explore the integration of artificial intelligence and power grid professional business. This paper analyzes the development trend of artificial intelligence technology in the fields of power grid distribution and power knowledge map, and proposes a distribution network virtual production commander engine with dispatch operation, remote monitoring, and intelligent screen monitoring capabilities based on the distribution network knowledge map to realize power grid dispatch Intelligent applications in the fields of operation command, emergency repair, and smart services, and some functions have been verified by the State Grid Hangzhou Electric Power Company.|10.1145/3512826.3512836|https://doi.org/10.1145/3512826.3512836|New York, NY, USA|Association for Computing Machinery|9781450395489|2022|Design and Application of Virtual Production Command Service in Power Distribution Network Based on Artificial Intelligence|Wu, Xueqiong and Chen, Lei and Ji, Kun and Wang, Huidong and Qian, Hao and Ma, Lidong|inproceedings|10.1145/3512826.3512836|||||||||||||||||||||||||||||562207941|42
APIT 2020|Bali Island, Indonesia|personal data, digital economy, Financial technology, data privacy, data protection|7|9–15|Proceedings of the 2020 2nd Asia Pacific Information Technology Conference|Financial Technology (fintech) has been immerged extensively in the last decade. In the realm of disruptive world, there are many areas in which startup companies are developing their business. There is always contradiction when dealing with innovation as core of digital disruption and how privacy remains as hot issues at the edge of everybody's talks. Internet plays important roles to sustain the trends. As rapidly growing country, 68% of Indonesian has access to the Internet. It drives startup companies on financial technology to innovate more and besides that they must comply to regulation in regard with personal data protection. This research aims to appraise how startup company on financial technology protect users' personal data. Personal data protection principles from international organization and Indonesian regulation regarding personal data protection are used to appraise how ABC Corp as a startup company that deliver financial technology service in Indonesian society. To ensure that its service is qualified and trustable, ABC Corp should be appraised using relevant criteria and qualitative approach. The results showed that most of regulations from sectorial supervising agency have been adhered by ABC Corp. The results bring meaningful insight to improve performance on personal data protection. They can became lessons for similar emerging startup companies in financial technology when acquiring their qualifications to protect users' personal data and keep their sustainability.|10.1145/3379310.3379322|https://doi.org/10.1145/3379310.3379322|New York, NY, USA|Association for Computing Machinery|9781450376853|2020|Appraising Personal Data Protection in Startup Companies in Financial Technology: A Case Study of ABC Corp|Rozi, Muhamad Fahru and Sucahyo, Yudho Giri and Gandhi, Arfive and Ruldeviyani, Yova|inproceedings|10.1145/3379310.3379322|||||||||||||||||||||||||||||563894187|42
||Big data;Databases;Labeling;Hybrid power systems;Crowdsourcing;Training;Cleaning;matching rules;deduplication;entity resolution;big data quality||2621-2627|2015 IEEE International Conference on Big Data (Big Data)|Matching dependencies (MDs) were recently introduced as quality rules for data cleaning and entity resolution. They are rules that specify what values should be considered duplicates, and have to be matched. Defining such quality rules on a database instance, is a very expensive and a time consuming process, and requires huge efforts to analyse the whole database. In this demo paper, we present CrowdMD, a hybrid machine-crowd system for generating MDs. It first asks the crowd to determine whether a given pair, from training sample pairs, match or not. Then, it uses data mining techniques to generate attributes constituting an MD. Using a Restaurant database, we will show how the crowders can help to generate MDs by labelling the training sample through the CrowdMD user interface and how MDs can be mined from this training set.|10.1109/BigData.2015.7364061|||||2015|CrowdMD: Crowdsourcing-based approach for deduplication|Abboura, Asma and Sahrl, Soror and Ouziri, Mourad and Benbernou, Salima|inproceedings|7364061||Oct|||||||||||||||||||||||||||567604738|42
||Data quality, Data integration, Data curation, Genomic datasets, Metadata, Interoperability||100009||Genomic data are growing at unprecedented pace, along with new protocols, update polices, formats and guidelines, terminologies and ontologies, which are made available every day by data providers. In this continuously evolving universe, enforcing quality on data and metadata is increasingly critical. While many aspects of data quality are addressed at each individual source, we focus on the need for a systematic approach when data from several sources are integrated, as such integration is an essential aspect for modern genomic data analysis. Data quality must be assessed from many perspectives, including accessibility, currency, representational consistency, specificity, and reliability. In this article we review relevant literature and, based on the analysis of many datasets and platforms, we report on methods used for guaranteeing data quality while integrating heterogeneous data sources. We explore several real-world cases that are exemplary of more general underlying data quality problems and we illustrate how they can be resolved with a structured method, sensibly applicable also to other biomedical domains. The overviewed methods are implemented in a large framework for the integration of processed genomic data, which is made available to the research community for supporting tertiary data analysis over Next Generation Sequencing datasets, continuously loaded from many open data sources, bringing considerable added value to biological knowledge discovery.|https://doi.org/10.1016/j.cmpbup.2021.100009|https://www.sciencedirect.com/science/article/pii/S2666990021000082||||2021|Data quality-aware genomic data integration|Anna Bernasconi|article|BERNASCONI2021100009|||Computer Methods and Programs in Biomedicine Update|26669900||1|||||||||||||||||||||||569612534|327184063
ICICSE 2021|Guilin, China|Data Center, GRU, AIOps, GAN, KPI Anomaly Detection|7|23–29|2021 10th International Conference on Internet Computing for Science and Engineering|The system architecture and application services of the data center are becoming increasingly large. To ensure the stable operation of the systems and businesses carried by the data center, the operations engineer needs to collect and monitor the generating KPIs during the operation of the systems and services. Traditional KPI anomaly detection methods are faced with the challenges of the huge amount of KPIs and constantly changing data characteristics, which are gradually no longer suitable for highly dynamic systems and services. With the popularity of artificial intelligence algorithms, machine learning and deep learning methods have also begun to be applied in operation and maintenance scenarios, that is the emergence of Artificial Intelligence for IT Operations (AIOps). KPI anomaly detection is the underlying core technology of AIOps. This paper proposes a hybrid model based on GRU-GAN (GGAN) for KPI anomaly detection in data center AIOps. The Gated Recurrent Unit (GRU) network is selected as the generator and discriminator of Generative adversarial network (GAN) in this model, which get the time correlation and data distribution of KPI through the adversarial training between the generator and the discriminator to make use of the reconstruction ability of the generator and the discriminant ability of the discriminator at the same time. At the anomaly detection stage, the anomaly score is formed by integrating reconstruction difference and discrimination loss to complete the anomaly detection task. Experimental results show that the proposed method can more accurately capture the variable data characteristics of KPI compared with the traditional KPI anomaly detection method and the general unsupervised method, as well as achieve better performance in the KPI anomaly detection task.|10.1145/3485314.3485323|https://doi.org/10.1145/3485314.3485323|New York, NY, USA|Association for Computing Machinery|9781450384957|2022|KPI Anomaly Detection Method for Data Center AIOps Based on GRU-GAN|Su, Hang and He, Qian and Guo, Biao|inproceedings|10.1145/3485314.3485323|||||||||||||||||||||||||||||569681589|42
||Building energy efficiency, Energy performance certificate, Deep learning, Google street view, SHapley additive explanations||112331||With buildings consuming nearly 40% of energy in developed countries, it is important to accurately estimate and understand the building energy efficiency in a city. A better understanding of building energy efficiency is beneficial for reducing overall household energy use and providing guidance for future housing improvement and retrofit. In this research, we propose a deep learning-based multi-source data fusion framework to estimate building energy efficiency. We consider the traditional factors associated with the building energy efficiency from the Energy Performance Certificate (EPC) for 160,000 properties (30,000 buildings) in Glasgow, UK (e.g., property structural attributes and morphological attributes), as well as the Google Street View (GSV) building façade images as a complement. We compare the performance improvements between our data-fusion framework with traditional morphological attributes and image-only models. The results show that including the building façade images from GSV, the overall model accuracy increases from 79.7% to 86.8%. A further investigation and explanation of the deep learning model are conducted to understand the relationships between building features and building energy efficiency by using SHapley Additive exPlanations (SHAP). Our research demonstrates the potential of using multi-source data in building energy efficiency prediction with high accuracy and short inference time. Our paper also helps understand building energy efficiency at the city level to help achieve the net-zero target by 2050.|https://doi.org/10.1016/j.enbuild.2022.112331|https://www.sciencedirect.com/science/article/pii/S0378778822005023||||2022|Understanding building energy efficiency with administrative and emerging urban big data by deep learning in Glasgow|Maoran Sun and Changyu Han and Quan Nie and Jingying Xu and Fan Zhang and Qunshan Zhao|article|SUN2022112331|||Energy and Buildings|03787788||273|||||29359|1,737|Q1|184|705|2402|39338|15832|2394|6,33|55,80|Netherlands|Western Europe|1970, 1977-2020|Building and Construction (Q1); Civil and Structural Engineering (Q1); Electrical and Electronic Engineering (Q1); Mechanical Engineering (Q1)|51,254|5.879|0.04387|572059930|1347637955
||ETL physical implementation, ETL conceptual design, ETL logical design, ETL workflow, ETL optimization|25|777–801||In this paper, we discuss the state of the art and current trends in designing and optimizing ETL workflows. We explain the existing techniques for: (1) constructing a conceptual and a logical model of an ETL workflow, (2) its corresponding physical implementation, and (3) its optimization, illustrated by examples. The discussed techniques are analyzed w.r.t. their advantages, disadvantages, and challenges in the context of metrics such as autonomous behavior, support for quality metrics, and support for ETL activities as user-defined functions. We draw conclusions on still open research and technological issues in the field of ETL. Finally, we propose a theoretical ETL framework for ETL optimization.|10.1007/s00778-017-0477-2|https://doi.org/10.1007/s00778-017-0477-2|Berlin, Heidelberg|Springer-Verlag||2017|From Conceptual Design to Performance Optimization of ETL Workflows: Current State of Research and Open Problems|Ali, Syed Muhammad and Wrembel, Robert|article|10.1007/s00778-017-0477-2||dec|The VLDB Journal|10668888|6|26|December  2017||||13646|0,653|Q1|90|64|117|4824|544|113|4,46|75,38|United States|Northern America|1992-2020|Hardware and Architecture (Q1); Information Systems (Q1)|2,106|2.868|0.0023|577932669|924310569
||Database design, Complex semantics, Disaggregation construct||||Conceptual modeling is important for developing databases that maintain the integrity and quality of stored information. However, classical conceptual models have often been assumed to work on well-maintained and high-quality data. With the advancement and expansion of data science, it is no longer the case. The need to model and store data has emerged for settings with lower data quality, which creates the need to update and augment conceptual models to represent lower-quality data. In this paper, we focus on the intersection between data completeness (an important aspect of data quality) and complex class semantics (where a complex class entity represents information that spans more than one simple class entity). We propose a new disaggregation construct to allow the modeling of incomplete information. We demonstrate the use of our disaggregation construct for diverse modeling problems and discuss the anomalies that could occur without this construct. We provide formal definitions and thorough comparisons between various types of complex constructs to guide future application and prove the unique interpretation of our newly proposed disaggregation construct.|10.1145/3532784|https://doi.org/10.1145/3532784|New York, NY, USA|Association for Computing Machinery||2022|Data Completeness and Complex Semantics in Conceptual Modeling: The Need for a Disaggregation Construct|Li, Yuanxia and Currim, Faiz and Ram, Sudha|article|10.1145/3532784||aug|J. Data and Information Quality|19361955||||Just Accepted|||||||||||||||||||||582740366|833754770
||Data analytics, Knowledge management, Big data, Business intelligence, Data discovery||156-165||Big data is significantly dependent on technologies such as cloud computing, machine learning and statistical models. However, its significance is becoming more dependent on human qualities e.g. judgment, value, intuition and experience. Therefore, the human knowledge presents a basis for knowledge management and big data, which are a major element of data analytics. This research contribution applies the process of Data, Information, Knowledge and Perception hierarchy as a structure to evaluate the end-users’ process. The framework in incorporating data analytics and display a conceptual data analytics process (with three phases) evaluated as knowledge management, including the creation, discovery and application of knowledge. Knowledge conversion theories are applicable in data analytics to emphasize on the typically overlooked organizational and human aspects, which are critical to the efficiency of data analytics. The synergy and alignment between knowledge management and data analytics is fundamental in fostering innovations and collaboration.|https://doi.org/10.1016/j.ijin.2021.09.004|https://www.sciencedirect.com/science/article/pii/S2666603021000208||||2021|Framework of Data Analytics and Integrating Knowledge Management|Camilla Schaefer and Ana Makatsaria|article|SCHAEFER2021156|||International Journal of Intelligent Networks|26666030||2|||||||||||||||||||||||585746481|2003593324
MISNC '18|Saint-Etienne, France|Privacy Protection, Personal Data, Legal Risk Assessment, Information system, Data Sharing|5||Proceedings of the 5th Multidisciplinary International Social Networks Conference|Regulations on protection of personal information vary from country to country. Therefore, when conducting international surveys for research, it is required to collect, manage and operate personal data properly complying with the laws and regulations of each country.We design a support system to fulfill conditions in terms of compliance for the proper and efficient management of data collection and utilization especially universities by making compliance management related to data cooperation a common foundation.This study aims to discuss requirements for the compliance management base system for data alliance and shared use of data.|10.1145/3227696.3227715|https://doi.org/10.1145/3227696.3227715|New York, NY, USA|Association for Computing Machinery|9781450364652|2018|Data Sharing System Based on Legal Risk Assessment|Tanaka, Yasuhiro and Kodate, Akihisa and Bolt, Timothy|inproceedings|10.1145/3227696.3227715|17||||||||||||||||||||||||||||587775757|42
LAK '14|Indianapolis, Indiana, USA|tools for sense-making in learning analytics, theories and theoretical concepts for understanding learning, data-driven decisions, learning analytics, educational data science, methods, analytic approaches, big data, learner analytics, educational data mining|10|193–202|Proceedings of the Fourth International Conference on Learning Analytics And Knowledge|In this paper, we develop a conceptual framework for organizing emerging analytic activities involving educational data that can fall under broad and often loosely defined categories, including Academic/Institutional Analytics, Learning Analytics/Educational Data Mining, Learner Analytics/Personalization, and Systemic Instructional Improvement. While our approach is substantially informed by both higher education and K-12 settings, this framework is developed to apply across all educational contexts where digital data are used to inform learners and the management of learning. Although we can identify movements that are relatively independent of each other today, we believe they will in all cases expand from their current margins to encompass larger domains and increasingly overlap. The growth in these analytic activities leads to the need to find ways to synthesize understandings, find common language, and develop frames of reference to help these movements develop into a field.|10.1145/2567574.2567582|https://doi.org/10.1145/2567574.2567582|New York, NY, USA|Association for Computing Machinery|9781450326643|2014|Educational Data Sciences: Framing Emergent Practices for Analytics of Learning, Organizations, and Systems|Piety, Philip J. and Hickey, Daniel T. and Bishop, M. J.|inproceedings|10.1145/2567574.2567582|||||||||||||||||||||||||||||590806588|42
MobiGIS '14|Dallas, Texas|geo-spatial data and knowledge, spatial data quality and uncertainty, statistical matching, urban and environmental planning, big data, context recognition, human activity recognition, bank card transactions|8|18–25|Proceedings of the Third ACM SIGSPATIAL International Workshop on Mobile Geographic Information Systems|Recent availability of big data of digital traces of human activity boosted research on human behavior. However, in most of the datasets such as mobile phone data or GPS traces, an important layer of information is typically missing: providing an extensive information of when and where people go typically does not allow understanding of what they do there. Predicting the context of human behavior in such cases where such information is not directly available from the data is a complex task that addresses context recognition problems. To fill in the contextual information for such data, we developed an ontological and stochastic model (HRBModel) that interprets semantic (high-level) human behaviors from geographical maps like OpenStreetMap, analyzing the distribution of Points of Interest(POIs), in a given region and time period. The semantic human behaviors are human activities that are accompanied by their likelihood, depending on their location and time. In this paper, we perform an extended evaluation of this model based on other qualitative data source, namely a country-wide anonymized bank card transaction data in Spain, which contains contextual information about the locations and the types of business categories where transactions occurred. This allows us to validate the model, by matching our predicted activity patterns with the actually observed ones, so that it can be later applied to the cases where such information is unavailable. This extended evaluation aimed to define the applicability of the predictive model, HRBModel, taking various type of spatial and temporal factors into account.|10.1145/2675316.2675321|https://doi.org/10.1145/2675316.2675321|New York, NY, USA|Association for Computing Machinery|9781450331425|2014|Human Activity Recognition from Spatial Data Sources|Dashdorj, Zolzaya and Sobolevsky, Stanislav and Serafini, Luciano and Ratti, Carlo|inproceedings|10.1145/2675316.2675321|||||||||||||||||||||||||||||592648385|42
dg.o '17|Staten Island, NY, USA|big data, e-government, open data, BOLD, infrastructure, enterprise architecture, ICT-architecture|6|505–510|Proceedings of the 18th Annual International Conference on Digital Government Research|Governments from all over the world are struggling to take advantage of big data developments. Enterprise Architecture (EA) can be used as an instrument to integrate big data (BD) in the existing business processes and ICT-landscape. In this policy paper, we explore the role of EA in the adoption of BD. For this, we adopted a qualitative case study approach and investigated a large administrative organization that was in the process of adopting BD. We found in our case study that the first attempts were focused on integrating big data in the current landscape, but this encountered too many challenges that halt progress. To overcome the challenges, a separate BD department and accompanying infrastructure was created. The strategy was first to reap the benefits of BD and to understand what should be done, and thereafter integrating the working systems in the existing landscape. The findings suggest that current infrastructures might not be suitable for integrating BD and substantial changes are needed first. In the case the role of BD needed to be first clarified before EA could play a role in adopting BD. EA should deal with the uncertainties and complexities by ensuring a configurable landscape, by providing an incremental approach for adapting the infrastructure step-by-step, before the benefits of big data can be gained. Developing an incremental migration plan was found to be a key aspect for the adoption of BD.|10.1145/3085228.3085275|https://doi.org/10.1145/3085228.3085275|New York, NY, USA|Association for Computing Machinery|9781450353175|2017|Enterprise Architectures for Supporting the Adoption of Big Data|Gong, Yiwei and Janssen, Marijn|inproceedings|10.1145/3085228.3085275|||||||||||||||||||||||||||||593516196|42
||Big Data, Systematic mapping study, Framework, Artefact, Usage, Analytics||105-115||Big Data has emerged as a significant area of study for both practitioners and researchers. Big Data is a term for massive data sets with large structure. In 2012, Big Data passed the top of the Gartner Hype Cycle, attesting the maturity level of this technology and its applications. The aim of this paper is to examine how do researchers grasp the big data concept? We will answer the following questions: How many research papers are produced? What is the annual trend of publications? What are the hot topics in big data research? What are the most investigated big data topics? Why the research is performed? What are the most frequently obtained research artefacts? What does big data research produces? Who are the active authors? Which journals include papers on Big Data? What are the active disciplines? For this purpose, we provide a framework identifying existing and emerging research areas of Big Data. This framework is based on eight dimensions, including the SMACIT (Social Mobile Analytics Cloud Internet of Things) perspective. Current and past research in Big Data are analyzed using a systematic mapping study of publications based on more than a decade of related academic publications. The results have shown that significant contributions have been made by the research community, attested by a continuous increase in the number of scientific publications that address Big Data. We found that researchers are increasingly involved in research combining Big Data and Analytics, Cloud, Internet of things, mobility or social media. As for quality objectives, besides an interest in performance, other topics as scalability is emerging. Moreover, security and quality aspects become important. Researchers on Big Data provide more algorithms, frameworks, and architectures than other artifacts. Finally, application domains such as earth, energy, medicine, ecology, marketing, and health attract more attention from researchers on big data. A complementary content analysis on a subset of papers sheds some light on the evolving field of big data research.|https://doi.org/10.1016/j.csi.2017.01.004|https://www.sciencedirect.com/science/article/pii/S0920548917300211||||2017|Research on Big Data – A systematic mapping study|Jacky Akoka and Isabelle Comyn-Wattiau and Nabil Laoufi|article|AKOKA2017105|||Computer Standards & Interfaces|09205489||54||SI: New modeling in Big Data|||24303|0,556|Q1|63|36|246|2302|1013|243|3,71|63,94|Netherlands|Western Europe|1985-2020|Law (Q1); Hardware and Architecture (Q2); Software (Q2)||||597346142|827980402
||Measurement;Internet of Things;Standards;Smart cities;Task analysis;Intelligent sensors;Data quality;Internet of Things (IoT);IoT architectures;IoT-based services;mobile crowdsensing (MCS);small data (SD)||10955-10968||Mobile crowdsensing (MCS) is a paradigm that exploits the presence of a crowd of moving human participants to acquire, or generate, data from their environment. As a part of the Internet-of-Things (IoT) paradigm, MCS serves the quest for a more efficient operation of a smart city. Big data techniques employed on this data produce inferences about the participants' environment, the smart city. However, sufficient amounts of data are not always available. Sometimes, the available data are scarce as it is obtained at different times, locations, and from different MCS participants who may not be present. As a consequence, the scale of data acquired may be small and susceptible to errors. In such scenarios, the MCS system requires techniques that acquire reliable inferences from such limited data sets. To that end, we resort to small data (SD) techniques that are relevant for scarce and erroneous scenarios. In this article, we discuss SD and propose schemes to tackle the problems associated with such limited data sets, in the context of the smart city. We propose two novel quality metrics: 1) MAD quality metric (MAD-Q) and 2) MAD bootstrap quality metric (MADBS-Q), to deal with SD, focusing on evaluating the quality of a data set within MCS. We also propose an MCS-specific coverage metric that combines the spatial dimension with MAD-Q and MADBS-Q. We show the performance of all the presented techniques through closed-form mathematical expressions, with which simulation results were found to be consistent.|10.1109/JIOT.2020.2994556|||||2020|Quality Estimation for Scarce Scenarios Within Mobile Crowdsensing Systems|Azmy, Sherif B. and Zorba, Nizar and Hassanein, Hossam S.|article|9093050||Nov|IEEE Internet of Things Journal|23274662|11|7|||||21100338350|2,075|Q1|97|1163|1594|40380|20461|1555|12,37|34,72|United States|Northern America|2014-2020|Computer Networks and Communications (Q1); Computer Science Applications (Q1); Hardware and Architecture (Q1); Information Systems (Q1); Information Systems and Management (Q1); Signal Processing (Q1)|21,151|9.471|0.03208|599289980|1395601152
||Itemsets;Soft sensors;Digital transformation;Data integrity;Conferences;Big Data;Feature extraction;Digital transformation;entity linking;topic extraction;word embedding;pattern search;frequent itemset mining;data quality||5900-5902|2021 IEEE International Conference on Big Data (Big Data)|This paper presents a data driven approach to replace expert driven business processes. The QMLEx methodology combines NLP algorithms to improve data quality, ML techniques to perform the task and exploits external data sources to eliminate the need for the expert input. The methodology is applied to our internal process devoted to creating groups of products with similar features, one of the most relevant use case in marketing analytics.|10.1109/BigData52589.2021.9671890|||||2021|QMLEx: Data Driven Digital Transformation in Marketing Analytics|Geronazzo, Angela and Ziegler, Markus|inproceedings|9671890||Dec|||||||||||||||||||||||||||600830580|42
||Risk analytics, Adaptation, Remote sensing, Big Data||598-612||Insurance plays a crucial role in human efforts to adapt to environmental hazards. Effective insurance can serve as both a measure to distribute, and a method to communicate risk. In order for insurance to fulfil these roles successfully, policy pricing and cover choices must be risk-based and founded on accurate information. This is reliant on a robust evidence base forming the foundation of policy choices. This paper focuses on the evidence available to insurers and emergent innovation in the use of data. The main risk considered is coastal flooding, for which the insurance sector offers an option for potential adaptation, capable of increasing resilience. However, inadequate supply and analysis of data have been highlighted as factors preventing insurance from fulfilling this role. Research was undertaken to evaluate how data are currently, and could potentially, be used within risk evaluations for the insurance industry. This comprised of 50 interviews with those working and associated with the London insurance market. The research reveals new opportunities, which could facilitate improvements in risk-reflective pricing of policies. These relate to a new generation of data collection techniques and analytics, such as those associated with satellite-derived data, IoT (Internet of Things) sensors, cloud computing, and Big Data solutions. Such technologies present opportunities to reduce moral hazard through basing predictions and pricing of risk on large empirical datasets. The value of insurers' claims data is also revealed, and is shown to have the potential to refine, calibrate, and validate models and methods. The adoption of such data-driven techniques could enable insurers to re-evaluate risk ratings, and in some instances, extend coverage to locations and developments, previously rated as too high a risk to insure. Conversely, other areas may be revealed more vulnerable, which could generate negative impacts for residents in these regions, such as increased premiums. However, the enhanced risk awareness generated, by new technology, data and data analytics, could positively alter future planning, development and investment decisions.|https://doi.org/10.1016/j.scitotenv.2019.01.114|https://www.sciencedirect.com/science/article/pii/S0048969719301317||||2019|Innovations in the use of data facilitating insurance as a resilience mechanism for coastal flood risk|Alexander G. Rumson and Stephen H. Hallett|article|RUMSON2019598|||Science of The Total Environment|00489697||661|||||25349|1,795|Q1|244|6929|13278|455970|106837|13125|7,96|65,81|Netherlands|Western Europe|1970, 1972-2021|Environmental Chemistry (Q1); Environmental Engineering (Q1); Pollution (Q1); Waste Management and Disposal (Q1)|210,143|7.963|0.23082|601277174|2019676356
||Big data, Data-driven culture, Competitive pressures, Mass customization, Marketing capability, CRM performance||483-494||This study investigates the driving forces of a firm's assimilation of big data analytical intelligence (BDAI) and how the assimilation of BDAI improve customer relationship management (CRM) performance. Drawing on the resource-based view, this study argues that a firm's data-driven culture and the competitive pressure it faces in the industry motivate a firm's assimilation of BDAI. As a firm resource, BDAI enables an organization to develop superior mass-customization capability, which in turn positively influences its CRM performance. In addition, this study proposes that a firm's marketing capability can moderate the impact of BDAI assimilation on its mass-customization capability. Using survey data collected from 147 business-to-business companies, this study finds support for most of the hypotheses. The findings of this study uncover compelling insights about the dynamics involved in the process of using BDAI to improve CRM performance.|https://doi.org/10.1016/j.indmarman.2020.10.012|https://www.sciencedirect.com/science/article/pii/S0019850120308762||||2020|Linking big data analytical intelligence to customer relationship management performance|Chubing Zhang and Xinchun Wang and Annie Peng Cui and Shenghao Han|article|ZHANG2020483|||Industrial Marketing Management|00198501||91|||||22792|2,022|Q1|136|314|465|27989|3787|450|6,86|89,14|United States|Northern America|1971-2020|Marketing (Q1)|16,291|6.960|0.00901|601390482|223518748
EBIMCS 2020|Wuhan, China|Big data, Information-oriented education, Smart education|6|228–233|Proceedings of the 2020 3rd International Conference on E-Business, Information Management and Computer Science|The big data technology can be applied to build the education service platforms and construct the big data analysis and application system as well as the multi-dimensional perception system. The big data analysis assists in the teaching process and breaks the temporal and spatial restrictions of educational resources, to realize the diversification of educational resources and improve the effectiveness of teaching feedback. This paper proposes a smart education service platform based on big data, which can promote the organic integration of educational communication, educational research, learning activities, teaching affairs administration, and information infrastructures. At the same time, the platform provides smarter, more efficient, and accurate services for teaching.|10.1145/3453187.3453340|https://doi.org/10.1145/3453187.3453340|New York, NY, USA|Association for Computing Machinery|9781450389099|2021|Research on Smart Education Service Platform Based on Big Data|Hu, Zhifeng and Zhao, Feng and Zhao, Xiaona|inproceedings|10.1145/3453187.3453340|||||||||||||||||||||||||||||603029788|42
|||||Data Cleaning|Data quality is one of the most important problems in data management, since dirty data often leads to inaccurate data analytics results and incorrect business decisions. Poor data across businesses and the U.S. government are reported to cost trillions of dollars a year. Multiple surveys show that dirty data is the most common barrier faced by data scientists. Not surprisingly, developing effective and efficient data cleaning solutions is challenging and is rife with deep theoretical and engineering problems.This book is about data cleaning, which is used to refer to all kinds of tasks and activities to detect and repair errors in the data. Rather than focus on a particular data cleaning task, we give an overview of the endto- end data cleaning process, describing various error detection and repair methods, and attempt to anchor these proposals with multiple taxonomies and views. Specifically, we cover four of the most common and important data cleaning tasks, namely, outlier detection, data transformation, error repair (including imputing missing values), and data deduplication. Furthermore, due to the increasing popularity and applicability of machine learning techniques, we include a chapter that specifically explores how machine learning techniques are used for data cleaning, and how data cleaning is used to improve machine learning models.This book is intended to serve as a useful reference for researchers and practitioners who are interested in the area of data quality and data cleaning. It can also be used as a textbook for a graduate course. Although we aim at covering state-of-the-art algorithms and techniques, we recognize that data cleaning is still an active field of research and therefore provide future directions of research whenever appropriate.||https://doi.org/10.1145/3310205.3310211|New York, NY, USA|Association for Computing Machinery|9781450371520|2019|Data Quality Rule Definition and Discovery||inbook|10.1145/3310205.3310211|||||||||||||||||||||||||||||604160963|42
SIGGRAPH '21|Virtual Event, USA||213||ACM SIGGRAPH 2021 Courses||10.1145/3450508.3464558|https://doi.org/10.1145/3450508.3464558|New York, NY, USA|Association for Computing Machinery|9781450383615|2021|Visual Analytics for Large Networks: Theory, Art and Practice|Bednarz, Tomasz and Hughes, Rowan T. and Mathews, Alex and Chen, Dawei and Zhu, Liming and Filonik, Daniel|inproceedings|10.1145/3450508.3464558|15||||||||||||||||||||||||||||604506339|42
||Monitoring;5G mobile communication;Green products;Big Data;Renewable energy sources;Quality of service;Computer architecture||116-123||The 5G system has been recognized as the most promising technology to provide high-quality network services. As a huge number of networking and computing equipments that generate big data are integrated into the 5G system, energy efficiency becomes the major challenge in building a green 5G system. In this article, we propose a software-defined green 5G system for big data, which consists of three planes: the control plane, the data plane and the energy plane. The data plane contains networking and computing equipments, which can be powered by both traditional grid and renewable energy sources in the energy plane. The control plane monitors the system status and configures the corresponding equipments to achieve energy efficiency and quality-of-service. Furthermore, to reduce the overhead of this software- defined architecture, we investigate a FRS to eliminate redundant system monitoring information. To integrate features in software-defined architecture, we propose an AIFS to mine latent rules among features. Simulation results indicate that our proposals achieve higher efficiency in the green 5G system.|10.1109/MCOM.2017.1700048|||||2018|Software-Defined Green 5G System for Big Data|Mi, Jun and Wang, Kun and Li, Peng and Guo, Song and Sun, Yanfei|article|8469815||November|IEEE Communications Magazine|15581896|11|56|||||||||||||||||||||||606184813|160873938
||Cloud computing;Security;Big Data;Software;Organizations;Social networking (online);STEM;Security challenges;big data;cloud computing;SLR;vendor;SPSS||107309-107332||Recently, its becomes easy to track down the data due to its availability in a large number. Although for data management, processing, and obtainability, cloud computing is considered a well-known approach for organizational development on the internet. Despite many advantages, cloud computing has still numerous security challenges that can affect the big-data usage on cloud computing. To find the security issues/challenges that are faced by software vendors’ organizations we conducted a systematic literature review (SLR) through which we have find out 103 relevant research publications by developing a search string that is inspired by the research questions. This relevant data was comprised from different databases e.g. Google Scholar, IEEE Explore, ScienceDirect, ACM Digital Library, and SpringerLink. Furthermore, for the detailed literature review, we have accomplished all the steps in SLR, for example, development of SLR protocol, Initials and final assortment of the relevant data, data extraction, data quality assessment, and data synthesis. We identified fifteen (15) critical security challenges which are: data secrecy, geographical data location, unauthorized data access, lack of control, lack of data management, network-level issues, data integrity, data recovery, lack of trust, data sharing, data availability, asset issues, legal amenabilities, lack of quality, and lack of consistency. Furthermore, sixty four (64) standard practices are identified for these critical security challenges using the proposed SLR that could help vendor organizations to overcome the security challenges for big data. The findings of our research study demonstrate the resemblances and divergences in the identified security challenges in different periods, continents, databases, and methods. The proposed SLR will also support software vendor organizations for securing big data on the cloud computing platforms. This paper has the following content: in Section II, we have describe the Literature review; in Section III, research methodology is specified; in Section IV, the findings of the SLR and the analysis of result are discussed; in Section V, the limitations of this research are given; in Section VI, we discussed our conclusions and future work.|10.1109/ACCESS.2021.3100287|||||2021|Analyzing and Evaluating Critical Challenges and Practices for Software Vendor Organizations to Secure Big Data on Cloud Computing: An AHP-Based Systematic Approach|Khan, Abudul Wahid and Khan, Maseeh Ullah and Khan, Javed Ali and Ahmad, Arshad and Khan, Khalil and Zamir, Muhammad and Kim, Wonjoon and Ijaz, Muhammad Fazal|article|9496639|||IEEE Access|21693536||9|||||21100374601|0,587|Q1|127|18036|24267|771081|116691|24200|4,48|42,75|United States|Northern America|2013-2020|Computer Science (miscellaneous) (Q1); Engineering (miscellaneous) (Q1); Materials Science (miscellaneous) (Q2)|105,968|3.367|0.15396|607425545|1905633267
||Computational modeling, Functional brain imaging, Signal-to-noise ratio, Reliability, Replicability||775-785||We analyzed factors that may hamper the advancement of computational cognitive neuroscience (CCN). These factors include a particular statistical mindset, which paves the way for the dominance of statistical power theory and a preoccupation with statistical replicability in the behavioral and neural sciences. Exclusive statistical concerns about sampling error occur at the cost of an inadequate representation of the problem of measurement error. We contrasted the manipulation of data quantity (sampling error, by varying the number of subjects) against the manipulation of data quality (measurement error, by varying the number of data per subject) in a simulated Bayesian model identifiability study. The results were clear-cut in showing that - across all levels of signal-to-noise ratios - varying the number of subjects was completely inconsequential, whereas the number of data per subject exerted massive effects on model identifiability. These results emphasize data quality over data quantity, and they call for the integration of statistics and measurement theory.|https://doi.org/10.1016/j.neuroimage.2018.01.005|https://www.sciencedirect.com/science/article/pii/S1053811918300053||||2018|Data quality over data quantity in computational cognitive neuroscience|Antonio Kolossa and Bruno Kopp|article|KOLOSSA2018775|||NeuroImage|10538119||172|||||||||||||||||||||||612353418|2006600757
||Energy consumption, Prediction, Optimization, Data Mining, Machine Learning, Forecasting, Industry 4.0||503-510||The fusion of emerged technologies such as Artificial Intelligence, cloud computing, big data, and the Internet of Things in manufacturing has pioneered this industry to meet the fourth stage of the industrial revolution (industry 4.0). One major approach to keeping this sector sustainable and productive is intelligent energy demand planning. Monitoring and controlling the consumption of energy under industry 4.0, directly results in minimizing the cost of operation and maximizing efficiency. To advance the research on the adoption of industry 4.0, this study examines CRISP-DM methodology to project data mining approach over data from 2020 to 2021 which was collected from industrial sensors to predict/forecast future electrical consumption at Bosch car multimedia facilities located at Braga, Portugal. Moreover, the influence of indicators such as humidity and temperature on electrical energy consumption was investigated. This study employed five promising regression algorithms and FaceBook prophet (FB prophet) to apply over data belonging to two HVAC (heating, ventilation, and air conditioning) sensors (E333, 3260). Results indicate Random Forest (RF) algorithms as a potential regression approach for prediction and the outcome of FB prophet to forecast the demand of future usage of electrical energy associated with HVAC presented. Based on that, it was concluded that predicting the usage of electrical energy for both data points requires time series techniques. Where “timestamp” was identified as the most effective feature to predict consume of electrical energy by regression technique (RF). The result of this study was integrated with Intelligent Industrial Management System (IIMS) at Bosch Portugal.|https://doi.org/10.1016/j.procs.2022.03.065|https://www.sciencedirect.com/science/article/pii/S1877050922004781||||2022|Intelligent energy management using data mining techniques at Bosch Car Multimedia Portugal facilities|Nasim Sadat Mosavi and Francisco Freitas and Rogério Pires and César Rodrigues and Isabel Silva and Manuel Santos and Paulo Novais|article|MOSAVI2022503|||Procedia Computer Science|18770509||201||The 13th International Conference on Ambient Systems, Networks and Technologies (ANT) / The 5th International Conference on Emerging Data and Industry 4.0 (EDI40)|||19700182801|0,334|-|76|1907|6483|38511|13722|6359|2,09|20,19|Netherlands|Western Europe|2010-2020|Computer Science (miscellaneous)||||612640948|2108686752
DanaC'14|Snowbird, UT, USA|Big data analytics, data integration, metadata, data discovery|4|1–4|Proceedings of Workshop on Data Analytics in the Cloud|"\"Current big data ecosystems lack a principled approach to metadata management. This impedes large organizations' ability to share data and data preparation and analysis code, to integrate data, and to ensure that analytic code makes compatible assumptions with the data it uses. This use-case paper describes the challenges and an in-progress effort to address them. We present a real application example, discuss requirements for \"\"big metadata\"\" drawn from that example as well as other U.S. government analytic applications, and briefly describe an effort to adapt an existing open source metadata manager to support the needs of big data ecosystems.\""|10.1145/2627770.2627776|https://doi.org/10.1145/2627770.2627776|New York, NY, USA|Association for Computing Machinery|9781450329972|2014|"\"\"\"Big Metadata\"\": The Need for Principled Metadata Management in Big Data Ecosystems\""|Smith, Ken and Seligman, Len and Rosenthal, Arnon and Kurcz, Chris and Greer, Mary and Macheret, Catherine and Sexton, Michael and Eckstein, Adric|inproceedings|10.1145/2627770.2627776|||||||||||||||||||||||||||||612787279|42
||Systems Bioscience, OMICs, Pathogenicity, Transmission, Adaptation, Data Mining, Big Data, Machine Learning, Association Mining, Host-Pathogen, Interaction, Infectious Disease, Vector-Borne Disease||1704-1721||Infectious diseases, including vector-borne diseases transmitted by arthropods, are a leading cause of morbidity and mortality worldwide. In the era of big data, addressing broad-scale, fundamental questions regarding the complex dynamics of these diseases will increasingly require the integration of diverse datasets to produce new biological knowledge. This review provides a current snapshot of the systematic assessment of the relationships between microbial pathogens, arthropod vectors and mammalian hosts using data mining and machine learning. We employ PRISMA to identify 32 key papers relevant to this topic. Our analysis shows an increasing use of data mining and machine learning tasks and techniques, including prediction, classification, clustering, association rules mining, and deep learning, over the last decade. However, it also reveals a number of critical challenges in applying these to the study of vector-host-pathogen interactions at various systems biology levels. Here, relevant studies, current limitations and future directions are discussed. Furthermore, the quality of data in relevant papers was assessed using the FAIR (Findable, Accessible, Interoperable, Reusable) compliance criteria to evaluate and encourage reproducibility and shareability of research outcomes. Although shortcomings in their application remain, data mining and machine learning have significant potential to break new ground in understanding fundamental aspects of vector-host-pathogen relationships and their application in this field should be encouraged. In particular, while predictive modeling, feature engineering and supervised machine learning are already being used in the field, other data mining and machine learning methods such as deep learning and association rules analysis lag behind and should be implemented in combination with established methods to accelerate hypothesis and knowledge generation in the domain.|https://doi.org/10.1016/j.csbj.2020.06.031|https://www.sciencedirect.com/science/article/pii/S2001037020303202||||2020|Assessment of vector-host-pathogen relationships using data mining and machine learning|Diing D.M. Agany and Jose E. Pietri and Etienne Z. Gnimpieba|article|AGANY20201704|||Computational and Structural Biotechnology Journal|20010370||18|||||21100318415|1,908|Q1|45|357|255|28093|2022|255|7,89|78,69|Sweden|Western Europe|2012-2020|Biochemistry (Q1); Biophysics (Q1); Biotechnology (Q1); Computer Science Applications (Q1); Genetics (Q1); Structural Biology (Q2)|3,620|7.271|0.00677|612878415|175296720
||Smart farming, Current Trends in smart farming, Precision agriculture, Agriculture 4.0, Machine Learning||107217||With increasing population, the demand for agricultural productivity is rising to meet the goal of “Zero Hunger”. Consequently, farmers have optimized the agricultural activities in a sustainable way with the modern technologies. This integration has boosted the agriculture production due to high potentiality in assisting the farmers. The impulse towards the technological advancement has revived the traditional agriculture methods and resulted in eco-friendly, sustainable, and efficient farming. This has revolutionized the era of smart farming which primarily alliance with modern technologies like, big data, machine learning, deep learning, swarm intelligence, internet-of-things, block chain, robotics and autonomous system, cloud-fog-edge computing, cyber physical systems, and generative adversarial networks (GAN). To cater the same, a detailed survey on ten hot-spots of smart farming is presented in this paper. The survey covers the technology-wise state-of-the-art methods along with their application domains. Moreover, the publicly available data sets with existing research challenges are investigated. Lastly, the paper concludes with suggestions to the identified problems and possible future research directions.|https://doi.org/10.1016/j.compag.2022.107217|https://www.sciencedirect.com/science/article/pii/S0168169922005324||||2022|Technological revolutions in smart farming: Current trends, challenges & future directions|Vivek Sharma and Ashish Kumar Tripathi and Himanshu Mittal|article|SHARMA2022107217|||Computers and Electronics in Agriculture|01681699||201|||||30441|1,208|Q1|115|648|1300|28725|9479|1298|7,27|44,33|Netherlands|Western Europe|1985-2020|Agronomy and Crop Science (Q1); Animal Science and Zoology (Q1); Computer Science Applications (Q1); Forestry (Q1); Horticulture (Q1)|17,657|5.565|0.01646|613705700|1531073408
|||9|35–43||We outline a call to action for promoting empiricism in data quality research. The action points result from an analysis of the landscape of data quality research. The landscape exhibits two dimensions of empiricism in data quality research relating to type of metrics and scope of method. Our study indicates the presence of a data continuum ranging from real to synthetic data, which has implications for how data quality methods are evaluated. The dimensions of empiricism and their inter-relationships provide a means of positioning data quality research, and help expose limitations, gaps and opportunities.|10.1145/3186549.3186559|https://doi.org/10.1145/3186549.3186559|New York, NY, USA|Association for Computing Machinery||2018|Data Quality: The Role of Empiricism|Sadiq, Shazia and Dasu, Tamraparni and Dong, Xin Luna and Freire, Juliana and Ilyas, Ihab F. and Link, Sebastian and Miller, Miller J. and Naumann, Felix and Zhou, Xiaofang and Srivastava, Divesh|article|10.1145/3186549.3186559||feb|SIGMOD Rec.|01635808|4|46|December 2017||||13622|0,372|Q2|142|39|81|808|127|57|1,67|20,72|United States|Northern America|1969, 1973-1978, 1981-2020|Information Systems (Q2); Software (Q2)|1,819|0.775|7.5E-4|616428151|962972343
|||||Data Cleaning|Data quality is one of the most important problems in data management, since dirty data often leads to inaccurate data analytics results and incorrect business decisions. Poor data across businesses and the U.S. government are reported to cost trillions of dollars a year. Multiple surveys show that dirty data is the most common barrier faced by data scientists. Not surprisingly, developing effective and efficient data cleaning solutions is challenging and is rife with deep theoretical and engineering problems.This book is about data cleaning, which is used to refer to all kinds of tasks and activities to detect and repair errors in the data. Rather than focus on a particular data cleaning task, we give an overview of the endto- end data cleaning process, describing various error detection and repair methods, and attempt to anchor these proposals with multiple taxonomies and views. Specifically, we cover four of the most common and important data cleaning tasks, namely, outlier detection, data transformation, error repair (including imputing missing values), and data deduplication. Furthermore, due to the increasing popularity and applicability of machine learning techniques, we include a chapter that specifically explores how machine learning techniques are used for data cleaning, and how data cleaning is used to improve machine learning models.This book is intended to serve as a useful reference for researchers and practitioners who are interested in the area of data quality and data cleaning. It can also be used as a textbook for a graduate course. Although we aim at covering state-of-the-art algorithms and techniques, we recognize that data cleaning is still an active field of research and therefore provide future directions of research whenever appropriate.||https://doi.org/10.1145/3310205.3310213|New York, NY, USA|Association for Computing Machinery|9781450371520|2019|Machine Learning and Probabilistic Data Cleaning||inbook|10.1145/3310205.3310213|||||||||||||||||||||||||||||617488816|42
||Cognitive security, Cognitive science, Situation awareness, Cyber operations||102352||Nowadays, IoT, cloud computing, mobile and social networks are generating a transformation in social processes. Nevertheless, this technological change rise to new threats and security attacks that produce new and complex cybersecurity scenarios with large volumes of data and different attack vectors that can exceeded the cognitive skills of security analysts. In this context, cognitive sciences can enhance the cognitive processes, which can help to security analysts to establish actions in less time and more efficiently within cybersecurity operations. This works presents a cognitive security model that integrates technological solutions such as Big Data, Machine Learning, and Support Decision Systems with the cognitive processes of security analysts used to generate knowledge, understanding and execution of security response actions. The model considers alternatives to establish the automation process in the execution of cognitive tasks defined in the cyber operations processes and includes the analyst as the central axis in the processes of validation and decision making through the use of MAPE-K, OODA and Human in the Loop.|https://doi.org/10.1016/j.jisa.2019.06.008|https://www.sciencedirect.com/science/article/pii/S2214212618307804||||2019|Cognitive security: A comprehensive study of cognitive science in cybersecurity|Roberto O Andrade and Sang Guun Yoo|article|ANDRADE2019102352|||Journal of Information Security and Applications|22142126||48|||||21100332403|0,610|Q2|40|183|297|8559|1526|292|5,43|46,77|United Kingdom|Western Europe|2013-2020|Computer Networks and Communications (Q2); Safety, Risk, Reliability and Quality (Q2); Software (Q2)|1,526|3.872|0.00209|618385226|68968737
||Magnetic resonance imaging;Feature extraction;Diseases;Positron emission tomography;Medical diagnosis;Brain modeling;Deep learning;Multi-modal neuroimage;incomplete data;generative adversarial network;fisher vector;brain disease diagnosis;MRI;PET||2965-2975||Multi-modal neuroimages, such as magnetic resonance imaging (MRI) and positron emission tomography (PET), can provide complementary structural and functional information of the brain, thus facilitating automated brain disease identification. Incomplete data problem is unavoidable in multi-modal neuroimage studies due to patient dropouts and/or poor data quality. Conventional methods usually discard data-missing subjects, thus significantly reducing the number of training samples. Even though several deep learning methods have been proposed, they usually rely on pre-defined regions-of-interest in neuroimages, requiring disease-specific expert knowledge. To this end, we propose a spatially-constrained Fisher representation framework for brain disease diagnosis with incomplete multi-modal neuroimages. We first impute missing PET images based on their corresponding MRI scans using a hybrid generative adversarial network. With the complete (after imputation) MRI and PET data, we then develop a spatially-constrained Fisher representation network to extract statistical descriptors of neuroimages for disease diagnosis, assuming that these descriptors follow a Gaussian mixture model with a strong spatial constraint (i.e., images from different subjects have similar anatomical structures). Experimental results on three databases suggest that our method can synthesize reasonable neuroimages and achieve promising results in brain disease identification, compared with several state-of-the-art methods.|10.1109/TMI.2020.2983085|||||2020|Spatially-Constrained Fisher Representation for Brain Disease Identification With Incomplete Multi-Modal Neuroimages|Pan, Yongsheng and Liu, Mingxia and Lian, Chunfeng and Xia, Yong and Shen, Dinggang|article|9046025||Sep.|IEEE Transactions on Medical Imaging|1558254X|9|39|||||||||||||||||||||||619475335|2077927198
||Big data analytics, Technology adoption, Literature review, Bibliometric analysis, Theoretical models, Adoption frameworks||102234||Incorporating big data analytics into a particular context brings various challenges that rest on the model or framework through which individuals or organisations adopt big data to achieve their objectives. Although these models have recently triggered scholars’ attention in various domains, in-depth knowledge of using each of these models in big data research is still blurred. This study enriches our knowledge on emerging models and theories that shape big data analytics adoption (BDAD) research through a bibliometric analysis of 229 studies (143 journal articles and 86 conference papers) published in indexed sources between 2013 and 2019. As a result, twenty models on BDAD have emerged (e.g., “Dynamic Capabilities”, “Resource-Based View”, “Technology Acceptance Model”, “Diffusion of Innovation”, etc.). The analysis reveals that BDAD research to demonstrate attributes suggestive of a topic at an initial stage of development as it is broadly dispersed across different domains employs a wide range of models, some of which overlap. Most of the applied models are generic in nature focusing on variance-based relationships and snapshot prediction with little consensus. There is a conspicuous dearth of process models, firm-level analysis and cultural orientation in contemporary BDAD research. Insights of this bibliometric study could guide rigorous big data research and practice in various contexts. The study concludes with research implications and limitations that offer promising prospects for forthcoming research.|https://doi.org/10.1016/j.ipm.2020.102234|https://www.sciencedirect.com/science/article/pii/S0306457319313366||||2020|Influencing models and determinants in big data analytics research: A bibliometric analysis|Mohamed Aboelmaged and Samar Mouakket|article|ABOELMAGED2020102234|||Information Processing & Management|03064573|4|57|||||||||||||||||||||||623879074|1769516999
MICRO-51|Fukuoka, Japan|data mining, performance counters, big data, computer architecture|14|613–626|Proceedings of the 51st Annual IEEE/ACM International Symposium on Microarchitecture|"\"Modern processors typically provide a small number of hardware performance counters to capture a large number of microarchitecture events1. These counters can easily generate a huge amount (e.g., GB or TB per day) of data, which we call big performance data in cloud computing platforms with more than thousands of servers and millions of complex workloads running ina\"\"24/7/365\"\" manner. The big performance data provides a precious foundation for root cause analysis of performance bottlenecks, architecture and compiler optimization, and many more. However, it is challenging to extract value from the big performance data due to: 1) the many unperceivable errors (e.g., outliers and missing values); and 2) the difficulty of obtaining insights, e.g., relating events to performance.In this paper, we propose CounterMiner, a rigorous methodology that enables the measurement and understanding of big performance data by using data mining and machine learning techniques. It includes three novel components: 1) using data cleaning to improve data quality by replacing outliers and filling in missing values; 2) iteratively quantifying, ranking, and pruning events based on their importance with respect to performance; 3) quantifying interaction intensity between two events by residual variance. We use sixteen benchmarks (eight from CloudSuite and eight from the Spark 2 version of HiBench) to evaluate CounterMiner. The experimental results show that CounterMiner reduces the average error from 28.3% to 7.7% when multiplexing 10 events on 4 hardware counters. We also conduct a real-world case study, showing that identifying important configuration parameters of Spark programs by event importance is much faster than directly ranking the importance of these parameters.\""|10.1109/MICRO.2018.00056|https://doi.org/10.1109/MICRO.2018.00056||IEEE Press|9781538662403|2018|CounterMiner: Mining Big Performance Data from Hardware Counters|Lv, Yirong and Sun, Bin and Luo, Qinyi and Wang, Jing and Yu, Zhibin and Qian, Xuehai|inproceedings|10.1109/MICRO.2018.00056|||||||||||||||||||||||||||||624965683|42
|||10|59–68||While it may not be possible to build a data brain identical to a human, data science can still aspire to imaginative machine thinking.|10.1145/3015456|https://doi.org/10.1145/3015456|New York, NY, USA|Association for Computing Machinery||2017|Data Science: Challenges and Directions|Cao, Longbing|article|10.1145/3015456||jul|Commun. ACM|00010782|8|60|August 2017||||||||||||||||||||||626246630|647144465
||Forecasting;Time series analysis;Market research;Mathematical model;Measurement;Predictive models;time series;forecasting method;data quality;error measurement;B2C/B2B forecasting||1-6|2016 IEEE International Conference on Big Data Analysis (ICBDA)|Since 1970s, many academic researchers and business practitioners have started to develop different forecasting methods and models. Most of them are still used in the IT-Systems nowadays. However, they don't perform well enough in practice. People pay much attention to data collection but ignore the data quality, which could lead to low forecasting accuracy. In this paper, we will introduce two new heuristic business forecasting techniques (Revinda and Metrix). Both methods utilize inherent structures of time series. The error analysis is based on B2C and B2B aggregated commercial data. In addition, these two methods will be compared with HOLT-WiNTERS-Methods (HWM) by using error measures MAPE, percentage better and THEIL's U2.|10.1109/ICBDA.2016.7509814|||||2016|Forecasting accuracy analysis based on two new heuristic methods and Holt-Winters-Method|Fang, Dianjun and Zhang, Yin and Spicher, Klaus|inproceedings|7509814||March|||||||||||||||||||||||||||626258325|42
||Internal audit, Data analytics, Big data, Soft skills, Fraud detection, IT audit||100357||In the big data era, internal audit functions (IAFs) should innovate their techniques so as to add value to their organizations. The use of data analytics (DA) increases IAFs’ ability to extract value from big data, helping IAFs to enhance their activities’ efficiency and effectiveness. We use responses from 1,681 Chief Audit Executives (CAEs) in 82 countries to investigate the correlates of IAFs’ DA usage. From the literature, we identify five main variables expected to be associated with IAFs’ DA use. We find a positive and significant association between DA use and (i) the IAF reporting to the audit committee (AC) and (ii) CAEs’ ability to build positive relationships with managers. These findings suggest that IAF independence and CAEs’ soft skills are important to innovate IAF techniques favoring DA use. We also find a positive association between DA use and IAFs’ involvement in the assurance of enterprise risk management, fraud detection, and IT risk audit activities. Our findings contribute to the internal auditing and DA literatures, and should be of interest to CAEs, ACs, corporate boards, and professional associations.|https://doi.org/10.1016/j.intaccaudtax.2020.100357|https://www.sciencedirect.com/science/article/pii/S1061951820300586||||2021|Correlates of the internal audit function’s use of data analytics in the big data era: Global evidence|Romina Rakipi and Federica {De Santis} and Giuseppe D'Onza|article|RAKIPI2021100357|||Journal of International Accounting, Auditing and Taxation|10619518||42|||||29884|0,444|Q2|41|32|66|2095|178|59|2,47|65,47|United Kingdom|Western Europe|1992-2020|Accounting (Q2); Finance (Q2)||||627084255|1227137157
ICBDR 2019|Cergy-Pontoise, France|Theory of evidence, Smart City, Data Reliability, IoT|6|18–23|Proceedings of the 2019 3rd International Conference on Big Data Research|Proliferation of IoT (Internet of Things) and sensor technology has expedited the realization of Smart City. To enable necessary functions, sensors distributed across the city generate a huge volume of stream data that are crucial for controlling Smart City devices. However, due to conditions such as wears and tears, battery drain, or malicious attacks, not all data are reliable even when they are accurately measured. These data could lead to invalid and devastating consequences (e.g., failed utility or transportation services). The assessment of data reliability is necessary and challenging especially for Smart City, as it has to keep up with velocity of big data stream to provide up-to-date results. Most research on data reliability has focused on data fusion and anomaly detection that lack a quantified measure of how much the data over a period of time are adequately reliable for decision-makings. This paper alleviates these issues and presents an online approach to assessing Big stream data reliability in a timely manner. By employing a well-studied evidence-based theory, our approach provides a computational framework that assesses data reliability in terms of belief likelihoods. The framework is lightweight and easy to scale, deeming fit for streaming data. We evaluate the approach using a real application of light sensing data of 1,323,298 instances. The preliminary results are consistent with logical rationales, confirming validity of the approach.|10.1145/3372454.3372478|https://doi.org/10.1145/3372454.3372478|New York, NY, USA|Association for Computing Machinery|9781450372015|2020|Assessing Reliability of Big Data Stream for Smart City|Puangpontip, Supadchaya and Hewett, Rattikorn|inproceedings|10.1145/3372454.3372478|||||||||||||||||||||||||||||628066472|42
CSAI 2020|Zhuhai, China|Analysis platform, Data mining, Cloud computing, Big data|5|29–33|2020 4th International Conference on Computer Science and Artificial Intelligence|The rapid development of big data has attracted extensive attention at home and abroad. Scientific and effective analysis and processing of big data is the core issue in the field of big data. The construction of a big data analysis platform in cloud environment can process complex data structures and highly correlated data, timely respond to user requests, realize intelligent and efficient data analysis, and mine more valuable data, providing technical support for the rapid construction of big data services.|10.1145/3445815.3445820|https://doi.org/10.1145/3445815.3445820|New York, NY, USA|Association for Computing Machinery|9781450388436|2021|Research on Architecture of Big Date Analysis Platform in Cloud Environment|Jin, Liya and Wang, Ronghui and Wang, Xuan|inproceedings|10.1145/3445815.3445820|||||||||||||||||||||||||||||631557348|42
MET '19|Montreal, Quebec, Canada|Oracle problem, natural language processing, metamorphic testing, data validation, Douban, social media, metamorphic relation, data quality assessment, sentiment analysis, machine translation|6|70–75|Proceedings of the 4th International Workshop on Metamorphic Testing|In conventional metamorphic testing, metamorphic relations (MRs) are identified as necessary properties of a computer program's intended functionality, whereby violations of MRs reveal faults in the program---under the assumption that the source and follow-up inputs (test cases used in metamorphic testing) are valid. In the present study, the authors argue that MRs can also be used to validate and assess the quality of the program's input data---under the assumption that the source or follow-up inputs can be inappropriately generated. Using this new perspective, a case study in the natural language processing domain is used to explore the different types of text messages that are difficult to interpret by (Chinese-English) machine translation. A total of 46,180 short user comments on Personal Tailor (a 2013 Chinese film), collected from Douban (a popular Chinese social media platform), has been used as the primary dataset of this study, and the analysis of results demonstrates that the proposed MR-based data validation method is useful for the automatic identification of poorly translated text messages.|10.1109/MET.2019.00018|https://doi.org/10.1109/MET.2019.00018||IEEE Press||2019|Metamorphic Relations for Data Validation: A Case Study of Translated Text Messages|Yan, Boyang and Yecies, Brian and Zhou, Zhi Quan|inproceedings|10.1109/MET.2019.00018|||||||||||||||||||||||||||||633589688|42
||Machine learning, Deep learning, Optimization algorithms, Geoengineering and geoscience, VOSviewer||1-17||The so-called Fourth Paradigm has witnessed a boom during the past two decades, with large volumes of observational data becoming available to scientists and engineers. Big data is characterized by the rule of the five Vs: Volume, Variety, Value, Velocity and Veracity. The concept of big data naturally matches well with the features of geoengineering and geoscience. Large-scale, comprehensive, multidirectional and multifield geotechnical data analysis is becoming a trend. On the other hand, Machine learning (ML), Deep Learning (DL) and Optimization Algorithm (OA) provide the ability to learn from data and deliver in-depth insight into geotechnical problems. Researchers use different ML, DL and OA models to solve various problems associated with geoengineering and geoscience. Consequently, there is a need to extend its research with big data research through integrating the use of ML, DL and OA techniques. This work focuses on a systematic review on the state-of-the-art application of ML, DL and OA algorithms in geoengineering and geoscience. Various ML, DL, and OA approaches are firstly concisely introduced, concerning mainly the supervised learning, unsupervised learning, deep learning and optimization algorithms. Then their representative applications in the geoengineering and geoscience are summarized via VOSviewer demonstration. The authors also provided their own thoughts learnt from these applications as well as work ongoing and future recommendations. This review paper aims to make a comprehensive summary and provide fundamental guidelines for researchers and engineers in the discipline of geoengineering and geoscience or similar research areas on how to integrate and apply ML, DL and OA methods.|https://doi.org/10.1016/j.gr.2022.03.015|https://www.sciencedirect.com/science/article/pii/S1342937X2200123X||||2022|Application of machine learning, deep learning and optimization algorithms in geoengineering and geoscience: Comprehensive review and future challenge|Wengang Zhang and Xin Gu and Libin Tang and Yueping Yin and Dongsheng Liu and Yanmei Zhang|article|ZHANG20221|||Gondwana Research|1342937X||109|||||22647|2,859|Q1|135|243|535|31951|3698|517|5,81|131,49|United States|Northern America|1997-2020|Geology (Q1)|18,040|6.051|0.01984|637468174|19400004
||Laser radar;Orbits (stellar);Bayes methods;Error correction;Calibration;Reliability;Remote sensing;Bayesian network;lidar;data error;correction methoIntroduction||486-491|2020 IEEE International Conference on Industrial Application of Artificial Intelligence (IAAI)|Data quality analysis is the first and key step of remote sensing mechanism/application research (especially quantitative remote sensing), which directly affects the accuracy of remote sensing inversion and the effect of remote sensing applications. Lidar (light detection and ranging, referred to as lidar) is a new active remote sensing technology. The research on hardware system development and data post-processing algorithm needs to be further strengthened, especially for the quality analysis of lidar data. After initializing the Bayesian network and checking the error, a correction mathematical model is established. Experiments have proved that after correction, the angle error of the radar is significantly improved, which verifies the feasibility and reliability of the precision orbit star calibration method.|10.1109/IAAI51705.2020.9332846|||||2020|Research on Lidar Data Error Correction Method Based on Bayesian Network|Fei, Chen|inproceedings|9332846||Dec|||||||||||||||||||||||||||637668904|42
||Big data, Analytics, Accounting, Data science, Business intelligence||121171||This study aims to develop accounting standards, curriculums, and research to cope with the rapid development of big data. The study presents several potential convergence points between big data and different accounting techniques and theories. The study discusses how big data can overcome the data limitations of six accounting issues: financial reporting, performance measurement, audit evidence, risk management, corporate budgeting and activity-based techniques. It presents six exciting research questions for future research. Then, the study explains the potential convergence between big data and agency theory, stakeholders theory, and legitimacy theory. This theoretical study develops new convergence points between big data and accounting by reviewing the literature and proposing new ideas and research questions. The conclusion indicates a significant convergence between big data and accounting on the premise that data is the heart of accounting. Big data and advanced analytics have the potential to overcome the data limitations of accounting techniques that require estimations and predictions. A remarkable convergence is argued between big data and three accounting theories. Overall, the study presents helpful insights to members of the accounting and auditing community on the potential of big data.|https://doi.org/10.1016/j.techfore.2021.121171|https://www.sciencedirect.com/science/article/pii/S0040162521006041||||2021|The convergence of big data and accounting: innovative research opportunities|Awad Elsayed Awad Ibrahim and Ahmed A. Elamer and Amr Nazieh Ezat|article|IBRAHIM2021121171|||Technological Forecasting and Social Change|00401625||173|||||14704|2,226|Q1|117|448|1099|35581|10127|1061|9,01|79,42|United States|Northern America|1970-2020|Applied Psychology (Q1); Business and International Management (Q1); Management of Technology and Innovation (Q1)|21,116|8.593|0.02416|638722335|1949868303
UbiComp/ISWC '19 Adjunct|London, United Kingdom|panel technique, human sensing, in situ, human subject studies, longitudinal studies, mobile devices|4|878–881|Adjunct Proceedings of the 2019 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2019 ACM International Symposium on Wearable Computers|Individuals increasingly use mobile, wearable, and ubiquitous devices capable of unobtrusive collection of vast amounts of scientifically rich personal data over long periods (months to years), and in the context of their daily life. However, numerous human and technological factors challenge longitudinal data collection, often limiting research studies to very short data collection periods (days to weeks), spawning recruitment biases, and affecting participant retention over time. This workshop is designed to bring together researchers involved in longitudinal data collection studies to foster an insightful exchange of ideas, experiences, and discoveries to improve the studies' reliability, validity, and perceived meaning of longitudinal mobile, wearable, and ubiquitous data collection for the participants.|10.1145/3341162.3347758|https://doi.org/10.1145/3341162.3347758|New York, NY, USA|Association for Computing Machinery|9781450368698|2019|LDC '19: International Workshop on Longitudinal Data Collection in Human Subject Studies|Manea, Vlad and Berrocal, Allan and De Masi, Alexandre and M\o{}ller, Naja Holten and Wac, Katarzyna and Bayer, Hannah and Lehmann, Sune and Ashley, Euan|inproceedings|10.1145/3341162.3347758|||||||||||||||||||||||||||||639428754|42
||Human resource management research, Big data, Integrative review, Inductive and deductive paradigms||34-50||The lack of sufficient big data-based approaches impedes the development of human resource management (HRM) research and practices. Although scholars have realized the importance of applying a big data approach to HRM research, clear guidance is lacking regarding how to integrate the two. Using a clustering algorithm based on the big data research paradigm, we first conduct a bibliometric review to quantitatively assess and scientifically map the evolution of the current big data HRM literature. Based on this systematic review, we propose a general theoretical framework described as “Inductive (Prediction paradigm: Data mining/Theory building) vs. Deductive (Explanation paradigm: Theory testing)”. In this framework, we discuss potential research questions, their corresponding levels of analysis, relevant methods, data sources and software. We then summarize the general procedures for conducting big data research within HRM research. Finally, we propose a future agenda for applying big data approaches to HRM research and identify five promising HRM research topics at the micro, meso and macro levels along with three challenges and limitations that HRM scholars may face in the era of big data.|https://doi.org/10.1016/j.jbusres.2021.04.019|https://www.sciencedirect.com/science/article/pii/S0148296321002563||||2021|Big data and human resource management research: An integrative review and new directions for future research|Yucheng Zhang and Shan Xu and Long Zhang and Mengxi Yang|article|ZHANG202134|||Journal of Business Research|01482963||133|||||20550|2,049|Q1|195|878|1342|73221|11507|1315|7,38|83,40|United States|Northern America|1973-2021|Marketing (Q1)|46,935|7.550|0.03523|648182526|1502892296
||Mobile phone, Call detail records, SMS, Disaster, Disease, Big data||253-264||Global health threats such as the recent Ebola and Zika virus outbreaks require rapid and robust responses to prevent, reduce and recover from disease dispersion. As part of broader big data and digital humanitarianism discourses, there is an emerging interest in data produced through mobile phone communications for enhancing the data environment in such circumstances. This paper assembles user perspectives and critically examines existing evidence and future potential of mobile phone data derived from call detail records (CDRs) and two-way short message service (SMS) platforms, for managing and responding to humanitarian disasters caused by communicable disease outbreaks. We undertake a scoping review of relevant literature and in-depth interviews with key informants to ascertain the: (i) information that can be gathered from CDRs or SMS data; (ii) phase(s) in the disease disaster management cycle when mobile data may be useful; (iii) value added over conventional approaches to data collection and transfer; (iv) barriers and enablers to use of mobile data in disaster contexts; and (v) the social and ethical challenges. Based on this evidence we develop a typology of mobile phone data sources, types, and end-uses, and a decision-tree for mobile data use, designed to enable effective use of mobile data for disease disaster management. We show that mobile data holds great potential for improving the quality, quantity and timing of selected information required for disaster management, but that testing and evaluation of the benefits, constraints and limitations of mobile data use in a wider range of mobile-user and disaster contexts is needed to fully understand its utility, validity, and limitations.|https://doi.org/10.1016/j.geoforum.2016.07.019|https://www.sciencedirect.com/science/article/pii/S0016718516301981||||2016|Evidence and future potential of mobile phone data for disease disaster management|Jonathan Cinnamon and Sarah K. Jones and W. Neil Adger|article|CINNAMON2016253|||Geoforum|00167185||75|||||28611|1,584|Q1|116|284|773|20662|3032|725|3,54|72,75|United Kingdom|Western Europe|1970-2020|Sociology and Political Science (Q1)|11,684|3.901|0.0159|649208376|951685985
ICCIR '22|Nanjing, China||5|479–483|Proceedings of the 2022 2nd International Conference on Control and Intelligent Robotics|In view of the current problems of time-consuming and poor effect of intelligent management and control of power material warehousing, this paper puts forward the research method of intelligent management and control platform of power material warehousing based on Web. First, combined with web technology, this paper constructs the ERP management model of power material warehousing, improves the ERP management function of power material intelligent warehousing, optimizes the information processing algorithm of power material warehousing, and optimizes the structure and function of the platform. The processing steps of the intelligent management and control platform for power material warehousing are simplified. Finally, the experiment proves that the intelligent management and control platform for power material warehousing based on Web can realize the management of massive warehousing information more quickly in the actual application process, and its running time is shortened by more than 4h, which has high practicability.|10.1145/3548608.3559245|https://doi.org/10.1145/3548608.3559245|New York, NY, USA|Association for Computing Machinery|9781450397179|2022|Research on Intelligent Management and Control Platform of Power Material Storage Based on Web|Chen, Jun and Song, Binghu and Li, Xiaopeng and Feng, Liang and Zhai, Yujia|inproceedings|10.1145/3548608.3559245|||||||||||||||||||||||||||||649398785|42
||Affective computing, Multimodal fusion, Sentiment databases, Sentiment analysis, Affective applications||102447||Affective computing is an emerging multidisciplinary research field that is increasingly drawing the attention of researchers and practitioners in various fields, including artificial intelligence, natural language processing, cognitive and social sciences. Research in affective computing includes areas such as sentiment, emotion, and opinion modelling. The internet is an excellent source of data required for sentiment analysis, such as customer reviews of products, social media, forums, blogs, etc. Most of these data, called big data, are unstructured and unorganized. Hence there is a strong demand for developing suitable data processing techniques to process these rich and valuable data to produce useful information. Early surveys on sentiment and emotion recognition in the literature have been limited to discussions using text, audio, and visual modalities. So far, to the author's knowledge, a comprehensive survey combining physiological modalities with these other modalities for affective computing has yet to be reported. The objective of this paper is to fill the gap in this surveyed area. The usage of physiological modalities for affective computing brings several benefits in that the signals can be used in different environmental conditions, more robust systems can be constructed in combination with other modalities, and it has increased anti-spoofing characteristics. The paper includes extensive reviews on different frameworks and categories for state-of-the-art techniques, critical analysis of their performances, and discussions of their applications, trends and future directions to serve as guidelines for readers towards this emerging research area.|https://doi.org/10.1016/j.jnca.2019.102447|https://www.sciencedirect.com/science/article/pii/S1084804519303078||||2020|Multimodal big data affective analytics: A comprehensive survey using text, audio, visual and physiological signals|Nusrat J. Shoumy and Li-Minn Ang and Kah Phooi Seng and D.M.Motiur Rahaman and Tanveer Zia|article|SHOUMY2020102447|||Journal of Network and Computer Applications|10848045||149|||||||||||||||||||||||651348063|2040591560
CCIOT 2018|Singapore, Singapore|Linked open data, Semantic Web, Quality of linked open data|7|33–39|Proceedings of the 2018 International Conference on Cloud Computing and Internet of Things|Linked Open Data refers to a set of best practices that empowers enterprises to publish and interlink their data using existing ontologies on the Semantic Web. The focus of linked open data is to move from document-based Web to a Web of interlinked data, created by typed links between data from different data sources. Linked open data expert group has taken cognizance of data quality importance, as the amount of linked data publications grown on the Web substantially. Measures have been taken to check the linked data quality. But, these measures are diverse in nature with respect to quality terms. This makes the comparison and evaluation difficult, leading to an incorrect selection of accurate data sources based on quality requirements. In this paper, we carried out an analysis on linked data, the quality of linked data, the frameworks to assess the quality of linked data and the challenges to achieve the quality of linked open data.|10.1145/3291064.3291074|https://doi.org/10.1145/3291064.3291074|New York, NY, USA|Association for Computing Machinery|9781450365765|2018|An Investigative Study on the Quality Aspects of Linked Open Data|H G, Monika Rani and R, Sapna and Mishra, Shakti|inproceedings|10.1145/3291064.3291074|||||||||||||||||||||||||||||652755402|42
ICSCA 2020|Langkawi, Malaysia|Cloud Computing, Retrofitting, Fog Computing, Industry 4.0, Middleware, Lean Production, Shop Floor Management, Smart Production, Mist Computing|5|41–45|Proceedings of the 2020 9th International Conference on Software and Computer Applications|In the context of manufacturing, shop floor management (SFM) is employed to ensure efficient production operations and workflows. Advanced technologies and methods can be used to improve the SFM and achieve close to real-time responsiveness. Even though there is a number of research available for the digitalized SFM (DSFM), a supportive framework for implementation purposes was not considered yet. Consequently, this paper utilizes concepts from related disciplines and research areas to derive an architectural framework for a DSFM. This particular architecture is then implemented to ensure its practicability and foster the understanding of challenges and opportunities. The proposed multi-layer framework and supportive methods can be employed by manufacturing companies to implement a DSFM focused on interoperability, security and low-latency.|10.1145/3384544.3384611|https://doi.org/10.1145/3384544.3384611|New York, NY, USA|Association for Computing Machinery|9781450376655|2020|Digital Shop Floor Management: A Practical Framework For Implementation|Hartner, Raphael and Mezhuyev, Vitaliy and Tschandl, Martin and Bischof, Christian|inproceedings|10.1145/3384544.3384611|||||||||||||||||||||||||||||652983430|42
||Big data;Quality assurance;Organizations;Q-factor;Standards organizations;Quality assurance;big data quality assurance;big data validation;data validation||433-441|2016 IEEE Symposium on Service-Oriented System Engineering (SOSE)|With the fast advance of big data technology and analytics solutions, big data computing and service is becoming a very hot research and application subject in academic research, industry community, and government services. Nevertheless, there are increasing data quality problems resulting in erroneous data costs in enterprises and businesses. Current research seldom discusses how to effectively validate big data to ensure data quality. This paper provides informative discussions for big data validation and quality assurance, including the essential concepts, focuses, and validation process. Moreover, the paper presents a comparison among big data validation tools and several major players in industry are discussed. Furthermore, the primary issues, challenges, and needs are discussed.|10.1109/SOSE.2016.63|||||2016|Big Data Validation and Quality Assurance -- Issuses, Challenges, and Needs|Gao, Jerry and Xie, Chunli and Tao, Chuanqi|inproceedings|7473058||March|||||||||||||||||||||||||||654784422|42
||Interregional travel survey, Web-based survey, Mobile phone data, Data quality, Nonnegative matrix factorization||100018||The Interregional Travel Survey in Japan (formerly the Net Passenger Transportation Survey [NPTS]) still has some limitations. New data sources have recently emerged, e.g., massive data from web-based surveys (WEB) or collecting passive mobile phone data (MOBI). Using or not using these data sources have been questioned for data integration or model estimation and validation. Therefore, as an initial step, the data quality of new data sources was evaluated to identify the potential for data integration with NPTS or new data collection methods to replace NPTS. This study focused on finding out the similarities in travel patterns extracted from these data sources using a nonnegative matrix factorization method. This study found that origin–destination pairs in the MOBI travel patterns were significantly different from those of NPTS and WEB, while there were some similarities between NPTS and WEB. However, some issues have been remaining and should be resolved in the future.|https://doi.org/10.1016/j.eastsj.2020.100018|https://www.sciencedirect.com/science/article/pii/S2185556020300183||||2020|Data quality analysis of interregional travel demand: Extracting travel patterns using matrix decomposition|Canh Xuan Do and Makoto Tsukai and Akimasa Fujiwara|article|DO2020100018|||Asian Transport Studies|21855560||6|||||||||||||||||||||||656333793|713949468
ICBDE'19|London, United Kingdom|Internet usage, Sequential pattern mining, Generalized Sequential Pattern, Inappropriate user pattern, Data mining, Event logs|5|93–97|Proceedings of the 2019 International Conference on Big Data and Education|Understanding users' behavior of internet usage is essential for the quality of service (QoS) analysis on the internet. If the internet providers can better understand their users, they may be able to provide better service, and also enhance the quality of the service. In general, the information about users' behavior is stored as the internet access log files, called event logs, on the server. To have the patterns of users' behavior from the event logs, this work aims to extract an interesting pattern of inappropriate user behaviors through the method of internet usage patterns mining. The primary mechanism of the proposed method is the Generalized Sequential Pattern (GSP) algorithm, which is an algorithm of sequential pattern mining. This study uses real event logs from an organization in Thailand. The results have identified exciting findings that have made possible to propose some improvements and increasing the QoS of the internet service.|10.1145/3322134.3322155|https://doi.org/10.1145/3322134.3322155|New York, NY, USA|Association for Computing Machinery|9781450361866|2019|Internet Usage Patterns Mining from Firewall Event Logs|Polpinij, Jantima and Namee, Khanista|inproceedings|10.1145/3322134.3322155|||||||||||||||||||||||||||||658091262|42
ICCDE 2022|Bangkok, Thailand|Model provenance, Machine learning engineering, Artificial Intelligence, MLOps|6|45–50|2022 The 8th International Conference on Computing and Data Engineering||10.1145/3512850.3512861|https://doi.org/10.1145/3512850.3512861|New York, NY, USA|Association for Computing Machinery|9781450395717|2022|Model Provenance Management in MLOps Pipeline|Mei, Songzhu and Liu, Cong and Wang, Qinglin and Su, Huayou|inproceedings|10.1145/3512850.3512861|||||||||||||||||||||||||||||659487388|42
||smart breeding, genomic selection, integrated genomic-enviromic selection, spatiotemporal omics, crop design, machine and deep learning, big data, artificial intelligence||||The first paradigm of plant breeding involves direct selection-based phenotypic observation, followed by predictive breeding using statistical models for quantitative traits constructed based on genetic experimental design and, more recently, by incorporation of molecular marker genotypes. However, plant performance or phenotype (P) is determined by the combined effects of genotype (G), envirotype (E), and genotype by environment interaction (GEI). Phenotypes can be predicted more precisely by training a model using data collected from multiple sources, including spatiotemporal omics (genomics, phenomics, and enviromics across time and space). Integration of 3D information profiles (G-P-E), each with multidimensionality, provides predictive breeding with both tremendous opportunities and great challenges. Here, we first review innovative technologies for predictive breeding. We then evaluate multidimensional information profiles that can be integrated with a predictive breeding strategy, particularly envirotypic data, which have largely been neglected in data collection and are nearly untouched in model construction. We propose a smart breeding scheme, integrated genomic-enviromic prediction (iGEP), as an extension of genomic prediction, using integrated multiomics information, big data technology, and artificial intelligence (mainly focused on machine and deep learning). We discuss how to implement iGEP, including spatiotemporal models, environmental indices, factorial and spatiotemporal structure of plant breeding data, and cross-species prediction. A strategy is then proposed for prediction-based crop redesign at both the macro (individual, population, and species) and micro (gene, metabolism, and network) scales. Finally, we provide perspectives on translating smart breeding into genetic gain through integrative breeding platforms and open-source breeding initiatives. We call for coordinated efforts in smart breeding through iGEP, institutional partnerships, and innovative technological support.|https://doi.org/10.1016/j.molp.2022.09.001|https://www.sciencedirect.com/science/article/pii/S1674205222002957||||2022|Smart breeding driven by big data, artificial intelligence, and integrated genomic-enviromic prediction|Yunbi Xu and Xingping Zhang and Huihui Li and Hongjian Zheng and Jianan Zhang and Michael S. Olsen and Rajeev K. Varshney and Boddupalli M. Prasanna and Qian Qian|article|XU2022|||Molecular Plant|16742052|||||||17600155011|4,588|Q1|115|153|474|8872|4792|380|9,60|57,99|United States|Northern America|2008-2020|Molecular Biology (Q1); Plant Science (Q1)|15,778|13.164|0.02686|661711710|1400021155
||Sentiment analysis, Twitter, Unstructured data analysis, Big data analytics, Machine learning algorithm||||The common and various forms of Twitter information render this one of the best controlling and recording virtual environments of information. The growth in social media nowadays gives internet users immense interest. In several pups like prediction, advertisement, sentiment analysis …, the data on such a social network platform is used. People exchange good or bad views on problems, items and administrations through the web and informal communities. The capacity to assess such a data productively is presently observed as a noteworthy upper hand in settling on choices all the more proficiently. In this sense, associations use methods, for example, Sentiment Analysis (SA). The utilization of web based life around the globe is growing, however, greatly speeding up mass data generation and stopping us from providing useful insights in conventional SA systems. These data volumes can be processed effectively, using SA and Big Data technology. Big data is not a luxury, in fact, but an important prediction.|https://doi.org/10.1016/j.matpr.2020.11.486|https://www.sciencedirect.com/science/article/pii/S2214785320391501||||2021|Analysis of twitter data through big data based sentiment analysis approaches|Harika Vanam and Jeberson {Retna Raj R}|article|VANAM2021|||Materials Today: Proceedings|22147853|||||||21100370037|0,341|-|47|3996|10585|88521|14096|10409|1,24|22,15|United Kingdom|Western Europe|2005, 2014-2020|Materials Science (miscellaneous)||||666043293|400517803
||Ecosystem monitoring, Fragile ecosystem, Internet of Things, Wireless sensor network, Smart device||1234-1245||Smart, real-time, low-cost, and distributed ecosystem monitoring is essential for understanding and managing rapidly changing ecosystems. However, new techniques in the big data era have rarely been introduced into operational ecosystem monitoring, particularly for fragile ecosystems in remote areas. We introduce the Internet of Things (IoT) techniques to establish a prototype ecosystem monitoring system by developing innovative smart devices and using IoT technologies for ecosystem monitoring in isolated environments. The developed smart devices include four categories: large-scale and nonintrusive instruments to measure evapotranspiration and soil moisture, in situ observing systems for CO2 and δ13C associated with soil respiration, portable and distributed devices for monitoring vegetation variables, and Bi-CMOS cameras and pressure trigger sensors for terrestrial vertebrate monitoring. These new devices outperform conventional devices and are connected to each other via wireless communication networks. The breakthroughs in the ecosystem monitoring IoT include new data loggers and long-distance wireless sensor network technology that supports the rapid transmission of data from devices to wireless networks. The applicability of this ecosystem monitoring IoT is verified in three fragile ecosystems, including a karst rocky desertification area, the National Park for Amur Tigers, and the oasis-desert ecotone in China. By integrating these devices and technologies with an ecosystem monitoring information system, a seamless data acquisition, transmission, processing, and application IoT is created. The establishment of this ecosystem monitoring IoT will serve as a new paradigm for ecosystem monitoring and therefore provide a platform for ecosystem management and decision making in the era of big data.|https://doi.org/10.1016/j.scib.2019.07.004|https://www.sciencedirect.com/science/article/pii/S2095927319304013||||2019|Internet of Things to network smart devices for ecosystem monitoring|Xin Li and Ning Zhao and Rui Jin and Shaomin Liu and Xiaomin Sun and Xuefa Wen and Dongxiu Wu and Yan Zhou and Jianwen Guo and Shiping Chen and Ziwei Xu and Mingguo Ma and Tianming Wang and Yonghua Qu and Xinwei Wang and Fangming Wu and Yuke Zhou|article|LI20191234|||Science Bulletin|20959273|17|64|||||21100405003|1,983|Q1|112|365|833|12806|5639|616|7,21|35,08|Netherlands|Western Europe|2015-2020|Multidisciplinary (Q1)|8,832|11.780|0.0164|667885612|895863808
||Big data analytics, Resource-based view, Data quality management, IT capability, Data usage||387-394||Big data analytics associated with database searching, mining, and analysis can be seen as an innovative IT capability that can improve firm performance. Even though some leading companies are actively adopting big data analytics to strengthen market competition and to open up new business opportunities, many firms are still in the early stage of the adoption curve due to lack of understanding of and experience with big data. Hence, it is interesting and timely to understand issues relevant to big data adoption. In this study, a research model is proposed to explain the acquisition intention of big data analytics mainly from the theoretical perspectives of data quality management and data usage experience. Our empirical investigation reveals that a firm's intention for big data analytics can be positively affected by its competence in maintaining the quality of corporate data. Moreover, a firm's favorable experience (i.e., benefit perceptions) in utilizing external source data could encourage future acquisition of big data analytics. Surprisingly, a firm's favorable experience (i.e., benefit perceptions) in utilizing internal source data could hamper its adoption intention for big data analytics.|https://doi.org/10.1016/j.ijinfomgt.2014.02.002|https://www.sciencedirect.com/science/article/pii/S0268401214000127||||2014|Data quality management, data usage experience and acquisition intention of big data analytics|Ohbyung Kwon and Namyeon Lee and Bongsik Shin|article|KWON2014387|||International Journal of Information Management|02684012|3|34|||||15631|2,770|Q1|114|224|382|20318|5853|373|16,16|90,71|United Kingdom|Western Europe|1970, 1986-2021|Artificial Intelligence (Q1); Computer Networks and Communications (Q1); Information Systems (Q1); Information Systems and Management (Q1); Library and Information Sciences (Q1); Management Information Systems (Q1); Marketing (Q1)|12,245|14.098|0.01167|669699431|747927863
||File systems;Algorithm design and analysis;Calibration;Reliability;Servers;Programming;Data processing;Algorithm prototyping;data processing;big data handling;map-reduce;calibration/validation||5278-5281|2012 IEEE International Geoscience and Remote Sensing Symposium|ESA's Earth Observation (EO) missions provide a unique dataset of observational data of our environment. Calibration, algorithm development and validation of the derived products are indispensable tasks for an efficient exploitation of EO data and form the basis for reliable scientific conclusions. In spite of its importance, the cal/val and algorithm development work is often hindered by insufficient means to access data, time consuming work used to identify suitable in-situ data matching the EO data, incompatible software and limited possibilities for a rapid prototyping and testing of ideas. In view of the amount of data produced by the future ESAs series of Sentinel satellites, a very efficient technological backbone is required to maintain the ability of ensuring data quality and algorithm performance. Brockmann Consult has developed such a backbone based on leading edge technologies within an ESA R&D study. Calvalus is a new processing system that utilises the map-reduce programming model with a distributed file system.|10.1109/IGARSS.2012.6352418|||||2012|Calvalus: Full-mission EO cal/val, processing and exploitation services|Fomferra, Norman and Böttcher, Martin and Zühlke, Marco and Brockmann, Carsten and Kwiatkowska, Ewa|inproceedings|6352418||July||21537003|||||||||||||||||||||||||671617081|1296020740
HILDA '22|Philadelphia, Pennsylvania||7||Proceedings of the Workshop on Human-In-the-Loop Data Analytics|"\"Most data scientists must build substantial data pipelines using scripting languages like Python and R. These pipelines are hard to get correct due to the large volume of data they process (thus the long execution time), and the fact that they are tested mainly by inspection of output data quality. It is therefore crucial for developers to reason about data through each step in the pipeline, starting from the raw input; this information is akin to data provenance in a relational setting. Past efforts for capturing data provenance for scripting languages have required substantial manual modifications to the scripts, or else yield information that is too inflexible for many debugging tasks.We instead propose a \"\"human-in-the-loop\"\" provenance generation model with three key improvements: (1) allowing humans to express the desired provenance through a provenance schema, (2) enabling one-time execution capture of scripts to produce traces that are later combined with different provenance schemata to yield useful provenance for different tasks, (3) providing a modular rule-based recommendation component to help design provenance schemata through a user interaction interface. We describe the concepts, the user experience with our system, explain the system components, and present preliminary experiment results.\""|10.1145/3546930.3547494|https://doi.org/10.1145/3546930.3547494|New York, NY, USA|Association for Computing Machinery|9781450394420|2022|Enabling Useful Provenance in Scripting Languages with a Human-in-the-Loop|Lou, Yuze and Cafarella, Michael|inproceedings|10.1145/3546930.3547494|5||||||||||||||||||||||||||||674131515|42
ESEC/FSE 2018|Lake Buena Vista, FL, USA|node failure, service availability, cloud service systems, Failure prediction, maintenance|11|480–490|Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering|In recent years, many traditional software systems have migrated to cloud computing platforms and are provided as online services. The service quality matters because system failures could seriously affect business and user experience. A cloud service system typically contains a large number of computing nodes. In reality, nodes may fail and affect service availability. In this paper, we propose a failure prediction technique, which can predict the failure-proneness of a node in a cloud service system based on historical data, before node failure actually happens. The ability to predict faulty nodes enables the allocation and migration of virtual machines to the healthy nodes, therefore improving service availability. Predicting node failure in cloud service systems is challenging, because a node failure could be caused by a variety of reasons and reflected by many temporal and spatial signals. Furthermore, the failure data is highly imbalanced. To tackle these challenges, we propose MING, a novel technique that combines: 1) a LSTM model to incorporate the temporal data, 2) a Random Forest model to incorporate spatial data; 3) a ranking model that embeds the intermediate results of the two models as feature inputs and ranks the nodes by their failure-proneness, 4) a cost-sensitive function to identify the optimal threshold for selecting the faulty nodes. We evaluate our approach using real-world data collected from a cloud service system. The results confirm the effectiveness of the proposed approach. We have also successfully applied the proposed approach in real industrial practice.|10.1145/3236024.3236060|https://doi.org/10.1145/3236024.3236060|New York, NY, USA|Association for Computing Machinery|9781450355735|2018|Predicting Node Failure in Cloud Service Systems|Lin, Qingwei and Hsieh, Ken and Dang, Yingnong and Zhang, Hongyu and Sui, Kaixin and Xu, Yong and Lou, Jian-Guang and Li, Chenggang and Wu, Youjiang and Yao, Randolph and Chintalapati, Murali and Zhang, Dongmei|inproceedings|10.1145/3236024.3236060|||||||||||||||||||||||||||||674895909|42
||adaptive evolution, probability distribution, power load forecasting, abnormal load recognition, Gene expression programming|28|||Load forecasting in short term is very important to economic dispatch and safety assessment of power system. Although existing load forecasting in short-term algorithms have reached required forecast accuracy, most of the forecasting models are black boxes and cannot be constructed to display mathematical models. At the same time, because of the abnormal load caused by the failure of the load data collection device, time synchronization, and malicious tampering, the accuracy of the existing load forecasting models is greatly reduced. To address these problems, this article proposes a Short-Term Load Forecasting algorithm by using Improved Gene Expression Programming and Abnormal Load Recognition (STLF-IGEP_ALR). First, the Recognition algorithm of Abnormal Load based on Probability Distribution and Cross Validation is proposed. By analyzing the probability distribution of rows and columns in load data, and using the probability distribution of rows and columns for cross-validation, misjudgment of normal load in abnormal load data can be better solved. Second, by designing strategies for adaptive generation of population parameters, individual evolution of populations and dynamic adjustment of genetic operation probability, an Improved Gene Expression Programming based on Evolutionary Parameter Optimization is proposed. Finally, the experimental results on two real load datasets and one open load dataset show that compared with the existing abnormal data detection algorithms, the algorithm proposed in this article have higher advantages in missing detection rate, false detection rate and precision rate, and STLF-IGEP_ALR is superior to other short-term load forecasting algorithms in terms of the convergence speed, MAE, MAPE, RSME, and R2.|10.1145/3447513|https://doi.org/10.1145/3447513|New York, NY, USA|Association for Computing Machinery||2021|Short-Term Load Forecasting by Using Improved GEP and Abnormal Load Recognition|Deng, Song and Chen, Fulin and Dong, Xia and Gao, Guangwei and Wu, Xindong|article|10.1145/3447513|95|jul|ACM Trans. Internet Technol.|15335399|4|21|November 2021||||||||||||||||||||||674968424|314651938
||information security governance, big data, framework, systematic mapping||401-408||Information security governance is an important aspects for all organizations. Given the crucial importance of IT systems and the increasing range of threats these systems are facing, there is an increasing interest on the topic. On the other hand, Big Data environments are also beginning to be more pervasive as IT is increasing its importance for organizations worldwide. In order to better know which aspects are the most important for the intersection of Big Data and information security governance, authors present in this paper a systematic mapping on this topic. Authors illustrate challenges and gaps concerning the topic and clarify these challenges by means of a classification of the environments they take place, the security risk spectrums they concern, and the security governance measures they take to mitigate them; by providing solutions as in a framework, model, software or tool, wherever possible. Results are expected to be useful for IT security professionals and information systems practitioners as a whole.|https://doi.org/10.1016/j.procs.2018.10.057|https://www.sciencedirect.com/science/article/pii/S1877050918316909||||2018|Information security governance in big data environments: A systematic mapping|Reza Saneei Moghadam and Ricardo Colomo-Palacios|article|MOGHADAM2018401|||Procedia Computer Science|18770509||138||CENTERIS 2018 - International Conference on ENTERprise Information Systems / ProjMAN 2018 - International Conference on Project MANagement / HCist 2018 - International Conference on Health and Social Care Information Systems and Technologies, CENTERIS/ProjMAN/HCist 2018|||19700182801|0,334|-|76|1907|6483|38511|13722|6359|2,09|20,19|Netherlands|Western Europe|2010-2020|Computer Science (miscellaneous)||||676709525|2108686752
https://doi.org/10.1016/j.ifacol.2016.12.205|https://www.sciencedirect.com/science/article/pii/S2405896316328774||||2016|Big Data Refining on the Base of Cognitive Modeling|Alexander N. Raikov and Z. Avdeeva and A. Ermakov|article|RAIKOV2016147|||IFAC-PapersOnLine|24058963|32|49||Cyber-Physical & Human-Systems CPHS 2016||||||||||||||||||||||||||||676980763|42
Mobihoc '20|Virtual Event, USA|connected and automated vehicles, 5G, vehicle-to-everything, cooperative sensing|6|333–338|Proceedings of the Twenty-First International Symposium on Theory, Algorithmic Foundations, and Protocol Design for Mobile Networks and Mobile Computing|Current vehicular communication technologies were designed for a so-called phase 1, where cars needed to advise of their presence. Several projects, research activities and field tests have proved their effectiveness to this scope. But entering the phase 2, where awareness needs to be improved with non-connected objects and vulnerable road users, and even more with phases 3 and 4, where also coordination is foreseen, the spectrum scarcity becomes a critical issue. In this work, we provide an overview of various 5G and beyond solutions currently under investigation that will be needed to tackle the challenge. We first recall the undergoing activities at the access layer aimed to satisfy capacity and bandwidth demands. We then discuss the role that emerging networking paradigms can play to improve vehicular data dissemination, while preventing congestion and better exploiting resources. Finally, we give a look into edge computing and machine learning techniques that will be determinant to efficiently process and mine the massive amounts of sensor data.|10.1145/3397166.3413465|https://doi.org/10.1145/3397166.3413465|New York, NY, USA|Association for Computing Machinery|9781450380157|2020|How to Deal with Data Hungry V2X Applications?|Bazzi, Alessandro and Campolo, Claudia and Masini, Barbara M. and Molinaro, Antonella|inproceedings|10.1145/3397166.3413465|||||||||||||||||||||||||||||677959544|42
ICSDE'18|Rabat, Morocco|Data placing, MapReduce jobs, Hadoop, Big Data, Multidimensional approach, Intelligent processing|6|42–47|Proceedings of the 2nd International Conference on Smart Digital Environment|Currently, the generated data flow is growing at a high rate resulting to the problem of data obesity and abundance, but yet a lack of pertinent information. To handle this Big Data, Hadoop is a distributed framework that facilitates data storage and processing. Although Hadoop is designed to deal with demands of storage and analysis of ever-growing Data, its performance characteristics are still to improve. In this regard, many approaches have been proposed to enhance Hadoop capabilities. Nevertheless, an overview of these approaches shows that several aspects need to be improved in terms of performance and data relevancy. The main challenge is how to extract efficiently value from the big data sources. For this purpose, we propose in this paper to discuss Hadoop architecture and intelligent data discovery, and propose an effective on-demand Big Data contribution enabling to process relevant data in efficient and effective way according to the stakeholder's needs, and aiming to boost Data appointment by integrating multidimensional approach.|10.1145/3289100.3289108|https://doi.org/10.1145/3289100.3289108|New York, NY, USA|Association for Computing Machinery|9781450365079|2018|Towards Efficient Big Data: Hadoop Data Placing and Processing|Bahadi, Jihane and El Asri, Bouchra and Courtine, M\'{e}lanie and Rhanoui, Maryem and Kergosien, Yannick|inproceedings|10.1145/3289100.3289108|||||||||||||||||||||||||||||678912014|42
GROUP '16|Sanibel Island, Florida, USA|supervised learning, axial coding, grounded theory, unsupervised learning, coding families, machine learning|6|3–8|Proceedings of the 19th International Conference on Supporting Group Work|Grounded Theory Method (GTM) and Machine Learning (ML) are often considered to be quite different. In this note, we explore unexpected convergences between these methods. We propose new research directions that can further clarify the relationships between these methods, and that can use those relationships to strengthen our ability to describe our phenomena and develop stronger hybrid theories.|10.1145/2957276.2957280|https://doi.org/10.1145/2957276.2957280|New York, NY, USA|Association for Computing Machinery|9781450342766|2016|Machine Learning and Grounded Theory Method: Convergence, Divergence, and Combination|Muller, Michael and Guha, Shion and Baumer, Eric P.S. and Mimno, David and Shami, N. Sadat|inproceedings|10.1145/2957276.2957280|||||||||||||||||||||||||||||680545361|42
ICSCA 2020|Langkawi, Malaysia|Spatial data, Sharing, Spatial Data Infrastructure, Geospatial data, Issues and Challenges|6|51–56|Proceedings of the 2020 9th International Conference on Software and Computer Applications|The rapid development of information technology has led to the demand for the latest, precise and easy to understand data. Data especially geospatial data is becoming increasingly crucial in all types of planning and decision making. Geospatial data sharing can be categorized into different disciplines such as public safety, disaster management, transportation, traffic control, tracking, health, environment, natural resources, mining, agriculture, utilities and many more. Whether as a way of distribution or retrieval of data, geospatial data has become an essential component of government GIS operations. Despite the prominence of this activity and its centrality to the day-to-day function of many government systems, the geospatial data sharing is still given less attention in the field of natural disaster management. Preliminary information is gathered from Literature Reviews (LR) and unstructured interviews with experts to seek information in depth. Thirteen (13) issues and challenges of geospatial data sharing in Malaysia Public Sector (MPS) for natural disaster management have been identified.|10.1145/3384544.3384596|https://doi.org/10.1145/3384544.3384596|New York, NY, USA|Association for Computing Machinery|9781450376655|2020|Geospatial Data Sharing: Preliminary Studies on Issues and Challenges in Natural Disaster Management|Valachamy, Mageshwari and Sahibuddin, Shamsul and Ahmad, Noor Azurati and Bakar, Nur Azaliah Abu|inproceedings|10.1145/3384544.3384596|||||||||||||||||||||||||||||681092862|42
||external validation, Clustering approach, optimal score, stability validation, internal validation|33|||Clustering approaches are extensively used by many areas such as IR, Data Integration, Document Classification, Web Mining, Query Processing, and many other domains and disciplines. Nowadays, much literature describes clustering algorithms on multivariate data sets. However, there is limited literature that presented them with exhaustive and extensive theoretical analysis as well as experimental comparisons. This experimental survey paper deals with the basic principle, and techniques used, including important characteristics, application areas, run-time performance, internal, external, and stability validity of cluster quality, etc., on five different data sets of eleven clustering algorithms. This paper analyses how these algorithms behave with five different multivariate data sets in data representation. To answer this question, we compared the efficiency of eleven clustering approaches on five different data sets using three validity metrics-internal, external, and stability and found the optimal score to know the feasible solution of each algorithm. In addition, we have also included four popular and modern clustering algorithms with only their theoretical discussion. Our experimental results for only traditional clustering algorithms showed that different algorithms performed different behavior on different data sets in terms of running time (speed), accuracy and, the size of data set. This study emphasized the need for more adaptive algorithms and a deliberate balance between the running time and accuracy with their theoretical as well as implementation aspects.|10.1145/3490384|https://doi.org/10.1145/3490384|New York, NY, USA|Association for Computing Machinery||2022|Experimental Comparisons of Clustering Approaches for Data Representation|Anand, Sanjay Kumar and Kumar, Suresh|article|10.1145/3490384|45|mar|ACM Comput. Surv.|03600300|3|55|April 2023||||23038|2,079|Q1|163|112|365|15859|6978|365|19,16|141,60|United States|Northern America|1969-2020|Computer Science (miscellaneous) (Q1); Theoretical Computer Science (Q1)|10,790|10.282|0.01438|682646151|1517405264
OpenSym '19|"\"Sk\"\"{o}vde, Sweden\""|web analytics, digital divides, Wikipedia, peer production, dwell time, quantitative methods, readership|14||Proceedings of the 15th International Symposium on Open Collaboration|Much existing knowledge about global consumption of peer-produced information goods is supported by data on Wikipedia page view counts and surveys. In 2017, the Wikimedia Foundation began measuring the time readers spend on a given page view (dwell time), enabling a more detailed understanding of such reading patterns. In this paper, we validate and model this new data source and, building on existing findings, use regression analysis to test hypotheses about how patterns in reading time vary between global contexts. Consistent with prior findings from self-report data, our complementary analysis of behavioral data provides evidence that Global South readers are more likely to use Wikipedia to gain in-depth understanding of a topic. We find that Global South readers spend more time per page view and that this difference is amplified on desktop devices, which are thought to be better suited for in-depth information seeking tasks.|10.1145/3306446.3340829|https://doi.org/10.1145/3306446.3340829|New York, NY, USA|Association for Computing Machinery|9781450363198|2019|Dwelling on Wikipedia: Investigating Time Spent by Global Encyclopedia Readers|TeBlunthuis, Nathan and Bayer, Tilman and Vasileva, Olga|inproceedings|10.1145/3306446.3340829|14||||||||||||||||||||||||||||683121789|42
||Agriculture, Data, Information and communication technology, Data infrastructure, Governance, Business modelling||69-80||Smart Farming is a development that emphasizes the use of information and communication technology in the cyber-physical farm management cycle. New technologies such as the Internet of Things and Cloud Computing are expected to leverage this development and introduce more robots and artificial intelligence in farming. This is encompassed by the phenomenon of Big Data, massive volumes of data with a wide variety that can be captured, analysed and used for decision-making. This review aims to gain insight into the state-of-the-art of Big Data applications in Smart Farming and identify the related socio-economic challenges to be addressed. Following a structured approach, a conceptual framework for analysis was developed that can also be used for future studies on this topic. The review shows that the scope of Big Data applications in Smart Farming goes beyond primary production; it is influencing the entire food supply chain. Big data are being used to provide predictive insights in farming operations, drive real-time operational decisions, and redesign business processes for game-changing business models. Several authors therefore suggest that Big Data will cause major shifts in roles and power relations among different players in current food supply chain networks. The landscape of stakeholders exhibits an interesting game between powerful tech companies, venture capitalists and often small start-ups and new entrants. At the same time there are several public institutions that publish open data, under the condition that the privacy of persons must be guaranteed. The future of Smart Farming may unravel in a continuum of two extreme scenarios: 1) closed, proprietary systems in which the farmer is part of a highly integrated food supply chain or 2) open, collaborative systems in which the farmer and every other stakeholder in the chain network is flexible in choosing business partners as well for the technology as for the food production side. The further development of data and application infrastructures (platforms and standards) and their institutional embedment will play a crucial role in the battle between these scenarios. From a socio-economic perspective, the authors propose to give research priority to organizational issues concerning governance issues and suitable business models for data sharing in different supply chain scenarios.|https://doi.org/10.1016/j.agsy.2017.01.023|https://www.sciencedirect.com/science/article/pii/S0308521X16303754||||2017|Big Data in Smart Farming – A review|Sjaak Wolfert and Lan Ge and Cor Verdouw and Marc-Jeroen Bogaardt|article|WOLFERT201769|||Agricultural Systems|0308521X||153|||||15061|1,694|Q1|107|197|511|12414|3229|503|5,52|63,02|United Kingdom|Western Europe|1976-2020|Agronomy and Crop Science (Q1); Animal Science and Zoology (Q1)|9,779|5.370|0.00937|685475904|947875286
||Atmospheric measurements;Particle measurements;Diabetes;Sociology;Statistics;Artificial neural networks;Big data;Healthcare;Predictive analytics;Diabetes||297-301|2014 Sixth International Conference on Advanced Computing (ICoAC)|From the banking to retail, many sectors have already embraced big data regardless of whether the information comes from public or private sources. In the clinical sphere, the amount of patient data has grown exponentially because of computer based information systems. E-Health monitoring applications have some particularities concerning the importance on data quality. This paper presents a novel solution using Hadoop Mapreduce to analyze large datasets and extract useful insights from the dataset which helps doctors to effectively allocate resources. The successful healthcare delivery and planning strongly rely on data (e.g. sensed data, diagnosis, administration information); the higher quality of the data, the better will be the patient assistance. The applications are also particularly exposed to a contextual environment (i.e., patient's mobility, communication technologies, performance, information heterogeneity, etc.) that has an important impact on information management and application achievement. The main objective of our system is to predict the risk of diabetic patients for readmission in the next 30 days by measuring the probability using MapReduce. This risk score helps the physicians in recommending appropriate care for the patients.|10.1109/ICoAC.2014.7229729|||||2014|Predicting the risk of readmission of diabetic patients using MapReduce|Gowsalya, M and Krushitha, K and Valliyammai, C|inproceedings|7229729||Dec||23776927|||||||||||||||||||||||||687017258|2125517372
ICPE '22|Bejing, China|benchmarking, standardization, data analytics, measurements, metrics, spec, data management|2|13–14|Companion of the 2022 ACM/SPEC International Conference on Performance Engineering|The research field of data analytics has grown significantly with the increase of gathered and available data. Accordingly, a large number of tools, metrics, and best practices have been proposed to make sense of this vast amount of data. To this end, benchmarking and standardization are needed to understand the proposed approaches better and continuously improve them. For this purpose, numerous associations and committees exist. One of them is SPEC (Standard Performance Evaluation Corporation), a non-profit corporation for the standardization and benchmarking of performance and energy evaluations. This paper gives an overview of the recently established SPEC RG Predictive Data Analytics Working Group. The mission of this group is to foster interaction between industry and academia by contributing research to the standardization and benchmarking of various aspects of data analytics.|10.1145/3491204.3527495|https://doi.org/10.1145/3491204.3527495|New York, NY, USA|Association for Computing Machinery|9781450391597|2022|SPEC Research - Introducing the Predictive Data Analytics Working Group: Poster Paper|"\"Bauer, Andr\\'{e} and Leznik, Mark and Iqbal, Md Shahriar and Seybold, Daniel and Trubin, Igor and Erb, Benjamin and Domaschka, J\"\"{o}rg and Jamshidi, Pooyan\""|inproceedings|10.1145/3491204.3527495|||||||||||||||||||||||||||||688679951|42
||Bibliometric analysis, Tourist arrival, Hospitality demand, Knowledge map, CiteSpace, Infographic||100715||Utilizing a scientometric review of global trends and structure from 388 bibliographic records over two decades (1999–2018), this study seeks to advance the building of comprehensive knowledge maps that draw upon global travel demand studies. The study, using the techniques of co-citation analysis, collaboration network and emerging trends analysis, identified major disciplines that provide knowledge and theories for tourism demand forecasting, many trending research topics, the most critical countries, institutions, publications, and articles, and the most influential researchers. The increasing interest and output for big data and machine learning techniques in the field were visualized via comprehensive knowledge maps. This research provides meaningful guidance for researchers, operators and decision makers who wish to improve the accuracy of tourism demand forecasting.|https://doi.org/10.1016/j.tmp.2020.100715|https://www.sciencedirect.com/science/article/pii/S2211973620300829||||2020|Knowledge mapping of tourism demand forecasting research|Chengyuan Zhang and Shouyang Wang and Shaolong Sun and Yunjie Wei|article|ZHANG2020100715|||Tourism Management Perspectives|22119736||35|||||21100202157|1,454|Q1|43|140|277|12102|1824|274|6,77|86,44|United States|Northern America|2012-2020|Tourism, Leisure and Hospitality Management (Q1)|3,902|6.586|0.00445|694100211|434694424
||Generators;Data integrity;XML;Cleaning;Tools;Databases;Pipelines;data generators;data quality dimensions;data cleaning;microservice architectures;data management||78-87|2020 IEEE Sixth International Conference on Big Data Computing Service and Applications (BigDataService)|In this paper, we present DWreck, a data wrecker framework for generating unclean datasets by counterproductively applying different data quality dimensions. In a typical data-analysis pipeline, data cleaning is the most cost-intensive, laborious, and time-consuming step. Unclean dataset or partially cleaned dataset can lead to incorrect training of machine learning models and result in wrong conclusions. Generally, data-scientists examine null, missing, or duplicate values, and the dataset is cleaned by removing the entire record or imputing the values. However, deleting the records, or imputing the values cannot be termed as comprehensive cleaning, as these cleaning techniques may result in a reduction in the population of data, and increased error in estimation due to biased values. For systematically cleaning an unclean dataset, it is necessary to comply with the data quality dimensions such as completeness, validity, consistency, accuracy, and conformity. The errors described as violations of expectations for completeness, accuracy, timeliness, consistency and other dimensions of data quality often impede the successful completion of information processing streams and consequently degrade the dependent business processes. Therefore, educating a data-scientist for comprehensively cleaning a raw-dataset acquired for analysis is an incremental learning process. Moreover, for extensive training on cleaning a dataset on different quality dimensions, it is necessary to provide a variety of datasets that are unclean on various data quality dimensions. Hence, in this paper, we present DWreck, a data wrecker framework for generating unclean datasets by counterproductively applying different data quality dimensions. The DWreck framework is designed on the principles of microservices architecture pattern. For allowing function-specific extensibility, the DWreck comprises four groups of microservices: (a) Dataset Profiling, (b) Data type Processing, (c) Counterproductive Dimensions, and (d) Miscellaneous. The orchestrator coordinates the different microservices in a complex workflow that is further split into three sub-workflows to generate an unclean (wrecked) dataset as an output. Finally, we evaluate the DWreck framework on twenty seed-datasets to generate corresponding wrecked datasets.|10.1109/BigDataService49289.2020.00020|||||2020|DWreck: A Data Wrecker Framework for Generating Unclean Datasets|Chouhan, Ashish and Prabhune, Ajinkya and Prabhuraj, Paneesh and Chaudhari, Hitesh|inproceedings|9179610||Aug|||||||||||||||||||||||||||694746673|42
||Measurement units;Measurement uncertainty;Area measurement;Feature extraction;Size measurement;Phasor measurement units;Time measurement;Big data;Event detection;Machine learning;Phasor measurement units;Power system faults;Time series analysis||1-6|2022 International Conference on Smart Grid Synchronized Measurements and Analytics (SGSMA)|This paper describes simple and efficient machine learning (ML) methods for efficiently detecting multiple types of power system events captured by PMUs scarcely placed in a large power grid. It uses a single feature from each PMU based on a rectangle area enclosing the event in a given data window. This single feature is sufficient to enable commonly used ML models to detect different types of events quickly and accurately. The feature is used by five ML models on four different data-window sizes. The results indicated a tradeoff between the execution speed and detection accuracy in variety of data-window size choices. The proposed method is insensitive to most data quality issues typical for data from field PMUs, and thus it does not require major data cleansing efforts prior to feature extraction.|10.1109/SGSMA51733.2022.9806000|||||2022|Machine Learning Using a Simple Feature for Detecting Multiple Types of Events From PMU Data|Dokic, Tatjana and Baembitov, Rashid and Hai, Ameen Abdel and Cheng, Zheyuan and Hu, Yi and Kezunovic, Mladen and Obradovic, Zoran|inproceedings|9806000||May|||||||||||||||||||||||||||697421407|42
||Big Data;Internet of Things;Intelligent sensors;Data analysis;Cloud computing;Smart manufacturing;Sensor phenomena and characterization;Big data;health state monitoring;Internet of Things (IoT);noisy data cleaning;real-time systems;sensor selection||2443-2454||With the rapid growth in the use of various smart digital sensors, the Internet of Things (IoT) is a swiftly growing technology, which has contributed significantly to Industry 4.0 and the promotion of IoT-based smart factories, which gives rise to the new challenges of big data analytics and the implementation of machine learning techniques. This article proposes a practical framework that combines IoT techniques, a data lake, data analysis, and cloud computing for manufacturing equipment health-state monitoring and diagnostics in smart manufacturing. It addresses all the required aspects in the realization of such a system and allows the seamless interchange of data and functionality. Due to the specific characteristics of IoT sensor data (low quality, redundant multisources, partial labeling), we not only provide a promising framework but also give detailed insights and pay considerable attention to data quality issues. In the proposed framework, an ingestion procedure is designed to manage data collection, data security, data transformation and data storage issues. To improve the quality of IoT big data, a high-noise feature filter is proposed for automated preliminary sensor selection to suppress noisy features, followed by a noisy data cleaning module to provide good quality data for unbiased diagnosis modeling. The proposed framework can achieve seamless integration between IoT big data ingestion from the physical factory and machine learning-based data analytics in the virtual systems. It is built on top of the Apache Spark processing engine, being capable of working in both big data and real-time environments. One case study has been conducted based on a four-stage syngas compressor from real industries, which won the Best Industry Application of IoT at the BigInsights Data & AI Innovation Awards. The experimental results demonstrate the effectiveness of both the proposed IoT-architecture and techniques to address the data quality issues.|10.1109/JIOT.2021.3096637|||||2022|An Integrated Framework for Health State Monitoring in a Smart Factory Employing IoT and Big Data Techniques|Yu, Wenjin and Liu, Yuehua and Dillon, Tharam and Rahayu, Wenny and Mostafa, Fahed|article|9481251||Feb|IEEE Internet of Things Journal|23274662|3|9|||||21100338350|2,075|Q1|97|1163|1594|40380|20461|1555|12,37|34,72|United States|Northern America|2014-2020|Computer Networks and Communications (Q1); Computer Science Applications (Q1); Hardware and Architecture (Q1); Information Systems (Q1); Information Systems and Management (Q1); Signal Processing (Q1)|21,151|9.471|0.03208|700112914|1395601152
||carpooling, ride-sharing, vehicular networks, Connected autonomous vehicles, intelligent transportation systems|36|||Owing to the advancements in communication and computation technologies, the dream of commercialized connected and autonomous cars is becoming a reality. However, among other challenges such as environmental pollution, cost, maintenance, security, and privacy, the ownership of vehicles (especially for Autonomous Vehicles) is the major obstacle in the realization of this technology at the commercial level. Furthermore, the business model of pay-as-you-go type services further attracts the consumer, because there is no need for upfront investment. In this vein, the idea of car-sharing (aka carpooling) is getting ground due to, at least in part, its simplicity, cost-effectiveness, and affordable choice of transportation. Carpooling systems are still in their infancy and face challenges such as scheduling, matching passengers interests, business model, security, privacy, and communication. To date, a plethora of research work has already been done covering different aspects of carpooling services (ranging from applications to communication and technologies); however, there is still a lack of a holistic, comprehensive survey that can be a one-stop-shop for the researchers in this area to (i) find all the relevant information and (ii) identify the future research directions. To fill these research challenges, this article provides a comprehensive survey on carpooling in autonomous and connected vehicles and covers architecture, components, and solutions, including scheduling, matching, mobility, pricing models of carpooling. We also discuss the current challenges in carpooling and identify future research directions. This survey is aimed to spur further discussion among the research community for the effective realization of carpooling.|10.1145/3501295|https://doi.org/10.1145/3501295|New York, NY, USA|Association for Computing Machinery||2022|Carpooling in Connected and Autonomous Vehicles: Current Solutions and Future Directions|Zafar, Farkhanda and Khattak, Hasan Ali and Aloqaily, Moayad and Hussain, Rasheed|article|10.1145/3501295|218|sep|ACM Comput. Surv.|03600300|10s|54|January 2022||||23038|2,079|Q1|163|112|365|15859|6978|365|19,16|141,60|United States|Northern America|1969-2020|Computer Science (miscellaneous) (Q1); Theoretical Computer Science (Q1)|10,790|10.282|0.01438|700130642|1517405264
||Big data analytics (BDA), Big data industry (BDI), Analytic hierarchy process (AHP), Business model innovation, Blockchain||119014||Big data can be used by enterprises to discover their operating rules and make a forecast of their future operation. To successfully realize the benefits of big data analytics (BDA), an enterprise must rely on external heterogeneous information that is effectively combined and integrated. Enterprises, their upstream and downstream organizations, that utilize big data together form a data industry chain. This study discussed the potential bottlenecks and obstacles of big data-based business model innovation and data transaction platform construction in the business environment. It also explored the successful influencing factors in the big data industry, and applied the analytic hierarchy process to collect opinions from information technology professionals. Regarding the critical factors associated with BDA success, this study found that the experts attached the greatest importance to the maintenance of personal and business privacy and security during the data used with data de-identification being of utmost concern. Based on the results, this study proposed three innovative services models for the data industry. It further analyzed a role model based on the three proposed service models, and defined the rights and obligations of each role holder. Based on decentralization of the blockchain, a multi-layer cloud-based services platform architecture was designed to support data transactions, thereby ensuring the fairness and transparency of big data transactions.|https://doi.org/10.1016/j.eswa.2022.119014|https://www.sciencedirect.com/science/article/pii/S0957417422020322||||2023|Critical success factors and architecture of innovation services models in data industry|Tsung-Yi Chen and Hsiu-Fang Chang|article|CHEN2023119014|||Expert Systems with Applications|09574174||213|||||24201|1,368|Q1|207|770|1945|42314|17345|1943|8,67|54,95|United Kingdom|Western Europe|1990-2021|Artificial Intelligence (Q1); Computer Science Applications (Q1); Engineering (miscellaneous) (Q1)|55,444|6.954|0.04053|703224862|1377770283
AIAM2021|Manchester, United Kingdom||5|1792–1796|2021 3rd International Conference on Artificial Intelligence and Advanced Manufacture|"\"The sculpture of Nanfeng Nuo mask originated in the Han Dynasty, developed in the Tang and Song Dynasties, and prospered in the Ming and Qing Dynasties. The carving art has been passed down to this day. The sculptures of Nanfeng Nuo masks are famous for their simple and profound, vivid shapes and delicate techniques. This article first stated that under the background of AR technology, in terms of digital protection mode, limited by ideological understanding and technology, the current digital protection of Chinese traditional culture is still at the stage of digital information collection and preservation. How to enrich and perfect the existing digital protection mode with new digital technology is an urgent problem to be solved in the new era. Taking the Nanfeng Nuo mask as an example, this research analyzes the inheritance dilemma of the Nanfeng Nuo mask wood carving and the modern and innovative protection model through the reading of historical documents, field inspections of existing wood carvings, survey visits to the protection status, and understanding of digital technology. Through the fuzzy KNN algorithm and AR compared to the database, the various databases are related to form a complete protection system; in the \"\"live inheritance protection mode\"\", AR technology is proposed as the basic technology, and AR image acquisition technology and AR display technology are proposed. , AR human-computer interaction technology and digital protection mode are combined, and then a digital protection platform based on AR technology is designed to achieve the realization of the live inheritance mode by the way audiences participate in the use of the digital protection platform. Experimental research results show that people in their 30s to 40s may take a long time to receive and learn when facing a new interactive operating system, but now due to the popularity of social software such as WeChat, the 30 to 40 age group 24.4% of the population can also perform basic operations, which provides a prerequisite for the use of AR technology to digitize the Nanfeng Nuo mask.\""|10.1145/3495018.3495486|https://doi.org/10.1145/3495018.3495486|New York, NY, USA|Association for Computing Machinery|9781450385046|2022|Digital Protection of Nanfeng Nuo Mask Based on AR Technology|Liu, Ying and Yu, Wei and Xiao, Suhong|inproceedings|10.1145/3495018.3495486|||||||||||||||||||||||||||||703403629|42
||Internet-of-Things, Smart manufacturing, Big data, Data analytics, Feature engineering, Deep learning, Statistical learning||106970||As IoT-enabled manufacturing is still in its infancy, there are several key research gaps that need to be addressed. These gaps include the understanding of the characteristics of the big data generated from industrial IoT sensors, the challenges they present to process data analytics, as well as the specific opportunities that the IoT big data could bring to advance manufacturing. In this paper, we use an inhouse-developed IoT-enabled manufacturing testbed to study the characteristics of the big data generated from the testbed. Since the quality of the data usually has the most impact on process modeling, data veracity is often the most challenging characteristic of big data. To address that, we explore the role of feature engineering in developing effective machine learning models for predicting key process variables. We compare complex deep learning approaches to a simple statistical learning approach, with different level or extent of feature engineering, to explore their pros and cons for potential industrial IoT-enabled manufacturing applications.|https://doi.org/10.1016/j.compchemeng.2020.106970|https://www.sciencedirect.com/science/article/pii/S0098135420300363||||2020|Feature engineering in big data analytics for IoT-enabled smart manufacturing – Comparison between deep learning and statistical learning|Devarshi Shah and Jin Wang and Q. Peter He|article|SHAH2020106970|||Computers & Chemical Engineering|00981354||141|||||24600|1,017|Q1|139|399|1010|19037|4474|1001|4,33|47,71|United Kingdom|Western Europe|1977-2020|Chemical Engineering (miscellaneous) (Q1); Computer Science Applications (Q1)||||704405991|624805784
|||12|432–443||Since regular expressions are often used to detect errors in sequences such as strings or date, it is natural to use them for data repair. Motivated by this, we propose a data repair method based on regular expression to make the input sequence data obey the given regular expression with minimal revision cost. The proposed method contains two steps, sequence repair and token value repair.For sequence repair, we propose the Regular-expression-based Structural Repair (RSR in short) algorithm. RSR algorithm is a dynamic programming algorithm that utilizes Nondeterministic Finite Automata (NFA) to calculate the edit distance between a prefix of the input string and a partial pattern regular expression with time complexity of O(nm2) and space complexity of O(mn) where m is the edge number of NFA and n is the input string length. We also develop an optimization strategy to achieve higher performance for long strings. For token value repair, we combine the edit-distance-based method and associate rules by a unified argument for the selection of the proper method. Experimental results on both real and synthetic data show that the proposed method could repair the data effectively and efficiently.|10.14778/2876473.2876478|https://doi.org/10.14778/2876473.2876478||VLDB Endowment||2016|Repairing Data through Regular Expressions|Li, Zeyu and Wang, Hongzhi and Shao, Wei and Li, Jianzhong and Gao, Hong|article|10.14778/2876473.2876478||jan|Proc. VLDB Endow.|21508097|5|9|January 2016||||21100199855|0,946|Q1|134|246|598|12401|3759|566|5,50|50,41|United States|Northern America|2008-2020|Computer Science (miscellaneous) (Q1)|7,198|2.047|0.00891|704677824|1216159931
||Cloud computing;Data security;Data integrity;Big Data;Maintenance engineering;Virtualization;Information technology;big data;cloud computing;data security;big data cloud computing;security policy||1446-1450|2021 IEEE 5th Advanced Information Technology, Electronic and Automation Control Conference (IAEAC)|In the big data cloud computing environment, data security issues have become a focus of attention. This paper delivers an overview of conceptions, characteristics and advanced technologies for big data cloud computing. Security issues of data quality and privacy control are elaborated pertaining to data access, data isolation, data integrity, data destruction, data transmission and data sharing. Eventually, a virtualization architecture and related strategies are proposed to against threats and enhance the data security in big data cloud environment.|10.1109/IAEAC50856.2021.9391025|||||2021|Research on Data Security in Big Data Cloud Computing Environment|Wang, Fengling and Wang, Han and Xue, Liang|inproceedings|9391025||March||26896621||5|||||||||||||||||||||||704872499|916005009
||neural networks, model scaling, many-to-many, bitext mining, multilingual machine translation|48|||Existing work in translation demonstrated the potential of massively multilingual machine translation by training a single model able to translate between any pair of languages. However, much of this work is English-Centric, training only on data which was translated from or to English. While this is supported by large sources of training data, it does not reflect translation needs worldwide. In this work, we create a true Many-to-Many multilingual translation model that can translate directly between any pair of 100 languages. We build and open-source a training data set that covers thousands of language directions with parallel data, created through large-scale mining. Then, we explore how to effectively increase model capacity through a combination of dense scaling and language-specific sparse parameters to create high quality models. Our focus on non-English-Centric models brings gains of more than 10 BLEU when directly translating between non-English directions while performing competitively to the best single systems from the Workshop on Machine Translation (WMT). We open-source our scripts so that others may reproduce the data, evaluation, and final M2M- 100 model||||JMLR.org||2022|Beyond English-Centric Multilingual Machine Translation|Fan, Angela and Bhosale, Shruti and Schwenk, Holger and Ma, Zhiyi and El-Kishky, Ahmed and Goyal, Siddharth and Baines, Mandeep and Celebi, Onur and Wenzek, Guillaume and Chaudhary, Vishrav and Goyal, Naman and Birch, Tom and Liptchinsky, Vitaliy and Edunov, Sergey and Grave, Edouard and Auli, Michael and Joulin, Armand|article|10.5555/3546258.3546365|107|jul|J. Mach. Learn. Res.|15324435|1|22|January 2021||||||||||||||||||||||706486459|1642452964
AICCC '21|Kyoto, Japan|Influencing factors, Government audit, Big data analysis capability|7|172–178|2021 4th Artificial Intelligence and Cloud Computing Conference|The widespread application of big data has had a profound impact on social and economic development. Government auditing is the guarantee for the modernization of national governance, and the development of big data audit capability has become the key to improving national governance capability. This paper summarizes the concept of government audit big data capability, and constructs the influencing factor model of government audit big data capability. This study finds that the construction degree of audit big data platform, big data management ability, big data audit technology and auditors' big data technology ability have a significant positive impact on the government audit big data ability, and the audit organization coordination ability plays a positive moderating effect in the whole impact process. This study provides guidance for the improvement and development of government audit big data capability.|10.1145/3508259.3508284|https://doi.org/10.1145/3508259.3508284|New York, NY, USA|Association for Computing Machinery|9781450384162|2022|Research on Influencing Factors of Government Audit Big Data Capability|Sun, Yu and Niu, Yanfang and Lu, Le|inproceedings|10.1145/3508259.3508284|||||||||||||||||||||||||||||708438516|42
||Perspective, Science, Statistics, Data science, Similarities, Differences, Pragmatism||121111||The importance and relevance of the discipline of statistics with the merits of the evolving field of data science continues to be debated in academia and industry. Following a narrative literature review with over 100 scholarly and practitioner-oriented publications from statistics and data science, this article generates a pragmatic perspective on the relationships and differences between statistics and data science. Some data scientists argue that statistics is not necessary for data science as statistics delivers simple explanations and data science delivers results. Therefore, this article aims to stimulate debate and discourse among both academics and practitioners in these fields. The findings reveal the need for stakeholders to accept the inherent advantages and disadvantages within the science of statistics and data science. The science of statistics enables data science (aiding its reliability and validity), and data science expands the application of statistics to Big Data. Data scientists should accept the contribution and importance of statistics and statisticians must humbly acknowledge the novel capabilities made possible through data science and support this field of study with their theoretical and pragmatic expertise. Indeed, the emergence of data science does pose a threat to statisticians, but the opportunities for synergies are far greater.|https://doi.org/10.1016/j.techfore.2021.121111|https://www.sciencedirect.com/science/article/pii/S0040162521005448||||2021|The science of statistics versus data science: What is the future?|Hossein Hassani and Christina Beneki and Emmanuel Sirimal Silva and Nicolas Vandeput and Dag Øivind Madsen|article|HASSANI2021121111|||Technological Forecasting and Social Change|00401625||173|||||14704|2,226|Q1|117|448|1099|35581|10127|1061|9,01|79,42|United States|Northern America|1970-2020|Applied Psychology (Q1); Business and International Management (Q1); Management of Technology and Innovation (Q1)|21,116|8.593|0.02416|708619198|1949868303
ICSE-NIER '22|Pittsburgh, Pennsylvania|safety, machine learning, data uncertainty, Dempster Shafer theory|5|11–15|Proceedings of the ACM/IEEE 44th International Conference on Software Engineering: New Ideas and Emerging Results|The latest trend of incorporating various data-centric machine learning (ML) models in software-intensive systems has posed new challenges in the quality assurance practice of software engineering, especially in a high-risk environment. ML experts are now focusing on explaining ML models to assure the safe behavior of ML-based systems. However, not enough attention has been paid to explain the inherent uncertainty of the training data. The current practice of ML-based system engineering lacks transparency in the systematic fitness assessment process of the training data before engaging in the rigorous ML model training. We propose a method of assessing the collective confidence in the quality of a training dataset by using Dempster Shafer theory and its modified combination rule (Yager's rule). With the example of training datasets for pedestrian detection of autonomous vehicles, we demonstrate how the proposed approach can be used by the stakeholders with diverse expertise to combine their beliefs in the quality arguments and evidences about the data. Our results open up a scope of future research on data requirements engineering that can facilitate evidence-based data assurance for ML-based safety-critical systems.|10.1145/3510455.3512779|https://doi.org/10.1145/3510455.3512779|New York, NY, USA|Association for Computing Machinery|9781450392242|2022|Are We Training with the Right Data? Evaluating Collective Confidence in Training Data Using Dempster Shafer Theory|Dey, Sangeeta and Lee, Seok-Won|inproceedings|10.1145/3510455.3512779|||||||||||||||||||||||||||||708706602|42
||Current measurement;Urban areas;Context;Big Data;Uncertainty;Decision making;Data quality;fuzzy measure;uncertainty modeling||627-639||In this paper, a novel framework for data quality measurement is proposed by adopting a measure-theoretic treatment of the problem. Instead of considering a specific setting in which quality must be assessed, our approach departs more formally from the concept of measurement. The basic assumption of the framework is that the highest possible quality can be described by means of a set of predicates. Quality of data is then measured by evaluating those predicates and by combining their evaluations. This combination is based on a capacity function (i.e., a fuzzy measure) that models for each combination of predicates the capacity with respect to the quality of the data. It is shown that expression of quality on an ordinal scale entails a high degree of interpretation and a compact representation of the measurement function. Within this purely ordinal framework for measurement, it is shown that reasoning about quality beyond the ordinal level naturally originates from the uncertainty about predicate evaluation. It is discussed how the proposed framework is positioned with respect to other approaches with particular attention to aggregation of measurements. The practical usability of the framework is discussed for several well known dimensions of data quality and demonstrated in a use-case study about clinical trials.|10.1109/TFUZZ.2017.2686807|||||2018|A Measure-Theoretic Foundation for Data Quality|Bronselaer, Antoon and De Mol, Robin and De Tré, Guy|article|7885523||April|IEEE Transactions on Fuzzy Systems|19410034|2|26|||||||||||||||||||||||710870901|1182942552
WWW '22|Virtual Event, Lyon, France|Graph Neural Network, Contrastive Learning, Collaborative Filtering, Recommender System|10|2320–2329|Proceedings of the ACM Web Conference 2022|Recently, graph collaborative filtering methods have been proposed as an effective recommendation approach, which can capture users’ preference over items by modeling the user-item interaction graphs. Despite the effectiveness, these methods suffer from data sparsity in real scenarios. In order to reduce the influence of data sparsity, contrastive learning is adopted in graph collaborative filtering for enhancing the performance. However, these methods typically construct the contrastive pairs by random sampling, which neglect the neighboring relations among users&nbsp;(or items) and fail to fully exploit the potential of contrastive learning for recommendation. To tackle the above issue, we propose a novel contrastive learning approach, named Neighborhood-enriched Contrastive Learning, named NCL, which explicitly incorporates the potential neighbors into contrastive pairs. Specifically, we introduce the neighbors of a user&nbsp;(or an item) from graph structure and semantic space respectively. For the structural neighbors on the interaction graph, we develop a novel structure-contrastive objective that regards users&nbsp;(or items) and their structural neighbors as positive contrastive pairs. In implementation, the representations of users&nbsp;(or items) and neighbors correspond to the outputs of different GNN layers. Furthermore, to excavate the potential neighbor relation in semantic space, we assume that users with similar representations are within the semantic neighborhood, and incorporate these semantic neighbors into the prototype-contrastive objective. The proposed NCL can be optimized with EM algorithm and generalized to apply to graph collaborative filtering methods. Extensive experiments on five public datasets demonstrate the effectiveness of the proposed NCL, notably with 26% and 17% performance gain over a competitive graph collaborative filtering base model on the Yelp and Amazon-book datasets, respectively. Our implementation code is available at: https://github.com/RUCAIBox/NCL.|10.1145/3485447.3512104|https://doi.org/10.1145/3485447.3512104|New York, NY, USA|Association for Computing Machinery|9781450390965|2022|Improving Graph Collaborative Filtering with Neighborhood-Enriched Contrastive Learning|Lin, Zihan and Tian, Changxin and Hou, Yupeng and Zhao, Wayne Xin|inproceedings|10.1145/3485447.3512104|||||||||||||||||||||||||||||712731890|42
||Couplings;Support vector machines;Training;Indexing;Manuals;Motion pictures;record linkage;internet video;support vector machine;manual review||31-38|2016 IEEE 14th Intl Conf on Dependable, Autonomic and Secure Computing, 14th Intl Conf on Pervasive Intelligence and Computing, 2nd Intl Conf on Big Data Intelligence and Computing and Cyber Science and Technology Congress(DASC/PiCom/DataCom/CyberSciTech)|Record linkage is widely used in many fields, which is a crucial step to increase data quality before data analyzing and data mining. The task of record linkage is to identify records that correspond to the same entities from multi-sources data. In this paper, we describe detailed process of record linkage through an application of internet video, with the purpose of guiding the practice. A method of combination of twice Iterative SVM (Support Vector Machine) training and controllable manual review has been presented. The experiment based on abundant actual data achieves over 98% in F-score.|10.1109/DASC-PICom-DataCom-CyberSciTec.2016.21|||||2016|Record Linkage Using the Combination of Twice Iterative SVM Training and Controllable Manual Review|Wang, Fei and Wang, Hongbo|inproceedings|7588816||Aug|||||||||||||||||||||||||||712803741|42
||Computer architecture;Microprocessors;Big Data;Quality of service;Anomaly detection;Cellular networks;Quality of experience||1-6|2018 IEEE Global Communications Conference (GLOBECOM)|5G is envisioned to have an artificial intelligence (AI)-empowerment to efficiently plan, manage and optimize the extremely complex network by leveraging colossal amount of data (big data) generated at different levels of the network architecture. Cell outages and congestion pose serious threat to the network management. Sleeping cell is a special case of cell outage in which the cell provides inferior services to its users. This peculiar behavior of the cell is particularly challenging to detect as it disguises itself from the network monitoring entity. Inadequate accuracy and high false alarms are two major constraints of state-of-the-art approaches for the anomaly-sleeping cell and surge in user traffic activity that may lead to congestion-detection in cellular networks. This implies squandering of scarce resources which ultimately results in increased operational expenditure (OPEX) while disrupting network's quality of service (QoS) and user's quality of experience (QoE). Inspired from the prominent success of deep learning (DL) technology in machine learning domain, this is the first study that applies DL for the detection of abovementioned anomalies. We utilized, and did a comprehensive study of, L-layer deep feedforward neural network fueled by real call detail record (CDR) dataset (big data) and achieved 94.6% accuracy with 1.7% false positive rate (FPR), that are remarkable improvements and overcome the limitations of the previous studies. The preliminary results elucidate the feasibility and preeminence of our proposed anomaly detection framework.|10.1109/GLOCOM.2018.8647366|||||2018|Deep Learning-Based Big Data-Assisted Anomaly Detection in Cellular Networks|Hussain, Bilal and Du, Qinghe and Ren, Pinyi|inproceedings|8647366||Dec||25766813|||||||||||||||||||||||||712862035|57625256
||Big data, Cloud computing, Bioinformatics, High throughput data, MapReduce, Machine learning||100869||The wave of new technologies has opened up the opportunity for cost-effective generation of high-throughput profiles of biological systems. This is generating tons of biological data. It is thus leading us towards the “big data” era which is creating a pressing need to bridge the gap between high-throughput technological development and our ability for managing, analyzing, and integrating the biological big data. To harness the maximum out of it, sufficient expertise needs to be developed for big data management and analysis. In this review, we discuss the challenges related to storage, transfer, access and analysis of unstructured and structured biological big data. Subsequently, it provides a comprehensive summary regarding the important strategies adopted for biological big data management which includes a discussion on all the recently used tools or software built for high throughput processing and analysis of biological big data. Finally it discusses the future perspectives of big data bioinformatics.|https://doi.org/10.1016/j.genrep.2020.100869|https://www.sciencedirect.com/science/article/pii/S2452014420302831||||2020|Big data in biology: The hope and present-day challenges in it|Subhajit Pal and Sudip Mondal and Gourab Das and Sunirmal Khatua and Zhumur Ghosh|article|PAL2020100869|||Gene Reports|24520144||21|||||21100445644|0,235|Q4|9|407|442|16946|331|441|0,72|41,64|Netherlands|Western Europe|2015-2020|Genetics (Q4)||||717403370|1089003252
|||8|12–19||Almost all organizations use some type of Entity Resolution (ER) methods to uniquely identify their customers and vendors across different channels of contact. In the case of persons, this requires the use of personally identifying information (PII) such as name, address, phone number, and email address. Because of the growing concerns over data privacy and identity theft, organizations are reluctant to release personally-identifiable customer information even for education and research purposes. An alternative is to generate synthetic data to use in student exercises and for research related to entity resolution methods and techniques. One advantage of synthetically generated data for ER is it can be fully annotated with the correct linking making it very easy to calculate the precision and recall of linking operations. This paper discusses a simple method to generate synthetic data as input for ER processes. The method allows the user to randomly assign certain types and levels of data quality errors along with other types of non-error variations to the data, such as nicknames, different date formats, and changes in address. For ER research in particular, the method can create introduce data redundancy by copying records referencing the same person into the same file or into different files with different record layouts.|||Evansville, IN, USA|Consortium for Computing Sciences in Colleges||2019|Generating Synthetic Data to Support Entity Resolution Education and Research|Ye, Yumeng and Talburt, John R.|article|10.5555/3344081.3344082||apr|J. Comput. Sci. Coll.|19374771|7|34|April 2019||||||||||||||||||||||717540667|1795572528
||Big data, Administrative data, Data management, Data quality, Data access||1-12||The term big data is currently a buzzword in social science, however its precise meaning is ambiguous. In this paper we focus on administrative data which is a distinctive form of big data. Exciting new opportunities for social science research will be afforded by new administrative data resources, but these are currently under appreciated by the research community. The central aim of this paper is to discuss the challenges associated with administrative data. We emphasise that it is critical for researchers to carefully consider how administrative data has been produced. We conclude that administrative datasets have the potential to contribute to the development of high-quality and impactful social science research, and should not be overlooked in the emerging field of big data.|https://doi.org/10.1016/j.ssresearch.2016.04.015|https://www.sciencedirect.com/science/article/pii/S0049089X1630206X||||2016|The role of administrative data in the big data revolution in social science research|Roxanne Connelly and Christopher J. Playford and Vernon Gayle and Chris Dibben|article|CONNELLY20161|||Social Science Research|0049089X||59||Special issue on Big Data in the Social Sciences|||||||||||||||||||||720422046|69629762
||software testing, metamorphic testing, software engineering, software verification and validation|4|56–59||MET is a relatively new workshop on metamorphic testing for academic researchers and industry practitioners. The first international workshop on MET (MET 2016) was co-located with the 38th International Conference on Software Engineering (ICSE 2016) in Austin TX, USA on May 16, 2016. Since then the workshop has become an annual event. This paper reports on the fourth International Workshop on Metamorphic Testing (MET 2019) held in Montr\'{e}al, Canada on May 26, 2019, as part of the 41st International Conference on Software Engineering (ICSE 2019). We first outline the aims of the workshop, followed by a discussion of its keynote speech and technical program.|10.1145/3356773.3356810|https://doi.org/10.1145/3356773.3356810|New York, NY, USA|Association for Computing Machinery||2020|Workshop Summary: 2019 IEEE / ACM Fourth International Workshop on Metamorphic Testing (MET 2019)|Xie, Xiaoyuan and Poon, Pak-Lok and Pullum, Laura L.|article|10.1145/3356773.3356810||oct|SIGSOFT Softw. Eng. Notes|01635948|3|44|July 2019||||||||||||||||||||||722456307|1286430388
||Artificial intelligence, Intelligent medicine, Big data, Data collection, Data storage, Data management||49-50||Medical artificial intelligence (AI) is an important technical asset to support medical supply-side reforms and national development in the big data era. Clinical data from multiple disciplines represent building blocks for the development and application of AI-aided diagnostic and treatment systems based on medical big data. However, the inconsistent quality of these data resources in AI research leads to waste and inefficiencies. Therefore, it is crucial that the field formulates the requirements and content related to data processing as part of the development of intelligent medicine. To promote medical AI research worldwide, the “Belt and Road” International Ophthalmic Artificial Intelligence Research and Development Alliance will establish a series of expert recommendations for data quality in intelligent medicine.|https://doi.org/10.1016/j.imed.2021.04.004|https://www.sciencedirect.com/science/article/pii/S2667102621000073||||2021|The critical need to establish standards for data quality in intelligent medicine|Ruiyang Li and Yahan Yang and Haotian Lin|article|LI202149|||Intelligent Medicine|26671026|2|1|||||||||||||||||||||||733230226|714385722
DSIT 2020|Xiamen, China|spatial-temporal data, Bi-LSTM, outlier detection, attention mechanism|7|220–226|Proceedings of the 3rd International Conference on Data Science and Information Technology|The rapid development of global positioning system has given birth to a large number of spatial-temporal data, and there are many outliers of points obviously in these trajectory data. It is very important to detect the outliers in the trajectory to improve the data quality and accuracy of trajectory mining. In this paper, we propose a trajectory outlier detection algorithm based on bi-directional long short-term memory model and attention mechanism. Firstly, an eight-dim eigenvector is extracted from each point of trajectory, and then a two-layer bi-directional long short-term memory model is constructed. Finally, representing the trajectory points in an interactive way which is called attention mechanism. The input of the model is the trajectory point with a certain length, and the output is the type of the trajectory point. The model can automatically learn the difference between the normal point and the adjacent abnormal point with motion features. Experimental dataset based on real trajectory data of taxi from Beijing, and results showed that the performance of this algorithm is significantly better than constant speed threshold method or classical machine learning classification. Especially the precision and recall reaches 0.93 and 0.90 separately, which proves the effectiveness of this algorithm.|10.1145/3414274.3414505|https://doi.org/10.1145/3414274.3414505|New York, NY, USA|Association for Computing Machinery|9781450376044|2020|An Automatic Learning Model for Trajectory Outlier Detection|Fu, Qingwen and Zhu, Jiahui and Chen, Yuepeng and Wan, Jintao and He, Bin|inproceedings|10.1145/3414274.3414505|||||||||||||||||||||||||||||733647923|42
||tensor, deployment, DTA, trajectory, recruitment, Participatory sensing|22|||Participatory sensing is a promising sensing paradigm that enables collection, processing, dissemination and analysis of the phenomena of interest by ordinary citizens through their handheld sensing devices. Participatory sensing has huge potential in many applications, such as smart transportation and air quality monitoring. However, participants may submit low-quality, misleading, inaccurate, or even malicious data if a participatory sensing campaign is not launched effectively. Therefore, it has become a significant issue to establish an efficient participatory sensing campaign for improving the data quality. This article proposes a novel five-tier framework of participatory sensing and addresses several technical challenges in this proposed framework including: (1) optimized deployment of data collection points (DC-points); and (2) efficient recruitment strategy of participants. Toward this end, the deployment of DC-points is formulated as an optimization problem with maximum utilization of sensor and then a Wise-Dynamic DC-points Deployment (WD3) algorithm is designed for high-quality sensing. Furthermore, to guarantee the reliable sensing data collection and communication, a trajectory-based strategy for participant recruitment is proposed to enable campaign organizers to identify well-suited participants for data sensing based on a joint consideration of temporal availability, trust, and energy. Extensive experiments and performance analysis of the proposed framework and associated algorithms are conducted. The results demonstrate that the proposed algorithm can achieve a good sensing coverage with a smaller number of DC-points, and the participants that are termed as social sensors are easily selected, to evaluate the feasibility and extensibility of the proposed recruitment strategies.|10.1145/2808198|https://doi.org/10.1145/2808198|New York, NY, USA|Association for Computing Machinery||2015|Launching an Efficient Participatory Sensing Campaign: A Smart Mobile Device-Based Approach|Hao, Fei and Jiao, Mingjie and Min, Geyong and Yang, Laurence T.|article|10.1145/2808198|18|oct|ACM Trans. Multimedia Comput. Commun. Appl.|15516857|1s|12|October 2015||||||||||||||||||||||734590787|343492898
PASC '22|Basel, Switzerland|juvenile delinquency, big data analysis system, natural language processing, information retrieval, data visualization, social science research, newspaper article segmentation, historical newspapers, social construction, image analysis, text analysis|11||Proceedings of the Platform for Advanced Scientific Computing Conference|The availability and generation of digitized newspaper collections have provided researchers in several domains with a powerful tool to advance their research. More specifically, digitized historical newspapers give us a magnifying glass into the past. In this paper, we propose a scalable and customizable big data analysis system that enables researchers to study complex questions about our society as depicted in news media for the past few centuries by applying cutting-edge text analysis tools to large historical newspaper collections. We discuss our experience with building a preliminary version of such a system, including how we have addressed the following challenges: processing millions of digitized newspaper pages from various publications worldwide, which amount to hundreds of terabytes of data; applying article segmentation and Optical Character Recognition (OCR) to historical newspapers, which vary between and within publications over time; retrieving relevant information to answer research questions from such data collections by applying human-in-the-loop machine learning; and enabling users to analyze topic evolution and semantic dynamics with multiple compatible analysis operators. We also present some preliminary results of using the proposed system to study the social construction of juvenile delinquency in the United States and discuss important remaining challenges to be tackled in the future.|10.1145/3539781.3539795|https://doi.org/10.1145/3539781.3539795|New York, NY, USA|Association for Computing Machinery|9781450394109|2022|Toward a Big Data Analysis System for Historical Newspaper Collections Research|Satheesan, Sandeep Puthanveetil and Bhavya and Davies, Adam and Craig, Alan B. and Zhang, Yu and Zhai, ChengXiang|inproceedings|10.1145/3539781.3539795|12||||||||||||||||||||||||||||735472454|42
||Machine learning;Big Data;Feature extraction;Spatial databases;Trajectory;Task analysis;Anomaly detection||4509-4518|2020 IEEE International Conference on Big Data (Big Data)|Outlier detection has become one of the core tasks in spatio-temporal data mining. It plays an essential role in data quality improvement for the machine learning models and recognizing the anomalous patterns, which may remarkably deviate from expected patterns among the trajectory datasets. In this work, we propose a clustering-based technique to detect local outliers in trajectory datasets by utilizing spatial and temporal attributes of moving objects. This local outlier detection involves three phases. In the first phase, we apply a temporal partition procedure to divide the raw trajectory into multiple trajectory segments and extract trajectory features from spatial and temporal attributes for each trajectory segment. Then, we generate template features of trajectory segments by applying a clustering schema in the second phase. Finally, we use the abnormal score - a novel dissimilarity measure, which quantifies the disparity among the query and template trajectory segments in terms of trajectory features and hence determines the local outliers based on the distribution of abnormal score. To demonstrate the effectiveness of our method, we conduct three case studies on the real-life spatio-temporal trajectory datasets from the solar astroinformatics domain (i.e., solar active regions, coronal mass ejections, polarity inversion lines (PIL)). Our experimental results show that our local outlier detection approach can effectively discover the erroneous reports from the reporting module and abnormal phenomenon in various spatio-temporal trajectory datasets.|10.1109/BigData50022.2020.9377801|||||2020|Local Outlier Detection for Multi-type Spatio-temporal Trajectories|Cai, Xumin and Aydin, Berkay and Maydeo, Saurabh and Ji, Anli and Angryk, Rafal|inproceedings|9377801||Dec|||||||||||||||||||||||||||735777776|42
||Big Data;Data analysis;Data privacy;Quality of service;Data models;Security;Biological system modeling;Cloud-Computing,-Big-Data,-Quality-of-Service,-Data-Analytics||47-48|2018 48th Annual IEEE/IFIP International Conference on Dependable Systems and Networks Workshops (DSN-W)|This paper describes the achievements of project EUBra-BIGSEA, which has delivered programming models and data analytics tools for the development of distributed Big Data applications. As framework components, multiple data models are supported (e.g. data streams, multidimensional data, etc.) and efficient mechanisms to ensure privacy and security, on top of a QoS-aware layer for the smart and rapid provisioning of resources in a cloud-based environment.|10.1109/DSN-W.2018.00023|||||2018|EUBra-BIGSEA, A Cloud-Centric Big Data Scientific Research Platform|Blanquer, Ignacio and Meira, Wagner|inproceedings|8416208||June||23256664|||||||||||||||||||||||||738851907|439168804
||Economics;Cloud computing;Data models;Contracts;Big Data;Computer architecture;Open Data Market;Data Marketplace;Trusted Data Market;Industrial Data Space;Data Economics;STREAM Data Properties||1017-1021|2019 International Conference on High Performance Computing & Simulation (HPCS)|This paper discusses the principles of organisation and infrastructure components of Open Data Markets (ODM) that would facilitate secure and trusted data exchange between data market participants, and other cooperating organisations. The paper provides a definition of the data properties as economic goods and identifies the generic characteristics of ODM as a Service. This is followed by a detailed description of the generic data market infrastructure that can be provisioned on demand for a group of cooperating parties. The proposed data market infrastructure and its operation are employing blockchain technologies for securing data provenance and providing a basis for data monetisation. Suggestions for trust management and data quality assurance are discussed.|10.1109/HPCS48598.2019.9188195|||||2019|Open Data Market Architecture and Functional Components|Demchenko, Yuri and Cushing, Reggie and Los, Wouter and Grosso, Paola and de Laat, Cees and Gommans, Leon|inproceedings|9188195||July|||||||||||||||||||||||||||741230009|42
||smart cities, urban sensor networks, sociotechnical system, civic data|22|||We use a sociotechnical perspective to expand upon prior characterizations of deploying end-to-end urban sensor networks that focus primarily on the technical aspects of such systems. Via exploratory, semi-structured interviews with those deploying a number of urban sensor networks in a single American city, we identify ways that human decision-making and collaborative processes influence how these infrastructures are built. We synthesize these findings into a framework in which sociotechnical factors show up across the phases of data collection, management, analysis, and impacts within smart city projects. Each phase can display variability in immediacy, automation, geographic scope, and ownership. Finally, we use our situated work to discuss a generalizable tension within smart city projects between cross-domain data integration and fragmentation and provide implications for CSCW research, the design of smart city data platforms, and municipal policy.|10.1145/3449252|https://doi.org/10.1145/3449252|New York, NY, USA|Association for Computing Machinery||2021|"\"\"\"Wiring a City\"\": A Sociotechnical Perspective on Deploying Urban Sensor Networks\""|Van Kleunen, Lucy and Muller, Brian and Voida, Stephen|article|10.1145/3449252|178|apr|Proc. ACM Hum.-Comput. Interact.||CSCW1|5|April 2021||||||||||||||||||||||743804366|42
||Cloud computing;Computer architecture;Semantics;Big data;Quality of service;Meteorology;NIST;Cloud Computing;Orchestration;Formal Semantics;Availability||46-51|2016 30th International Conference on Advanced Information Networking and Applications Workshops (WAINA)|Every-Day lives are becoming increasingly instrumented by electronic devices and any kind of computer-based (distributed) service. As a result, organizations need to analyse an enormous amounts of data in order to increase their incomings or to improve their services. Anyway, setting-up a private infrastructure to execute analytics over Big Data is still expensive. The exploitation of Cloud infrastructure in Big Data management is appealing because of costs reductions and potentiality of storage, network and computing resources. The Cloud can consistently reduce the cost of analysis of data from different sources, opening analytics to big storages in a multi-cloud environment. Anyway, creating and executing this kind of service is very complex since different resources have to be provisioned and coordinated depending on users' needs. Orchestration is a solution to this problem, but it requires proper languages and methodologies for automatic composition and execution. In this work we propose a methodology for composition of services used for analyses of different Big Data sources: in particular an Orchestration language is reported able to describe composite services and resources in a multi-cloud environment.|10.1109/WAINA.2016.169|||||2016|Automatic Cloud Services Composition for Big Data Management|Amato, Flora and Moscato, Francesco|inproceedings|7471171||March|||||||||||||||||||||||||||744067152|42
||Databases;Tools;Software;Big Data;Data mining;Companies;Zero Deviation Life Cycle (ZDLC);ZDLC;layered lineage;lineage report;data lineage;data quality||638-641|2016 IEEE Intl Conference on Computational Science and Engineering (CSE) and IEEE Intl Conference on Embedded and Ubiquitous Computing (EUC) and 15th Intl Symposium on Distributed Computing and Applications for Business Engineering (DCABES)|As technology moves from being an enabler to become the lifeblood of businesses in the digital era, so does the data used to support the implementation of these technological shifts. In order for the data to be an asset and not a liability, it is primordial to ensure that the data captured and maintained over time is accurate, traceable, reliable and current. However, very often, in order to complete a business function the data from one system may be exported to another system or to a third party tool, which also changes the data. In order to keep track of the data and maximize the use and analysis of data, the Zero Deviation Life Cycle (ZDLC) framework proposes a series of tools to which can trace the data lineage, across several database technologies.|10.1109/CSE-EUC-DCABES.2016.252|||||2016|ZDLC : Layered Lineage Report across Technologies|Makoondlall, Y.K. and Khaddaj, S. and Makoond, B. and Kethan, K.|inproceedings|7982314||Aug|||||||||||||||||||||||||||744089640|42
KDD '19|Anchorage, AK, USA|schema mapping, data cleaning, data integration, entity linkage, data fusion|2|3193–3194|Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining|As data volume and variety have increased, so have the ties between machine learning and data integration become stronger. For machine learning to be effective, one must utilize data from the greatest possible variety of sources; and this is why data integration plays a key role. At the same time machine learning is driving automation in data integration, resulting in overall reduction of integration costs and improved accuracy. This tutorial focuses on three aspects of the synergistic relationship between data integration and machine learning: (1) we survey how state-of-the-art data integration solutions rely on machine learning-based approaches for accurate results and effective human-in-the-loop pipelines, (2) we review how end-to-end machine learning applications rely on data integration to identify accurate, clean, and relevant data for their analytics exercises, and (3) we discuss open research challenges and opportunities that span across data integration and machine learning.|10.1145/3292500.3332296|https://doi.org/10.1145/3292500.3332296|New York, NY, USA|Association for Computing Machinery|9781450362016|2019|Data Integration and Machine Learning: A Natural Synergy|Dong, Xin Luna and Rekatsinas, Theodoros|inproceedings|10.1145/3292500.3332296|||||||||||||||||||||||||||||745921270|42
HILDA'19|Amsterdam, Netherlands||7||Proceedings of the Workshop on Human-In-the-Loop Data Analytics|Data Cleaning refers to the process of detecting and fixing errors in the data. Human involvement is instrumental at several stages of this process such as providing rules or validating computed repairs. There is a plethora of data cleaning algorithms addressing a wide range of data errors (e.g., detecting duplicates, violations of integrity constraints, and missing values). Many of these algorithms involve a human in the loop, however, this latter is usually coupled to the underlying cleaning algorithms. In a real data cleaning pipeline, several data cleaning operations are performed using different tools. A high-level reasoning on these tools, when combined to repair the data, has the potential to unlock useful use cases to involve humans in the cleaning process. Additionally, we believe there is an opportunity to benefit from recent advances in active learning methods to minimize the effort humans have to spend to verify data items produced by tools or humans. There is currently no end-to-end data cleaning framework that systematically involves humans in the cleaning pipeline regardless of the underlying cleaning algorithms. In this paper, we present opportunities that this framework could offer, and highlight key challenges that need to be addressed to realize this vision. We present a design vision and discuss scenarios that motivate the need for this framework to judiciously assist humans in the cleaning process.|10.1145/3328519.3329133|https://doi.org/10.1145/3328519.3329133|New York, NY, USA|Association for Computing Machinery|9781450367912|2019|Towards an End-to-End Human-Centric Data Cleaning Framework|Rezig, El Kindi and Ouzzani, Mourad and Elmagarmid, Ahmed K. and Aref, Walid G. and Stonebraker, Michael|inproceedings|10.1145/3328519.3329133|1||||||||||||||||||||||||||||746574129|42
||Distributed databases;Metadata;Big Data;Standards;File systems;Data integrity;Big Data;data quality;unstructured Data Distributed data file system;and statistical model.||357-362|2019 12th International Conference on Developments in eSystems Engineering (DeSE)|Currently, as a result of the continuous increase of data, one of the key issues is the development of systems and applications to deal with storage, management and processing of big numbers of data. These data are found in unstructured ways. Data management with traditional approaches is inappropriate because of the large and complex data sizes. Hadoop is a suitable solution for the continuous increase in data sizes. The important characteristics of the Hadoop are distributed processing, high storage space, and easy administration. Hadoop is better known for distributed file systems. In this paper, we have proposed techniques and algorithms that deal with big data including data collecting, data preprocessing, algorithms for data cleaning, A Technique for Converting Unstructured Data to Structured Data using metadata, distributed data file system (fragmentation algorithm) and Quality assurance algorithms by using the model is the statistical model to evaluate the highest educational institutions. We concluded that Metadata accelerates query response required and facilitates query execution, metadata will be content for reports, fields and descriptions. Total time access for three complex queries in distributed processing it is 00: 03: 00 per second while in nondistributed processing it is at 00: 15: 77 per second, average is approximately five minutes per second. Quality assurance note values (T-test) is 0.239 and values (T-dis) is 1.96, as a result of dealing with scientific sets and humanities sets. In the comparison law, it can be deduced that if the t-test is smaller than the t-dis; so there is no difference between the mean of the scientific and humanities samples, the values of C.V for both scientific is (8.585) and humanities sets is (7.427), using the law of homogeneity know whether any sets are more homogeneous whenever the value of a small C.V was more homogeneous however the humanity set is more homogeneity.|10.1109/DeSE.2019.00072|||||2019|Data Quality Management for Big Data Applications|Khaleel, Majida Yaseen and Hamad, Murtadha M.|inproceedings|9073586||Oct||21611351|||||||||||||||||||||||||749198915|335367811
||Big Data, clinical database, GDPR, healthcare, orthopaedics, privacy||84-89||The enormous amount of data created daily within healthcare has become so complex, that it cannot be effectively handled by routine analytical methods. Such large data sets can be processed, looking for correlation not otherwise obvious in smaller patient samples. Further advances in terms of data processing as well as significant infrastructure and personnel investments are required to fully reap the benefits of Big Data, in terms of research and financial sense. However, despite its popularity and promise, Big Data in orthopaedics has attracted a number of criticisms, not only in terms of data input and processing, but particularly with regards to analysis of the output, which are explored within the article. Moreover, use of Big Data within healthcare carries the ethical question of privacy and consent.|https://doi.org/10.1016/j.mporth.2021.01.004|https://www.sciencedirect.com/science/article/pii/S187713272100004X||||2021|Introduction to Big Data in trauma and orthopaedics|Michal Koziara and Andrew Gaukroger and Caroline Hing and Will Eardley|article|KOZIARA202184|||Orthopaedics and Trauma|18771327|2|35|||||16800154701|0,194|Q4|27|68|208|1441|82|160|0,28|21,19|United Kingdom|Western Europe|2009-2020|Orthopedics and Sports Medicine (Q4)||||749703861|1981343898
||Big data analytics, Dynamic capabilities, Operational capabilities, Business value, Resource-based view||103169||A central question for information systems (IS) researchers and practitioners is if, and how, big data can help attain a competitive advantage. To address this question, this study draws on the resource-based view, dynamic capabilities view, and on recent literature on big data analytics, and examines the indirect relationship between a firm’s big data analytics capability (BDAC) and competitive performance. The study extends existing research by proposing that BDACs enable firms to generate insight that can help strengthen their dynamic capabilities, which, in turn, positively impact marketing and technological capabilities. To test our proposed research model, we used survey data from 202 chief information officers and IT managers working in Norwegian firms. By means of partial least squares structural equation modeling, results show that a strong BDAC can help firms build a competitive advantage. This effect is not direct but fully mediated by dynamic capabilities, which exerts a positive and significant effect on two types of operational capabilities: marketing and technological capabilities. The findings suggest that IS researchers should look beyond direct effects of big data investments and shift their attention on how a BDAC can be leveraged to enable and support organizational capabilities.|https://doi.org/10.1016/j.im.2019.05.004|https://www.sciencedirect.com/science/article/pii/S0378720618301022||||2020|Exploring the relationship between big data analytics capability and competitive performance: The mediating roles of dynamic and operational capabilities|Patrick Mikalef and John Krogstie and Ilias O. Pappas and Paul Pavlou|article|MIKALEF2020103169|||Information & Management|03787206|2|57|||||12303|2,147|Q1|162|130|248|11385|2482|246|8,94|87,58|Netherlands|Western Europe|1977-2020|Information Systems (Q1); Information Systems and Management (Q1); Management Information Systems (Q1)||||752531273|1945939487
IDEAS '18|Villa San Giovanni, Italy|Big Data, Data Quality Assessment, Veracity|8|37–44|Proceedings of the 22nd International Database Engineering &amp; Applications Symposium|The combination of data and technology is having a high impact on the way we live. The world is getting smarter thanks to the quantity of collected and analyzed data. However, it is necessary to consider that such amount of data is continuously increasing and it is necessary to deal with novel requirements related to variety, volume, velocity, and veracity issues. In this paper we focus on veracity that is related to the presence of uncertain or imprecise data: errors, missing or invalid data can compromise the usefulness of the collected values. In such a scenario, new methods and techniques able to evaluate the quality of the available data are needed. In fact, the literature provides many data quality assessment and improvement techniques, especially for structured data, but in the Big Data era new algorithms have to be designed. We aim to provide an overview of the issues and challenges related to Data Quality assessment in the Big Data scenario. We also propose a possible solution developed by considering a smart city case study and we describe the lessons learned in the design and implementation phases.|10.1145/3216122.3216124|https://doi.org/10.1145/3216122.3216124|New York, NY, USA|Association for Computing Machinery|9781450365277|2018|Quality Awareness for a Successful Big Data Exploitation|Cappiello, Cinzia and Sam\'{a}, Walter and Vitali, Monica|inproceedings|10.1145/3216122.3216124|||||||||||||||||||||||||||||755853542|42
FAccT '21|Virtual Event, Canada|machine learning, datasets, requirements engineering|16|560–575|Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency|Datasets that power machine learning are often used, shared, and reused with little visibility into the processes of deliberation that led to their creation. As artificial intelligence systems are increasingly used in high-stakes tasks, system development and deployment practices must be adapted to address the very real consequences of how model development data is constructed and used in practice. This includes greater transparency about data, and accountability for decisions made when developing it. In this paper, we introduce a rigorous framework for dataset development transparency that supports decision-making and accountability. The framework uses the cyclical, infrastructural and engineering nature of dataset development to draw on best practices from the software development lifecycle. Each stage of the data development lifecycle yields documents that facilitate improved communication and decision-making, as well as drawing attention to the value and necessity of careful data work. The proposed framework makes visible the often overlooked work and decisions that go into dataset creation, a critical step in closing the accountability gap in artificial intelligence and a critical/necessary resource aligned with recent work on auditing processes.|10.1145/3442188.3445918|https://doi.org/10.1145/3442188.3445918|New York, NY, USA|Association for Computing Machinery|9781450383097|2021|Towards Accountability for Machine Learning Datasets: Practices from Software Engineering and Infrastructure|Hutchinson, Ben and Smart, Andrew and Hanna, Alex and Denton, Emily and Greer, Christina and Kjartansson, Oddur and Barnes, Parker and Mitchell, Margaret|inproceedings|10.1145/3442188.3445918|||||||||||||||||||||||||||||756170280|42
WWW '19|San Francisco, CA, USA|replication, regulation, food safety, consumer reviews, Yelp|8|2543–2550|The World Wide Web Conference|Social media provides the government with novel methods to improve regulation. One leading case has been the use of Yelp reviews to target food safety inspections. While previous research on data from Seattle finds that Yelp reviews can predict unhygienic establishments, we provide a more cautionary perspective. First, we show that prior results are sensitive to what we call “Extreme Imbalanced Sampling”: extreme because the dataset was restricted from roughly 13k inspections to a sample of only 612 inspections with only extremely high or low inspection scores, and imbalanced by not accounting for class imbalance in the population. We show that extreme imbalanced sampling is responsible for claims about the power of Yelp information in the original classification setup. Second, a re-analysis that utilizes the full dataset of 13k inspections and models the full inspection score (regression instead of classification) shows that (a) Yelp information has lower predictive power than prior inspection history and (b) Yelp reviews do not significantly improve predictions, given existing information about restaurants and inspection history. Contrary to prior claims, Yelp reviews do not appear to aid regulatory targeting. Third, this case study highlights critical issues when using social media for predictive models in governance and corroborates recent calls for greater transparency and reproducibility in machine learning.|10.1145/3308558.3313683|https://doi.org/10.1145/3308558.3313683|New York, NY, USA|Association for Computing Machinery|9781450366748|2019|Is Yelp Actually Cleaning Up the Restaurant Industry? A Re-Analysis on the Relative Usefulness of Consumer Reviews|Altenburger, Kristen M. and Ho, Daniel E.|inproceedings|10.1145/3308558.3313683|||||||||||||||||||||||||||||756591777|42
MoMM '20|Chiang Mai, Thailand|business process, feature engineering, risk-based customer segmentation, banking processes|10|74–83|Proceedings of the 18th International Conference on Advances in Mobile Computing &amp; Multimedia|Business Processes, i.e., a set of coordinated tasks and activities to achieve a business goal, and their continuous improvements are key to the operation of any organization. In banking, business processes are increasingly dynamic as various technologies have made dynamic processes more prevalent. For example, customer segmentation, i.e., the process of grouping related customers based on common activities and behaviors, could be a data-driven and knowledge-intensive process. In this paper, we present an intelligent data-driven pipeline composed of a set of processing elements to move customers' data from one system to another, transforming the data into the contextualized data and knowledge along the way. The goal is to present a novel intelligent customer segmentation process which automates the feature engineering, i.e., the process of using (banking) domain knowledge to extract features from raw data via data mining techniques, in the banking domain. We adopt a typical scenario for analyzing customer transaction records, to highlight how the presented approach can significantly improve the quality of risk-based customer segmentation in the absence of feature engineering.|10.1145/3428690.3429172|https://doi.org/10.1145/3428690.3429172|New York, NY, USA|Association for Computing Machinery|9781450389242|2021|Towards Intelligent Feature Engineering for Risk-Based Customer Segmentation in Banking|Khadivizand, Sam and Beheshti, Amin and Sobhanmanesh, Fariborz and Sheng, Quan Z. and Istanbouli, Elias and Wood, Steven and Pezaro, Damon|inproceedings|10.1145/3428690.3429172|||||||||||||||||||||||||||||759030664|42
ICSCC 2019|Wuhan, China|Picture management, convolutional neural network, deep learning|5|71–75|Proceedings of the 2019 5th International Conference on Systems, Control and Communications|"\"With the advent of the era of big data, power supply security management systems will get a lot of picture data. In the face of massive image data, this paper studies the image management technology based on convolutional neural network. Aiming at the high repetition rate of self built image database samples and the problem that many sample classes contain uncorrelated images, two algorithms are proposed to improve the quality of the database: de duplication and de uncorrelation. By using the depth convolution neural network, the Embedding represented by the corresponding image is taken, and the distance between Embedding is calculated in the Euclidean space to achieve the purpose of de duplication and de uncorrelation. In this paper, \"\"time\"\" and \"\"accuracy\"\" are used to evaluate the performance of de duplication and de uncorrelation algorithms. The comparison examples of some sample classes before and after removing repetition and before and after removing uncorrelation are shown. The Recall-value of the database after removing duplicate and uncorrelated is tested based on the GoogLe Netplus-model respectively, which proves the effectiveness of the two filtering algorithms and overcomes the complexity of the traditional filtering process.\""|10.1145/3377458.3377464|https://doi.org/10.1145/3377458.3377464|New York, NY, USA|Association for Computing Machinery|9781450372640|2020|Picture Management of Power Supply Safety Management System Based on Deep Learning Technology|Yuwei, Sun and Jianbao, Zhu and Qingshan, Ma and Xinchun, Yu and Ye, Shi and Yu, Chen|inproceedings|10.1145/3377458.3377464|||||||||||||||||||||||||||||761763360|42
||multi-life-cycle remanufacturing, Sustainable products, Big data, CPS-Digital-twin(CPSDT), IoT-cloud, Reconfiguration||119299||Remanufacturing is deemed to be an effective method for recycling resources, achieving sustainable production. However, little importance of remanufacturing has been attached in PLM. Surely, there are many problems in implementation of the remanufacturing strategy, such as inability to effectively reduce uncertainty, lack of product multi-life-cycle remanufacturing process tracking management, lack of smart enabling technology application in the full lifecycle that focusing on multi-life-cycle remanufacturing. After analyzing the reasons, through integrating smart enabling technologies, a new PLM paradigm focusing on the multi-life-cycle remanufacturing process: Big Data driven Hierarchical Digital Twin Predictive Remanufacturing (BDHDTPREMfg) is proposed. And the definition of BDHDTPREMfg is proposed. A big data driven layered architecture and the hierarchical CPS-Digital-Twin(CPSDT) reconfiguration control mechanism of BDHDTPREMfg are respectively developed. Then, this paper presents an application scenario of BDHDTPREMfg to validate the feasibility and effectiveness. Based on the above application analysis, the benefits of penetrating BDHDTPREMfg into the entire lifecycle are demonstrated. The summary of this paper and future research work is discussed in the end.|https://doi.org/10.1016/j.jclepro.2019.119299|https://www.sciencedirect.com/science/article/pii/S0959652619341691||||2020|Big data driven Hierarchical Digital Twin Predictive Remanufacturing paradigm: Architecture, control mechanism, application scenario and benefits|Yankai Wang and Shilong Wang and Bo Yang and Lingzi Zhu and Feng Liu|article|WANG2020119299|||Journal of Cleaner Production|09596526||248|||||19167|1,937|Q1|200|5126|10603|304498|103295|10577|9,56|59,40|United Kingdom|Western Europe|1993-2021|Environmental Science (miscellaneous) (Q1); Industrial and Manufacturing Engineering (Q1); Renewable Energy, Sustainability and the Environment (Q1); Strategy and Management (Q1)|170,352|9.297|0.18299|762342175|1121054297
||Analytics, Delphi, Management challenges, Value creation, Ecosystem||626-639||The popularity of big data and business analytics has increased tremendously in the last decade and a key challenge for organizations is in understanding how to leverage them to create business value. However, while the literature acknowledges the importance of these topics little work has addressed them from the organization's point of view. This paper investigates the challenges faced by organizational managers seeking to become more data and information-driven in order to create value. Empirical research comprised a mixed methods approach using (1) a Delphi study with practitioners through various forums and (2) interviews with business analytics managers in three case organizations. The case studies reinforced the Delphi findings and highlighted several challenge focal areas: organizations need a clear data and analytics strategy, the right people to effect a data-driven cultural change, and to consider data and information ethics when using data for competitive advantage. Further, becoming data-driven is not merely a technical issue and demands that organizations firstly organize their business analytics departments to comprise business analysts, data scientists, and IT personnel, and secondly align that business analytics capability with their business strategy in order to tackle the analytics challenge in a systemic and joined-up way. As a result, this paper presents a business analytics ecosystem for organizations that contributes to the body of scholarly knowledge by identifying key business areas and functions to address to achieve this transformation.|https://doi.org/10.1016/j.ejor.2017.02.023|https://www.sciencedirect.com/science/article/pii/S0377221717301455||||2017|Management challenges in creating value from business analytics|Richard Vidgen and Sarah Shaw and David B. Grant|article|VIDGEN2017626|||European Journal of Operational Research|03772217|2|261|||||22489|2,161|Q1|260|657|2080|32022|13341|2068|6,02|48,74|Netherlands|Western Europe|1977-2021|Computer Science (miscellaneous) (Q1); Information Systems and Management (Q1); Management Science and Operations Research (Q1); Modeling and Simulation (Q1)|64,756|5.334|0.04764|766996954|1844564743
ICFET '18|Moscow, Russian Federation|artificial intelligence, body-in-white, automotive, knowledge-engineering, data mining, process automatization, ruled-based algorithms, optimization, production system design|5|192–196|Proceedings of the 4th International Conference on Frontiers of Educational Technologies|Design processes for production systems (PS) in the automotive body-in-white (BIW) sector tie up tremendous resources. Current challenges like the continuous increase of product variants and product complexity have direct impact on the required planning effort in production system design (PSD), which is currently increasing significantly. Analysis of these design processes have revealed a high potential for process automatization. In order to achieve this, suitable methods are required as well as a data basis of reasonable quality. Both methods and data basis are deeply investigated in this paper. The investigations' results create a solid basis for further research in the young field of automated BIW PSD.|10.1145/3233347.3233373|https://doi.org/10.1145/3233347.3233373|New York, NY, USA|Association for Computing Machinery|9781450364720|2018|Automated Body-in-White Production System Design: Data-Based Generation of Production System Configurations|Hagemann, Simon and Stark, Rainer|inproceedings|10.1145/3233347.3233373|||||||||||||||||||||||||||||771742479|42
||Big data, Data mining, Process mining, Proximity of events, Causality, Health and safety, Cause of incidents||345-354||The paper at hand motivates, proposes, demonstrates, and evaluates a novel systematic approach to discovering causal dependencies between events encoded in large arrays of data, called causality mining. The approach has emerged in the discussions with our project partner, an Australian public energy company. It was successfully evaluated in a case study with the project partner to extract valuable, and otherwise unknown, information on the causal dependencies between observations reported by the company’s employees as part of the organizational health and safety management practices and incidents that had occurred at the organization’s sites. The dependencies were derived based on the notion of proximity of the observations and incidents. The setup and results of the evaluation are reported in this paper. The new approach and the delivered insights aim at improving the overall health and safety culture of the project partner practices, as they can be applied to caution and, thus, prevent future incidents.|https://doi.org/10.1016/j.ssci.2019.04.045|https://www.sciencedirect.com/science/article/pii/S0925753518316230||||2019|A systematic approach for discovering causal dependencies between observations and incidents in the health and safety domain|Artem Polyvyanyy and Anastasiia Pika and Moe T. Wynn and Arthur H.M. {ter Hofstede}|article|POLYVYANYY2019345|||Safety Science|09257535||118|||||12332|1,178|Q1|111|451|1047|26276|5909|1020|5,49|58,26|Netherlands|Western Europe|1991-2020|Public Health, Environmental and Occupational Health (Q1); Safety Research (Q1); Safety, Risk, Reliability and Quality (Q1)|17,184|4.877|0.01488|771986127|1544923827
||Industry 4.0, Digital Twin, Geometry Assurance||3-10||Product realization processes are undergoing radical change considering the increasing digitalization of manufacturing fostered by cyber-physical production systems, the internet of things, big data, cloud computing, and the advancing use of digital twins. These trends are subsumed under the term “industry 4.0” describing the vision of a digitally connected manufacturing environment. The contribution gives an overview of future challenges and potentials for next generation geometry assurance and geometrical variations management in the context of industry 4.0. Particularly, the focus is set on potentials and risks of increasingly available manufacturing data and the use of digital twins in geometrical variations management.|https://doi.org/10.1016/j.procir.2018.04.078|https://www.sciencedirect.com/science/article/pii/S2212827118305948||||2018|Geometrical Variations Management 4.0: towards next Generation Geometry Assurance|Benjamin Schleich and Kristina Wärmefjord and Rikard Söderberg and Sandro Wartzack|article|SCHLEICH20183|||Procedia CIRP|22128271||75||The 15th CIRP Conference on Computer Aided Tolerancing, CIRP CAT 2018, 11-13 June 2018, Milan, Italy|||21100243809|0,683|-|65|1456|3191|30451|8225|3145|2,40|20,91|Netherlands|Western Europe|2012-2020|Control and Systems Engineering; Industrial and Manufacturing Engineering||||772012832|2127027836
SWIM '13|New York, New York|data quality, ETL, interoperability, linked data, linked open data, provenance|8||Proceedings of the Fifth Workshop on Semantic Web Information Management|The Web of Data has emerged as a means to expose, share, reuse, and connect information on the Web identified by URIs using RDF as a data model, following Linked Data Principles. However, the reuse of third party data can be compromised without proper data quality assessments. In this context, important questions emerge: how can one trust on published data and links? Which manipulation, modification and integration operations have been applied to the data before its publication? What is the nature of comparisons or transformations applied to data during the interlinking process? In this scenario, provenance becomes a fundamental element. In this paper, we describe an approach for generating and capturing Linked Open Provenance (LOP) to support data quality and trustworthiness assessments, which covers preparation and format transformation of traditional data sources, up to dataset publication and interlinking. The proposed architecture takes advantage of provenance agents, orchestrated by an ETL workflow approach, to collect provenance at any specified level and also link it with its corresponding data. We also describe a real use case scenario where the architecture was implemented to evaluate the proposal.|10.1145/2484712.2484715|https://doi.org/10.1145/2484712.2484715|New York, NY, USA|Association for Computing Machinery|9781450321945|2013|LOP: Capturing and Linking Open Provenance on LOD Cycle|de Mendon\c{c}a, Rogers Reiche and da Cruz, S\'{e}rgio Manuel Serra and De La Cerda, Jonas F. S. M. and Cavalcanti, Maria Cl\'{a}udia and Cordeiro, Kelli Faria and Campos, Maria Luiza M.|inproceedings|10.1145/2484712.2484715|3||||||||||||||||||||||||||||775197568|42
https://doi.org/10.1016/j.exis.2022.101089|https://www.sciencedirect.com/science/article/pii/S2214790X22000508||||2022|Framework components for data-centric dry laboratories in the minerals industry: A path to science-and-technology-led innovation|Yousef Ghorbani and Steven E. Zhang and Glen T. Nwaila and Julie E. Bourdeau|article|GHORBANI2022101089|||The Extractive Industries and Society|2214790X||10||||||||||||||||||||||||||||||775347404|42
ICEMC '22|Seoul, Republic of Korea|CPI, BP neural network, Big data, Quantitative economy|5|54–58|Proceedings of the 2022 International Conference on E-Business and Mobile Commerce|With the rapid development of China's economy, the degree of integration of economics and management is deepening. Based on the statistical analysis method of big data, the trend and law of economic development can be obtained. Big data statistical methods are widely used in the field of economics and improve work efficiency. Effective statistical analysis of data can not only reflect the operation of the product in time, but also reflect the market demand for the product. Therefore, this paper studies the role of big data statistical analysis methods in the field of economic management. This paper first classifies and sorts out the representative quantitative research methods and models in the era of big data, and then based on the BP neural network model and combines 36 indicator data to make multivariate forecasts for China's consumer price index (CPI). The research results show that the prediction results of the BP neural network are good.|10.1145/3543106.3543115|https://doi.org/10.1145/3543106.3543115|New York, NY, USA|Association for Computing Machinery|9781450397162|2022|A Study on the Economic Model of Volume in the Age of Big Data|Li, Kai|inproceedings|10.1145/3543106.3543115|||||||||||||||||||||||||||||780424054|42
IMMS 2019|Chengdu, China|Big data, performance management, human resources|6|12–17|Proceedings of the 2019 2nd International Conference on Information Management and Management Sciences|With the development of technology in the era of digital big data in the network and the promotion of network technology, big data is simultaneously integrated into different industry sectors to achieve Internet performance management, and enhance the new perspective of enterprise human resources performance management activities. Today's Internet, cloud computing, Internet of Things and other industrial technologies have undergone repeated changes, showing an unprecedented picture. At present, the subjective awareness of enterprise human resources performance management is too strong, lack of objective data understanding, and the theoretical framework of big data human resource management is not fully applied. This paper reconstructs the data system from four aspects: data source, collection, integration and analysis. Innovate the human resources performance management method from the system to provide more scientific and specific ideas for human resource performance management.|10.1145/3357292.3357302|https://doi.org/10.1145/3357292.3357302|New York, NY, USA|Association for Computing Machinery|9781450371445|2019|Big Data Informatization Applied to Optimization of Human Resource Performance Management|Kun-fa, Li and Jing-chun, Chen and Yan-xi, Wang|inproceedings|10.1145/3357292.3357302|||||||||||||||||||||||||||||780684566|42
WWW '19|San Francisco, USA|epidemics, call detail records, health vulnerability, social network analysis, migrations, neglected tropical diseases, Chagas disease|10|262–271|Companion Proceedings of The 2019 World Wide Web Conference|A map of potential prevalence of Chagas disease (ChD) with high spatial disaggregation is presented. It aims to detect areas outside the Gran Chaco ecoregion (hyperendemic for the ChD), characterized by high affinity with ChD and high health vulnerability.To quantify potential prevalence, we developed several indicators: an Affinity Index which quantifies the degree of linkage between endemic areas of ChD and the rest of the country. We also studied favorable habitability conditions for Triatoma infestans, looking for areas where the predominant materials of floors, roofs and internal ceilings favor the presence of the disease vector.We studied determinants of a more general nature that can be encompassed under the concept of Health Vulnerability Index. These determinants are associated with access to health providers and the socio-economic level of different segments of the population.Finally we constructed a Chagas Potential Prevalence Index (ChPPI) which combines the affinity index, the health vulnerability index, and the population density. We show and discuss the maps obtained. These maps are intended to assist public health specialists, decision makers of public health policies and public officials in the development of cost-effective strategies to improve access to diagnosis and treatment of ChD.|10.1145/3308560.3316485|https://doi.org/10.1145/3308560.3316485|New York, NY, USA|Association for Computing Machinery|9781450366755|2019|Detecting Areas of Potential High Prevalence of Chagas in Argentina|Vazquez Brust, Antonio and Olego, Tom\'{a}s and Rosati, Germ\'{a}n and Lang, Carolina and Bozzoli, Guillermo and Weinberg, Diego and Chuit, Roberto and Minnoni, Martin and Sarraute, Carlos|inproceedings|10.1145/3308560.3316485|||||||||||||||||||||||||||||781112768|42
||Microwave radiometry;Meteorology;Satellite broadcasting;Sensors;Soil moisture;Synthetic aperture radar;Soil measurements;Gap-filling;satellite retrieved soil moisture;the essential climate variable soil moisture;the soil moisture active passive soil moisture||133114-133127||The Essential Climate Variable (ECV) soil moisture (SM) datasets, originated from the European Space Agency, have revealed great potential for application in hydrology and agriculture. Hence, it is essential to continuously enhance the data quality and spatial completeness to satisfy the increasing scientific research requirements. In this study, we explore the potential possibility of Soil Moisture Active Passive (SMAP) datasets in filling the gaps of ECV SM. The comprehensive assessment results show that: (1) The data missing percent of gap-filled ECV decreases 20% on average, which can be one step closer to generate a seamlessly covered global land surface SM product with favorable quality. (2) Compared to the original ECV, the gap-filled ECV products express similar good response to the in-situ measurements, suggesting that the SMAP SM products could be taken to efficiently fill the gaps and consistently maintain favorable accuracy at the same time. (3) Compared to the in-situ measurements, the original ECV SM products demonstrate extremely high probability density peak percentages. Fortunately, this eminent high value could be effectively rectified through gap-filling progress using SMAP. Overall, this study conducts objective and detailed evaluation on the performance of applying SMAP to fill the gaps of ECV, and is expected to act as a valuable reference in ECV SM gap-filling method.|10.1109/ACCESS.2020.3009977|||||2020|Potential Applicability of SMAP in ECV Soil Moisture Gap-Filling: A Case Study in Europe|Liu, Yangxiaoyue and Yang, Yaping and Jing, Wenlong|article|9143114|||IEEE Access|21693536||8|||||21100374601|0,587|Q1|127|18036|24267|771081|116691|24200|4,48|42,75|United States|Northern America|2013-2020|Computer Science (miscellaneous) (Q1); Engineering (miscellaneous) (Q1); Materials Science (miscellaneous) (Q2)|105,968|3.367|0.15396|781599724|1905633267
ISSTA 2018|Amsterdam, Netherlands|Deep Learning, Empirical Study, TensorFlow Program Bug|12|129–140|Proceedings of the 27th ACM SIGSOFT International Symposium on Software Testing and Analysis|Deep learning applications become increasingly popular in important domains such as self-driving systems and facial identity systems. Defective deep learning applications may lead to catastrophic consequences. Although recent research efforts were made on testing and debugging deep learning applications, the characteristics of deep learning defects have never been studied. To fill this gap, we studied deep learning applications built on top of TensorFlow and collected program bugs related to TensorFlow from StackOverflow QA pages and Github projects. We extracted information from QA pages, commit messages, pull request messages, and issue discussions to examine the root causes and symptoms of these bugs. We also studied the strategies deployed by TensorFlow users for bug detection and localization. These findings help researchers and TensorFlow users to gain a better understanding of coding defects in TensorFlow programs and point out a new direction for future research.|10.1145/3213846.3213866|https://doi.org/10.1145/3213846.3213866|New York, NY, USA|Association for Computing Machinery|9781450356992|2018|An Empirical Study on TensorFlow Program Bugs|Zhang, Yuhao and Chen, Yifan and Cheung, Shing-Chi and Xiong, Yingfei and Zhang, Lu|inproceedings|10.1145/3213846.3213866|||||||||||||||||||||||||||||781812855|42
||Control loop performance assessment, Industrial alarm system, Process knowledge, Root cause diagnosis, Big data||952-962||Owing to wide applications of automatic control systems in the process industries, the impacts of controller performance on industrial processes are becoming increasingly significant. Consequently, controller maintenance is critical to guarantee routine operations of industrial processes. The workflow of controller maintenance generally involves the following steps: monitor operating controller performance and detect performance degradation, diagnose probable root causes of control system malfunctions, and take specific actions to resolve associated problems. In this article, a comprehensive overview of the mainstream of control loop monitoring and diagnosis is provided, and some existing problems are also analyzed and discussed. From the viewpoint of synthesizing abundant information in the context of big data, some prospective ideas and promising methods are outlined to potentially solve problems in industrial applications.|https://doi.org/10.1016/j.cjche.2016.05.039|https://www.sciencedirect.com/science/article/pii/S1004954116305134||||2016|A review of control loop monitoring and diagnosis: Prospects of controller maintenance in big data era|Xinqing Gao and Fan Yang and Chao Shang and Dexian Huang|article|GAO2016952|||Chinese Journal of Chemical Engineering|10049541|8|24|||||12888|0,595|Q2|54|355|874|15618|2755|871|3,16|43,99|China|Asiatic Region|1993-2020|Chemical Engineering (miscellaneous) (Q2); Chemistry (miscellaneous) (Q2); Environmental Engineering (Q2); Biochemistry (Q3)|6,469|3.171|0.00619|784561453|738319868
||Smart farm, Lossy compression, IoT, Signal processing, Data fidelity||304-313||As the volume of data collected by various IoT sensors used in smart farm applications increases, the storing and processing of big data for agricultural applications become a huge challenge. The insight of this paper is that lossy compression can unleash the power of compression to IoT because, as compared with its counterpart (a lossless one), it can significantly reduce the data volume when the spatiotemporal characteristics of IoT sensor data are properly exploited. However, lossy compression faces the challenge of compressing too much data thus losing data fidelity, which might affect the quality of the data and potential analytics outcomes. To understand the impact of lossy compression on IoT data management and analytics, we evaluated four classification algorithms with reconstructed agricultural sensor data based on various energy concentration. Specifically, we applied three transformation-based lossy compression mechanisms to five real-world weather datasets collected at different sampling granularities from IoT weather stations. Our experimental results indicate that there is a strong positive correlation between the concentrated energy of the transformed coefficients and the compression ratio as well as the data quality. While we observed a general trend where much higher compression ratios can be achieved at the cost of a decrease in quality, we also observed that the impact on the classification accuracy varies among the data sets and algorithms we evaluated. Lastly, we show that the sampling granularity also influences the data fidelity in terms of the prediction performance and compression ratio.|https://doi.org/10.1016/j.compag.2018.08.045|https://www.sciencedirect.com/science/article/pii/S0168169918303697||||2018|Evaluating fidelity of lossy compression on spatiotemporal data from an IoT enabled smart farm|Aekyeung Moon and Jaeyoung Kim and Jialing Zhang and Seung Woo Son|article|MOON2018304|||Computers and Electronics in Agriculture|01681699||154|||||30441|1,208|Q1|115|648|1300|28725|9479|1298|7,27|44,33|Netherlands|Western Europe|1985-2020|Agronomy and Crop Science (Q1); Animal Science and Zoology (Q1); Computer Science Applications (Q1); Forestry (Q1); Horticulture (Q1)|17,657|5.565|0.01646|785102996|1531073408
||Manufacturing process, Bigdata, Structured data, Unstructured data||6864-6866||Manufacturing is a technique that produce finished goods after taking supplies, raw materials and ingredients. Manufacturing process is frequently used to produce food, chemicals and other things those have very important place in human’s life. This manufacturing process involve data for analysis and management of the process, but in current scenario the data that are generated by the process is increasing day by day. This huge amount of data in known as big data. Big data is difficult to handle by traditional data management tools. Data that are generated by the manufacturing process collected by the logs records and may be as structured or unstructured. Generally analysis is performed by structured data. Unstructured data also provide good insights in the manufacturing process if analysed in proper manner. This paper involved an efficient approach of big data analysis for manufacturing process.|https://doi.org/10.1016/j.matpr.2021.05.146|https://www.sciencedirect.com/science/article/pii/S2214785321037226||||2021|An efficient approach for manufacturing process using Big data analytics|Devendra Kumar Mishra and Arvind Kumar Upadhyay and Sanjiv Sharma|article|MISHRA20216864|||Materials Today: Proceedings|22147853||47||International Conference on Advances in Design, Materials and Manufacturing|||21100370037|0,341|-|47|3996|10585|88521|14096|10409|1,24|22,15|United Kingdom|Western Europe|2005, 2014-2020|Materials Science (miscellaneous)||||785347234|400517803
CSAE '18|Hohhot, China|maximum dependency set (MDS), dynamic domain adjustment, inconsistent data, Conditional functional dependency (CFDs)|8||Proceedings of the 2nd International Conference on Computer Science and Application Engineering|For1 the incomplete detection of inconsistent data by conditional functional dependency(CFDs), this paper proposes a dependency lifting algorithm (DLA) based on maximum dependency set (MDS), which detects inconsistent data in data set by acquiring recessive conditional functional dependencies (RCFDs) in CFDs. Presenting the dynamic domain adjustment, setting forward and backward pointers of numerical change to improve the enumeration process in original algorithm, the applicability of the algorithm to the continuous attributes is raised too. Then, this paper provides the algorithm flow and pseudo code of dynamic domain adjustment and the DLA, analyses the convergence and time complexity of them. Finally, the validity of the DLA is verified by comparing the detection accuracy and time-cost.|10.1145/3207677.3277983|https://doi.org/10.1145/3207677.3277983|New York, NY, USA|Association for Computing Machinery|9781450365123|2018|Inconsistent Data Detection Based on Maximum Dependency Set|Li, Pei and Dai, Chaofan and Wang, Wenqian|inproceedings|10.1145/3207677.3277983|63||||||||||||||||||||||||||||785732148|42
GEOIT4W-2020|Al-Hoceima, Morocco|Big data, IoT, Cloud Computing, Information System|4||Proceedings of the 4th Edition of International Conference on Geo-IT and Water Resources 2020, Geo-IT and Water Resources 2020|The integration of new technologies such as big data, cloud computing, and the Internet of Things (IoT), constitute a new challenge for existing information systems of local administrations.In this work, we present the results of interviews conducted with the public administrations in the South-eastern region of Morocco, and as conclusion we expose two proposed models induced by this study.|10.1145/3399205.3399228|https://doi.org/10.1145/3399205.3399228|New York, NY, USA|Association for Computing Machinery|9781450375788|2020|Adoption of Big Data, Cloud Computing &amp; IoT in Morocco Perception of Public Administrations Collaborators|Moumen, Aniss|inproceedings|10.1145/3399205.3399228|21||||||||||||||||||||||||||||787424415|42
||Big Data Engineering, Big Data Analytics, Construction industry, Machine learning||500-521||The ability to process large amounts of data and to extract useful insights from data has revolutionised society. This phenomenon—dubbed as Big Data—has applications for a wide assortment of industries, including the construction industry. The construction industry already deals with large volumes of heterogeneous data; which is expected to increase exponentially as technologies such as sensor networks and the Internet of Things are commoditised. In this paper, we present a detailed survey of the literature, investigating the application of Big Data techniques in the construction industry. We reviewed related works published in the databases of American Association of Civil Engineers (ASCE), Institute of Electrical and Electronics Engineers (IEEE), Association of Computing Machinery (ACM), and Elsevier Science Direct Digital Library. While the application of data analytics in the construction industry is not new, the adoption of Big Data technologies in this industry remains at a nascent stage and lags the broad uptake of these technologies in other fields. To the best of our knowledge, there is currently no comprehensive survey of Big Data techniques in the context of the construction industry. This paper fills the void and presents a wide-ranging interdisciplinary review of literature of fields such as statistics, data mining and warehousing, machine learning, and Big Data Analytics in the context of the construction industry. We discuss the current state of adoption of Big Data in the construction industry and discuss the future potential of such technologies across the multiple domain-specific sub-areas of the construction industry. We also propose open issues and directions for future work along with potential pitfalls associated with Big Data adoption in the industry.|https://doi.org/10.1016/j.aei.2016.07.001|https://www.sciencedirect.com/science/article/pii/S1474034616301938||||2016|Big Data in the construction industry: A review of present status, opportunities, and future trends|Muhammad Bilal and Lukumon O. Oyedele and Junaid Qadir and Kamran Munir and Saheed O. Ajayi and Olugbenga O. Akinade and Hakeem A. Owolabi and Hafiz A. Alaka and Maruf Pasha|article|BILAL2016500|||Advanced Engineering Informatics|14740346|3|30|||||23640|1,107|Q1|81|146|295|8184|1973|289|6,41|56,05|United Kingdom|Western Europe|2002-2020|Artificial Intelligence (Q1); Information Systems (Q1)|4,432|5.603|0.00428|791830440|876312848
||Machine learning, Deep learning, Forecasting, Drought, Big data||105327||Machine learning is a dynamic field with wide-ranging applications, including drought modeling and forecasting. Drought is a complex, devastating natural disaster for which it is challenging to develop effective prediction models. Therefore, our review focuses on basic information about machine learning methods (MLMs) and their potential applications in developing efficient and effective drought forecasting models. We observed that MLMs have achieved significant advances in the robustness, effectiveness, and accuracy of the algorithms for drought modelling in recent years. The performance comparison of MLMs with other models provides a comprehensive conception of different model evaluation metrics. Further challenges of MLMs, such as inadequate training data sets, noise, outliers, and observation bias for spatial data sets, are explored. Finally, our review conveys in-depth understanding to researchers on machine learning applications in forecasting and modeling and provides drought mitigation strategy guidance for policymakers.|https://doi.org/10.1016/j.envsoft.2022.105327|https://www.sciencedirect.com/science/article/pii/S1364815222000330||||2022|A review of machine learning methods for drought hazard monitoring and forecasting: Current research trends, challenges, and future research directions|Foyez Ahmed Prodhan and Jiahua Zhang and Shaikh Shamim Hasan and Til Prasad {Pangali Sharma} and Hasiba Pervin Mohana|article|PRODHAN2022105327|||Environmental Modelling & Software|13648152||149|||||23295|1,828|Q1|136|223|717|14218|4373|711|5,47|63,76|Netherlands|Western Europe|1997-2020|Ecological Modeling (Q1); Environmental Engineering (Q1); Software (Q1)||||793156057|665084965
||Systematic review, Big data analytics, Health-medicine, Decision-making, Organizational and societal values, Preferred reporting items for systematic reviews and meta-analyses||112533||The emergence of powerful software has created conditions and approaches for large datasets to be collected and analyzed which has led to informed decision-making towards tackling health issues. The objective of this study is to systematically review 804 scholarly publications related to big data analytics in health in order to identify the organizational and social values along with associated challenges. Key principles of Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) methodology were followed for conducting systematic reviews. Following a research path, we present the values, challenges and future directions of the scientific area using indicative examples from relevant published articles. The study reveals that one of the main values created is the development of analytical techniques which provides personalized health services to users and supports human decision-making using automated algorithms, challenging the power issues in the doctor-patient relationship and creating new working conditions. A main challenge to data analytics is data management and security when processing large volumes of sensitive, personal health data. Future research is directed towards the development of systems that will standardize and secure the process of extracting private healthcare datasets from relevant organizations. Our systematic literature review aims to provide to governments and health policy-makers a better understanding of how the development of a data driven strategy can improve public health and the functioning of healthcare organizations but also how can create challenges that need to be addressed in the near future to avoid societal malfunctions.|https://doi.org/10.1016/j.socscimed.2019.112533|https://www.sciencedirect.com/science/article/pii/S0277953619305271||||2019|Values, challenges and future directions of big data analytics in healthcare: A systematic review|P. Galetsi and K. Katsaliaki and S. Kumar|article|GALETSI2019112533|||Social Science & Medicine|02779536||241|||||||||||||||||||||||794083641|693055954
||Big data, Data marketplace, Data pricing, Production management, Bi-level programming model||1-10||This paper presents a bi-level mathematical programming model for the data-pricing problem that considers both data quality and data versioning strategies. Data products and data-related services differ from information products or services in terms of quality assessment methods. For this problem, we consider two aspects of data quality: (1) its multidimensionality and (2) the interaction between the dimensions. We designed a multi-version data strategy and propose a data-pricing bi-level programming model based on the data quality to maximize the profit by the owner of the data platform and the utility to consumers. A genetic algorithm was used to solve the model. The numerical solutions for the data-pricing model indicate that the multi-version strategy achieves a better market segmentation and is more profitable and feasible when the multiple dimensions of data quality are considered. These results also provide managerial guidance on data provision and data pricing for platform owners.|https://doi.org/10.1016/j.cie.2017.08.008|https://www.sciencedirect.com/science/article/pii/S0360835217303509||||2017|Data pricing strategy based on data quality|Haifei Yu and Mengxiao Zhang|article|YU20171|||Computers & Industrial Engineering|03608352||112|||||18164|1,315|Q1|128|641|1567|31833|9597|1557|5,97|49,66|United Kingdom|Western Europe|1976-2020|Computer Science (miscellaneous) (Q1); Engineering (miscellaneous) (Q1)||||797873537|1798521593
||Big data, Smart data, Development, Automobile, SME||101602||Many SMEs still seem reluctant to accept the management of large datasets, which still appear to be too complex for them. However, our study reveals that the majority of small French car dealers are developing Big data and Smart data policies to improve the quality of their offers, the dynamism of their sales and their access to new opportunities. However, not every policy has the same effects on the development of their business. Whereas Big data improves all the components of SME development in a global, short-term and operational way, Smart data presents itself as a more targeted, prospective and strategic approach.|https://doi.org/10.1016/j.jengtecman.2020.101602|https://www.sciencedirect.com/science/article/pii/S0923474820300503||||2020|Data determinants of the activity of SMEs automobile dealers|David Salvetat and Jean-Sébastien Lacam|article|SALVETAT2020101602|||Journal of Engineering and Technology Management|09234748||58|||||||||||||||||||||||799001803|2132231255
||Big Data, Data governance, Self-service business intelligence||23-36||This case increases your understanding of data governance in an era of sophisticated analytics and Big Data where corporate data integrity and data quality may be at risk. KrauseMcMahon, a large certified public accounting and business consulting firm, faces a tradeoff of increasing control of the company’s data assets versus unleashing end user innovation due to the proliferation of self-service business intelligence tools. You are required to analyze the issues in the case from organizational, financial, and technical perspectives to propose alternatives the organization should consider and make specific recommendations on how the company should proceed. By completing this case, you will demonstrate cross-disciplinary abilities related to foundational business, accounting, and broad management competencies. By addressing such competencies, the case requires your use of accounting, MIS, and upper-level business skills.|https://doi.org/10.1016/j.jaccedu.2016.12.002|https://www.sciencedirect.com/science/article/pii/S0748575116301208||||2017|Data governance case at KrauseMcMahon LLP in an era of self-service BI and Big Data|Frederick J. Riggins and Bonnie K. Klamm|article|RIGGINS201723|||Journal of Accounting Education|07485751||38||Special Issue on Big Data|||29852|0,931|Q1|35|24|73|831|227|69|2,76|34,63|United Kingdom|Western Europe|1983-2020|Accounting (Q1); Education (Q1)||||799425663|1852131396
||Big data, Additive manufacturing, Sustainable manufacturing, Smart manufacturing, Optimization||102026||From the last decade, additive manufacturing (AM) has been evolving speedily and has revealed the great potential for energy-saving and cleaner environmental production due to a reduction in material and resource consumption and other tooling requirements. In this modern era, with the advancements in manufacturing technologies, academia and industry have been given more interest in smart manufacturing for taking benefits for making their production more sustainable and effective. In the present study, the significant techniques of smart manufacturing, sustainable manufacturing, and additive manufacturing are combined to make a unified term of sustainable and smart additive manufacturing (SSAM). The paper aims to develop framework by combining big data analytics, additive manufacturing, and sustainable smart manufacturing technologies which is beneficial to the additive manufacturing enterprises. So, a framework of big data-driven sustainable and smart additive manufacturing (BD-SSAM) is proposed which helped AM industry leaders to make better decisions for the beginning of life (BOL) stage of product life cycle. Finally, an application scenario of the additive manufacturing industry was presented to demonstrate the proposed framework. The proposed framework is implemented on the BOL stage of product lifecycle due to limitation of available resources and for fabrication of AlSi10Mg alloy components by using selective laser melting (SLM) technique of AM. The results indicate that energy consumption and quality of the product are adequately controlled which is helpful for smart sustainable manufacturing, emission reduction, and cleaner production.|https://doi.org/10.1016/j.rcim.2020.102026|https://www.sciencedirect.com/science/article/pii/S0736584520302374||||2021|A big data-driven framework for sustainable and smart additive manufacturing|Arfan Majeed and Yingfeng Zhang and Shan Ren and Jingxiang Lv and Tao Peng and Saad Waqar and Enhuai Yin|article|MAJEED2021102026|||Robotics and Computer-Integrated Manufacturing|07365845||67|||||18080|1,561|Q1|93|139|404|6448|2949|400|7,35|46,39|United Kingdom|Western Europe|1984-1994, 1996-2021|Computer Science Applications (Q1); Control and Systems Engineering (Q1); Industrial and Manufacturing Engineering (Q1); Mathematics (miscellaneous) (Q1); Software (Q1)|6,215|5.666|0.0057|803222092|1967447291
BDIOT '22|Chongqing, China|Data cleaning. Big data. Industrial data flow. Cleaning model. Multi-dimensional|5|77–81|Proceedings of the 2022 5th International Conference on Big Data and Internet of Things|With the development of the era of big data, the quality of data has become a growing concern of people. Improving data quality has become a very hot topic at present. In this paper, we propose a data cleaning method for industrial data flow based on multistage combinational optimization of rule set. According to the characteristics of the data, excellent cleaning algorithm is selected for the data. The data is evaluated and the cleaning rules are updated. In the first step, feature detection is carried out on the data, and high-quality data is selected as training samples to match the optimal data cleaning algorithm for them. In the second step, the model uses a random forest algorithm to learn the relationship between data features and data cleansing algorithms, and constructs multi-level filtering rules. In the third step, the data is cleansed and iterated to ensure that the rules are updated automatically. Finally, the model can automatically clean the data with a good cleaning effect. The results show that the method presented in this paper can achieve automatic cleaning effect on real industrial data sets, and the cleaning effect can reach 99% accuracy. This method effectively solves the problem of automatic data cleaning and can be used in the actual industrial data system.|10.1145/3561801.3561814|https://doi.org/10.1145/3561801.3561814|New York, NY, USA|Association for Computing Machinery|9781450390361|2022|A Data Cleaning Method for Industrial Data Flow Based on Multistage Combinational Optimization of Rule Set|Zhang, Cheng and Lin, Songbo and Zhang, Dang|inproceedings|10.1145/3561801.3561814|||||||||||||||||||||||||||||803673828|42
WWW '17 Companion|Perth, Australia|semantic web, linked data|2|1679–1680|Proceedings of the 26th International Conference on World Wide Web Companion|The 10th Linked Data on the Web workshop (LDOW2017) was held in Perth, Western Australia on April 3, 2017, co-located with the 26th International World Wide Web Conference (WWW2017). In its 10th anniversary edition, the LDOW workshop aims to stimulate discussion and further research into the challenges of publishing, consuming, and integrating structured data on the Web as well as mining knowledge from said data.|10.1145/3041021.3055510|https://doi.org/10.1145/3041021.3055510|Republic and Canton of Geneva, CHE|International World Wide Web Conferences Steering Committee|9781450349147|2017|LDOW2017: 10th Workshop on Linked Data on the Web|"\"Lehmann, Jens and Auer, S\"\"{o}ren and Capadisli, Sarven and Janowicz, Krzysztof and Bizer, Christian and Heath, Tom and Hogan, Aidan and Berners-Lee, Tim\""|inproceedings|10.1145/3041021.3055510|||||||||||||||||||||||||||||804514563|42
||Electrocardiography;Feature extraction;Biomedical monitoring;Sensors;Cloud computing;Computational modeling;Data analysis||1-9|2019 28th International Conference on Computer Communication and Networks (ICCCN)|The electrocardiogram (ECG) signal, as one of the most important vital signs, can provide indications of many heart-related diseases. Nonetheless, in the case of telehealth context, the automated analysis and accurate detection of ECG signals remain unsolved issues, because the poor data quality collected by the wearable devices and unprofessional users further increases the complexity of hand-crafted feature extraction, ultimately affecting the efficiency of feature extraction and the detection accuracy. To address this issue and improve the detection accuracy, in this paper we present a novel detection scheme with the raw ECG signal in wearable telehealth system. Our system benefits from the concept of big data, sensing and pervasive computing and the emerging deep learning technology. In particular, a Deep Heartbeat Classification (DHC) scheme is proposed to analyze the ECG signal for arrhythmia detection. Distinct from existing solutions, the detection model in DHC can be trained directly on the raw ECG signal without hand-crafted feature extraction. A cloud-based prototypical system is also designed and implemented with the functions of data acquisition, wireless transmission, back-end data management, and ECG detection. The experimental results demonstrate that our prototypical system is feasible and effective in real-world practice, and extensive experimentation based on the MIT-BIH database demonstrates that the proposed DHC scheme outperforms baseline schemes.|10.1109/ICCCN.2019.8847069|||||2019|Towards Deep Learning-Based Detection Scheme with Raw ECG Signal for Wearable Telehealth Systems|Zhao, Peng and Quan, Dekui and Yu, Wei and Yang, Xinyu and Fu, Xinwen|inproceedings|8847069||July||26379430|||||||||||||||||||||||||807310576|947309988
MEDES '18|Tokyo, Japan|semantics, multimedia ontologies, semantic bigdata|8|31–38|Proceedings of the 10th International Conference on Management of Digital EcoSystems|The use of formal representation is a key task in the era of big data. In the context of multimedia big data this issue is stressed due to the intrinsic complexity nature of this kind of data. Moreover, the relations among objects should be clearly expressed and formalized to give the right meaning of data correlation. For this reason the design of formal models to represent and manage information is a necessary task to implement intelligent information systems. In this latter some approaches related to the semantic web could be used to improve the data models which underlie the implementation of big data applications. Using these models the visualization of data and information become an intrinsic and strategic task for the analysis and exploration of multimedia BigData. In this paper we propose the use of a semantic approach to formalize the model structure of multimedia BigData. In addition, the recognition of multimodal features to represent concepts and linguistic properties to relate them are an effective way to bridge the gap between the target semantic classes and the available low-level multimedia descriptors. The proposed model has been implemented in a NoSQL graph database populated from different knowledge sources and a visualization of this very large knowledge base has been presented and discussed as a case study.|10.1145/3281375.3281386|https://doi.org/10.1145/3281375.3281386|New York, NY, USA|Association for Computing Machinery|9781450356220|2018|A Semantic-Based Model to Represent Multimedia Big Data|Rinaldi, Antonio M. and Russo, Cristiano|inproceedings|10.1145/3281375.3281386|||||||||||||||||||||||||||||807714635|42
||Structured Query Language;Computer languages;Data analysis;NoSQL databases;Data integrity;Scalability;Transforms;Big Data;Data Transformation;SABR Algorithm;NoSQL;ETL||399-403|2021 5th International Symposium on Multidisciplinary Studies and Innovative Technologies (ISMSIT)|Owing to their high availability and scalability, NoSQL databases are becoming more popular for Big data applications in web analytics and supporting large websites. Moreover, each NoSQL system has its API which does not support industry standards like SQL and JDBC, integrating these systems with other enterprise and reporting software takes more time. The main requirements of Big data and data analytics are transforming the data from SQL databases to NoSQL data structures to represent the data. In this work, we presented a method to transform the data from different types of SQL databases to the desired NoSQL database based on the R programming language. The proposed work is based on the R environment used to handle the data from the source system to the target databases and meet the data quality requirements in data transformation. The results confirmed that the development provided a good solution for the data transformation from SQL to NoSQL by taking into account the data quality requirements.|10.1109/ISMSIT52890.2021.9604548|||||2021|Data Transformation from SQL to NoSQL MongoDB Based on R Programming Language|Hasan, Forat Falih and Bakar, Muhamad Shahbani Abu|inproceedings|9604548||Oct|||||||||||||||||||||||||||809140501|42
||graph dependencies, Numeric errors, incremental validation|47|||Numeric inconsistencies are common in real-life knowledge bases and social networks. To catch such errors, we extend graph functional dependencies with linear arithmetic expressions and built-in comparison predicates, referred to as numeric graph dependencies (NGDs). We study fundamental problems for NGDs. We show that their satisfiability, implication, and validation problems are Σp2-complete, Πp2-complete, and coNP-complete, respectively. However, if we allow non-linear arithmetic expressions, even of degree at most 2, the satisfiability and implication problems become undecidable. In other words, NGDs strike a balance between expressivity and complexity. To make practical use of NGDs, we develop an incremental algorithm IncDect to detect errors in a graph G using NGDs in response to updates ΔG to G. We show that the incremental validation problem is coNP-complete. Nonetheless, algorithm IncDect is localizable, i.e., its cost is determined by small neighbors of nodes in ΔG instead of the entire G. Moreover, we parallelize IncDect such that it guarantees to reduce running time with the increase of processors. In addition, to strike a balance between the efficiency and accuracy, we also develop polynomial-time parallel algorithms for detection and incremental detection of top-ranked inconsistencies. Using real-life and synthetic graphs, we experimentally verify the scalability and efficiency of the algorithms.|10.1145/3385031|https://doi.org/10.1145/3385031|New York, NY, USA|Association for Computing Machinery||2020|Catching Numeric Inconsistencies in Graphs|Fan, Wenfei and Liu, Xueli and Lu, Ping and Tian, Chao|article|10.1145/3385031|9|jun|ACM Trans. Database Syst.|03625915|2|45|June 2020||||||||||||||||||||||809144427|1726010902
||Big data, Parallel and distributed computing, Cloud computing, Internet of things, Social media, Analytics||1231-1247||Big data is a potential research area receiving considerable attention from academia and IT communities. In the digital world, the amounts of data generated and stored have expanded within a short period of time. Consequently, this fast growing rate of data has created many challenges. In this paper, we use structuralism and functionalism paradigms to analyze the origins of big data applications and its current trends. This paper presents a comprehensive discussion on state-of-the-art big data technologies based on batch and stream data processing. Moreover, strengths and weaknesses of these technologies are analyzed. This study also discusses big data analytics techniques, processing methods, some reported case studies from different vendors, several open research challenges, and the opportunities brought about by big data. The similarities and differences of these techniques and technologies based on important parameters are also investigated. Emerging technologies are recommended as a solution for big data problems.|https://doi.org/10.1016/j.ijinfomgt.2016.07.009|https://www.sciencedirect.com/science/article/pii/S0268401216304753||||2016|Big data: From beginning to future|Ibrar Yaqoob and Ibrahim Abaker Targio Hashem and Abdullah Gani and Salimah Mokhtar and Ejaz Ahmed and Nor Badrul Anuar and Athanasios V. Vasilakos|article|YAQOOB20161231|||International Journal of Information Management|02684012|6, Part B|36|||||15631|2,770|Q1|114|224|382|20318|5853|373|16,16|90,71|United Kingdom|Western Europe|1970, 1986-2021|Artificial Intelligence (Q1); Computer Networks and Communications (Q1); Information Systems (Q1); Information Systems and Management (Q1); Library and Information Sciences (Q1); Management Information Systems (Q1); Marketing (Q1)|12,245|14.098|0.01167|809870612|747927863
WebSci '16|Hannover, Germany|legal issues, data archives, privacy, data protection, reproducibility, archiving, social media, methodology, data sharing|7|166–172|Proceedings of the 8th ACM Conference on Web Science|More and more researchers want to share research data collected from social media to allow for reproducibility and comparability of results. With this paper we want to encourage them to pursue this aim -- despite initial obstacles that they may face. Sharing can occur in various, more or less formal ways. We provide background information that allows researchers to make a decision about whether, how and where to share depending on their specific situation (data, platform, targeted user group, research topic etc.). Ethical, legal and methodological considerations are important for making this decision. Based on these three dimensions we develop a framework for social media sharing that can act as a first set of guidelines to help social media researchers make practical decisions for their own projects. In the long run, different stakeholders should join forces to enable better practices for data sharing for social media researchers. This paper is intended as our call to action for the broader research community to advance current practices of data sharing in the future.|10.1145/2908131.2908172|https://doi.org/10.1145/2908131.2908172|New York, NY, USA|Association for Computing Machinery|9781450342087|2016|A Manifesto for Data Sharing in Social Media Research|Weller, Katrin and Kinder-Kurlanda, Katharina E.|inproceedings|10.1145/2908131.2908172|||||||||||||||||||||||||||||810385199|42
KDD '13|Chicago, Illinois, USA|a/b testing, controlled experiments, randomized experiments|9|1168–1176|Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining|Web-facing companies, including Amazon, eBay, Etsy, Facebook, Google, Groupon, Intuit, LinkedIn, Microsoft, Netflix, Shop Direct, StumbleUpon, Yahoo, and Zynga use online controlled experiments to guide product development and accelerate innovation. At Microsoft's Bing, the use of controlled experiments has grown exponentially over time, with over 200 concurrent experiments now running on any given day. Running experiments at large scale requires addressing multiple challenges in three areas: cultural/organizational, engineering, and trustworthiness. On the cultural and organizational front, the larger organization needs to learn the reasons for running controlled experiments and the tradeoffs between controlled experiments and other methods of evaluating ideas. We discuss why negative experiments, which degrade the user experience short term, should be run, given the learning value and long-term benefits. On the engineering side, we architected a highly scalable system, able to handle data at massive scale: hundreds of concurrent experiments, each containing millions of users. Classical testing and debugging techniques no longer apply when there are billions of live variants of the site, so alerts are used to identify issues rather than relying on heavy up-front testing. On the trustworthiness front, we have a high occurrence of false positives that we address, and we alert experimenters to statistical interactions between experiments. The Bing Experimentation System is credited with having accelerated innovation and increased annual revenues by hundreds of millions of dollars, by allowing us to find and focus on key ideas evaluated through thousands of controlled experiments. A 1% improvement to revenue equals more than $10M annually in the US, yet many ideas impact key metrics by 1% and are not well estimated a-priori. The system has also identified many negative features that we avoided deploying, despite key stakeholders' early excitement, saving us similar large amounts.|10.1145/2487575.2488217|https://doi.org/10.1145/2487575.2488217|New York, NY, USA|Association for Computing Machinery|9781450321747|2013|Online Controlled Experiments at Large Scale|Kohavi, Ron and Deng, Alex and Frasca, Brian and Walker, Toby and Xu, Ya and Pohlmann, Nils|inproceedings|10.1145/2487575.2488217|||||||||||||||||||||||||||||810840270|42
|||12|80–91||Exploring the vision of a model-based framework that may enable broader engagement with and informed decision making about sustainability issues.|10.1145/3371906|https://doi.org/10.1145/3371906|New York, NY, USA|Association for Computing Machinery||2020|Toward Model-Driven Sustainability Evaluation|"\"Kienzle, J\"\"{o}rg and Mussbacher, Gunter and Combemale, Benoit and Bastin, Lucy and Bencomo, Nelly and Bruel, Jean-Michel and Becker, Christoph and Betz, Stefanie and Chitchyan, Ruzanna and Cheng, Betty H. C. and Klingert, Sonja and Paige, Richard F. and Penzenstadler, Birgit and Seyff, Norbert and Syriani, Eugene and Venters, Colin C.\""|article|10.1145/3371906||feb|Commun. ACM|00010782|3|63|March 2020||||||||||||||||||||||811417815|647144465
||Big data analytics, DEMATEL, Indian manufacturing supply chains, Interpretive structural modeling, MICMAC analysis||103368||Big Data Analytics (BDA) has attracted significant attention from both academicians and practitioners alike as it provides several ways to improve strategic, tactical and operational capabilities to eventually create a positive impact on the economic performance of organizations. In the present study, twelve significant barriers against BDA implementation are identified and assessed in the context of Indian manufacturing Supply Chains (SC). These barriers are modeled using an integrated two-stage approach, consisting of Interpretive Structural Modeling (ISM) in the first stage and Decision-Making Trial and Evaluation Laboratory (DEMATEL) in the second stage. The approach developed provides the interrelationships between the identified constructs and their intensities. Moreover, Fuzzy MICMAC technique is applied to analyze the high impact (i.e., high driving power) barriers. Results show that four constructs, namely lack of top management support, lack of financial support, lack of skills, and lack of techniques or procedures, are the most significant barriers. This study aids policy-makers in conceptualizing the mutual interaction of the barriers for developing policies and strategies to improve the penetration of BDA in manufacturing SC.|https://doi.org/10.1016/j.compind.2020.103368|https://www.sciencedirect.com/science/article/pii/S0166361520306023||||2021|Big data analytics: Implementation challenges in Indian manufacturing supply chains|Rakesh D. Raut and Vinay Surendra Yadav and Naoufel Cheikhrouhou and Vaibhav S. Narwane and Balkrishna E. Narkhede|article|RAUT2021103368|||Computers in Industry|01663615||125|||||19080|1,432|Q1|100|115|323|6926|3165|319|9,96|60,23|Netherlands|Western Europe|1979-2020|Computer Science (miscellaneous) (Q1); Engineering (miscellaneous) (Q1)|6,018|7.635|0.00584|815632847|605146181
GeoRich '17|Chicago, Illinois|cyber-poaching, animal, data sharing, security, hunting, crime, GPS, wildlife, blockchain, privacy, geospatial, species protection|6||Proceedings of the Fourth International ACM Workshop on Managing and Mining Enriched Geo-Spatial Data|Modern tracking technologies enables new ways for data mining in the wild. It allows wildlife monitoring centers to permanently collect geospatial data in a non-intrusive manner in real-time and at low cost. Unfortunately, wildlife data is exposed to crime and there is already a first reported case of 'cyber-poaching'. Based on stolen geospatial data, poachers can easily track and kill animals. As a result, cautious monitoring centers limited data access for research and public use. This means that the data cannot fully exploit its potential. We propose a novel solution to overcome the security problem. It allows monitoring centers to securely answer questions from the research community and to provide aggregated data to the public while the raw data is protected against unauthorized third parties. This data service can also be monetized. Several new applications are conceivable, such as a mobile app for preventing conflicts between human and wildlife or for engaging people in wildlife donation. Besides presenting the solution and potential use cases, the intention of present article is to start a discussion about the need for data protection and privacy in the animal world.|10.1145/3080546.3080550|https://doi.org/10.1145/3080546.3080550|New York, NY, USA|Association for Computing Machinery|9781450350471|2017|Secure Sharing of Geospatial Wildlife Data|Frey, Remo Manuel and Hardjono, Thomas and Smith, Christian and Erhardt, Keeley and Pentland, Alex 'Sandy'|inproceedings|10.1145/3080546.3080550|5||||||||||||||||||||||||||||815791960|42
BuildSys '21|Coimbra, Portugal|computer vision and language, COVID-19, urban sensing|5|302–306|Proceedings of the 8th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation|Sensing activities at the city scale using big data can enable applications to improve the quality of citizen life. While there are approaches to sense the urban heartbeat using sound, vision, radio frequency (RF), and other sensors, capturing changes at urban scale using such sensing modalities is challenging. Due to the enormous amount of data they produce and the associated annotation and processing requirement, such data can be of limited use. In this paper, we present a vision-to-language modeling approach to capture patterns and transitions that occur in New York City from March 2020 to August 2020. We use the model on ~1 million street images captured by dashcams over 6 months. We then use the captions to train a language model based on Latent Dirichlet Allocation [4] and compare models from different periods using probabilistic distance measures. We observe distribution shifts in the model that correlate well with social distancing policies and are corroborated by different data sources, such as mobility traces. This language-based sensing introduces a new sensing modality to capture dynamics in the city with lower storage requirements and privacy concerns.|10.1145/3486611.3491133|https://doi.org/10.1145/3486611.3491133|New York, NY, USA|Association for Computing Machinery|9781450391146|2021|Tracking Urban Heartbeat and Policy Compliance through Vision and Language-Based Sensing|Chowdhury, Tahiya and Ding, Qizhen and Mandel, Ilan and Ju, Wendy and Ortiz, Jorge|inproceedings|10.1145/3486611.3491133|||||||||||||||||||||||||||||816597934|42
||Electric vehicles, Battery energy storage, Cyber-physical battery management system, Big data, Deep learning||102064||The establishment of an accurate battery model is of great significance to improve the reliability of electric vehicles (EVs). However, the battery is a complex electrochemical system with hardly observable and simulatable internal chemical reactions, and it is challenging to estimate the state of battery accurately. This paper proposes a novel flexible and reliable battery management method based on the battery big data platform and Cyber-Physical System (CPS) technology. First of all, to integrate the battery big data resources in the cloud, a Cyber-physical battery management framework is defined and served as the basic data platform for battery modeling issues. And to improve the quality of the collected battery data in the database, this work reports the first attempt to develop an adaptive data cleaning method for the cloud battery management issue. Furthermore, a deep learning algorithm-based feature extraction model, as well as a feature-oriented battery modeling method, is developed to mitigate the under-fitting problem and improve the accuracy of the cloud-based battery model. The actual operation data of electric buses is used to validate the proposed methodologies. The maximum data restoring error can be limited within 1.3% in the experiments, which indicates that the proposed data cleaning method is able to improve the cloud battery data quality effectively. Meanwhile, the maximum SoC estimation error in the proposed feature-oriented battery modeling method is within 2.47%, which highlights the effectiveness of the proposed method.|https://doi.org/10.1016/j.est.2020.102064|https://www.sciencedirect.com/science/article/pii/S2352152X20318971||||2021|Big data driven vehicle battery management method: A novel cyber-physical system perspective|Shuangqi Li and Pengfei Zhao|article|LI2021102064|||Journal of Energy Storage|2352152X||33|||||21100400826|1,088|Q1|42|860|831|44642|5383|828|6,87|51,91|Netherlands|Western Europe|2015-2020|Electrical and Electronic Engineering (Q1); Energy Engineering and Power Technology (Q1); Renewable Energy, Sustainability and the Environment (Q2)|7,765|6.583|0.00926|817491816|593635875
CGI 2018|Bintan, Island, Indonesia|virtual worlds, social behavioral biometrics, online networks, machine learning, Human behavior recognition, decision-making|6|247–252|Proceedings of Computer Graphics International 2018|Human brain has an ability to perform a massive processing of auxiliary information such as visual cues, cognitive and social interactions, contextual and spatio-temporal data. Similarly to a human brain, social behavioral cues can aid the reliable decision-making of a biometric security system. Being an integral part of human behavior, social interactions are likely to possess unique behavioral patterns. This state-of-the-art review paper discusses an emerging person recognition approach based on the in-depth analysis of individuals' social behavior in order to enhance the performance of a traditional biometric system. The social behavioral information can be mined from their offline or online interactions, and can be identified as a set of Social Behavioral Biometric (SBB) features. These features could be used on their own or further combined with other behavioral and physiological patters, and classification can be enhanced by the use of machine learning approaches. An overview of open problems and challenges as well as applications of studying social behavior in various domains concludes this paper.|10.1145/3208159.3208187|https://doi.org/10.1145/3208159.3208187|New York, NY, USA|Association for Computing Machinery|9781450364010|2018|Machine Learning for Social Behavior Understanding|Gavrilova, Marina L.|inproceedings|10.1145/3208159.3208187|||||||||||||||||||||||||||||818828550|42
ACSW '18|Brisband, Queensland, Australia|SDN, cloud computing, cost optimization, resource optimization, big data, big data stream processing|11||Proceedings of the Australasian Computer Science Week Multiconference|Big data is the term which denotes data with features such as voluminous data, a variety of data and streaming data as well. Processing big data became essential for enterprises to garner general intelligence and avoid biased conclusions. Due to these features, big data processing is considered to be a challenging task. Big data Processing should rely on a robust network. Cloud computing offers a suitable environment for these processes. However, it is more challenging when we move big data to the cloud, as managing the cloud resources is the main issue. Software Defined Network (SDN) has a potential solution to this issue. In this paper, first, we survey the present state of the art of SDN, cloud computing, and Big data Stream processing (BDSP). Then, we discuss SDN in the context of Big Data Stream Processing in Cloud environment. Finally, critical issues and research opportunity are discussed.|10.1145/3167918.3167924|https://doi.org/10.1145/3167918.3167924|New York, NY, USA|Association for Computing Machinery|9781450354363|2018|A Survey on Big Data Stream Processing in SDN Supported Cloud Environment|Al-Mansoori, Ahmed and Yu, Shui and Xiang, Yong and Sood, Keshav|inproceedings|10.1145/3167918.3167924|12||||||||||||||||||||||||||||818917052|42
||Data quality, Statistical process control, Knowledge-based view, Organizational information processing view, Systems theory||72-80||Today׳s supply chain professionals are inundated with data, motivating new ways of thinking about how data are produced, organized, and analyzed. This has provided an impetus for organizations to adopt and perfect data analytic functions (e.g. data science, predictive analytics, and big data) in order to enhance supply chain processes and, ultimately, performance. However, management decisions informed by the use of these data analytic methods are only as good as the data on which they are based. In this paper, we introduce the data quality problem in the context of supply chain management (SCM) and propose methods for monitoring and controlling data quality. In addition to advocating for the importance of addressing data quality in supply chain research and practice, we also highlight interdisciplinary research topics based on complementary theory.|https://doi.org/10.1016/j.ijpe.2014.04.018|https://www.sciencedirect.com/science/article/pii/S0925527314001339||||2014|Data quality for data science, predictive analytics, and big data in supply chain management: An introduction to the problem and suggestions for research and applications|Benjamin T. Hazen and Christopher A. Boone and Jeremy D. Ezell and L. Allison Jones-Farmer|article|HAZEN201472|||International Journal of Production Economics|09255273||154|||||19165|2,406|Q1|185|327|891|21712|8124|881|8,31|66,40|Netherlands|Western Europe|1991-2021|Business, Management and Accounting (miscellaneous) (Q1); Economics and Econometrics (Q1); Industrial and Manufacturing Engineering (Q1); Management Science and Operations Research (Q1)|32,606|7.885|0.0228|819669810|850534974
||Privacy, Standardization, Clinical data warehousing, Data modeling, Big data analytics, Decision supports||100238||A centralized clinical data repository is essential for inspecting patients’ medical history, disease analysis, population-wide disease research, treatment decision support, and improving existing healthcare policies and services. Bangladesh, a rapidly developing country, poses several unusual challenges for developing such a centralized clinical data repository as the existing Electronic Health Records (EHR) are stored in unconnected, heterogeneous sources with no unique patient identifier and consistency. Data integration with secure record linkage, privacy preservation, quality control, and data standardization are the main challenges for developing a consistent and interoperable centralized clinical data repository. Based on the findings from our previous researches, we have designed an anonymous National Clinical Data Warehouse (NCDW) framework to reinforce research and analysis. The architecture of NCDW is divided into five stages to overcome the challenges: (1) Wrapper-based anonymous data acquisition; (2) Data loading and staging; (3) Transformation, standardization, and uploading to the data warehouse; (4) Management and monitoring; (5) Data Mart design, OLAP server, data mining, and applications. A prototype of NCDW has been developed with a complete pipeline from data collection to analytics by integrating three data sources. The proposed NCDW model facilitates regional and national decision support, intelligent disease analysis, knowledge discovery, and data-driven research. We have inspected the analytical efficacy of the framework by qualitative evaluation of the national decision support from two derived disease data marts. The experimental result based on the analysis is satisfactory to extend the NCDW on a large scale.|https://doi.org/10.1016/j.smhl.2021.100238|https://www.sciencedirect.com/science/article/pii/S2352648321000544||||2022|A privacy-preserving National Clinical Data Warehouse: Architecture and analysis|Md Raihan Mia and Abu Sayed Md Latiful Hoque and Shahidul Islam Khan and Sheikh Iqbal Ahamed|article|MIA2022100238|||Smart Health|23526483||23|||||21100894507|0,410|Q2|9|28|74|1456|210|69|2,71|52,00|Netherlands|Western Europe|2018-2020|Information Systems (Q2); Computer Science Applications (Q3); Health Informatics (Q3); Health Information Management (Q3); Medicine (miscellaneous) (Q3)||||821087239|1897124289
||||102477||The recent development and diffusion of next-generation digital technologies (NGDTs) such as artificial intelligence, the Internet of Things, big data, 3D printing, and so on are expected to have an immense impact on businesses, innovation, and society. While we know from extant research that a firm's R&D investment, intangible assets, and productivity are factors that influence technology use more generally, to date there is little known about the factors that determine how these emerging tools are used, and by who. Using Probit and OLS modeling on a survey of 12,579 South Korean firms in 2017, we conduct one of the first comprehensive examinations highlighting various firm characteristics that drive NGDT implementation. While much of the literature assesses the use of individual technologies, our research attempts to unveil the extent to which firms implement NGDTs in bundles. Our investigation shows that more than half of the firms that use NGDTs deployed multiple technologies simultaneously. One of the insightful complementarities identified in this research exists amongst technologies that generate, facilitate and demand large sums of data, including big data, IoT, cloud computing and AI. Such technologies also appear important for innovative tools such as 3D printing and robotics.|https://doi.org/10.1016/j.technovation.2022.102477|https://www.sciencedirect.com/science/article/pii/S0166497222000244||||2022|What's driving the diffusion of next-generation digital technologies?|Jaehan Cho and Timothy DeStefano and Hanhin Kim and Inchul Kim and Jin Hyun Paik|article|CHO2022102477|||Technovation|01664972|||||||14726|2,300|Q1|130|53|102|5179|837|93|6,66|97,72|United Kingdom|Western Europe|1981-2020|Engineering (miscellaneous) (Q1); Management of Technology and Innovation (Q1)|8,486|6.606|0.00351|821489243|1912566877
ICISE 2021|Shanghai, China|Fault data detection, Pre-Processing, Big Data, Map-reduce, Classification using SVM|6|1–6|2021 the 6th International Conference on Information Systems Engineering|In many health care domains, big data has arrived. How to manage and use big data better has become the focus of all walks of life. Many data sources provide the repeated fault data—the repeated fault data forming the delay of processing time and storage capacity. Big data includes properties like volume, velocity, variety, variability, value, complexity, and performance put forward more challenges. Most healthcare domains face the problem of testing for structured and unstructured data validation in big data. It provides low-quality data and delays in response. In testing process is delay and not provide the correct response. In Proposed, pre-testing and post-testing are used for big data testing. In pre-testing, classify fault data from different data sources. After Classification to group big data using SVM algorithms such as Text, Image, Audio, and Video file. In post-testing, to implement the pre-processing, remove the zero file size, unrelated file extension, and de-duplication after pre-processing to implement the Map-reduce algorithm to find out the big data efficiently. This process reduces the pre-processing time, reduces the server energy, and increases the processing time. To remove the fault data before pre-processing means to increase the processing time and data storage.|10.1145/3503928.3503929|https://doi.org/10.1145/3503928.3503929|New York, NY, USA|Association for Computing Machinery|9781450385220|2022|Big Data: Finding Frequencies of Faulty Multimedia Data|Barzan Abdalla, Hemn and Mustafa, Nasser and Ihnaini, Baha|inproceedings|10.1145/3503928.3503929|||||||||||||||||||||||||||||821972351|42
||Analytics, Classification, Clustering, Decision, Diabetic, Healthcare||179-199|Big Data Analytics for Healthcare|Big data analytics and machine learning are the promising fields of the present time and playing important role in the healthcare sector. Big data analytical techniques help in analyzing a huge volume of data which may be in structured, semistructured, or unstructured form, and extract meaningful information for effective decision-making. Machine learning techniques help in performing predictions with the trained models on the input datasets and perform classification, clustering of data. In this chapter, the author has performed data analysis on diabetic patients dataset categorical in nature using big data analytical techniques, i.e., MapReduce, Apache Pig, Apache Hive, Apache Spark, and their architectures are discussed. Apart from big data analytics, machine learning techniques, i.e., K-Nearest Neighbor, Decision Trees, Bagged Trees, are implemented on the female diabetic patient dataset which is categorical and numerical for performing predictions based on the attributes like Age, Body Mass Index, Glucose, Blood Pressure, etc. The sensitivity achieved by the decision tree is 61.2% which is higher compared to KNN and bagged tree, whereas the Specificity achieved by the KNN is 89.2% which is higher than the other two algorithms.|https://doi.org/10.1016/B978-0-323-91907-4.00018-2|https://www.sciencedirect.com/science/article/pii/B9780323919074000182||Academic Press|978-0-323-91907-4|2022|Chapter 15 - Predictions on diabetic patient datasets using big data analytics and machine learning techniques|Pratiyush Guleria|incollection|GULERIA2022179||||||||||Pantea Keikhosrokiani|||||||||||||||||||823263923|42
SIGMOD '15|Melbourne, Victoria, Australia|data pipeline, big data, data visualization, data analysis|16|1925–1940|Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data|The field of data analysis seeks to extract value from data for either business or scientific benefit. This field has seen a renewed interest with the advent of big data technologies and a new organizational role called data scientist. Even with the new found focus, the task of analyzing large amounts of data is still challenging and time-consuming.The essence of data analysis involves setting up data pipe-lines which consists of several operations that are chained together - starting from data collection, data quality checks, data integration, data analysis and data visualization (including the setting up of interaction paths in that visualization).In our opinion, the challenges stem from from the technology diversity at each stage of the data pipeline as well as the lack of process around the analysis.In this paper we present a platform that aims to significantly reduce the time it takes to build data pipelines. The platform attempts to achieve this in following ways. Allow the user to describe the entire data pipeline with a single language and idioms - all the way from data ingestion to insight expression (via visualization and end-user interaction).Provide a rich library of parts that allow users to quickly assemble a data analysis pipeline in the language.Allow for a collaboration model that allows multiple users to work together on a data analysis pipeline as well as leverage and extend prior work with minimal effort.We studied the efficacy of the platform for a data hackathon competition conducted in our organization. The hackathon provided us with a way to study the impact of the approach. Rich data pipelines which traditionally took weeks to build were constructed and deployed in hours. Consequently, we believe that the complexity of designing and running the data analysis pipeline can be significantly reduced; leading to a marked improvement in the productivity of data analysts/data scientists.|10.1145/2723372.2742800|https://doi.org/10.1145/2723372.2742800|New York, NY, USA|Association for Computing Machinery|9781450327589|2015|ShareInsights: An Unified Approach to Full-Stack Data Processing|Deshpande, Mukund and Ray, Dhruva and Dixit, Sameer and Agasti, Avadhoot|inproceedings|10.1145/2723372.2742800|||||||||||||||||||||||||||||824295227|42
||Genetic algorithms;Machine learning;Oils;Optimization;Search problems;Big Data;Quality assessment;Operating envelope;Genetic algorithm;Penalty approach;Generalization||1506-1516|2019 IEEE International Conference on Big Data (Big Data)|Operating envelope is an important concept in industrial operations. Accurate identification for operating envelope can be extremely beneficial to stakeholders as it provides a set of operational parameters that optimizes some key performance indicators (KPI) such as product quality, operational safety, equipment efficiency, environmental impact, etc. Given the importance, data-driven approaches for computing the operating envelope are gaining popularity. These approaches typically use classifiers such as support vector machines, to set the operating envelope by learning the boundary in the operational parameter spaces between the manually assigned `large KPI' and `small KPI' groups. One challenge to these approaches is that the assignment to these groups is often ad-hoc and hence arbitrary. However, a bigger challenge with these approaches is that they don't take into account two key features that are needed to operationalize operating envelopes: (i) interpretability of the envelope by the operator and (ii) implementability of the envelope from a practical standpoint. In this work, we propose a new definition for operating envelope which directly targets the expected magnitude of KPI (i.e., no need to arbitrarily bin the data instances into groups) and accounts for the interpretability and the implementability. We then propose a regularized `GA +penalty' algorithm that outputs an envelope where the user can tradeoff between bias and variance. The validity of our proposed algorithm is demonstrated by two sets of simulation studies and an application to a real-world challenge in the mining processes of a flotation plant.|10.1109/BigData47090.2019.9005484|||||2019|Regularized Operating Envelope with Interpretability and Implementability Constraints|Wang, Qiyao and Wang, Haiyan and Gupta, Chetan and Serita, Susumu|inproceedings|9005484||Dec|||||||||||||||||||||||||||826697918|42
||Buildings Performance Database, Building performance, Big data, Building data collection, Data-driven decision support||85-93||Building energy data has been used for decades to understand energy flows in buildings and plan for future energy demand. Recent market, technology and policy drivers have resulted in widespread data collection by stakeholders across the buildings industry. Consolidation of independently collected and maintained datasets presents a cost-effective opportunity to build a database of unprecedented size. Applications of the data include peer group analysis to evaluate building performance, and data-driven algorithms that use empirical data to estimate energy savings associated with building retrofits. This paper discusses technical considerations in compiling such a database using the DOE Buildings Performance Database (BPD) as a case study. We gathered data on over 750,000 residential and commercial buildings. We describe the process and challenges of mapping and cleansing data from disparate sources. We analyze the distributions of buildings in the BPD relative to the Commercial Building Energy Consumption Survey (CBECS) and Residential Energy Consumption Survey (RECS), evaluating peer groups of buildings that are well or poorly represented, and discussing how differences in the distributions of the three datasets impact use-cases of the data. Finally, we discuss the usefulness and limitations of the current dataset and the outlook for increasing its size and applications.|https://doi.org/10.1016/j.apenergy.2014.11.042|https://www.sciencedirect.com/science/article/pii/S0306261914012112||||2015|Big-data for building energy performance: Lessons from assembling a very large national database of building energy use|Paul A. Mathew and Laurel N. Dunn and Michael D. Sohn and Andrea Mercado and Claudine Custudio and Travis Walter|article|MATHEW201585|||Applied Energy|03062619||140|||||28801|3,035|Q1|212|1729|5329|100144|56804|5304|10,59|57,92|United Kingdom|Western Europe|1975-2020|Building and Construction (Q1); Energy (miscellaneous) (Q1); Management, Monitoring, Policy and Law (Q1); Mechanical Engineering (Q1)|122,712|9.746|0.15329|828630342|892883729
MK Series on Business Intelligence||metadata, master data, machine learning, algorithms, semantic libraries, data governance||219-240|Data Warehousing in the Age of Big Data|The goal of this chapter is to provide readers with data governance in the age of Big Data. We will discuss the goals of what managing data means with respect to the next generation of data warehousing and the role of metadata and master data in integrating Big Data into the data warehouse.|https://doi.org/10.1016/B978-0-12-405891-0.00011-8|https://www.sciencedirect.com/science/article/pii/B9780124058910000118|Boston|Morgan Kaufmann|978-0-12-405891-0|2013|Chapter 11 - Data-Driven Architecture for Big Data|Krish Krishnan|incollection|KRISHNAN2013219||||||||||Krish Krishnan|||||||||||||||||||828730064|42
|||2||||10.1145/2435221.2435222|https://doi.org/10.1145/2435221.2435222|New York, NY, USA|Association for Computing Machinery||2013|SPECIAL ISSUE ON ENTITY RESOLUTION Overview: The Criticality of Entity Resolution in Data and Information Quality|Talburt, John R.|article|10.1145/2435221.2435222|6|mar|J. Data and Information Quality|19361955|2|4|March 2013||||||||||||||||||||||828756327|833754770
||Causes of death data, Data linkages, Big data||37-40||The study of cause-specific mortality data is one of the main sources of information for public health monitoring. In most industrialized countries, when a death occurs, it is a legal requirement that a medical certificate based on the international form recommended by World Health Organization's (WHO) is filled in by a physician. The physician reports the causes of death that directly led or contributed to the death on the death certificate. The death certificate is then forwarded to a coding office, where each cause is coded, and one underlying cause is defined, using the rules of the International Classification of Diseases and Related Health Problems, now in its 10th Revision (ICD-10). Recently, a growing number of countries have adopted, or have decided to adopt, the coding software Iris, developed and maintained by an international consortium1. This whole standardized production process results in a high and constantly increasing international comparability of cause-specific mortality data. While these data could be used for international comparisons and benchmarking of global burden of diseases, quality of care and prevention policies, there are also many other ways and methods to explore their richness, especially when they are linked with other data sources. Some of these methods are potentially referring to the so-called “big data” field. These methods could be applied both to the production of the data, to the statistical processing of the data, and even more to process these data linked to other databases. In the present note, we depict the main domains in which this new field of methods could be applied. We focus specifically on the context of France, a 65 million inhabitants country with a centralized health data system. Finally we will insist on the importance of data quality, and the specific problematics related to death certification in the forensic medicine domain.|https://doi.org/10.1016/j.jflm.2016.12.004|https://www.sciencedirect.com/science/article/pii/S1752928X16301652||||2018|Causes of deaths data, linkages and big data perspectives|Grégoire Rey and Karim Bounebache and Claire Rondet|article|REY201837|||Journal of Forensic and Legal Medicine|1752928X||57||Thematic section: Big dataGuest editor: Thomas LefèvreThematic section: Health issues in police custodyGuest editors: Patrick Chariot and Steffen Heide|||5100154502|0,569|Q1|47|134|428|3921|668|407|1,57|29,26|United Kingdom|Western Europe|2007-2020|Law (Q1); Medicine (miscellaneous) (Q2); Pathology and Forensic Medicine (Q2)|2,328|1.614|0.00305|829330684|1636581161
||Time series analysis;Temperature sensors;Mobile handsets;Temperature distribution;Databases;Data models;Machine Learning;Crowd Sensing;Big Data Analysis;Abnormal Data Detection Management;Clustering Method||185-188|2020 International Conference on Computer Information and Big Data Applications (CIBDA)|Recently, research on crowd sensing data quality management is a new subject area developed based on wireless sensor network related concepts. Crowd sensing data has also experienced many links in the process of network propagation, so it is inevitable that there are abnormal data in the database. Therefore, how to filter these unreliable data to get more real data is particularly important. This paper takes somatosensory temperature as an example, solves the problem of calculating the similarity of unequal long-term sequences by using DTW technology, and then clusters and compares the data in the database to find out the abnormal data. Thereby, the accuracy of the somatosensory temperature database is improved, and relevant users can obtain more accurate information. The experimental results show that when the minimum DTW value exceeds the threshold t = 0.7, the more the number of simulation sequences, the more stable the accuracy rate, and the faster the growth rate of the running time.|10.1109/CIBDA50819.2020.00049|||||2020|Quality Management of Crowd Sensing Data Based on Machine Learning|Bai, Zhongxian and Zhuo, Rongqing|inproceedings|9148148||April|||||||||||||||||||||||||||832577370|42
||Big Data, Data Mesh, Data Architectures, Data Lake||263-271||Inherent to the growing use of the most varied forms of software (e.g., social applications), there is the creation and storage of data that, due to its characteristics (volume, variety, and velocity), make the concept of Big Data emerge. Big Data Warehouses and Data Lakes are concepts already well established and implemented by several organizations, to serve their decision-making needs. After analyzing the various problems demonstrated by those monolithic architectures, it is possible to conclude about the need for a paradigm shift that will make organizations truly data-oriented. In this new paradigm, data is seen as the main concern of the organization, and the pipelining tools and the Data Lake itself are seen as a secondary concern. Thus, the Data Mesh consists in the implementation of an architecture where data is intentionally distributed among several Mesh nodes, in such a way that there is no chaos or data silos, since there are centralized governance strategies and the guarantee that the core principles are shared throughout the Mesh nodes. This paper presents the motivation for the appearance of the Data Mesh paradigm, its features, and approaches for its implementation.|https://doi.org/10.1016/j.procs.2021.12.013|https://www.sciencedirect.com/science/article/pii/S1877050921022365||||2022|Data Mesh: Concepts and Principles of a Paradigm Shift in Data Architectures|Inês Araújo Machado and Carlos Costa and Maribel Yasmina Santos|article|MACHADO2022263|||Procedia Computer Science|18770509||196||International Conference on ENTERprise Information Systems / ProjMAN - International Conference on Project MANagement / HCist - International Conference on Health and Social Care Information Systems and Technologies 2021|||19700182801|0,334|-|76|1907|6483|38511|13722|6359|2,09|20,19|Netherlands|Western Europe|2010-2020|Computer Science (miscellaneous)||||833465465|2108686752
||validation, Dependencies, dependency discovery, implication, graphs, certain fixes, satisfiability, error detection|12|||What are graph dependencies? What do we need them for? What new challenges do they introduce? This article tackles these questions. It aims to incite curiosity and interest in this emerging area of research.|10.1145/3310230|https://doi.org/10.1145/3310230|New York, NY, USA|Association for Computing Machinery||2019|Dependencies for Graphs: Challenges and Opportunities|Fan, Wenfei|article|10.1145/3310230|5|feb|J. Data and Information Quality|19361955|2|11|June 2019||||||||||||||||||||||834310773|833754770
ICCBDC '22|Birmingham, United Kingdom|Unstructured data, Applications, Analytics, Big Data, Management, Structured data, Accounting|5|37–41|Proceedings of the 2022 6th International Conference on Cloud and Big Data Computing|Generating value from big data is a task that requires models’ preparation and use of advanced technologies but which, above all, is based on the ability to extract, manage and analyse data. These processes’ effectiveness depends on the data's quality and their structured or unstructured nature. We are witnessing a growing number of applications based on unstructured data mining in the accounting and management fields. This research aims to demonstrating that despite the traditional association between accounting and quantitative analyses (expected to be based mainly on structured financial data). The findings show that several useful applications now rely on unstructured data in this field. A basic analysis of the cybersecurity risks is also presented, along with mitigating strategies to allow companies to comply with current regulations such as the GDPR. The result might appear surprising from the business perspective, but it is not from a data science perspective. In conclusion the growing number of unsctructured data business applications should orientate a better understanding of their potential and target better training of finance specialist on data processing skills.|10.1145/3555962.3555969|https://doi.org/10.1145/3555962.3555969|New York, NY, USA|Association for Computing Machinery|9781450396578|2022|Unstructured Over Structured, Big Data Analytics and Applications In Accounting and Management|Faccia, Alessio and Cavaliere, Luigi Pio Leonardo and Petratos, Pythagoras and Mosteanu, Narcisa Roxana|inproceedings|10.1145/3555962.3555969|||||||||||||||||||||||||||||838012065|42
||Data, Sampling, Variability, Inference, Uncertainty||20-24||What is the role of Statistics in the era of big data, or is Statistics still relevant? I will start this rather personal view with my answer. Statistics remains highly relevant irrespective of ‘bigness’ of data, its role remains what is has always been, but is even more important now. As a community, we need to improve our explanations and presentations to make more visible our relevance.|https://doi.org/10.1016/j.spl.2018.02.050|https://www.sciencedirect.com/science/article/pii/S0167715218300956||||2018|The role of Statistics in the era of big data: Crucial, critical and under-valued|E. Marian Scott|article|SCOTT201820|||Statistics & Probability Letters|01677152||136||The role of Statistics in the era of big data|||14794|0,576|Q2|66|241|862|4066|1014|860|1,17|16,87|Netherlands|Western Europe|1982-2021|Statistics and Probability (Q2); Statistics, Probability and Uncertainty (Q2)||||838896297|1688735168
||Mobile sensing network, High performance sensing, Big data perception, Node self-deployment, On-demand coverage, Mobile cellular learning automata||220-234||Mobile Sensing Networks have been widely applied to many fields for big data perception such as intelligent transportation, medical health and environment sensing. However, in some complex environments and unreachable regions of inconvenience for human, the establishment of the mobile sensing networks, the layout of the nodes and the control of the network topology to achieve high performance sensing of big data are increasingly becoming a main issue in the applications of the mobile sensing networks. To deal with this problem, we propose a novel on-demand coverage based self-deployment algorithm for big data perception based on mobile sensing networks in this paper. Firstly, by considering characteristics of mobile sensing nodes, we extend the cellular automata model and propose a new mobile cellular automata model for effectively characterizing the spatial–temporal evolutionary process of nodes. Secondly, based on the learning automata theory and the historical information of node movement, we further explore a new mobile cellular learning automata model, in which nodes can self-adaptively and intelligently decide the best direction of movement with low energy consumption. Finally, we propose a new optimization algorithm which can quickly solve the node self-adaptive deployment problem, thus, we derive the best deployment scheme of nodes in a short time. The extensive simulation results show that the proposed algorithm in this paper outperforms the existing algorithms by as much as 40% in terms of the degree of satisfaction of network coverage, the iterations of the algorithm, the average moving steps of nodes and the energy consumption of nodes. Hence, we believe that our work will make contributions to large-scale adaptive deployment and high performance sensing scenarios of the mobile sensing networks.|https://doi.org/10.1016/j.future.2018.01.007|https://www.sciencedirect.com/science/article/pii/S0167739X17313262||||2018|An on-demand coverage based self-deployment algorithm for big data perception in mobile sensing networks|Yaguang Lin and Xiaoming Wang and Fei Hao and Liang Wang and Lichen Zhang and Ruonan Zhao|article|LIN2018220|||Future Generation Computer Systems|0167739X||82|||||12264|1,262|Q1|119|798|1935|36054|16877|1868|9,11|45,18|Netherlands|Western Europe|1984-2021|Computer Networks and Communications (Q1); Hardware and Architecture (Q1); Software (Q1)||||842905171|562237118
||Big data, Measurement, Implementation challenges, Analysis tools, Transit best practices||236-241||A group of researchers, consultants, software developers, and transit agencies convened in Santiago, Chile over 3 days as part of the Thredbo workshop titled “Harnessing Big Data”, to present their recent research and discuss the state of practice, state of the art, and future directions of big data in public transportation. This report documents their discussion. The key conclusion of the workshop is that, although much progress has been made in utilizing big data to improve transportation planning and operations, much remains to be done, both in terms of developing further analysis tools and use cases of big data, and of disseminating best practices so that they are adopted across the industry.|https://doi.org/10.1016/j.retrec.2016.10.008|https://www.sciencedirect.com/science/article/pii/S0739885916301494||||2016|Workshop 5 report: Harnessing big data|Gabriel E. Sánchez-Martínez and Marcela Munizaga|article|SANCHEZMARTINEZ2016236|||Research in Transportation Economics|07398859||59||Competition and Ownership in Land Passenger Transport (selected papers from the Thredbo 14 conference)|||4100151536|1,019|Q1|46|160|220|7437|694|205|2,79|46,48|United States|Northern America|1994, 1996, 1999, 2001, 2004-2020|Economics, Econometrics and Finance (miscellaneous) (Q1); Transportation (Q1)|2,095|2.627|0.00218|843564274|1377536709
WI-IAT '21|Melbourne, VIC, Australia|mobile crowdsensing, mobile sensing devices, crowdsourcing|8|54–61|IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology|The amount of gadgets connected to the internet has grown rapidly in the recent years. These human owned devices can potentially be used to gather sensor data without active involvement of their owners. One of the types of platforms that contribute to the utilisation of these devices are mobile crowdsensing systems. These systems can be used for different tasks including different types of community support. While these systems are quite widely used, yet little research has been done for integration of imagery data into them which require also human involvement. This paper considers a mobile crowdsensing system where gathering data from sensors is supported by crowdsourcing human intelligence for providing both textual and visual information. We also explore the best settings for such a system. Imagery processing is integrated into an already existing mobile crowdsensing platform CrowdS. The solution was evaluated both by a limited number of real life users and by conducting simulations. The simulations represent complex scenarios with multi-level variables. The results of simulation allow suggest an efficient configuration for the parameters and characteristics of the environment used in imagery integration.|10.1145/3498851.3498929|https://doi.org/10.1145/3498851.3498929|New York, NY, USA|Association for Computing Machinery|9781450391870|2022|Mobile Crowdsensing with Imagery Tasks|Dautaras, Justas and Matskin, Mihhail|inproceedings|10.1145/3498851.3498929|||||||||||||||||||||||||||||843814037|42
||Master data, master data management, MDM, Big Data, reference data management, RDM||1-16|Entity Information Life Cycle for Big Data|This chapter gives a definition of master data management (MDM) and describes how it generates value for organizations. It also provides an overview of Big Data and the challenges it brings to MDM.|https://doi.org/10.1016/B978-0-12-800537-8.00001-6|https://www.sciencedirect.com/science/article/pii/B9780128005378000016|Boston|Morgan Kaufmann|978-0-12-800537-8|2015|Chapter 1 - The Value Proposition for MDM and Big Data|John R. Talburt and Yinle Zhou|incollection|TALBURT20151||||||||||John R. Talburt and Yinle Zhou|||||||||||||||||||844234621|42
||High-speed railway, big data, train running status, monitoring, early warning||247-266|Safety Theory and Control Technology of High-Speed Train Operation|In the view of the whole system this chapter introduces the train state monitoring and early warning, which is based on big data and combines big data theory with human and signaling systems, comprehensively considering the coordination among the TCC, CBI, CTC, and other subsystems. We analyze the factors that can affect the train state of operation systematically, including the operation action of the signaling system and the operator, realizing the real-time and online monitoring and early warning of train running state then to ensure the safety of train operation.|https://doi.org/10.1016/B978-0-12-813304-0.00008-6|https://www.sciencedirect.com/science/article/pii/B9780128133040000086||Academic Press|978-0-12-813304-0|2018|Chapter 8 - Real-Time Monitoring and Early Warning of a Train’s Running State and Operation Behavior|Junfeng Wang|incollection|WANG2018247||||||||||Junfeng Wang|||||||||||||||||||845709342|42
|||14|2294–2307||Ubiquitous mobile devices with rich sensors and advanced communication capabilities have given rise to mobile crowdsensing systems. The diverse reliabilities of mobile users and the openness of sensing paradigms raise concerns for data trustworthiness, user privacy, and incentive provision. Instead of considering these issues as isolated modules in most existing researches, we comprehensively capture both conflict and inner-relationship among them. In this paper, we propose a holistic solution for trustworthy and privacy-aware mobile crowdsensing with no need of a trusted third party. Specifically, leveraging cryptographic technologies, we devise a series of protocols to enable benign users to request tasks, contribute their data, and earn rewards anonymously without any data linkability. Meanwhile, an anonymous trust/reputation model is seamlessly integrated into our scheme, which acts as reference for our fair incentive design, and provides evidence to detect malicious users who degrade the data trustworthiness. Particularly, we first propose the idea of limiting the number of issued pseudonyms which serves to efficiently tackle the anonymity abuse issue. Security analysis demonstrates that our proposed scheme achieves stronger security with resilience against possible collusion attacks. Extensive simulations are presented which demonstrate the efficiency and practicality of our scheme.|10.1109/TNET.2019.2944984|https://doi.org/10.1109/TNET.2019.2944984||IEEE Press||2019|Enabling Data Trustworthiness and User Privacy in Mobile Crowdsensing|Wu, Haiqin and Wang, Liangmin and Xue, Guoliang and Tang, Jian and Yang, Dejun|article|10.1109/TNET.2019.2944984||dec|IEEE/ACM Trans. Netw.|10636692|6|27|Dec. 2019||||27237|1,022|Q1|174|187|653|6994|3573|652|5,40|37,40|United States|Northern America|1993-2020|Computer Networks and Communications (Q1); Computer Science Applications (Q1); Electrical and Electronic Engineering (Q1); Software (Q1)||||846897570|1453185914
||||100279|||https://doi.org/10.1016/j.xinn.2022.100279|https://www.sciencedirect.com/science/article/pii/S2666675822000753||||2022|Geographic information science in the era of geospatial big data: A cyberspace perspective|Xintao Liu and Min Chen and Christophe Claramunt and Michael Batty and Mei-Po Kwan and Ahmad M. Senousi and Tao Cheng and Josef Strobl and Arzu Cöltekin and John Wilson and Temenoujka Bandrova and Milan Konecny and Paul M. Torrens and Fengyuan Zhang and Li He and Jinfeng Wang and Carlo Ratti and Olaf Kolditz and Alexander Klippel and Songnian Li and Hui Lin and Guonian Lü|article|LIU2022100279|||The Innovation|26666758|5|3|||||||||||||||||||||||847589024|1734043345
||Process modeling, Small data, Deep neural network, Embedding, Autoencoder||48-57||In the big data era, small data problems still exist in many industrial sectors. Taking the high-value process industries as an example, a large number of materials and processing methods are often tested at the design stage. However, only a small amount of data can be collected for each material-process combination, which poses a serious challenge to data-driven process modeling. There is a great necessity to integrate the small data measured in different tasks and build the process model by sharing the information. In this work, a deep embedding neural network is proposed to extract the qualitative task information for process modeling. Specifically, an autoencoder is used to learn embeddings which are combined with the quantitative process conditions as the inputs of a feed-forward neural network to produce the final predictions. The feasibility, including interpretability and prediction accuracy, of the developed method is illustrated with an extrusion process.|https://doi.org/10.1016/j.jprocont.2022.04.018|https://www.sciencedirect.com/science/article/pii/S0959152422000749||||2022|Process modeling by integrating quantitative and qualitative information using a deep embedding network and its application to an extrusion process|Haibin Wu and Yu-Han Lo and Le Zhou and Yuan Yao|article|WU202248|||Journal of Process Control|09591524||115|||||14414|1,102|Q1|114|136|413|5639|1820|408|4,39|41,46|United Kingdom|Western Europe|1991-2020|Computer Science Applications (Q1); Control and Systems Engineering (Q1); Industrial and Manufacturing Engineering (Q1); Modeling and Simulation (Q1)|7,446|3.666|0.00525|847808038|26970220
SIGMOD '16|San Francisco, California, USA|implication, satisfiability, functional dependencies, validation, graphs|15|1843–1857|Proceedings of the 2016 International Conference on Management of Data|We propose a class of functional dependencies for graphs, referred to as GFDs. GFDs capture both attribute-value dependencies and topological structures of entities, and subsume conditional functional dependencies (CFDs) as a special case. We show that the satisfiability and implication problems for GFDs are coNP-complete and NP-complete, respectively, no worse than their CFD counterparts. We also show that the validation problem for GFDs is coNP-complete. Despite the intractability, we develop parallel scalable algorithms for catching violations of GFDs in large-scale graphs. Using real-life and synthetic data, we experimentally verify that GFDs provide an effective approach to detecting inconsistencies in knowledge and social graphs.|10.1145/2882903.2915232|https://doi.org/10.1145/2882903.2915232|New York, NY, USA|Association for Computing Machinery|9781450335317|2016|Functional Dependencies for Graphs|Fan, Wenfei and Wu, Yinghui and Xu, Jingbo|inproceedings|10.1145/2882903.2915232|||||||||||||||||||||||||||||850768299|42
||||306-310||In oncology, the term “big data” broadly describes the rapid acquisition and generation of massive amounts of information, typically from population cancer registries, electronic health records, or large-scale genetic sequencing studies. The challenge of using big data in cancer research lies in interdisciplinary collaboration and information processing to unify diverse data sources and provide valid analytics to harness meaningful information. This article provides an overview of how big data approaches can be applied in cancer research, and how they can be used to translate information into new ways to ultimately make informed decisions that improve cancer care and delivery.|https://doi.org/10.1016/j.semradonc.2019.05.002|https://www.sciencedirect.com/science/article/pii/S1053429619300347||||2019|Big Data in Cancer Research: Real-World Resources for Precision Oncology to Improve Cancer Care Delivery|Chiaojung Jillian Tsai and Nadeem Riaz and Scarlett Lin Gomez|article|TSAI2019306|||Seminars in Radiation Oncology|10534296|4|29||Big Data in Radiation Oncology|||13121|1,761|Q1|93|40|129|2575|654|118|5,29|64,38|United Kingdom|Western Europe|1991-2020|Oncology (Q1); Radiology, Nuclear Medicine and Imaging (Q1); Cancer Research (Q2)|2,837|5.934|0.00271|850794697|665117643
||Deep learning, Big data, Stacked auto-encoders, Deep belief networks, Convolutional neural networks, Recurrent neural networks||146-157||Deep learning, as one of the most currently remarkable machine learning techniques, has achieved great success in many applications such as image analysis, speech recognition and text understanding. It uses supervised and unsupervised strategies to learn multi-level representations and features in hierarchical architectures for the tasks of classification and pattern recognition. Recent development in sensor networks and communication technologies has enabled the collection of big data. Although big data provides great opportunities for a broad of areas including e-commerce, industrial control and smart medical, it poses many challenging issues on data mining and information processing due to its characteristics of large volume, large variety, large velocity and large veracity. In the past few years, deep learning has played an important role in big data analytic solutions. In this paper, we review the emerging researches of deep learning models for big data feature learning. Furthermore, we point out the remaining challenges of big data deep learning and discuss the future topics.|https://doi.org/10.1016/j.inffus.2017.10.006|https://www.sciencedirect.com/science/article/pii/S1566253517305328||||2018|A survey on deep learning for big data|Qingchen Zhang and Laurence T. Yang and Zhikui Chen and Peng Li|article|ZHANG2018146|||Information Fusion|15662535||42|||||||||||||||||||||||851890475|1192976563
||||37-41||In the era of big data medicinal chemists are exposed to an enormous amount of bioactivity data. Numerous public data sources allow for querying across medium to large data sets mostly compiled from literature. However, the data available are still quite incomplete and of mixed quality. This mini review will focus on how medicinal chemists might use such resources and how valuable the current data sources are for guiding drug discovery.|https://doi.org/10.1016/j.ddtec.2015.06.001|https://www.sciencedirect.com/science/article/pii/S1740674915000141||||2015|Medicinal chemistry in the era of big data|Lars Richter and Gerhard F. Ecker|article|RICHTER201537|||Drug Discovery Today: Technologies|17406749||14||From Chemistry to Biology Database Curation|||21207|1,676|Q1|51|16|99|1002|521|89|5,64|62,63|United Kingdom|Western Europe|2004-2020|Biotechnology (Q1); Drug Discovery (Q1); Molecular Medicine (Q1)||||854524485|1563558628
|||8|72–79||Every five years, a group of the leading database researchers meet to reflect on their community's impact on the computing industry as well as examine current research challenges.|10.1145/3524284|https://doi.org/10.1145/3524284|New York, NY, USA|Association for Computing Machinery||2022|The Seattle Report on Database Research|Abadi, Daniel and Ailamaki, Anastasia and Andersen, David and Bailis, Peter and Balazinska, Magdalena and Bernstein, Philip A. and Boncz, Peter and Chaudhuri, Surajit and Cheung, Alvin and Doan, Anhai and Dong, Luna and Franklin, Michael J. and Freire, Juliana and Halevy, Alon and Hellerstein, Joseph M. and Idreos, Stratos and Kossmann, Donald and Kraska, Tim and Krishnamurthy, Sailesh and Markl, Volker and Melnik, Sergey and Milo, Tova and Mohan, C. and Neumann, Thomas and Ooi, Beng Chin and Ozcan, Fatma and Patel, Jignesh and Pavlo, Andrew and Popa, Raluca and Ramakrishnan, Raghu and Re, Christopher and Stonebraker, Michael and Suciu, Dan|article|10.1145/3524284||jul|Commun. ACM|00010782|8|65|August 2022||||||||||||||||||||||857748702|647144465
|||10|44–53||Approximately every five years, a group of database researchers meet to do a self-assessment of our community, including reflections on our impact on the industry as well as challenges facing our research community. This report summarizes the discussion and conclusions of the 9th such meeting, held during October 9-10, 2018 in Seattle.|10.1145/3385658.3385668|https://doi.org/10.1145/3385658.3385668|New York, NY, USA|Association for Computing Machinery||2020|The Seattle Report on Database Research|Abadi, Daniel and Ailamaki, Anastasia and Andersen, David and Bailis, Peter and Balazinska, Magdalena and Bernstein, Philip and Boncz, Peter and Chaudhuri, Surajit and Cheung, Alvin and Doan, AnHai and Dong, Luna and Franklin, Michael J. and Freire, Juliana and Halevy, Alon and Hellerstein, Joseph M. and Idreos, Stratos and Kossmann, Donald and Kraska, Tim and Krishnamurthy, Sailesh and Markl, Volker and Melnik, Sergey and Milo, Tova and Mohan, C. and Neumann, Thomas and Chin Ooi, Beng and Ozcan, Fatma and Patel, Jignesh and Pavlo, Andrew and Popa, Raluca and Ramakrishnan, Raghu and R\'{e}, Christopher and Stonebraker, Michael and Suciu, Dan|article|10.1145/3385658.3385668||feb|SIGMOD Rec.|01635808|4|48|December 2019||||13622|0,372|Q2|142|39|81|808|127|57|1,67|20,72|United States|Northern America|1969, 1973-1978, 1981-2020|Information Systems (Q2); Software (Q2)|1,819|0.775|7.5E-4|857748702|962972343
||machine learning, robustness, safety, uncertainty quantification, verification||||The open-world deployment of Machine Learning (ML) algorithms in safety-critical applications such as autonomous vehicles needs to address a variety of ML vulnerabilities such as interpretability, verifiability, and performance limitations. Research explores different approaches to improve ML dependability by proposing new models and training techniques to reduce generalization error, achieve domain adaptation, and detect outlier examples and adversarial attacks. However, there is a missing connection between ongoing ML research and well-established safety principles. In this paper, we present a structured and comprehensive review of ML techniques to improve the dependability of ML algorithms in uncontrolled open-world settings. From this review, we propose the Taxonomy of ML Safety that maps state-of-the-art ML techniques to key engineering safety strategies. Our taxonomy of ML safety presents a safety-oriented categorization of ML techniques to provide guidance for improving dependability of the ML design and development. The proposed taxonomy can serve as a safety checklist to aid designers in improving coverage and diversity of safety strategies employed in any given ML system.|10.1145/3551385|https://doi.org/10.1145/3551385|New York, NY, USA|Association for Computing Machinery||2022|Taxonomy of Machine Learning Safety: A Survey and Primer|Mohseni, Sina and Wang, Haotao and Xiao, Chaowei and Yu, Zhiding and Wang, Zhangyang and Yadawa, Jay|article|10.1145/3551385||jul|ACM Comput. Surv.|03600300||||Just Accepted|||23038|2,079|Q1|163|112|365|15859|6978|365|19,16|141,60|United States|Northern America|1969-2020|Computer Science (miscellaneous) (Q1); Theoretical Computer Science (Q1)|10,790|10.282|0.01438|858080322|1517405264
CIKM '18|Torino, Italy|entity linking, concepts, short text, fine-grained topics|10|457–466|Proceedings of the 27th ACM International Conference on Information and Knowledge Management|A wide range of web corpora are in the form of short text, such as QA queries, search queries and news titles. Entity linking for these short texts is quite important. Most of supervised approaches are not effective for short text entity linking. The training data for supervised approaches are not suitable for short text and insufficient for low-resourced languages. Previous unsupervised methods are incapable of handling the sparsity and noisy problem of short text. We try to solve the problem by mapping the sparse short text to a topic space. We notice that the concepts of entities have rich topic information and characterize entities in a very fine-grained granularity. Hence, we use the concepts of entities as topics to explicitly represent the context, which helps improve the performance of entity linking for short text. We leverage our linking approach to segment the short text semantically, and build a system for short entity text recognition and linking. Our entity linking approach exhibits the state-of-the-art performance on several datasets for the realistic short text entity linking problem.|10.1145/3269206.3271809|https://doi.org/10.1145/3269206.3271809|New York, NY, USA|Association for Computing Machinery|9781450360142|2018|Short Text Entity Linking with Fine-Grained Topics|Chen, Lihan and Liang, Jiaqing and Xie, Chenhao and Xiao, Yanghua|inproceedings|10.1145/3269206.3271809|||||||||||||||||||||||||||||860435769|42
||Training;Testing;Annotations;Predictive models;Noise reduction;Feature extraction;Dictionaries;Annotation Noise;Regression Testing;Machine Learning Models||191-194|2020 46th Euromicro Conference on Software Engineering and Advanced Applications (SEAA)|Big data and machine learning models have been increasingly used to support software engineering processes and practices. One example is the use of machine learning models to improve test case selection in continuous integration. However, one of the challenges in building such models is the identification and reduction of noise that often comes in large data. In this paper, we present a noise reduction approach that deals with the problem of contradictory training entries. We empirically evaluate the effectiveness of the approach in the context of selective regression testing. For this purpose, we use a curated training set as input to a tree-based machine learning ensemble and compare the classification precision, recall, and f-score against a non-curated set. Our study shows that using the noise reduction approach on the training instances gives better results in prediction with an improvement of 37% on precision, 70% on recall, and 59% on f-score.|10.1109/SEAA51224.2020.00042|||||2020|Improving Data Quality for Regression Test Selection by Reducing Annotation Noise|Al-Sabbagh, Khaled Walid and Staron, Miroslaw and Hebig, Regina and Meding, Wilhelm|inproceedings|9226358||Aug|||||||||||||||||||||||||||862161069|42
||Human mobility, Spatiotemporal heterogeneity, Remote sensing, Big data, Environmental health||288-296||In the past few decades, extensive epidemiological studies have focused on exploring the adverse effects of PM2.5 (particulate matters with aerodynamic diameters less than 2.5 μm) on public health. However, most of them failed to consider the dynamic changes of population distribution adequately and were limited by the accuracy of PM2.5 estimations. Therefore, in this study, location-based service (LBS) data from social media and satellite-derived high-quality PM2.5 concentrations were collected to perform highly spatiotemporal exposure assessments for thirteen cities in the Beijing-Tianjin-Hebei (BTH) region, China. The city-scale exposure levels and the corresponding health outcomes were first estimated. Then the uncertainties in exposure risk assessments were quantified based on in-situ PM2.5 observations and static population data. The results showed that approximately half of the population living in the BTH region were exposed to monthly mean PM2.5 concentration greater than 80 μg/m3 in 2015, and the highest risk was observed in December. In terms of all-cause, cardiovascular, and respiratory disease, the premature deaths attributed to PM2.5 were estimated to be 138,150, 80,945, and 18,752, respectively. A comparative analysis between five different exposure models further illustrated that the dynamic population distribution and accurate PM2.5 estimations showed great influence on environmental exposure and health assessments and need be carefully considered. Otherwise, the results would be considerably over- or under-estimated.|https://doi.org/10.1016/j.envpol.2019.06.057|https://www.sciencedirect.com/science/article/pii/S026974911930418X||||2019|Dynamic assessment of PM2.5 exposure and health risk using remote sensing and geo-spatial big data|Yimeng Song and Bo Huang and Qingqing He and Bin Chen and Jing Wei and Rashed Mahmood|article|SONG2019288|||Environmental Pollution|02697491||253|||||23916|2,136|Q1|227|2109|4204|129684|35383|4169|8,04|61,49|United Kingdom|Western Europe|1970-1980, 1986-2020|Health, Toxicology and Mutagenesis (Q1); Medicine (miscellaneous) (Q1); Pollution (Q1); Toxicology (Q1)|84,491|8.071|0.07982|862196364|354628351
PIKM '12|Maui, Hawaii, USA|scientific data, ranked data search|8|1–8|Proceedings of the 5th Ph.D. Workshop on Information and Knowledge|"\"For decades, scientists bemoaned the scarcity of observational data to analyze and against which to test their models. Exponential growth in data volumes from ever-cheaper environmental sensors has provided scientists with the answer to their prayers: \"\"big data\"\". Now, scientists face a new challenge: with terabytes, petabytes or exabytes of data at hand, stored in thousands of heterogeneous datasets, how can scientists find the datasets most relevant to their research interests? If they cannot find the data, then they may as well never have collected it; that data is lost to them. Our research addresses this challenge, using an existing scientific archive as our test-bed. We approach this problem in a new way: by adapting Information Retrieval techniques, developed for searching text documents, into the world of (primarily numeric) scientific data. We propose an approach that uses a blend of automated and \"\"semi-curated\"\" methods to extract metadata from large archives of scientific data. We then perform searches over the extracted metadata, returning results ranked by similarity to the query terms. We briefly describe an implementation performed at an ocean observatory to validate the proposed approach. We propose performance and scalability research to explore how continued archive growth will affect our goal of interactive response, no matter the scale.\""|10.1145/2389686.2389688|https://doi.org/10.1145/2389686.2389688|New York, NY, USA|Association for Computing Machinery|9781450317191|2012|When Big Data Leads to Lost Data|Megler, V. M. and Maier, David|inproceedings|10.1145/2389686.2389688|||||||||||||||||||||||||||||863324327|42
||Big data;Distributed databases;Decision support systems;Power industry;Electric vehicles;Systems operation;Maintenance engineering;electric vehicle;operations management system;big data;data quality;data selection and processing||2715-2720|2015 5th International Conference on Electric Utility Deregulation and Restructuring and Power Technologies (DRPT)|It is very important for Operations Management System (OMS) and big data analysis application to improve the data quality of Electric Vehicle (EV) charging service. This paper focuses on the charging transaction record data from the Beijing EV charging OMS, and analyzes error types and distributed locations of the abnormal data. Based on the mathematical logic among various kinds of operation data, the data selection rules and processing method are proposed, and the system improved scheme is given. Through the design and application of data selection and processing module, the abnormal data can be timely detected and corrected. It is also beneficial for the system operation and maintenance to improve the acquisition data quality. The comparative analysis results verify the feasibility and effectivity of the proposed scheme. This research is a necessary guarantee for the big data technology application.|10.1109/DRPT.2015.7432708|||||2015|Data quality analysis and improved strategy research on operations management system for electric vehicles|Zhang, Lu and Chen, Yanxia and Zhu, Jie and Pan, Mingyu and Sun, Zhou and Wang, Weixian|inproceedings|7432708||Nov|||||||||||||||||||||||||||872347779|42
WWW '17 Companion|Perth, Australia|topic analysis, academic papers, study map, reference injection, double-damping pagerank|8|1225–1232|Proceedings of the 26th International Conference on World Wide Web Companion|As the number of academic papers and new technologies soars, it has been increasingly difficult for researchers, especially beginners, to enter a new research field. Researchers often need to study a promising paper in depth to keep up with the forefront of technology. Traditional Query-Oriented study method is time-consuming and even tedious. For a given paper, existent academic search engines like Google Scholar tend to recommend relevant papers, failing to reveal the knowledge structure. The state-of-the-art Map-Oriented study methods such as AMiner and AceMap can structure scholar information, but they're too coarse-grained to dig into the underlying principles of a specific paper. To address this problem, we propose a Study-Map Oriented method and a novel model called RIDP (Reference Injection based Double-Damping PageRank) to help researchers study a given paper more efficiently and thoroughly. RIDP integrates newly designed Reference Injection based Topic Analysis method and Double-Damping PageRank algorithm to mine a Study Map out of massive academic papers in order to guide researchers to dig into the underlying principles of a specific paper. Experiment results on real datasets and pilot user studies indicate that our method can help researchers acquire knowledge more efficiently, and grasp knowledge structure systematically.|10.1145/3041021.3053059|https://doi.org/10.1145/3041021.3053059|Republic and Canton of Geneva, CHE|International World Wide Web Conferences Steering Committee|9781450349147|2017|From Citation Network to Study Map: A Novel Model to Reorganize Academic Literatures|Tao, Shibo and Wang, Xiaorong and Huang, Weijing and Chen, Wei and Wang, Tengjiao and Lei, Kai|inproceedings|10.1145/3041021.3053059|||||||||||||||||||||||||||||873584454|42
||Indexes;Mathematical model;Data models;Manufacturing industries;Computational modeling;Big data;Customer satisfaction;macro-quality evaluation;big data;macro-quality index;customer satisfaction index;PLS-SEM||181-186|2016 International Conference on Industrial Informatics - Computing Technology, Intelligent Technology, Industrial Information Integration (ICIICII)|The aim of this paper was to develop a novel macro-quality index driven by big quality data regarding the macro-quality management demands of manufacturing enterprises in Industry 4.0. Firstly, the connotation of big data of macro-quality management in manufacturing industry is expounded, which is the collection of the quality data in product lifecycle including the product quality data, process quality data and organizational ability data. Secondly, referring to the customer satisfaction index theory, a new big data oriented macro-quality index computation model based on the partial least square-structural equation modeling(PLS-SEM) theory is proposed, and the partial least square(PLS) is adopted to estimate the path parameters. Finally, a case study of macro-quality situation evaluation for the manufacturing industry of a city in China is presented. The final result shows that the proposed macro-quality index model is applicable and predictable.|10.1109/ICIICII.2016.0052|||||2016|Big Data Oriented Macro-Quality Index Based on Customer Satisfaction Index and PLS-SEM for Manufacturing Industry|Li, Tao and He, Yihai and Zhu, Chunling|inproceedings|7823519||Dec|||||||||||||||||||||||||||875178826|42
ICCDA '17|Lakeland, FL, USA|Data, ETL, intelligence, business, warehouse|5|104–108|Proceedings of the International Conference on Compute and Data Analysis|This paper provides an overview of the history and current state of data warehousing and corporate analytics. It begins with a quick review of the history of the data warehouse and then does a deeper dive into subsets of this space including data integration, the DBMS, business intelligence and analytics, advanced analytics, and information stewardship. It finishes with a quick review of some of the leading trends in data warehousing including Big Data and the Logical Data Warehouse, Hybrid Transaction Analytical Processing and In-Memory Computing.|10.1145/3093241.3093268|https://doi.org/10.1145/3093241.3093268|New York, NY, USA|Association for Computing Machinery|9781450352413|2017|The Data Warehousing (R) Evolution: Where's It Headed Next?|Smith, Jeffrey and Rege, Manjeet|inproceedings|10.1145/3093241.3093268|||||||||||||||||||||||||||||879552562|42
|||51|143–193|Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice|||https://doi.org/10.1145/3447404.3447414|New York, NY, USA|Association for Computing Machinery|9781450390293|2021|Machine Learning Basics|Chatzilygeroudis, Konstantinos and Hatzilygeroudis, Ioannis and Perikos, Isidoros|inbook|10.1145/3447404.3447414|||||||||1||||||||||||||||||||881723655|42
||convolutional neural network, diffraction image, Data quality, deep learning, support vector machine, machine learning|22|||Deep learning has been widely used for extracting values from big data. As many other machine learning algorithms, deep learning requires significant training data. Experiments have shown both the volume and the quality of training data can significantly impact the effectiveness of the value extraction. In some cases, the volume of training data is not sufficiently large for effectively training a deep learning model. In other cases, the quality of training data is not high enough to achieve the optimal performance. Many approaches have been proposed for augmenting training data to mitigate the deficiency. However, whether the augmented data are “fit for purpose” of deep learning is still a question. A framework for comprehensively evaluating the effectiveness of the augmented data for deep learning is still not available. In this article, we first discuss a data augmentation approach for deep learning. The approach includes two components: the first one is to remove noisy data in a dataset using a machine learning based classification to improve its quality, and the second one is to increase the volume of the dataset for effectively training a deep learning model. To evaluate the quality of the augmented data in fidelity, variety, and veracity, a data quality evaluation framework is proposed. We demonstrated the effectiveness of the data augmentation approach and the data quality evaluation framework through studying an automated classification of biology cell images using deep learning. The experimental results clearly demonstrated the impact of the volume and quality of training data to the performance of deep learning and the importance of the data quality evaluation. The data augmentation approach and the data quality evaluation framework can be straightforwardly adapted for deep learning study in other domains.|10.1145/3317573|https://doi.org/10.1145/3317573|New York, NY, USA|Association for Computing Machinery||2019|A Case Study of the Augmentation and Evaluation of Training Data for Deep Learning|Ding, Junhua and Li, Xinchuan and Kang, Xiaojun and Gudivada, Venkat N.|article|10.1145/3317573|20|aug|J. Data and Information Quality|19361955|4|11|December 2019||||||||||||||||||||||882362107|833754770
https://doi.org/10.1016/j.jfma.2021.12.024|https://www.sciencedirect.com/science/article/pii/S092966462100591X||||2022|Quality assurance of integrative big data for medical research within a multihospital system|Yi-Chia Lee and Ying-Ting Chao and Pei-Ju Lin and Yen-Yun Yang and Yu-Cih Yang and Cheng-Chieh Chu and Yu-Chun Wang and Chin-Hao Chang and Shu-Lin Chuang and Wei-Chun Chen and Hsing-Jen Sun and Hsin-Cheng Tsou and Cheng-Fu Chou and Wei-Shiung Yang|article|LEE20221728|||Journal of the Formosan Medical Association|09296646|9|121||||||||||||||||||||||||||||||882927725|42
RACS '16|Odense, Denmark|Crowdsourcing, HITs, Quality Control, Big Data, Reputation, Crowd Computing, MTurk, Ontology, Human Computation|7|22–28|Proceedings of the International Conference on Research in Adaptive and Convergent Systems|In the era of big data, a vast amount of data is created every day. Crowdsourcing systems have recently gained significance as an interesting practice in managing and performing big data operations. Crowdsourcing has facilitated the process of performing tasks that cannot be adequately solved by machines including image labeling, transcriptions, data validation and sentiment analysis. However, quality control remains one of the biggest challenges for crowdsourcing. Current crowdsourcing systems use the same quality control mechanism for evaluating different types of tasks. In this paper, we argue that quality mechanisms vary by task type. We propose a task ontology-based model to identify the most appropriate quality mechanism for a given task. The proposed model has been enriched by a reputation system to collect requesters' feedback on quality mechanisms. Accordingly, the reputation of each mechanism can be established and used for mapping between tasks and mechanisms. Description of the model's framework, algorithms, and its components' interaction are presented.|10.1145/2987386.2987413|https://doi.org/10.1145/2987386.2987413|New York, NY, USA|Association for Computing Machinery|9781450344555|2016|A Task Ontology-Based Model for Quality Control in Crowdsourcing Systems|Alabduljabbar, Reham and Al-Dossari, Hmood|inproceedings|10.1145/2987386.2987413|||||||||||||||||||||||||||||884799486|42
|||14|3362–3375||Modern data processing systems require optimization at massive scale, and using machine learning to optimize these systems (ML-for-systems) has shown promising results. Unfortunately, ML-for-systems is subject to over generalizations that do not capture the large variety of workload patterns, and tend to augment the performance of certain subsets in the workload while regressing performance for others. In this paper, we introduce a performance safeguard system, called PerfGuard, that designs pre-production experiments for deploying ML-for-systems. Instead of searching the entire space of query plans (a well-known, intractable problem), we focus on query plan deltas (a significantly smaller space). PerfGuard formalizes these differences, and correlates plan deltas to important feedback signals, like execution cost. We describe the deep learning architecture and the end-to-end pipeline in PerfGuard that could be used with general relational databases. We show that this architecture improves on baseline models, and that our pipeline identifies key query plan components as major contributors to plan disparity. Offline experimentation shows PerfGuard as a promising approach, with many opportunities for future improvement.|10.14778/3484224.3484233|https://doi.org/10.14778/3484224.3484233||VLDB Endowment||2021|PerfGuard: Deploying ML-for-Systems without Performance Regressions, Almost!|Ammerlaan, Remmelt and Antonius, Gilbert and Friedman, Marc and Hossain, H M Sajjad and Jindal, Alekh and Orenberg, Peter and Patel, Hiren and Qiao, Shi and Ramani, Vijay and Rosenblatt, Lucas and Roy, Abhishek and Shaffer, Irene and Srinivasan, Soundarajan and Weimer, Markus|article|10.14778/3484224.3484233||oct|Proc. VLDB Endow.|21508097|13|14|September 2021||||21100199855|0,946|Q1|134|246|598|12401|3759|566|5,50|50,41|United States|Northern America|2008-2020|Computer Science (miscellaneous) (Q1)|7,198|2.047|0.00891|885576917|1216159931
||Environmental monitoring, Plastic pollution, Bioregional management, Litter, Citizen science, Marine debris||150742||Anthropogenic marine debris is a persistent threat to oceans, imposing risks to ecosystems and the communities they support. Whilst an understanding of marine debris risks is steadily advancing, monitoring at spatial and temporal scales relevant to management remains limited. Citizen science projects address this shortcoming but are often critiqued on data accuracy and potential bias in sampling efforts. Here we present 10-years of Australia's largest marine debris database - the Australian Marine Debris Initiative (AMDI), in which we perform systematic data filtering, test for differences between collecting groups, and report patterns in marine debris. We defined five stages of data filtering to address issues in data quality and to limit inference to ocean-facing sandy beaches. Significant differences were observed in the average accumulation of items between filtered and remaining data. Further, differences in sampling were compared between collecting groups at the same site (e.g., government, NGOs, and schools), where no significant differences were observed. The filtering process removed 21% of events due to data quality issues and a further 42% of events to restrict analyses to ocean-facing sandy beaches. The remaining 7275 events across 852 sites allowed for an assessment of debris patterns at an unprecedented spatial and temporal resolution. Hard plastics were the most common material found on beaches both nationally and regionally, consisting of up to 75% of total debris. Nationally, land and sea-sourced items accounted for 48% and 7% of debris, respectively, with most debris found on the east coast of Australia. This study demonstrates the value of citizen science datasets with broad spatial and temporal coverage, and the importance of data filtering to improve data quality. The citizen science presented provides an understanding of debris patterns on Australia's ocean beaches and can serve as a foundation for future source reduction plans.|https://doi.org/10.1016/j.scitotenv.2021.150742|https://www.sciencedirect.com/science/article/pii/S0048969721058204||||2022|Continental patterns in marine debris revealed by a decade of citizen science|Jordan Gacutan and Emma L. Johnston and Heidi Tait and Wally Smith and Graeme F. Clark|article|GACUTAN2022150742|||Science of The Total Environment|00489697||807|||||25349|1,795|Q1|244|6929|13278|455970|106837|13125|7,96|65,81|Netherlands|Western Europe|1970, 1972-2021|Environmental Chemistry (Q1); Environmental Engineering (Q1); Pollution (Q1); Waste Management and Disposal (Q1)|210,143|7.963|0.23082|887120009|2019676356
||Big Data, Machine Learning, Concussion, Informatics mild Traumatic Brain Injury||265-273||A concussion is an invisible and poorly understood mild traumatic brain injury (mTBI) that can alter the way the brain functions. Patients who have screened positive for mTBI are at an increased risk of depression, post-traumatic stress disorder (PTSD), headaches, sleep disorders, and other neurological and psychological problems. Early detection of psychological conditions such as PTSD following a concussion might improve the overall outcome of a patient and could potentially reduce the cost associated with intense interventions often required when conditions go untreated for a long time. Statistical and predictive models that leverage large-scale clinical repositories and use pre-existing conditions to determine the probability of a patient developing psychological conditions following a concussion have not been widely studied. This paper presents an SVM-based model that has been trained with a longitudinal dataset of over 5.3 million clinical encounters of 89,840 service members that have sustained a concussion. The model has been tested and validated with over 16,045 patients that developed PTSD and it has shown an accuracy of over 85% (AUC of 86.52%) at predicting the condition within the first year following the injury.|https://doi.org/10.1016/j.procs.2015.07.303|https://www.sciencedirect.com/science/article/pii/S1877050915018062||||2015|Leveraging Big Data to Model the Likelihood of Developing Psychological Conditions After a Concussion|Filip Dabek and Jesus J. Caban|article|DABEK2015265|||Procedia Computer Science|18770509||53||INNS Conference on Big Data 2015 Program San Francisco, CA, USA 8-10 August 2015|||19700182801|0,334|-|76|1907|6483|38511|13722|6359|2,09|20,19|Netherlands|Western Europe|2010-2020|Computer Science (miscellaneous)||||887259920|2108686752
||Task analysis;Crowdsourcing;Libraries;Media;Uniform resource locators;Web pages;Big Data;Evidence;Assumption;Crowdsourcing||3560-3563|2018 IEEE International Conference on Big Data (Big Data)|Crowdsourcing is a promising tool involving multiple people in completing tasks that are difficult to complete by an individual, a small team or a computer. Ensuring the quality of the results is also one of the primary problems in crowdsourcing. One of the major approaches to improve the data quality to aggregate answers from more than one workers. This study explores a different approach - we ask workers to prove facts. We devise a general framework for collecting and ranking evidence-based proofs. The experiments results show that the proposed framework works and how diverse the collected proofs are. Our results clearly indicate that the crowd-based approach to prove facts is promising.|10.1109/BigData.2018.8622185|||||2018|Finding Evidences by Crowdsourcing|Wijerathna, Nadeesha and Matsubara, Masaki and Morishima, Atsuyuki|inproceedings|8622185||Dec|||||||||||||||||||||||||||889703393|42
ICBIM 2017|Bei Jing, China|Business Intelligence, Institutional Research, Big data, Database|5|121–125|Proceedings of the International Conference on Business and Information Management|The applications on business intelligence and big data analysis to extract useful information are getting more mature, but the development and operations in higher education institutions are still be lack. This study aims to explore how HEIs employ business intelligence to analysis and mining student learning and HEIs' operation data from database. The outcomes are benefit for universities to support the management of decision-making.|10.1145/3134271.3134296|https://doi.org/10.1145/3134271.3134296|New York, NY, USA|Association for Computing Machinery|9781450352765|2017|Establishment of Business Intelligence and Big Data Analysis for Higher Education|Peng, Michael Yao-Ping and Tuan, Sheng-Hwa and Liu, Feng-Chi|inproceedings|10.1145/3134271.3134296|||||||||||||||||||||||||||||892165308|42
||Measurement;Data integrity;Data acquisition;Learning (artificial intelligence);Interference;Data models;Real-time systems;multi-mode Data;Learning behavior;data quality;data acquisition||64-69|2021 2nd International Symposium on Computer Engineering and Intelligent Communications (ISCEIC)|With the rapid development of new technologies such as artificial intelligence, big data, and the Internet of Things, many researchers have probed into the study of learning analysis, trying to solve the problems of teaching by analyzing the learning behavior data from learning process. And in many learning behavior research, the sensor network usually consists of a host of mutually independent data sources, which can be used to monitor measured objects from multiple dimensions thereby obtaining the multi-source multi-modal sensory data. However, there still exist false negative readings, false positive readings and environmental interference, etc. Therefore, we propose a multi-source multimode sensory data acquisition method based on Date Quality(DQ). We first define the data quality in terms of four aspects-accuracy, integrity, consistency and instantaneity. Then, by the modeling there aspects respectively, we propose metrics to estimate the comprehensive data quality method of multi-source multi-mode sensory data. Finally, a data acquisition method is presented based on data quality, which selects a part of data sources for data transmission according to the given precision. This method aims at reducing the consumption of the sensory network on the premise of the data quality guarantee. An extensive experimental evaluation demonstrates the efficiency and effectiveness of the algorithm.|10.1109/ISCEIC53685.2021.00021|||||2021|A Multi-Mode Learning Behavior Real-time Data Acquisition Method Based on Data Quality|Zhang, Zhenwei and Wu, Wenyan and Wu, Dongjie|inproceedings|9599192||Aug|||||||||||||||||||||||||||893012828|42
WWW '18|Lyon, France|terminology evolution, web science, content evolution on the web, web spam evolution, temporal web analytics, data aggregation, web trends, web scale data analytics, community detection and evolution, large scale data storage, systematic exploitation of web archives, web dynamics, distributed data analytics, time aware web archiving, large scale data processing, data quality metrics, topic mining|2|1729–1730|Companion Proceedings of the The Web Conference 2018|Time is a key dimension to understand the Web. It is fair to say that it has not received yet all the attention it deserves and TempWeb is an attempt to help remedy this situation by putting time as the center of its reflection. Studying time in this context actually covers a large spectrum, from the extraction of temporal information and knowledge, to diachronic studies for the design of infrastructural and experimental settings enabling a proper observation of this dimension.|10.1145/3184558.3192324|https://doi.org/10.1145/3184558.3192324|Republic and Canton of Geneva, CHE|International World Wide Web Conferences Steering Committee|9781450356404|2018|TempWeb 2018 Chairs' Welcome and Organization|Spaniol, Marc and Baeza-Yates, Ricardo and Masan\`{e}s, Julien|inproceedings|10.1145/3184558.3192324|||||||||||||||||||||||||||||894987235|42
WISCS '14|Scottsdale, Arizona, USA|cryptographic protocols, data quality assessment, privacy and confidentiality|9|21–29|Proceedings of the 2014 ACM Workshop on Information Sharing &amp; Collaborative Security|In a data-driven economy that struggles to cope with the volume and diversity of information, data quality assessment has become a necessary precursor to data analytics. Real-world data often contains inconsistencies, conflicts and errors. Such dirty data increases processing costs and has a negative impact on analytics. Assessing the quality of a dataset is especially important when a party is considering acquisition of data held by an untrusted entity. In this scenario, it is necessary to consider privacy risks of the stakeholders.This paper examines challenges in privacy-preserving data quality assessment. A two-party scenario is considered, consisting of a client that wishes to test data quality and a server that holds the dataset. Privacy-preserving protocols are presented for testing important data quality metrics: completeness, consistency, uniqueness, timeliness and validity. For semi-honest parties, the protocols ensure that the client does not discover any information about the data other than the value of the quality metric. The server does not discover the parameters of the client's query, the specific attributes being tested and the computed value of the data quality metric. The proposed protocols employ additively homomorphic encryption in conjunction with condensed data representations such as counting hash tables and histograms, serving as efficient alternatives to solutions based on private set intersection.|10.1145/2663876.2663885|https://doi.org/10.1145/2663876.2663885|New York, NY, USA|Association for Computing Machinery|9781450331517|2014|Privacy Preserving Data Quality Assessment for High-Fidelity Data Sharing|Freudiger, Julien and Rane, Shantanu and Brito, Alejandro E. and Uzun, Ersin|inproceedings|10.1145/2663876.2663885|||||||||||||||||||||||||||||896683140|42
||Pressure transient analysis, Cloud computing, ISIP, Flow regime, G-function, Web-based application||107627||Pressure transient analysis provides essential information to evaluate the dimensions of injection induced fractures, permeability damage near the wellbore, and pressure elevation in the injection horizon. For injection wells, shut-in data can be collected and analyzed after each injection cycle to evaluate the well injectivity and predict the well longevity. However, any interactive analysis of the pressure data could be subjective and time-consuming. In this study a novel cloud-based approach to automatically analyzing pressure data is presented, which aims to improve the reliability and efficiency of pressure transient analysis. There are two fundamental requirements for automated pressure transient analysis: 1) Pressure data needs to be automatically retrieved from field sites and fed to the analyzer; 2) The engineer can automatically select instantaneous shut-in pressure (ISIP), identify flow regimes, and determine the fracture closure point if any. To meet these requirements as well as to take advantage of cloud storage and computing technologies, a web-based application has been developed to pull real time injection data from any field sites and push it to a cloud database. A built-in pressure transient workflow has been also proposed to detect any stored or real-time pressure data and perform pressure analysis automatically if the required data is available. The automated pressure transient analysis technology has been applied to multiple injection projects. In general, the analysis results including formation and fracture properties (i.e. permeability, fracture half length, skin factor, and fracture closure pressure) are comparable to results from interactive analysis. Any discrepancies are mainly caused by poor data quality. Issues such as inconsistent selections of ISIP and different slopes defined for pre and after closure analyses also contribute to the divergence. Overall, the automated pressure transient analysis provides consistent results as the exact same criteria are applied to the pressure data, and analysis results are independent of the analyzer's experience and knowledge. As data from oil/gas industry increases exponentially over time, automated data transmission, storage, analysis and access are becoming necessary to maximize the value of the data and reduce operation cost. The automated pressure transient analysis presented here demonstrates that cloud storage and computing combined with automated analysis tools is a viable way to overcome big data challenges faced by oil/gas industry professionals.|https://doi.org/10.1016/j.petrol.2020.107627|https://www.sciencedirect.com/science/article/pii/S0920410520306951||||2021|Automated pressure transient analysis: A cloud-based approach|Yonggui Guo and Ibrahim Mohamed and Ali Zidane and Yashesh Panchal and Omar Abou-Sayed and Ahmed Abou-Sayed|article|GUO2021107627|||Journal of Petroleum Science and Engineering|09204105||196|||||17013|0,975|Q1|111|1174|2764|59146|13310|2752|4,78|50,38|Netherlands|Western Europe|1987-2021|Fuel Technology (Q1); Geotechnical Engineering and Engineering Geology (Q1)|25,616|4.346|0.02651|896969084|2008704552
||data quality control, data quality assessment, ontologies, Data streams|34|||Data Stream Management Systems (DSMS) provide real-time data processing in an effective way, but there is always a tradeoff between data quality (DQ) and performance. We propose an ontology-based data quality framework for relational DSMS that includes DQ measurement and monitoring in a transparent, modular, and flexible way. We follow a threefold approach that takes the characteristics of relational data stream management for DQ metrics into account. While (1) Query Metrics respect changes in data quality due to query operations, (2) Content Metrics allow the semantic evaluation of data in the streams. Finally, (3) Application Metrics allow easy user-defined computation of data quality values to account for application specifics. Additionally, a quality monitor allows us to observe data quality values and take counteractions to balance data quality and performance. The framework has been designed along a DQ management methodology suited for data streams. It has been evaluated in the domains of transportation systems and health monitoring.|10.1145/2968332|https://doi.org/10.1145/2968332|New York, NY, USA|Association for Computing Machinery||2016|Ontology-Based Data Quality Management for Data Streams|Geisler, Sandra and Quix, Christoph and Weber, Sven and Jarke, Matthias|article|10.1145/2968332|18|oct|J. Data and Information Quality|19361955|4|7|October 2016||||||||||||||||||||||899208086|833754770
||Supply Chain Management, Project Management, Laws of Geography, Domain Ontologies, Data Mining||1155-1164||We examine the existing goals of business- and geographic - information systems and their influence on logistics and supply chain management systems. Modelling supply chain management systems is held back because of lack of consistent and poorly aligned data with supply chain elements and processes. The issues constraining the decision-making process limit the connectivity between supply chains and geographically controlled database systems. The heterogeneous and unstructured data are added challenges to connectivity and integration processes. The research focus is on analysing the data heterogeneity and multidimensionality relevant to supply chain systems and geographically controlled databases. In pursuance of the challenges, a unified methodological framework is designed with data structuring, data warehousing and mining, visualization and interpretation artefacts to support connectivity and integration process. Multidimensional ontologies, ecosystem conceptualization and Big Data novelty are added motivations, facilitating the relationships between events of supply chain operations. The models construed for optimizing the resources are analysed in terms of effectiveness of the integrated framework articulations in global supply chains that obey laws of geography. The integrated articulations analysed with laws of geography can affect the operational costs, sure for better with reduced lead times and enhanced stock management.|https://doi.org/10.1016/j.procs.2019.09.284|https://www.sciencedirect.com/science/article/pii/S1877050919314814||||2019|On Modelling Big Data Guided Supply Chains in Knowledge-Base Geographic Information Systems|Shastri L Nimmagadda and Torsten Reiners and Lincoln C Wood|article|NIMMAGADDA20191155|||Procedia Computer Science|18770509||159||Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 23rd International Conference KES2019|||19700182801|0,334|-|76|1907|6483|38511|13722|6359|2,09|20,19|Netherlands|Western Europe|2010-2020|Computer Science (miscellaneous)||||900952911|2108686752
||Data models;Analytical models;Task analysis;Predictive models;Uncertainty;Biological system modeling;Numerical models;modeling;data quality;big data;Machine Learning;scheduling||33-40|2020 IEEE Sixth International Conference on Big Data Computing Service and Applications (BigDataService)|"\"Efforts on Big Data technologies have been highly directed towards the amount of data a task can access or crunch. Yet, for content-driven decision making, it is not (only) about the size, but about the \"\"right\"\" data: The number of available datasets (a different type of volume) can reach astronomical sizes, making a thorough evaluation of each input prohibitively expensive. The problem is exacerbated as data sources regularly exhibit varying levels of uncertainty and velocity/churn. To date, there exists no efficient method to quantify the impact of numerous available datasets over different analytics tasks and workflows. This visionary work puts the spotlight on data content rather than size. It proposes a novel modeling, planning and processing research bundle that assesses data quality in terms of analytics performance. The main expected outcome is to provide efficient, continuous and intelligent management and execution of content-driven data analytics. Intelligent dataset selection can achieve massive gains on both accuracy and time required to reach a desired level of performance. This work introduces the notion of utilizing dataset similarity to infer operator behavior and, consequently, be able to build scalable, operator-agnostic performance models for Big Data tasks over different domains. We present an overview of the promising results from our initial work with numerical and graph data and respective operators. We then describe a reference architecture with specific areas of research that need to be tackled in order to provide a data-centric analytics ecosystem.\""|10.1109/BigDataService49289.2020.00013|||||2020|Content-Based Analytics: Moving beyond Data Size|Tsoumakos, Dimitrios and Giannakopoulos, Ioannis|inproceedings|9179628||Aug|||||||||||||||||||||||||||902674298|42
||Big data, Analytics, Literature review, Value realization, Portability, Interconnectivity||191-209||Big data has been considered to be a breakthrough technological development over recent years. Notwithstanding, we have as yet limited understanding of how organizations translate its potential into actual social and economic value. We conduct an in-depth systematic review of IS literature on the topic and identify six debates central to how organizations realize value from big data, at different levels of analysis. Based on this review, we identify two socio-technical features of big data that influence value realization: portability and interconnectivity. We argue that, in practice, organizations need to continuously realign work practices, organizational models, and stakeholder interests in order to reap the benefits from big data. We synthesize the findings by means of an integrated model.|https://doi.org/10.1016/j.jsis.2017.07.003|https://www.sciencedirect.com/science/article/pii/S0963868717302615||||2017|Debating big data: A literature review on realizing value from big data|Wendy Arianne Günther and Mohammad H. {Rezazade Mehrizi} and Marleen Huysman and Frans Feldberg|article|GUNTHER2017191|||The Journal of Strategic Information Systems|09638687|3|26|||||12396|3,133|Q1|88|24|75|2111|965|61|12,45|87,96|Netherlands|Western Europe|1991-2020|Information Systems (Q1); Information Systems and Management (Q1); Management Information Systems (Q1)|2,945|11.022|0.00233|904170497|1997649283
||Java;Quality assurance;Semantics;Big Data;Tools;Syntactics;Cleaning;Data Collection;Data Analytics;Model-Driven Development;Historical Data;Data Parsing;Data Cleaning;Ethics;Data Assurance;Data Quality||1914-1923|2020 IEEE International Conference on Big Data (Big Data)|The project Death and Burial Data: Ireland 1864-1922 (DBDIrl) examines the relationship between historical death registration data and burial data to explore the history of power in Ireland from 1864 to 1922. Its core Big Data arises from historical records from a variety of heterogeneous sources, some aspects are pre-digitized and machine readable. A huge data set (over 4 million records in each source) and its slow manual enrichment (ca 7,000 records processed so far) pose issues of quality, scalability, and creates the need for a quality assurance technology that is accessible to non-programmers. An important goal for the researcher community is to produce a reusable, high-level quality assurance tool for the ingested data that is domain specific (historic data), highly portable across data sources, thus independent of storage technology.This paper outlines the step-wise design of the finer granular digital format, aimed for storage and digital archiving, and the design and test of two generations of the techniques, used in the first two data ingestion and cleaning phases.The first small scale phase was exploratory, based on metadata enrichment transcription to Excel, and conducted in parallel with the design of the final digital format and the discovery of all the domain-specific rules and constraints for the syntax and semantic validity of individual entries. Excel embedded quality checks or database-specific techniques are not adequate due to the technology independence requirement. This first phase produced a Java parser with an embedded data cleaning and evaluation classifier, continuously improved and refined as insights grew. The next, larger scale phase uses a bespoke Historian Web Application that embeds the Java validator from the parser, as well as a new Boolean classifier for valid and complete data assurance built using a Model-Driven Development technique that we also describe. This solution enforces property constraints directly at data capture time, removing the need for additional parsing and cleaning stages. The new classifier is built in an easy to use graphical technology, and the ADD-Lib tool it uses is a modern low-code development environment that auto-generates code in a large number of programming languages. It thus meets the technology independence requirement and historians are now able to produce new classifiers themselves without being able to program. We aim to infuse the project with computational and archival thinking in order to produce a robust data set that is FAIR compliant (Free Accessible Inter-operable and Re-useable).|10.1109/BigData50022.2020.9378148|||||2020|Towards Automatic Data Cleansing and Classification of Valid Historical Data An Incremental Approach Based on MDD|O’Shea, Enda and Khan, Rafflesia and Breathnach, Ciara and Margaria, Tiziana|inproceedings|9378148||Dec|||||||||||||||||||||||||||908804933|42
|||25|557–581||Profiling data to determine metadata about a given dataset is an important and frequent activity of any IT professional and researcher and is necessary for various use-cases. It encompasses a vast array of methods to examine datasets and produce metadata. Among the simpler results are statistics, such as the number of null values and distinct values in a column, its data type, or the most frequent patterns of its data values. Metadata that are more difficult to compute involve multiple columns, namely correlations, unique column combinations, functional dependencies, and inclusion dependencies. Further techniques detect conditional properties of the dataset at hand. This survey provides a classification of data profiling tasks and comprehensively reviews the state of the art for each class. In addition, we review data profiling tools and systems from research and industry. We conclude with an outlook on the future of data profiling beyond traditional profiling tasks and beyond relational databases.|10.1007/s00778-015-0389-y|https://doi.org/10.1007/s00778-015-0389-y|Berlin, Heidelberg|Springer-Verlag||2015|Profiling Relational Data: A Survey|Abedjan, Ziawasch and Golab, Lukasz and Naumann, Felix|article|10.1007/s00778-015-0389-y||aug|The VLDB Journal|10668888|4|24|August    2015||||13646|0,653|Q1|90|64|117|4824|544|113|4,46|75,38|United States|Northern America|1992-2020|Hardware and Architecture (Q1); Information Systems (Q1)|2,106|2.868|0.0023|915439956|924310569
||Design, Product development, Big data||149-152||Big data of online product purchases is an emerging source for obtaining customers’ preferences of product features for new product development. This paper proposes a framework and associated method for product features characterization and customers’ preference prediction based on online product purchase data. Specifications and components of products are firstly analyzed and the relationships between product specifications and components are then established for features characterization. The customers preferred specifications, features and their combinations are predicted for development of new products. The features characterization and customers’ preferences prediction of toy cars were used as an example of illustrating the proposed method.|https://doi.org/10.1016/j.cirp.2018.04.020|https://www.sciencedirect.com/science/article/pii/S0007850618300441||||2018|Product features characterization and customers’ preferences prediction based on purchasing data|Jian Zhang and Alessandro Simeone and Peihua Gu and Bo Hong|article|ZHANG2018149|||CIRP Annals|00078506|1|67|||||19804|2,370|Q1|155|144|472|4478|3073|472|5,27|31,10|United States|Northern America|1969-2020|Industrial and Manufacturing Engineering (Q1); Mechanical Engineering (Q1)||||916345384|1674304376
SIGMOD '15|Melbourne, Victoria, Australia|big data, telco churn prediction, customer retention|12|607–618|Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data|We show that telco big data can make churn prediction much more easier from the $3$V's perspectives: Volume, Variety, Velocity. Experimental results confirm that the prediction performance has been significantly improved by using a large volume of training data, a large variety of features from both business support systems (BSS) and operations support systems (OSS), and a high velocity of processing new coming data. We have deployed this churn prediction system in one of the biggest mobile operators in China. From millions of active customers, this system can provide a list of prepaid customers who are most likely to churn in the next month, having $0.96$ precision for the top $50000$ predicted churners in the list. Automatic matching retention campaigns with the targeted potential churners significantly boost their recharge rates, leading to a big business value.|10.1145/2723372.2742794|https://doi.org/10.1145/2723372.2742794|New York, NY, USA|Association for Computing Machinery|9781450327589|2015|Telco Churn Prediction with Big Data|Huang, Yiqing and Zhu, Fangzhou and Yuan, Mingxuan and Deng, Ke and Li, Yanhua and Ni, Bing and Dai, Wenyuan and Yang, Qiang and Zeng, Jia|inproceedings|10.1145/2723372.2742794|||||||||||||||||||||||||||||917234310|42
KDD '18|London, United Kingdom|data sturcture, data stream processing, probabilistic and approximate data|10|2584–2593|Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining|Data stream processing is a fundamental issue in many fields, such as data mining, databases, network traffic measurement. There are five typical tasks in data stream processing: frequency estimation, heavy hitter detection, heavy change detection, frequency distribution estimation, and entropy estimation. Different algorithms are proposed for different tasks, but they seldom achieve high accuracy and high speed at the same time. To address this issue, we propose a novel data structure named HeavyGuardian. The key idea is to intelligently separate and guard the information of hot items while approximately record the frequencies of cold items. We deploy HeavyGuardian on the above five typical tasks. Extensive experimental results show that HeavyGuardian achieves both much higher accuracy and higher speed than the state-of-the-art solutions for each of the five typical tasks. The source codes of HeavyGuardian and other related algorithms are available at GitHub.|10.1145/3219819.3219978|https://doi.org/10.1145/3219819.3219978|New York, NY, USA|Association for Computing Machinery|9781450355520|2018|HeavyGuardian: Separate and Guard Hot Items in Data Streams|Yang, Tong and Gong, Junzhi and Zhang, Haowei and Zou, Lei and Shi, Lei and Li, Xiaoming|inproceedings|10.1145/3219819.3219978|||||||||||||||||||||||||||||918230962|42
||Business, Economics, Information science, Big data analytics capabilities, Co-innovation, Big data, Co-creation||e02541||There are numerous emerging studies addressing big data and its application in different organizational aspects, especially regarding its impact on the business innovation process. This study in particular aims at analyzing the existing relationship between Big Data Analytics Capabilities and Co-innovation. To test the hypothesis model, structural equations by the partial least squares method were used in a sample of 112 Colombian firms. The main findings allow to positively relate Big Data Analytics Capabilities with better and more agile processes of product and service co-creation and with more robust collaboration networks with stakeholders internal and external to the firm.|https://doi.org/10.1016/j.heliyon.2019.e02541|https://www.sciencedirect.com/science/article/pii/S2405844019362012||||2019|Big data analytics capability and co-innovation: An empirical study|Nelson Lozada and Jose Arias-Pérez and Geovanny Perdomo-Charry|article|LOZADA2019e02541|||Heliyon|24058440|10|5|||||21100411756|0,455|Q1|28|2721|2689|132717|7639|2688|2,85|48,78|Netherlands|Western Europe|2015-2020|Multidisciplinary (Q1)||||920022417|1953619154
WSDM '20|Houston, TX, USA|controlled experiments, online metrics, a/b testing, user experience evaluation|4|877–880|Proceedings of the 13th International Conference on Web Search and Data Mining|"\"A/B Testing is the gold standard to estimate the causal relationship between a change in a product and its impact on key outcome measures. It is widely used in the industry to test changes ranging from simple copy change or UI change to more complex changes like using machine learning models to personalize user experience. The key aspect of A/B testing is evaluation of experiment results. Designing the right set of metrics - correct outcome measures, data quality indicators, guardrails that prevent harm to business, and a comprehensive set of supporting metrics to understand the \"\"why\"\" behind the key movements is the #1 challenge practitioners face when trying to scale their experimentation program [11, 14]. On the technical side, improving sensitivity of experiment metrics is a hard problem and an active research area, with large practical implications as more and more small and medium size businesses are trying to adopt A/B testing and suffer from insufficient power. In this tutorial we will discuss challenges, best practices, and pitfalls in evaluating experiment results, focusing on both lessons learned and practical guidelines as well as open research questions. A version of this tutorial was also present at KDD 2019 [23]. It was attended by around 150 participants.\""|10.1145/3336191.3371871|https://doi.org/10.1145/3336191.3371871|New York, NY, USA|Association for Computing Machinery|9781450368223|2020|Challenges, Best Practices and Pitfalls in Evaluating Results of Online Controlled Experiments|Gupta, Somit and Shi, Xiaolin and Dmitriev, Pavel and Fu, Xin and Mukherjee, Avijit|inproceedings|10.1145/3336191.3371871|||||||||||||||||||||||||||||920092231|42
|||1||||10.1145/3450751|https://doi.org/10.1145/3450751|New York, NY, USA|Association for Computing Machinery||2021|Introduction to the Special Issue on Learning-Based Support for Data Science Applications|Zhou, Ke and Song, Jingkuan|article|10.1145/3450751|9|apr|ACM/IMS Trans. Data Sci.|26911922|2|2|May 2021||||||||||||||||||||||920432679|961429963
||Digital manufacturing system, Information, Learning||668-692||Continued advancement of sensors has led to an ever-increasing amount of data of various physical nature to be acquired from production lines. As rich information relevant to the machines and processes are embedded within these “big data”, how to effectively and efficiently discover patterns in the big data to enhance productivity and economy has become both a challenge and an opportunity. This paper discusses essential elements of and promising solutions enabled by data science that are critical to processing data of high volume, velocity, variety, and low veracity, towards the creation of added-value in smart factories of the future.|https://doi.org/10.1016/j.cirp.2020.05.002|https://www.sciencedirect.com/science/article/pii/S0007850620301359||||2020|Big data analytics for smart factories of the future|Robert X. Gao and Lihui Wang and Moneer Helu and Roberto Teti|article|GAO2020668|||CIRP Annals|00078506|2|69|||||19804|2,370|Q1|155|144|472|4478|3073|472|5,27|31,10|United States|Northern America|1969-2020|Industrial and Manufacturing Engineering (Q1); Mechanical Engineering (Q1)||||925502787|1674304376
SCC '18|Portland, OR, USA|Transportation Data, Smart Cities, Data Management|8||Proceedings of the 1st ACM/EIGSCC Symposium on Smart Cities and Communities|From the time and money lost sitting in congestion and waiting for traffic signals to change, to the many people injured and killed in traffic crashes each year, to the emissions and energy consumption from our vehicles, the effects of transportation on our daily lives are immense. A wealth of transportation data is available to help address these problems; from data from sensors installed to monitor and operate the roadways and traffic signals to data from cell phone apps and -- just over the horizon -- data from connected vehicles and infrastructure. However, this wealth of data has yet to be effectively leveraged, thus providing opportunities in areas such as improving traffic safety, reducing congestion, improving traffic signal timing, personalizing routing, coordinating across transportation agencies and more. This paper presents opportunities and challenges in applying data management technology to the transportation domain.|10.1145/3236461.3241971|https://doi.org/10.1145/3236461.3241971|New York, NY, USA|Association for Computing Machinery|9781450357869|2018|Challenges and Opportunities in Transportation Data|Tufte, Kristin and Datta, Kushal and Jindal, Alekh and Maier, David and Bertini, Robert L.|inproceedings|10.1145/3236461.3241971|2||||||||||||||||||||||||||||925811400|42
||Cloud computing;Medical services;Mobile communication;Smart cities;Servers;Real-time systems;Quality of service;Smart health care;smart city;big data;quality of service (QoS);virtual machine migration;ant colony optimization||11887-11899||In recent years, the Smart City concept has become popular for its promise to improve the quality of life of urban citizens. The concept involves multiple disciplines, such as Smart health care, Smart transportation, and Smart community. Most services in Smart Cities, especially in the Smart healthcare domain, require the real-time sharing, processing, and analyzing of Big Healthcare Data for intelligent decision making. Therefore, a strong wireless and mobile communication infrastructure is necessary to connect and access Smart healthcare services, people, and sensors seamlessly, anywhere at any time. In this scenario, mobile cloud computing (MCC) can play a vital role by offloading Big Healthcare Data related tasks, such as sharing, processing, and analysis, from mobile applications to cloud resources, ensuring quality of service demands of end users. Such resource migration, which is also termed virtual machine (VM) migration, is effective in the Smart healthcare scenario in Smart Cities. In this paper, we propose an ant colony optimization-based joint VM migration model for a heterogeneous, MCC-based Smart Healthcare system in Smart City environment. In this model, the user’s mobility and provisioned VM resources in the cloud address the VM migration problem. We also present a thorough performance evaluation to investigate the effectiveness of our proposed model compared with the state-of-the-art approaches.|10.1109/ACCESS.2017.2707439|||||2017|Mobile Cloud-Based Big Healthcare Data Processing in Smart Cities|Islam, MD. Mofijul and Razzaque, MD. Abdur and Hassan, Mohammad Mehedi and Ismail, Walaa Nagy and Song, Biao|article|7933943|||IEEE Access|21693536||5|||||21100374601|0,587|Q1|127|18036|24267|771081|116691|24200|4,48|42,75|United States|Northern America|2013-2020|Computer Science (miscellaneous) (Q1); Engineering (miscellaneous) (Q1); Materials Science (miscellaneous) (Q2)|105,968|3.367|0.15396|927638597|1905633267
JCDL '14|London, United Kingdom|astronomy, digital libraries, sensor networks, small science, data management, biology, little science, big science, knowledge infrastructures, big data|10|257–266|Proceedings of the 14th ACM/IEEE-CS Joint Conference on Digital Libraries|The promise of technology-enabled, data-intensive scholarship is predicated upon access to knowledge infrastructures that are not yet in place. Scientific data management requires expertise in the scientific domain and in organizing and retrieving complex research objects. The Knowledge Infrastructures project compares data management activities of four large, distributed, multidisciplinary scientific endeavors as they ramp their activities up or down; two are big science and two are small science. Research questions address digital library solutions, knowledge infrastructure concerns, issues specific to individual domains, and common problems across domains. Findings are based on interviews (n=113 to date), ethnography, and other analyses of these four cases, studied since 2002. Based on initial comparisons, we conclude that the roles of digital libraries in scientific data management often depend upon the scale of data, the scientific goals, and the temporal scale of the research projects being supported. Digital libraries serve immediate data management purposes in some projects and long-term stewardship in others. In small science projects, data management tools are selected, designed, and used by the same individuals. In the multi-decade time scale of some big science research, data management technologies, policies, and practices are designed for anticipated future uses and users. The need for library, archival, and digital library expertise is apparent throughout all four of these cases. Managing research data is a knowledge infrastructure problem beyond the scope of individual researchers or projects. The real challenges lie in designing digital libraries to assist in the capture, management, interpretation, use, reuse, and stewardship of research data.||||IEEE Press|9781479955695|2014|The Ups and Downs of Knowledge Infrastructures in Science: Implications for Data Management|Borgman, Christine L. and Darch, Peter T. and Sands, Ashley E. and Wallis, Jillian C. and Traweek, Sharon|inproceedings|10.5555/2740769.2740814|||||||||||||||||||||||||||||929069584|42
||||640-642||The term big data has been popularized over the past decade and is often used to refer to data sets that are too large or complex to be analyzed by traditional means. Although the term has been utilized for some time in business and engineering, the concept of big data is relatively new to medicine. The reception from the medical community has been mixed; however, the widespread utilization of electronic health records in the United States, the creation of large clinical data sets and national registries that capture information on numerous vectors affecting healthcare delivery and patient outcomes, and the sequencing of the human genome are all opportunities to leverage big data. This review was inspired by a lively panel discussion on big data that took place at the 75th Central Surgical Association Annual Meeting. The authors’ aim was to describe big data, the methodologies used to analyze big data, and their practical clinical application.|https://doi.org/10.1016/j.surg.2018.06.022|https://www.sciencedirect.com/science/article/pii/S0039606018303660||||2018|Big data: More than big data sets|Adrienne N. Cobb and Andrew J. Benjamin and Erich S. Huang and Paul C. Kuo|article|COBB2018640|||Surgery|00396060|4|164|||||||||||||||||||||||931302690|1032196881
SIGMOD '16|San Francisco, California, USA|sampling, data quality, data cleaning, statistical cleaning, integrity constraints|6|2201–2206|Proceedings of the 2016 International Conference on Management of Data|Detecting and repairing dirty data is one of the perennial challenges in data analytics, and failure to do so can result in inaccurate analytics and unreliable decisions. Over the past few years, there has been a surge of interest from both industry and academia on data cleaning problems including new abstractions, interfaces, approaches for scalability, and statistical techniques. To better understand the new advances in the field, we will first present a taxonomy of the data cleaning literature in which we highlight the recent interest in techniques that use constraints, rules, or patterns to detect errors, which we call qualitative data cleaning. We will describe the state-of-the-art techniques and also highlight their limitations with a series of illustrative examples. While traditionally such approaches are distinct from quantitative approaches such as outlier detection, we also discuss recent work that casts such approaches into a statistical estimation framework including: using Machine Learning to improve the efficiency and accuracy of data cleaning and considering the effects of data cleaning on statistical analysis.|10.1145/2882903.2912574|https://doi.org/10.1145/2882903.2912574|New York, NY, USA|Association for Computing Machinery|9781450335317|2016|Data Cleaning: Overview and Emerging Challenges|Chu, Xu and Ilyas, Ihab F. and Krishnan, Sanjay and Wang, Jiannan|inproceedings|10.1145/2882903.2912574|||||||||||||||||||||||||||||931337114|42
SACMAT '20|Barcelona, Spain|privacy, data protection, access control, partial consent|10|71–80|Proceedings of the 25th ACM Symposium on Access Control Models and Technologies|The consent to personal data sharing is an integral part of modern access control models on smart devices. This paper examines the possibility of registering conditional consent which could potentially increase trust in data sharing. We introduce an indecisive state of consenting to policies that will enable consumers to evaluate data services before fully committing to their data sharing policies. We address technical, regulatory, social, individual and economic perspectives for inclusion of partial consent within an access control mechanism. Then, we look into the possibilities to integrate it within the access control model of Android by introducing an additional button in the interface--Maybe. This article also presents a design for such implementation and demonstrates feasibility by showcasing a prototype built on Android platform. Our effort is exploratory and aims to shed light on the probable research direction.|10.1145/3381991.3395603|https://doi.org/10.1145/3381991.3395603|New York, NY, USA|Association for Computing Machinery|9781450375689|2020|Accept - Maybe - Decline: Introducing Partial Consent for the Permission-Based Access Control Model of Android|Momen, Nurul and Bock, Sven and Fritsch, Lothar|inproceedings|10.1145/3381991.3395603|||||||||||||||||||||||||||||932366548|42
||Data integrity;Taxonomy;Standards organizations;Decision making;Organizations;Big Data;Data models;Data Quality;Big Data;Quality Dimensions;Quality Metrics;Metamodel;Assessment process;Improvement||1-9|2021 International Conference on Information Systems and Advanced Technologies (ICISAT)|Nowadays, data generation keeps increasing exponentially due to the emergence of the Internet of Things (IoT) and Big data technologies. The manipulation of such Big amount of data becomes more and more difficult because of its size and its variety. For better governance of organizations (decision making, data analysis, earnings increase …), data quality and data governance at present of Big data are two major pillars for the design of any system handling data within the organization. This explains the number of researches conducted as it constitutes a research subject with several gaps and opportunities. Many works were conducted to define and standardize Data Quality (DQ) and its dimensions, others were directed to design and propose data quality assessment and improvement models or frameworks. This work aims to recall the data quality principles starting by the needed background knowledge, then identify and compare the relevant taxonomies existing in the literature, next surveys and compares the available Data quality assessment and improvement approaches. After that, we propose a metamodel highlighting the main concepts of DQ assessment and we describe a generic process for DQ assessment and improvement. Finally, we evoke the main challenges in the field of DQ before and after the emergence of Big Data.|10.1109/ICISAT54145.2021.9678209|||||2021|A survey on data quality: principles, taxonomies and comparison of approaches|Yalaoui, Mehdi and Boukhedouma, Saida|inproceedings|9678209||Dec|||||||||||||||||||||||||||934849086|42
|||4|3441–3444||Since the introduction of Bitcoin---the first widespread application driven by blockchains---the interest in the design of blockchain-based applications has increased tremendously. At the core of these applications are consensus protocols that securely replicate client requests among all replicas, even if some replicas are Byzantine faulty. Unfortunately, these consensus protocols typically have low throughput, and this lack of performance is often cited as the reason for the slow wider adoption of blockchain technology. Consequently, many works focus on designing more efficient consensus protocols to increase throughput of consensus.We believe that this focus on consensus protocols only explains part of the story. To investigate this belief, we raise a simple question: Can a well-crafted system using a classical consensus protocol outperform systems using modern protocols? In this tutorial, we answer this question by diving deep into the design of blockchain systems. Further, we take an in-depth look at the theory behind consensus, which can help users select the protocol that best-fits their requirements. Finally, we share our vision of high-throughput blockchain systems that operate at large scales.|10.14778/3415478.3415565|https://doi.org/10.14778/3415478.3415565||VLDB Endowment||2020|Building High Throughput Permissioned Blockchain Fabrics: Challenges and Opportunities|Gupta, Suyash and Hellings, Jelle and Rahnama, Sajjad and Sadoghi, Mohammad|article|10.14778/3415478.3415565||sep|Proc. VLDB Endow.|21508097|12|13|August 2020||||21100199855|0,946|Q1|134|246|598|12401|3759|566|5,50|50,41|United States|Northern America|2008-2020|Computer Science (miscellaneous) (Q1)|7,198|2.047|0.00891|935517322|1216159931
||Digital Shadow, Information quality, Data quality, Latent dirichlet allocation (LDA)||304-310||The amount of data, which is created in companies is increasing due to modern communication technologies and decreasing costs for storing data. This leads to an advancement of methods for data analyses as well as to an increasing awareness of benefits resulting from data-based knowledge. In the context of product service systems and product development, there are two major concepts for providing product information. The digital twin collects every information possible, while the digital shadow provides a sufficient and content-related picture of the product. Since these concepts merge data from different sources, comprehension about information quality and its relation to the data quality becomes immanently important. This paper introduces a framework to determine information quality with respect to data-related and system-related attributes. An extensive literature review with focus on “information quality” and “data quality” identifies the important approaches for describing information and data quality. A latent dirichlet allocation (LDA) algorithm is applied on 371 definitions and identify 12 data-related and system-related attributes for information quality. Those attributes are assigned to six dimensions for information quality. So the proposed framework depicts the relationships between data attributes and the influence on information quality.|https://doi.org/10.1016/j.procir.2019.03.131|https://www.sciencedirect.com/science/article/pii/S2212827119304366||||2019|Framework for defining information quality based on data attributes within the digital shadow using LDA|Michael Riesener and Christian Dölle and Günther Schuh and Christian Tönnes|article|RIESENER2019304|||Procedia CIRP|22128271||83||11th CIRP Conference on Industrial Product-Service Systems|||21100243809|0,683|-|65|1456|3191|30451|8225|3145|2,40|20,91|Netherlands|Western Europe|2012-2020|Control and Systems Engineering; Industrial and Manufacturing Engineering||||936466924|2127027836
||Analytical models;Biological system modeling;Games;Machine learning;Nash equilibrium;Linear programming;Collaborative work;Federal learning;Incentive mechanism;Stackelberg game;Nash equilibrium;Specific indicators||475-478|2021 2nd International Conference on Big Data Economy and Information Management (BDEIM)|As a privacy-focused distributed machine learning, federated learning can not only train models effectively but also prevent data sets from being leaked easily. However, like crowdsensing perception and other technologies, participants often lack the motivation to learn and the quality of participation is not high. Therefore, this paper mainly designs a two-stage federal learning incentive mechanism based on the Stackelberg game model under a specific model accuracy index. Firstly, we combine data quality and data quantity to construct a federal learning incentive mechanism model under specific indicators. Then, we conduct a two-stage Stackelberg game analysis on the incentive mechanism model based on the utility function construction of the server platform and data island. In the first stage, the platform server is the leader and the data island is the follower. The second stage is a Nash equilibrium game between data islands. Finally, we construct the objective function of the server platform and data island, namely utility maximization, deduce the optimal equilibrium solution of the two-stage game, and determine the optimal strategy of the platform server and data island.|10.1109/BDEIM55082.2021.00103|||||2021|Design of two-stage federal learning incentive mechanism under specific indicators|Hu, Pan and Gu, Hailin and Qi, Jun and Gao, Qiang and Xia, Yu and Qu, Ruiting|inproceedings|9708977||Dec|||||||||||||||||||||||||||936467679|42
||||50-67||The era of big data has resulted in the development and applications of technologies and methods aimed at effectively using massive amounts of data to support decision-making and knowledge discovery activities. In this paper, the five Vs of big data, volume, velocity, variety, veracity, and value, are reviewed, as well as new technologies, including NoSQL databases that have emerged to accommodate the needs of big data initiatives. The role of conceptual modeling for big data is then analyzed and suggestions made for effective conceptual modeling efforts with respect to big data.|https://doi.org/10.1016/j.datak.2017.01.001|https://www.sciencedirect.com/science/article/pii/S0169023X17300277||||2017|Big data technologies and Management: What conceptual modeling can do|Veda C. Storey and Il-Yeol Song|article|STOREY201750|||Data & Knowledge Engineering|0169023X||108|||||24437|0,480|Q2|87|37|155|1512|440|152|2,55|40,86|Netherlands|Western Europe|1985, 1987-2020|Information Systems and Management (Q2)||||937975055|1516868485
PETRA '22|Corfu, Greece|Privacy Enhancing Technology, Data donation, Medical data protection, Differential Privacy|9|446–454|Proceedings of the 15th International Conference on PErvasive Technologies Related to Assistive Environments|Through the growing amount of personal health data collected by the individual itself digital data donations become more and more attractive. Wearables like Apple Watch or Fitbit trackers make tracking of heart rate, daily step counts and other lifestyle data easier than ever. While this data is collected on the dedicated device, it can help research in many promising ways. Even if the potential benefit of this data is very clear, there are open questions regarding privacy. Traditional privatization measures like anonymization and pseudonymization can only provide limited privacy guarantees especially with the growing amount of personalized data. To mitigate those risks privacy enhancing technologies like differential privacy can be used. While the theoretical foundation of such technologies is strong, only limited data is available about their practical use in large scale applications and the trade-off between privacy and utility. In this paper we will present a data donation scenario that is inspired by a real-world use case using lifestyle data for its analyses. We will apply the local differential privacy technology ”RAPPOR” to improve the privacy protection for the data donors and evaluate the impact of this technique to the data utility.|10.1145/3529190.3534768|https://doi.org/10.1145/3529190.3534768|New York, NY, USA|Association for Computing Machinery|9781450396318|2022|Towards Private Medical Data Donations by Using Privacy Preserving Technologies|"\"Appenzeller, Arno and Terzer, Nick and Krempel, Erik and Beyerer, J\"\"{u}rgen\""|inproceedings|10.1145/3529190.3534768|||||||||||||||||||||||||||||938106718|42
||Big Data Streams, Data Analytics, Condition Based Maintenance, Intelligent Transporta- tion Systems, Online Learning, Model Selection||437-446||Streaming Data Analysis (SDA) of Big Data Streams (BDS) for Condition Based Maintenance (CBM) in the context of Rail Transportation Systems (RTS) is a state-of-the-art field of re- search. SDA of BDS is the problem of analyzing, modeling and extracting information from huge amounts of data that continuously come from several sources in real time through com- putational aware solutions. Among others, CBM for Rail Transportation is one of the most challenging SDA problems, consisting of the implementation of a predictive maintenance system for evaluating the future status of the monitored assets in order to reduce risks related to failures and to avoid service disruptions. The challenge is to collect and analyze all the data streams that come from the numerous on-board sensors monitoring the assets. This paper deals with the problem of CBM applied to the condition monitoring and predictive maintenance of train axle bearings based on sensors data collection, with the purpose of maximizing their Remaining Useful Life (RUL). In particular we propose a novel algorithm for CBM based on SDA that takes advantage of the Online Support Vector Regression (OL-SVR) for predicting the RUL. The novelty of our proposal is the heuristic approach for optimizing the trade-off between the accuracy of the OL-SVR models and the computational time and resources needed in order to build them. Results from tests on a real-world dataset show the actual benefits brought by the proposed methodology.|https://doi.org/10.1016/j.procs.2015.07.321|https://www.sciencedirect.com/science/article/pii/S1877050915018244||||2015|Condition Based Maintenance in Railway Transportation Systems Based on Big Data Streaming Analysis|Emanuele Fumeo and Luca Oneto and Davide Anguita|article|FUMEO2015437|||Procedia Computer Science|18770509||53||INNS Conference on Big Data 2015 Program San Francisco, CA, USA 8-10 August 2015|||19700182801|0,334|-|76|1907|6483|38511|13722|6359|2,09|20,19|Netherlands|Western Europe|2010-2020|Computer Science (miscellaneous)||||944480417|2108686752
ICSE '22|Pittsburgh, Pennsylvania||12|262–273|Proceedings of the 44th International Conference on Software Engineering|Massive data from software repositories and collaboration tools are widely used to study social aspects in software development. One question that several recent works have addressed is how a software project's size and structure influence team productivity, a question famously considered in Brooks' law. Recent studies using massive repository data suggest that developers in larger teams tend to be less productive than smaller teams. Despite using similar methods and data, other studies argue for a positive linear or even super-linear relationship between team size and productivity, thus contesting the view of software economics that software projects are diseconomies of scale.In our work, we study challenges that can explain the disagreement between recent studies of developer productivity in massive repository data. We further provide, to the best of our knowledge, the largest, curated corpus of GitHub projects tailored to investigate the influence of team size and collaboration patterns on individual and collective productivity. Our work contributes to the ongoing discussion on the choice of productivity metrics in the operationalisation of hypotheses about determinants of successful software projects. It further highlights general pitfalls in big data analysis and shows that the use of bigger data sets does not automatically lead to more reliable insights.|10.1145/3510003.3510619|https://doi.org/10.1145/3510003.3510619|New York, NY, USA|Association for Computing Machinery|9781450392211|2022|Big Data = Big Insights? Operationalising Brooks' Law in a Massive GitHub Data Set|Gote, Christoph and Mavrodiev, Pavlin and Schweitzer, Frank and Scholtes, Ingo|inproceedings|10.1145/3510003.3510619|||||||||||||||||||||||||||||945808780|42
|||6|35–40||For big data, data quality problem is more serious. Big data cleaning system requires scalability and the abilityof handling mixed errors. Motivated by this, we develop Cleanix, a prototype system for cleaning relational Big Data. Cleanix takes data integrated from multiple data sources and cleans them on a shared-nothing machine cluster. The backend system is built on-top-of an extensible and flexible data-parallel substrate the Hyracks framework. Cleanix supports various data cleaning tasks such as abnormal value detection and correction, incomplete data filling, de-duplication, and conflict resolution. In this paper, we show the organization, data cleaning algorithms as well as the design of Cleanix.|10.1145/2935694.2935702|https://doi.org/10.1145/2935694.2935702|New York, NY, USA|Association for Computing Machinery||2016|Cleanix: A Parallel Big Data Cleaning System|Wang, Hongzhi and Li, Mingda and Bu, Yingyi and Li, Jianzhong and Gao, Hong and Zhang, Jiacheng|article|10.1145/2935694.2935702||may|SIGMOD Rec.|01635808|4|44|December 2015||||13622|0,372|Q2|142|39|81|808|127|57|1,67|20,72|United States|Northern America|1969, 1973-1978, 1981-2020|Information Systems (Q2); Software (Q2)|1,819|0.775|7.5E-4|946101782|962972343
||Big data, Tourism studies, Co-citation analysis, Network analysis, Research trends||100608||This study aims to provide a comprehensive network analysis to understand the current state of big data research in tourism by investigating multi-disciplinary contributions relevant to big data. A comprehensive network analytical method, which includes co-citation, clustering and trend analysis, is applied to systematically analyse publications from 2008 to 2017. Two unique data sets from Web of Science are collected. The first data set focuses on big data research in tourism and hospitality. The second data set involves other disciplines, such as computer science, for a comparison with tourism. Results suggest that applications of social media and user-generated content are gaining momentum, whereas theory-based studies on big data in tourism remain limited. Tourism and other relevant domains have similar concerns with the challenges involved in big data, such as privacy, data quality and appropriate data use. This comparative network analysis has implications for future big data research in tourism.|https://doi.org/10.1016/j.tmp.2019.100608|https://www.sciencedirect.com/science/article/pii/S2211973619301400||||2020|Network analysis of big data research in tourism|Xin Li and Rob Law|article|LI2020100608|||Tourism Management Perspectives|22119736||33|||||21100202157|1,454|Q1|43|140|277|12102|1824|274|6,77|86,44|United States|Northern America|2012-2020|Tourism, Leisure and Hospitality Management (Q1)|3,902|6.586|0.00445|946439305|434694424
||Smart agriculture, Pest identification, Information entropy, Data-centric||108322||Deep learning has played a crucial role in the field of smart agriculture and been widely used in various applications. However, the deep learning models are constrained by data quality, which means poor data quality and unreliable data annotation will seriously restrict the performance of smart applications. In this paper, we proposed two methods to assess data quality, named Bound-DE and Multi-Branch. In experiments, the IP06 dataset and the ResNet-18 backbone network were adopted. The results show that the redundancy of the used public dataset is so large that about 50% of the data can achieve the similar test accuracy. Furthermore, we also analyzed the high contributive samples and summarized the rules of those selected informative samples, which is significant for the design of high-efficiency datasets. In summary, this study guides and promotes the following data-centric research in the field of smart agriculture.|https://doi.org/10.1016/j.compeleceng.2022.108322|https://www.sciencedirect.com/science/article/pii/S0045790622005444||||2022|Data quality assessment and analysis for pest identification in smart agriculture|Jiachen Yang and Guipeng Lan and Yang Li and Yicheng Gong and Zhuo Zhang and Sezai Ercisli|article|YANG2022108322|||Computers and Electrical Engineering|00457906||103|||||18159|0,630|Q1|64|298|1040|7464|4626|979|4,79|25,05|United Kingdom|Western Europe|1973-1984, 1986-2020|Computer Science (miscellaneous) (Q1); Control and Systems Engineering (Q2); Electrical and Electronic Engineering (Q2)||||946872334|1285201041
||World Wide Web;Databases;Knowledge based systems;Computational fluid dynamics;Data mining;Web sites;Strain||261-264|2016 International Conference on Big Data and Smart Computing (BigComp)|It is critical to detect and correct information errors effectively to achieve higher data quality in many applications. Most existing techniques only use the intrinsic information to detect and correct a database, provided that data is adequate and well-structured. These techniques will not work properly if there is no sufficient data available. Integrating the information from external sources, like the World Wide Web (WWW), can help us overcome the shortcomings of existing techniques to a great extent. In this paper, we introduce our on-going work that is capable of detecting and correcting data errors in a database by integrating external information from the WWW. The goal of our research is to pursuit another effective way to enhance information quality.|10.1109/BIGCOMP.2016.7425923|||||2016|Web-based techniques for automatically detecting and correcting information errors in a database|Hailong Liu and Zhanhuai Li and Cheqing Jin and Qun Chen|inproceedings|7425923||Jan||23759356|||||||||||||||||||||||||951817450|1627774957
PyHPC'17|Denver, CO, USA||10||Proceedings of the 7th Workshop on Python for High-Performance and Scientific Computing|"\"We describe a new effort at the National Energy Research Scientific Computing Center (NERSC) in performance analysis and optimization of scientific Python applications targeting the Intel Xeon Phi (Knights Landing, KNL) manycore architecture. The Python-centered work outlined here is part of a larger effort called the NERSC Exascale Science Applications Program (NESAP) for Data. NESAP for Data focuses on applications that process and analyze high-volume, high-velocity data sets from experimental or observational science (EOS) facilities supported by the US Department of Energy Office of Science. We present three case study applications from NESAP for Data that use Python. These codes vary in terms of \"\"Python purity\"\" from applications developed in pure Python to ones that use Python mainly as a convenience layer for scientists without expertise in lower level programming languages like C, C++ or Fortran. The science case, requirements, constraints, algorithms, and initial performance optimizations for each code are discussed. Our goal with this paper is to contribute to the larger conversation around the role of Python in high-performance computing today and tomorrow, highlighting areas for future work and emerging best practices.\""|10.1145/3149869.3149873|https://doi.org/10.1145/3149869.3149873|New York, NY, USA|Association for Computing Machinery|9781450351249|2017|Python in the NERSC Exascale Science Applications Program for Data|Ronaghi, Zahra and Thomas, Rollin and Deslippe, Jack and Bailey, Stephen and Gursoy, Doga and Kisner, Theodore and Keskitalo, Reijo and Borrill, Julian|inproceedings|10.1145/3149869.3149873|4||||||||||||||||||||||||||||953882655|42
||Correlation;Agriculture;Vegetation;Geographic information systems;Economics;Information management;GIS;land circulation;right registration;spatial evolution||1-6|2016 Fifth International Conference on Agro-Geoinformatics (Agro-Geoinformatics)|The scale operation in various forms of rural land circulation in China is the way for the development of modern agriculture, and is also the basic direction of agricultural reform. The right to rural land contractual management registration makes the position of cadastral land and its interrelated information, as contractor and area, clear by Geographic Information Systems (GIS). That supplied a data base for land circulation spatial management. In this paper, we discussed the correlation between the management and the spatial position of transferred rural land, in order to support the policy and decision for agriculture. Taking Zhengjia Town as an example, which located in the west of Dongchangfu District, Liaocheng, Shandong, we made a survey and got the information about the land circulation, the operators who engaged in the transferred land, and the farmers who had transferred their farmlands from 2011 to 2015. Based on the outcome data of right to rural land contractual management registration, the circulation parcels and the cadastral parcels were linked by their only parcel code, and then formed the land circulation spatial information. Using ArcGIS 10.1 for spatial analysis and correlation analysis, we analyzed the correlation between the management and the spatial evolution of the transferred land on some indicators including acquirement intention, operation style, land use, circulation variability, distribution of circulation management right, and compared the income of the operators and the farmers. According to the survey, the scope of the transferred land was gradually increased. The area was 16.09ha in 2011 and 147.27ha in 2015, 19.22% of the contracted land. The results show that the main form of land circulation was to rent, and scale operation in northern and fragmented operation in southern. The transferred lands which were acquired by family contract have the largest area. However, in the south, the farmers seemed like to transfer their lands which were acquired by bidding. The operators aged in 40-49 managed the most transferred land, accounting for 77.00% of the total circulation area. Followed by 39 years old under, and the over 50 years old was the least. Before transferred, all farmland was used to grow grain. But 93% farmlands were changed the crops after land circulation. As the investment of time, capital, farmland and implied labor increased, there was a certain increase in income of the operators and the farmers, however, and a few operators had no profit because of the business model. There was no spatial correlation between the farmlands position and the farmers' income at the fragmented operation region. The results can provide an idea for spatial information management of land circulation based on GIS, and defend the farmers' contractual rights when the boundaries are destroyed. With the spatial informatization of farmland and the improvement of data quality and quantity, big data will further predict land circulation spatial arrangement and business model.|10.1109/Agro-Geoinformatics.2016.7577612|||||2016|Management and spatial evolution of rural land circulation: Taking Zhengjia Town as an example|Xue, Chunlu and Guo, Lin and Hu, Hualang and Pei, Zhiyuan|inproceedings|7577612||July|||||||||||||||||||||||||||954934799|42
BDE '22|Beijing, China|Drug Traceability, Traceability Verification, Big Data|7|1–7|Proceedings of the 4th International Conference on Big Data Engineering|In order to solve the storage and management problems of heterogeneous drug data, this paper uses big data technology to complete the cleaning and distributed storage of drug data, and improve the function of data sharing and traceability. At the same time, in order to improve the drug traceability function, ensure the reliable storage of traceability information, and make the traceability process more reliable. This paper will put forward a drug traceability system model based on big data on the basis of existing research. Secondly, an evidence chain framework is proposed to verify evidence files. At last, the simulation experiment is carried out to test and illustrate the credibility of the traceability verification model.|10.1145/3538950.3538951|https://doi.org/10.1145/3538950.3538951|New York, NY, USA|Association for Computing Machinery|9781450395632|2022|A Drug Safety Traceability Model Based on Big Data|Zhang, Lin and Jiang, Rong and Wang, Meng and Yang, Yue and Wang, Chenguang|inproceedings|10.1145/3538950.3538951|||||||||||||||||||||||||||||958890845|42
SIGMOD '14|Snowbird, Utah, USA|failures, result semantics, partial results|12|1275–1286|Proceedings of the 2014 ACM SIGMOD International Conference on Management of Data|"\"As the size and complexity of analytic data processing systems continue to grow, the effort required to mitigate faults and performance skew has also risen. However, in some environments we have encountered, users prefer to continue query execution even in the presence of failures (e.g., the unavailability of certain data sources), and receive a \"\"partial\"\" answer to their query. We explore ways to characterize and classify these partial results, and describe an analytical framework that allows the system to perform coarse to fine-grained analysis to determine the semantics of a partial result. We propose that if the system is equipped with such a framework, in some cases it is better to return and explain partial results than to attempt to avoid them.\""|10.1145/2588555.2612176|https://doi.org/10.1145/2588555.2612176|New York, NY, USA|Association for Computing Machinery|9781450323765|2014|Partial Results in Database Systems|Lang, Willis and Nehme, Rimma V. and Robinson, Eric and Naughton, Jeffrey F.|inproceedings|10.1145/2588555.2612176|||||||||||||||||||||||||||||960398830|42
MK Series on Business Intelligence||Hadoop, Big Data, cloud computing||185-197|Implementing Analytics|When the idea for this book was originally conceived, Big Data and Hadoop were not the most popular themes on the tech circuit, although cloud computing was somewhat more prominent. Some of the reviewer feedback suggested that these topics should be addressed in the context of the conceptual layout of analytics solutions. In this chapter their use in an overall analytics solution will be explained using the previous chapters as a foundation. Big Data, Hadoop, and cloud computing are presented as standalone material, each tying back into the overall analytics solution implementations presented in preceding chapters.|https://doi.org/10.1016/B978-0-12-401696-5.00011-6|https://www.sciencedirect.com/science/article/pii/B9780124016965000116|Boston|Morgan Kaufmann|978-0-12-401696-5|2013|Chapter 11 - Big Data, Hadoop, and Cloud Computing|Nauman Sheikh|incollection|SHEIKH2013185||||||||||Nauman Sheikh|||||||||||||||||||961387937|42
||Big Data;Data integration;Data analysis;Dimensionality reduction;Feature extraction;Data integrity;big data processing;data pre-processing;data cleansing;dimension reduction;data quality||241-247|2017 5th Intl Conf on Applied Computing and Information Technology/4th Intl Conf on Computational Science/Intelligence and Applied Informatics/2nd Intl Conf on Big Data, Cloud Computing, Data Science (ACIT-CSII-BCD)|In this paper, we briefly introduce some basic concepts and characteristics of big data. We are surrounded by massive amount of data but starving for knowledge. In the era of Big Data, how to quickly obtain high-quality and valuable information from massive amounts of data has become an important research direction. Hence, we focus our attention to the data pre-processing which is a sub-content of the data processing workflow. In this paper, the four phases of data pre-processing, including data cleansing, data integration, data reduction, and data transformation, have been discussed. And different approaches for a variety of purposes have been presented, which show current methods and techniques need to be further modified in order to improve the quality of data before data analysis.|10.1109/ACIT-CSII-BCD.2017.49|||||2017|A Survey on Big Data Pre-processing|Guan, Zhibin and Ji, Tongkai and Qian, Xu and Ma, Yan and Hong, Xuehai|inproceedings|8787092||July|||||||||||||||||||||||||||961750208|42
SoCC '17|Santa Clara, California|real-time analysis, oversampling, on-demand streaming, adaptive sampling, user-defined sampling, sensor sharing, sensor data|12|586–597|Proceedings of the 2017 Symposium on Cloud Computing|Real-time sensor data enables diverse applications such as smart metering, traffic monitoring, and sport analysis. In the Internet of Things, billions of sensor nodes form a sensor cloud and offer data streams to analysis systems. However, it is impossible to transfer all available data with maximal frequencies to all applications. Therefore, we need to tailor data streams to the demand of applications.We contribute a technique that optimizes communication costs while maintaining the desired accuracy. Our technique schedules reads across huge amounts of sensors based on the data-demands of a huge amount of concurrent queries. We introduce user-defined sampling functions that define the data-demand of queries and facilitate various adaptive sampling techniques, which decrease the amount of transferred data. Moreover, we share sensor reads and data transfers among queries. Our experiments with real-world data show that our approach saves up to 87% in data transmissions.|10.1145/3127479.3131621|https://doi.org/10.1145/3127479.3131621|New York, NY, USA|Association for Computing Machinery|9781450350280|2017|Optimized On-Demand Data Streaming from Sensor Nodes|Traub, Jonas and Bre\ss{}, Sebastian and Rabl, Tilmann and Katsifodimos, Asterios and Markl, Volker|inproceedings|10.1145/3127479.3131621|||||||||||||||||||||||||||||965594456|42
https://doi.org/10.1016/j.ajo.2015.09.028|https://www.sciencedirect.com/science/article/pii/S000293941500598X||||2015|How Big Data Informs Us About Cataract Surgery: The LXXII Edward Jackson Memorial Lecture|Anne Louise Coleman|article|COLEMAN20151091|||American Journal of Ophthalmology|00029394|6|160||||||||||||||||||||||||||||||966461419|42
WWW '21|Ljubljana, Slovenia|Data poisoning attacks, crowdsourcing, truth discovery|12|969–980|Proceedings of the Web Conference 2021|A key challenge of big data analytics is how to collect a large volume of (labeled) data. Crowdsourcing aims to address this challenge via aggregating and estimating high-quality data (e.g., sentiment label for text) from pervasive clients/users. Existing studies on crowdsourcing focus on designing new methods to improve the aggregated data quality from unreliable/noisy clients. However, the security aspects of such crowdsourcing systems remain under-explored to date. We aim to bridge this gap in this work. Specifically, we show that crowdsourcing is vulnerable to data poisoning attacks, in which malicious clients provide carefully crafted data to corrupt the aggregated data. We formulate our proposed data poisoning attacks as an optimization problem that maximizes the error of the aggregated data. Our evaluation results on one synthetic and two real-world benchmark datasets demonstrate that the proposed attacks can substantially increase the estimation errors of the aggregated data. We also propose two defenses to reduce the impact of malicious clients. Our empirical results show that the proposed defenses can substantially reduce the estimation errors of the data poisoning attacks.|10.1145/3442381.3450066|https://doi.org/10.1145/3442381.3450066|New York, NY, USA|Association for Computing Machinery|9781450383127|2021|Data Poisoning Attacks and Defenses to Crowdsourcing Systems|Fang, Minghong and Sun, Minghao and Li, Qi and Gong, Neil Zhenqiang and Tian, Jin and Liu, Jia|inproceedings|10.1145/3442381.3450066|||||||||||||||||||||||||||||970191497|42
||Artificial intelligence, Big data, Laboratory medicine, Machine learning, Pediatrics, Regulation||37-70|Biochemical and Molecular Basis of Pediatric Disease (Fifth Edition)|Clinical laboratories generate a large number of test results, creating opportunities for improved data management and the use of analytics. Aggregate analyses of these data have potential diagnostic value but require labs to utilize computational tools for the analysis of high-dimensional data. Machine learning can be used to aid decision-making, whether for clinical or operational purposes, using a variety of algorithms to analyze complex data sets and make reliable predictions. This chapter discusses key concepts related to big data and its application to pediatric laboratory medicine. Machine learning workflows, concepts, common algorithms, and related infrastructure requirements are also covered.|https://doi.org/10.1016/B978-0-12-817962-8.00018-4|https://www.sciencedirect.com/science/article/pii/B9780128179628000184||Academic Press|978-0-12-817962-8|2021|Chapter 3 - Machine learning and big data in pediatric laboratory medicine|Shannon Haymond and Randall K. Julian and Emily L. Gill and Stephen R. Master|incollection|HAYMOND202137|||||||||Fifth Edition|Dennis Dietzen and Michael Bennett and Edward Wong and Shannon Haymond|||||||||||||||||||971230326|42
||Human resource analytics, HRM analytics, People analytics, Adoption of HR analytics, Challenges, Implementation of HR analytics, Big data, Data analytics, Framework synthesis||311-326||Data analytics has gained importance in human resource management (HRM) for its ability to provide insights based on data-driven decision-making processes. However, integrating an analytics-based approach in HRM is a complex process, and hence, many organizations are unable to adopt HR Analytics (HRA). Using a framework synthesis approach, we first identify the challenges that hinder the practice of HRA and then develop a framework to explain the different factors that impact the adoption of HRA within organizations. This study identifies the key aspects related to the technological, organizational, environmental, data governance, and individual factors that influence the adoption of HRA. In addition, this paper determines 23 sub-dimensions of these five factors as the crucial aspects for successfully implementing and practicing HRA within organizations. We also discuss the implications of the framework for HR leaders, HR Managers, CEOs, IT Managers and consulting practitioners for effective adoption of HRA in organization.|https://doi.org/10.1016/j.jbusres.2021.03.054|https://www.sciencedirect.com/science/article/pii/S0148296321002174||||2021|Examining the determinants of successful adoption of data analytics in human resource management – A framework for implications|Sateesh.V. Shet and Tanuj Poddar and Fosso {Wamba Samuel} and Yogesh K. Dwivedi|article|SHET2021311|||Journal of Business Research|01482963||131|||||20550|2,049|Q1|195|878|1342|73221|11507|1315|7,38|83,40|United States|Northern America|1973-2021|Marketing (Q1)|46,935|7.550|0.03523|973104136|1502892296
BDIoT'19|Rabat, Morocco|Deep Learning, Arabic, Multi-layer Perceptron, Machine Learning, Sentiment Analysis|6||Proceedings of the 4th International Conference on Big Data and Internet of Things|Sentiment Analysis (SA), commonly known as opinion mining, during last couple of years, it becomes the fastest growing research areas in computer science. Conventionally, it helps to automatically detect if a text express is a positive, negative or neutral opinion. It enables us to identify and extract subjective information in a piece of writing, and this leads to gain an overview of wider public opinions or attitudes toward topics, products or services. Many researches have been done in this area, but most of them have focused on English and other Indo-European languages. Insufficient studies have actually accosted Sentiment Analysis in morphologically rich language such as Arabic. Regardless, given the increasing number of Arabic users and the exponential growth of online content, SA in this language has gained the attention of many researches last years, since Arabic raises many challenges because of its derivational, inflectional and agglutinative morphology. The objective of this paper is to promote the performance of Arabic Sentiment Analysis (ASA) by using Deep learning techniques. For that we implement Multi-Layer perceptron model in order to process and classify a dataset (Tweets). In fact, the experimental results prove that MLP as a deep learning model has a better performance for ASA than classical approaches.|10.1145/3372938.3372950|https://doi.org/10.1145/3372938.3372950|New York, NY, USA|Association for Computing Machinery|9781450372404|2020|Impact of Neural Network Architectures on Arabic Sentiment Analysis|Chahidi, Hamza and Omara, Hicham and Lazaar, Mohamed and Al Achhab, Mohammed|inproceedings|10.1145/3372938.3372950|12||||||||||||||||||||||||||||974494213|42
||Big Earth Data, Big data, Sustainable Development Goals (SDGs), Decision support, CASEarth, Digital Earth||1792-1801||The United Nations 2030 Agenda for Sustainable Development provides an important framework for economic, social, and environmental action. A comprehensive indicator system to aid in the systematic implementation and monitoring of progress toward the Sustainable Development Goals (SDGs) is unfortunately limited in many countries due to lack of data. The availability of a growing amount of multi-source data and rapid advancements in big data methods and infrastructure provide unique opportunities to mitigate these data shortages and develop innovative methodologies for comparatively monitoring SDGs. Big Earth Data, a special class of big data with spatial attributes, holds tremendous potential to facilitate science, technology, and innovation toward implementing SDGs around the world. Several programs and initiatives in China have invested in Big Earth Data infrastructure and capabilities, and have successfully carried out case studies to demonstrate their utility in sustainability science. This paper presents implementations of Big Earth Data in evaluating SDG indicators, including the development of new algorithms, indicator expansion (for SDG 11.4.1) and indicator extension (for SDG 11.3.1), introduction of a biodiversity risk index as a more effective analysis method for SDG 15.5.1, and several new high-quality data products, such as global net ecosystem productivity, high-resolution global mountain green cover index, and endangered species richness. These innovations are used to present a comprehensive analysis of SDGs 2, 6, 11, 13, 14, and 15 from 2010 to 2020 in China utilizing Big Earth Data, concluding that all six SDGs are on schedule to be achieved by 2030.|https://doi.org/10.1016/j.scib.2022.07.015|https://www.sciencedirect.com/science/article/pii/S2095927322002997||||2022|Measuring and evaluating SDG indicators with Big Earth Data|Huadong Guo and Dong Liang and Zhongchang Sun and Fang Chen and Xinyuan Wang and Junsheng Li and Li Zhu and Jinhu Bian and Yanqiang Wei and Lei Huang and Yu Chen and Dailiang Peng and Xiaosong Li and Shanlong Lu and Jie Liu and Zeeshan Shirazi|article|GUO20221792|||Science Bulletin|20959273|17|67|||||21100405003|1,983|Q1|112|365|833|12806|5639|616|7,21|35,08|Netherlands|Western Europe|2015-2020|Multidisciplinary (Q1)|8,832|11.780|0.0164|976987848|895863808
||Data profiling, Distributed processing engine, Data quality, Data management, Knowledge graph|26|851–876||Processing large-scale and highly interconnected Knowledge Graphs (KG) is becoming crucial for many applications such as recommender systems, question answering, etc. Profiling approaches have been proposed to summarize large KGs with the aim to produce concise and meaningful representation so that they can be easily managed. However, constructing profiles and calculating several statistics such as cardinality descriptors or inferences are resource expensive. In this paper, we present ABSTAT-HD, a highly distributed profiling tool that supports users in profiling and understanding big and complex knowledge graphs. We demonstrate the impact of the new architecture of ABSTAT-HD by presenting a set of experiments that show its scalability with respect to three dimensions of the data to be processed: size, complexity and workload. The experimentation shows that our profiling framework provides informative and concise profiles, and can process and manage very large KGs.|10.1007/s00778-021-00704-2|https://doi.org/10.1007/s00778-021-00704-2|Berlin, Heidelberg|Springer-Verlag||2021|ABSTAT-HD: A Scalable Tool for Profiling Very Large Knowledge Graphs|Alva Principe, Renzo Arturo and Maurino, Andrea and Palmonari, Matteo and Ciavotta, Michele and Spahiu, Blerina|article|10.1007/s00778-021-00704-2||sep|The VLDB Journal|10668888|5|31|Sep 2022||||13646|0,653|Q1|90|64|117|4824|544|113|4,46|75,38|United States|Northern America|1992-2020|Hardware and Architecture (Q1); Information Systems (Q1)|2,106|2.868|0.0023|978889408|924310569
||Crowdsourcing;Databases;Task analysis;Object recognition;Monitoring;Error analysis;Big Data;Quality management;quality control;data quality;duplicate detection;in-house crowdsourcing||90715-90730||While organizations in the current era of big data are generating massive volumes of data, they also need to ensure that its quality is maintained for it to be useful in decision-making purposes. The problem of dirty data plagues every organization. One aspect of dirty data is the presence of duplicate data records that negatively impact the organization's operations in many ways. Many existing approaches attempt to address this problem by using traditional data cleansing methods. In this paper, we address this problem by using an in-house crowdsourcing-based framework, namely, DedupCrowd. One of the main obstacles of crowdsourcing-based approaches is to monitor the performance of the crowd, by which the integrity of the whole process is maintained. In this paper, a statistical quality control-based technique is proposed to regulate the performance of the crowd. We apply our proposed framework in the context of a contact center, where the Customer Service Representatives are used as the crowd to assist in the process of deduplicating detection. By using comprehensive working examples, we show how the different modules of the DedupCrowd work not only to monitor the performance of the crowd but also to assist in duplicate detection.|10.1109/ACCESS.2019.2924979|||||2019|Quality Management of Workers in an In-House Crowdsourcing-Based Framework for Deduplication of Organizations’ Databases|Saberi, Morteza and Hussain, Omar Khadeer and Chang, Elizabeth|article|8746175|||IEEE Access|21693536||7|||||21100374601|0,587|Q1|127|18036|24267|771081|116691|24200|4,48|42,75|United States|Northern America|2013-2020|Computer Science (miscellaneous) (Q1); Engineering (miscellaneous) (Q1); Materials Science (miscellaneous) (Q2)|105,968|3.367|0.15396|980825032|1905633267
||Inverse theory;Time-series analysis;Seismic tomography||1145-1164||Seismic tomography has arrived at the threshold of the era of big data. However, how to extract information optimally from every available time-series remains a challenge; one that is directly related to the objective function chosen as a distance metric between observed and synthetic data. Time-domain cross-correlation and frequency-dependent multitaper traveltime measurements are generally tied to window selection algorithms in order to balance the amplitude differences between seismic phases. Even then, such measurements naturally favour the dominant signals within the chosen windows. Hence, it is difficult to select all usable portions of seismograms with any sort of optimality. As a consequence, information ends up being lost, in particular from scattered waves. In contrast, measurements based on instantaneous phase allow extracting information uniformly over the seismic records without requiring their segmentation. And yet, measuring instantaneous phase, like any other phase measurement, is impeded by phase wrapping. In this paper, we address this limitation by using a complex-valued phase representation that we call ‘exponentiated phase’. We demonstrate that the exponentiated phase is a good substitute for instantaneous-phase measurements. To assimilate as much information as possible from every seismogram while tackling the non-linearity of inversion problems, we discuss a flexible hybrid approach to combine various objective functions in adjoint seismic tomography. We focus on those based on the exponentiated phase, to take into account relatively small-magnitude scattered waves; on multitaper measurements of selected surface waves; and on cross-correlation measurements on specific windows to select distinct body-wave arrivals. Guided by synthetic experiments, we discuss how exponentiated-phase, multitaper and cross-correlation measurements, and their hybridization, affect tomographic results. Despite their use of multiple measurements, the computational cost to evaluate gradient kernels for the objective functions is scarcely affected, allowing for issues with data quality and measurement challenges to be simultaneously addressed efficiently.|10.1093/gji/ggaa063|||||2019|The exponentiated phase measurement, and objective-function hybridization for adjoint waveform tomography|Yuan, Yanhua O and Bozdağ, Ebru and Ciardelli, Caio and Gao, Fuchun and Simons, F J|article|9267091||Dec|Geophysical Journal International|1365246X|1|221|||||||||||||||||||||||982809708|1499633960
||Anomaly detection, Data repair, Geo-distributed big data, Spatial autocorrelation, Neural networks, Gradient-boosting||18-35||The increasing presence of geo-distributed sensor networks implies the generation of huge volumes of data from multiple geographical locations at an increasing rate. This raises important issues which become more challenging when the final goal is that of the analysis of the data for forecasting purposes or, more generally, for predictive tasks. This paper proposes a framework which supports predictive modeling tasks from streaming data coming from multiple geo-referenced sensors. In particular, we propose a distance-based anomaly detection strategy which considers objects described by embedding features learned via a stacked auto-encoder. We then devise a repair strategy which repairs the data detected as anomalous exploiting non-anomalous data measured by sensors in nearby spatial locations. Subsequently, we adopt Gradient Boosted Trees (GBTs) to predict/forecast values assumed by a target variable of interest for the repaired newly arriving (unlabeled) data, using the original feature representation or the embedding feature representation learned via the stacked auto-encoder. The workflow is implemented with distributed Apache Spark programming primitives and tested on a cluster environment. We perform experiments to assess the performance of each module, separately and in a combined manner, considering the predictive modeling of one-day-ahead energy production, for multiple renewable energy sites. Accuracy results show that the proposed framework allows reducing the error up to 13.56%. Moreover, scalability results demonstrate the efficiency of the proposed framework in terms of speedup, scaleup and execution time under a stress test.|https://doi.org/10.1016/j.bdr.2019.04.001|https://www.sciencedirect.com/science/article/pii/S2214579618302119||||2019|Anomaly Detection and Repair for Accurate Predictions in Geo-distributed Big Data|Roberto Corizzo and Michelangelo Ceci and Nathalie Japkowicz|article|CORIZZO201918|||Big Data Research|22145796||16|||||21100356018|0,565|Q2|25|11|76|547|324|69|4,06|49,73|United States|Northern America|2014-2020|Computer Science Applications (Q2); Information Systems (Q2); Information Systems and Management (Q2); Management Information Systems (Q2)|586|3.578|0.00126|982876618|1627174784
BIGDSE '15|Florence, Italy|system engineering, software architecture, embedded case study methodology, data system design methods, collaborative practice research, big data|7|44–50|Proceedings of the First International Workshop on BIG Data Software Engineering|Big data system development is dramatically different from small (traditional, structured) data system development. At the end of 2014, big data deployment is still scarce and failures abound. Outsourcing has become a main strategy for many enterprises. We therefore selected an outsourcing company who has successfully deployed big data projects for our study. Our research results from analyzing 10 outsourced big data projects provide a glimpse into early adopters of big data, illuminates the challenges for system development that stem from the 5Vs of big data and crystallizes the importance of architecture design choices and technology selection. We followed a collaborative practice research (CPR) method to develop and validate a new method, called BDD. BDD is the first attempt to systematically combine architecture design with data modeling approaches to address big data system development challenges. The use of reference architectures and a technology catalog are advancements to architecture design methods and are proving to be well-suited for big data system architecture design and system development.||||IEEE Press||2015|Big Data System Development: An Embedded Case Study with a Global Outsourcing Firm|Chen, Hong-Mei and Kazman, Rick and Haziyev, Serge and Hrytsay, Olha|inproceedings|10.5555/2819289.2819302|||||||||||||||||||||||||||||988358248|42
Emerging Topics in Computer Science and Applied Computing||Elderly care, personalized care, independent living, ACTVAGE, Big Data analytics, CAPIM, lifestyle-oriented, context-awareness, framework||99-124|Applied Computing in Medicine and Health|Owing to the growing increase in the world’s ageing population, research has focused on developing information and communication technology (ICT)–based services for personalized care, improved health, and quality social life for the elderly. Recent efforts explore Big Data in order to build mathematical models of personal behavior and lifestyle for analytics. Leveraging Big Data analytics holds enormous potential for solving some of the biggest and most intractable challenges in personalized elderly care through quantified modeling of a person’s lifestyle in a way that takes cognizance of their beliefs, values, and preferences, and connects to a history of events, things, and places around which they have progressively built their lives. However, the idea of discovering patterns to personalize care and inform critical health care decisions for the elderly is challenged as data grow exponentially in volume, become faster and increasingly unstructured, and are generated from sociodigital engagements that often may not accurately reflect the real-world entities and contexts they represent. As a result, the idea raises issues along several dimensions, including social, technical, and context-aware challenges. In this chapter, we present an overview of the state of the art in personalized elderly care, and explore the opportunities and inherent sociotechnical challenges in leveraging Big Data analytics to support elderly care and independent living. Based on this discussion, and arguing that analytics need to take account of the contexts that shape the generation and use of data, ACTVAGE, a context-aware lifestyle-oriented framework for personalized elderly care and independent living is proposed.|https://doi.org/10.1016/B978-0-12-803468-2.00005-9|https://www.sciencedirect.com/science/article/pii/B9780128034682000059|Boston|Morgan Kaufmann|978-0-12-803468-2|2016|Chapter 5 - Leveraging Big Data Analytics for Personalized Elderly Care: Opportunities and Challenges|Obinna Anya and Hissam Tawfik|incollection|ANYA201699||||||||||Dhiya Al-Jumeily and Abir Hussain and Conor Mallucci and Carol Oliver|||||||||||||||||||990057136|42
ICEME 2020|Beijing, China|Case study, Data empowerment, Media organization, Big data|4|153–156|2020 The 11th International Conference on E-Business, Management and Economics|Big data brings opportunities for the development of the media industry and also poses serious challenges. During the digital transformation period, media organizations have used third-party big data to achieve digital transformation and build a media digital ecology, which has become the focus of strategic development at this stage. This article takes the application of the data assets of China Unicom Big Data Co., Ltd. (UBD) in the media industry as an example, and explores the difficulties and attempts made by UBD. through in-depth interviews. The countermeasures are summarized from different aspects such as technology and data governance, which provide a reference for the digital transformation of media organizations.|10.1145/3414752.3414772|https://doi.org/10.1145/3414752.3414772|New York, NY, USA|Association for Computing Machinery|9781450388016|2020|How to Be an Enabler of Digital Transformation for Media Organizations?|Qin, Haiqing and Liu, Xiaohan and Qin, Haiqi and Zhu, Jiang|inproceedings|10.1145/3414752.3414772|||||||||||||||||||||||||||||991272897|42
||Recurrent neural networks;Power supplies;Time series analysis;Predictive models;Big Data applications;Prediction algorithms;Data models;Recurrent neural network;Long Short-Term Memory neural network;regional monthly electricity sales;electricity sales forecast;big data application service platform||229-233|2020 IEEE 3rd International Conference on Electronics and Communication Engineering (ICECE)|Regional monthly electricity sales forecast is an important basis for regional power grid planning and construction, evaluation of regional economic development and operation, and protection of residents' lives. It is also an important work of regional power regulation and management, decision-making of power generation and purchase, improvement of power supply equipment utilization rate and deepening of power system reform. Based on the current situation of power supply enterprise information development, distribution network business status and characteristics, this paper analyzes the factors affecting electricity sales. According to the characteristics of annual changes in electricity sales and data quality factors, the recurrent neural network model is selected based on the big data application service platform. The long short term memory neural network model performs multi-step multivariate prediction on time series, and uses the attention mechanism to combine two independent models for prediction. Experiments conducted on the historical electricity sales data set of a power supply company show that compared with traditional machine learning methods, this method has advantages in accuracy and efficiency.|10.1109/ICECE51594.2020.9352886|||||2020|Regional Electricity Sales Forecasting Research Based on Big Data Application Service Platform|Qi, Cui and Mingyue, Sun and Na, Mi and Honggang, Wang and Yanhong, Jian and Jing, Zhu|inproceedings|9352886||Dec|||||||||||||||||||||||||||992128706|42
||Aviation, Flight turnaround events, Digital twin, Internet of Things, Data modelling, Big data||101723||The aerospace sector is one of the many sectors in which large amounts of data are generated. Thanks to the evolution of technology, these data can be exploited in several ways to improve the operation and management of industrial processes. However, to achieve this goal, it is necessary to define architectures and data models that allow to manage and homogenise the heterogeneous data collected. In this paper, we present an Airport Digital Twin Reference Conceptualisation’s and data model based on FIWARE Generic Enablers and the Next Generation Service Interfaces-Linked Data standard. Concretely, we particularise the Airport Digital Twin to improve the efficiency of flight turnaround events. The architecture proposed is validated in the Aberdeen International Airport with the aim of reducing delays in commercial flights. The implementation includes an application that shows the real state of the airport, combining two-dimensional and three-dimensional virtual reality representations of the stands, and a mobile application that helps ground operators to schedule departure and arrival flights.|https://doi.org/10.1016/j.aei.2022.101723|https://www.sciencedirect.com/science/article/pii/S1474034622001811||||2022|Applying digital twins for the management of information in turnaround event operations in commercial airports|Javier Conde and Andres Munoz-Arcentales and Mario Romero and Javier Rojo and Joaquín Salvachúa and Gabriel Huecas and Álvaro Alonso|article|CONDE2022101723|||Advanced Engineering Informatics|14740346||54|||||23640|1,107|Q1|81|146|295|8184|1973|289|6,41|56,05|United Kingdom|Western Europe|2002-2020|Artificial Intelligence (Q1); Information Systems (Q1)|4,432|5.603|0.00428|994510479|876312848
BigDataScience '14|Beijing, China|Hadoop, Big data, R, Mining, Visualization|6||Proceedings of the 2014 International Conference on Big Data Science and Computing|Compared to the traditional data storing, processing, analyzing and visualization which have been performed, Big data requires evolutionary technologies of massive data processing on distributed and parallel systems, such as Hadoop system. Big data analytic systems, thus, have been popular to derive important decision making in various areas. However, visualization on analytic system faces various limitation due to the huge amount of data. This brings the necessity of interactive visualization techniques beyond the traditional static visualization. R has been used and improved for a big data analysis and mining tool. Also, R is supported with various and abundant packages for different targets with visualization. However interactive visualization packages are not easily found in the market. This paper compares and analyzes interactive web packages with visualization packages for R. This paper also proposes interactive web visualized analysis environment for big data with a combination of interactive web packages and visualization packages. In particular, Big data analysis techniques with sensed data are presented as the result by reflecting the decision view on sensing field.|10.1145/2640087.2644168|https://doi.org/10.1145/2640087.2644168|New York, NY, USA|Association for Computing Machinery|9781450328913|2014|Big Data Analysis with Interactive Visualization Using R Packages|Cho, Wonhee and Lim, Yoojin and Lee, Hwangro and Varma, Mohan Krishna and Lee, Moonsoo and Choi, Eunmi|inproceedings|10.1145/2640087.2644168|18||||||||||||||||||||||||||||996654302|42
SA '18|Tokyo, Japan||96||SIGGRAPH Asia 2018 Courses|"\"Four \"\"Paradigms\"\" of Science• Empirical Science• Theoretical Science• Computational Science• Data Science\""|10.1145/3277644.3277803|https://doi.org/10.1145/3277644.3277803|New York, NY, USA|Association for Computing Machinery|9781450360265|2018|Visual Analytics of Big Networks: Novel Approaches for Exploring Complex Networks in Big Data|Filonik, Daniel and Bednarz, Tomasz|inproceedings|10.1145/3277644.3277803|18||||||||||||||||||||||||||||1001925193|42
KDD '21|Virtual Event, Singapore|fairness, convergence, machine learning, robustness|2|4046–4047|Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining|Responsible AI becomes critical where robustness and fairness must be satisfied together. Traditionally, the two topics have been studied by different communities for different applications. Robust training is designed for noisy or poisoned data where image data is typically considered. In comparison, fair training primarily deals with biased data where structured data is typically considered. Nevertheless, robust training and fair training are fundamentally similar in considering that both of them aim at fixing the inherent flaws of real-world data. In this tutorial, we first cover state-of-the-art robust training techniques where most of the research is on combating various label noises. In particular, we cover label noise modeling, robust training approaches, and real-world noisy data sets. Then, proceeding to the related fairness literature, we discuss pre-processing, in-processing, and post-processing unfairness mitigation techniques, depending on whether the mitigation occurs before, during, or after the model training. Finally, we cover the recent trend emerged to combine robust and fair training in two flavors: the former is to make the fair training more robust (i.e., robust fair training), and the latter is to consider robustness and fairness as two equals to incorporate them into a holistic framework. This tutorial is indeed timely and novel because the convergence of the two topics is increasingly common, but yet to be addressed in tutorials. The tutors have extensive experience publishing papers in top-tier machine learning and data mining venues and developing machine learning platforms.|10.1145/3447548.3470799|https://doi.org/10.1145/3447548.3470799|New York, NY, USA|Association for Computing Machinery|9781450383325|2021|Machine Learning Robustness, Fairness, and Their Convergence|Lee, Jae-Gil and Roh, Yuji and Song, Hwanjun and Whang, Steven Euijong|inproceedings|10.1145/3447548.3470799|||||||||||||||||||||||||||||1002229236|42
||data profession, data scientist, informatics, advanced analytics, computing, data analysis, data economy, big data analytics, data DNA, data analytics, Big data, data education, data engineering, data innovation, data science, statistics, data service, data industry|42|||The 21st century has ushered in the age of big data and data economy, in which data DNA, which carries important knowledge, insights, and potential, has become an intrinsic constituent of all data-based organisms. An appropriate understanding of data DNA and its organisms relies on the new field of data science and its keystone, analytics. Although it is widely debated whether big data is only hype and buzz, and data science is still in a very early phase, significant challenges and opportunities are emerging or have been inspired by the research, innovation, business, profession, and education of data science. This article provides a comprehensive survey and tutorial of the fundamental aspects of data science: the evolution from data analysis to data science, the data science concepts, a big picture of the era of data science, the major challenges and directions in data innovation, the nature of data analytics, new industrialization and service opportunities in the data economy, the profession and competency of data education, and the future of data science. This article is the first in the field to draw a comprehensive big picture, in addition to offering rich observations, lessons, and thinking about data science and analytics.|10.1145/3076253|https://doi.org/10.1145/3076253|New York, NY, USA|Association for Computing Machinery||2017|Data Science: A Comprehensive Overview|Cao, Longbing|article|10.1145/3076253|43|jun|ACM Comput. Surv.|03600300|3|50|May 2018||||23038|2,079|Q1|163|112|365|15859|6978|365|19,16|141,60|United States|Northern America|1969-2020|Computer Science (miscellaneous) (Q1); Theoretical Computer Science (Q1)|10,790|10.282|0.01438|1003216690|1517405264
ARES 21|Vienna, Austria|Incident Response, Incident Handling, Cyber Situational Awareness, Network Monitoring, Cybersecurity|8||Proceedings of the 16th International Conference on Availability, Reliability and Security|In this paper, we describe a system for the continuous collection of data for the needs of network security management. When a cybersecurity incident occurs in the network, the contextual information on the involved assets facilitates estimating the severity and impact of the incident and selecting an appropriate incident response. We propose a system based on the combination of active and passive network measurements and the correlation of the data with third-party systems. The system enumerates devices and services in the network and their vulnerabilities via fingerprinting of operating systems and applications. Further, the system pairs the hosts in the network with contacts on responsible administrators and highlights critical infrastructure and its dependencies. The system concentrates all the information required for common incident handling procedures and aims to speed up incident response, reduce the time spent on the manual investigation, and prevent errors caused by negligence or lack of information.|10.1145/3465481.3470037|https://doi.org/10.1145/3465481.3470037|New York, NY, USA|Association for Computing Machinery|9781450390514|2021|System for Continuous Collection of Contextual Information for Network Security Management and Incident Handling|Hus\'{a}k, Martin and La\v{s}tovi\v{c}ka, Martin and Tovar\v{n}\'{a}k, Daniel|inproceedings|10.1145/3465481.3470037|112||||||||||||||||||||||||||||1004963636|42
||Data life cycle, Data creation, Data usability, Healthcare, Big data||364-371||Data life cycle management is very much useful for any enterprise or application where data is being used and processed for producing results. Data’s appearance for a certain period time ensures accessibility and usability in the system. Data generated through different sources and it is available in various forms for accessibility. A big data-based application such as the healthcare sector generates lots of data through sensors and other electronic devices which can be further classified into a model for report generations and predictions for various purposes for the benefits of patients and hospitals as well. The data life cycle presents the entire data process in the system. The lifecycle of data starts from creation, store, usability, sharing, and archive and destroy in the system and applications. It defines the data flow in an organization. For the successful implementation of the model, there is a need to maintain the life cycle of data under a secured system of data management. This paper deals with the data life cycle with different steps and various works are done for data management in different sectors and benefits of the data life cycle for industrial and healthcare applications including challenges, conclusions, and future scope.|https://doi.org/10.1016/j.procs.2020.06.042|https://www.sciencedirect.com/science/article/pii/S1877050920315465||||2020|Data Life Cycle Management in Big Data Analytics|Kumar Rahul and Rohitash Kumar Banyal|article|RAHUL2020364|||Procedia Computer Science|18770509||173||International Conference on Smart Sustainable Intelligent Computing and Applications under ICITETM2020|||19700182801|0,334|-|76|1907|6483|38511|13722|6359|2,09|20,19|Netherlands|Western Europe|2010-2020|Computer Science (miscellaneous)||||1008390095|2108686752
||Interactive Visualization, Magic Leap, Human-Computer Interaction, Immersive Technologies, Biomedical Visualization, Spatial Computing, Mixed Reality||||Many recent breakthroughs in medical diagnostics and drug discovery arise from deploying machine learning algorithms to large-scale data sets. However, a significant obstacle to such approaches is that they depend on high-quality annotations generated by domain experts. This study develops and evaluates BioLumin, a novel immersive mixed reality environment that enables users to virtually shrink down to the microscopic level for navigation and annotation of 3D reconstructed images. We discuss how domain experts were consulted in the specification of a pipeline to enable automatic reconstruction of biological models for mixed reality environments, driving the design of a 3DUI system to explore whether such a system allows accurate annotation of complex medical data by non-experts. To examine the usability and feasibility of BioLumin, we evaluated our prototype through a multi-stage mixed-method approach. First, three domain experts offered expert reviews, and subsequently, nineteen non-expert users performed representative annotation tasks in a controlled setting. The results indicated that the mixed reality system was learnable and that non-experts could generate high-quality 3D annotations after a short training session. Lastly, we discuss design considerations for future tools like BioLumin in medical and more general scientific contexts.|10.1145/3548777|https://doi.org/10.1145/3548777|New York, NY, USA|Association for Computing Machinery||2022|BioLumin: An Immersive Mixed Reality Experience for Interactive Microscopic Visualization and Biomedical Research Annotation|Elor, Aviv and Whittaker, Steve and Kurniawan, Sri and Michael, Sam|article|10.1145/3548777||jul|ACM Trans. Comput. Healthcare|26911957||||Just Accepted|||||||||||||||||||||1009200795|1983512862
||Big data, Floating car data, Urban road network, Traffic congestion, Road intersection||320-339||Locating the bottlenecks in cities where traffic congestion usually occurs is essential prior to solving congestion problems. Therefore, this paper proposes a low-frequency probe vehicle data (PVD)-based method to identify turn-level intersection traffic congestion in an urban road network. This method initially divides an urban area into meter-scale square cells and maps PVD into those cells and then identifies the cells that correspond to road intersections by taking advantage of the fixed-location stop-and-go characteristics of traffic passing through intersections. With those rasterized road intersections, the proposed method recognizes probe vehicles’ turning directions and provides preliminary analysis of traffic conditions at all turning directions. The proposed method is map-independent (i.e., no digital map is needed) and computationally efficient and is able to rapidly screen most of the intersections for turn-level congestion in a road network. Thereby, this method is expected to greatly decrease traffic engineers’ workloads by providing information regarding where and when to investigate and solve traffic congestion problems.|https://doi.org/10.1016/j.trc.2019.10.001|https://www.sciencedirect.com/science/article/pii/S0968090X18312543||||2019|Network-wide identification of turn-level intersection congestion using only low-frequency probe vehicle data|Zhengbing He and Geqi Qi and Lili Lu and Yanyan Chen|article|HE2019320|||Transportation Research Part C: Emerging Technologies|0968090X||108|||||20893|3,185|Q1|133|327|834|17795|8956|824|10,15|54,42|United Kingdom|Western Europe|1993-2020|Automotive Engineering (Q1); Civil and Structural Engineering (Q1); Computer Science Applications (Q1); Management Science and Operations Research (Q1); Transportation (Q1)||||1009685244|307592163
||Big data analytics, Manufacturing firms, Supply chain and logistics management, LARG, Business performance and innovation, Sustainability||102170||The effect of big data on the lean, agile, resilient, and green (LARG) supply chain has not been explored much in the literature. This study investigates the role of ‘Big Data Analytics’ (BDA) as a mediator between ‘sustainable supply chain business performance’ and key factors, namely, lean practices, social practices, environmental practices, organisational practices, supply chain practices, financial practices, and total quality management. A sample of 297 responses from thirty-seven Indian manufacturing firms was collected. The paper is beneficial for managers and practitioners to understand supply chain analytics, and it addresses challenges in the management of LARG practices to contribute to a sustainable supply chain.|https://doi.org/10.1016/j.tre.2020.102170|https://www.sciencedirect.com/science/article/pii/S1366554520308139||||2021|Big Data Analytics as a mediator in Lean, Agile, Resilient, and Green (LARG) practices effects on sustainable supply chains|Rakesh D. Raut and Sachin Kumar Mangla and Vaibhav S. Narwane and Manoj Dora and Mengqi Liu|article|RAUT2021102170|||Transportation Research Part E: Logistics and Transportation Review|13665545||145|||||20909|2,042|Q1|110|233|507|13126|4018|499|7,63|56,33|United Kingdom|Western Europe|1997-2020|Business and International Management (Q1); Civil and Structural Engineering (Q1); Management Science and Operations Research (Q1); Transportation (Q1)||||1010891757|1729083653
||Robust statistics, L-moments, L-statistics, Skewness, Kurtosis||107632||An important challenge in big data is identification of important variables. For this purpose, methods of discovering variables with non-standard univariate marginal distributions are proposed. The conventional moments based summary statistics can be well-adopted, but their sensitivity to outliers can lead to selection based on a few outliers rather than distributional shape such as bimodality. To address this type of non-robustness, the L-moments are considered. Using these in practice, however, has a limitation since they do not take zero values at the Gaussian distributions to which the shape of a marginal distribution is most naturally compared. As a remedy, Gaussian Centered L-moments are proposed, which share advantages of the L-moments, but have zeros at the Gaussian distributions. The strength of Gaussian Centered L-moments over other conventional moments is shown in theoretical and practical aspects such as their performances in screening important genes in cancer genetics data.|https://doi.org/10.1016/j.csda.2022.107632|https://www.sciencedirect.com/science/article/pii/S0167947322002122||||2023|Variable screening based on Gaussian Centered L-moments|Hyowon An and Kai Zhang and Hannu Oja and J.S. Marron|article|AN2023107632|||Computational Statistics & Data Analysis|01679473||179|||||28461|1,093|Q1|115|168|502|6401|1122|495|2,14|38,10|Netherlands|Western Europe|1983-2021|Applied Mathematics (Q1); Computational Mathematics (Q1); Computational Theory and Mathematics (Q1); Statistics and Probability (Q2)||||1011877652|1791479902
||Carbon emission, data-oriented, MRV, big data||537-544||Shipping is a heavily regulated industry and responsible for around 3% of global carbon emissions. Global trade is highly dependent on shipping which covers around 90% of commercial demand. Now the industry is expected to navigate through many twists and turns of different situations like upcoming regulations, climate change, energy shortages and technological revolutions. Technological development is apparent across all marine sectors due to the rapid development of sensor technology, IT, automation and robotics. The industry must continue to develop at a rapid pace over the next decade in order to be able to adapt to upcoming regulations and market pressure. Ship intelligence will be the driving force shaping the future of the industry. Ships generate a large volume of data from different sources and in different formats. So big data has become the talk of the industry nowadays. Big data analysis discovers correlations between different measurable or unmeasurable parameters to determine hidden patterns and trends. This analysis will have a significant impact on vessel performance monitoring and provide performance prediction, real-time transparency, and decision-making support to the ship operator. Big data will also bring new opportunities and challenges for the maritime industry. It will increase the capability of performance monitoring, remove human error and increase interdependencies of components. However, the industry will have to face many challenges such as data processing, reliability, and data security. Many regulations rely on ship data including the new EU MRV (Monitoring, Reporting and Verification) regulation to quantify the CO2 emissions for ships above 5000 gross tonnage. As a result, ship operators will have to monitor and report the verified amount of CO2 emitted by their vessels on voyages to, from and between EU ports and will also be required to provide information on energy efficiency parameters. The MRV is a data-oriented regulation requiring ship operators to capture and monitor the ship emissions and other related data and although it is a regional regulation at the moment there is scope for the International Maritime Organisation (IMO) to implement it globally in the near future.|https://doi.org/10.1016/j.proeng.2017.08.182|https://www.sciencedirect.com/science/article/pii/S1877705817333386||||2017|Challenges and Opportunities of Big Data Analytics for Upcoming Regulations and Future Transformation of the Shipping Industry|Ibna Zaman and Kayvan Pazouki and Rose Norman and Shervin Younessi and Shirley Coleman|article|ZAMAN2017537|||Procedia Engineering|18777058||194||10th International Conference on Marine Technology, MARTEC 2016|||18700156717|0,320|-|74|0|5873|0|9870|5804|1,88|0,00|Netherlands|Western Europe|2009-2019|Engineering (miscellaneous)||||1012350096|506091674
||Algorithm design and analysis;Cleaning;Maintenance engineering;Feature extraction;Sorting;Classification algorithms;data quality;template;NFA;extensional B tree||1563-1567|2017 4th International Conference on Systems and Informatics (ICSAI)|Data quality is the core problem in the field of big data application. In practical applications, data are often derived from multi-source databases, which can cause named conflict and similar duplicate record problems. This paper proposed a method to solve the named conflict problem and similar duplicate record based on a given template. In order to find the relationship between the template data and the data to be unified, we segmented the data that to be unified, then built an extensional B tree based on these data. At the same time, we construct a NFA for each template value. By using these NFA, we can get the language pattern for each column in the template relation. Finally, we search each template value in the extensional B tree, if the template value can be found and the corresponding data to be unified can be accepted by the NFA based on the template value, we can use the template value to replace the data that need to be recovered. Then, the data can be consolidated and integrated to ensure the consistency and integrity of the data.|10.1109/ICSAI.2017.8248534|||||2017|Incomplete relation revision method based on template|Zhang, Heng and Liu, Guohua and Zhao, Wenfeng and Ni, Mengfei|inproceedings|8248534||Nov|||||||||||||||||||||||||||1013128816|42
||Data integrity;Knowledge based systems;Transportation;Big Data;knowledge graph;entity alignment;conflict resolution||104-111|2022 8th International Conference on Big Data and Information Analytics (BigDIA)|Knowledge graphs (KGs) have provided a better approach to organize and manage multi-source knowledge in the field of civilian transportation equipment. However, due to the existence of isolated data islands, there are still problems such as unidentified attribute relations in the KGs for civilian transportation equipment. An effective solution to these problems can be achieved through entity alignment. In this paper, we propose two entity alignment strategies for different purposes. Specifically, aiming at the incompleteness of single-source knowledge in the construction of KGs for civilian transportation equipment, we employ an instance alignment strategy to enable multi-source fusion of attribute relationships. In addition, aiming at the inconsistency between different attribute values of the same entity, we propose a conflict resolution strategy based on multi-source data quality assessment, and achieve the unity of attribute values. Compared with previous KGs, our proposed entity alignment method is able to moderately enrich the attribute relations in KGs for civilian transportation equipment.|10.1109/BigDIA56350.2022.9874246|||||2022|Entity Alignment in the Construction of Knowledge Graphs for Civilian Transportation Equipment|Zhang, Liang and Wang, Jia and Cui, Dandan and Fu, Anna and Liu, Xiulei and Xu, Daozhu|inproceedings|9874246||Aug||27716902|||||||||||||||||||||||||1013798292|980068981
||Federated learning, participant contribution estimation, neural network|24|||The rapid development of modern artificial intelligence technique is mainly attributed to sufficient and high-quality data. However, in the data collection, personal privacy is at risk of being leaked. This issue can be addressed by federated learning, which is proposed to achieve efficient model training among multiple data providers without direct data access and aggregation. To encourage more parties owning high-quality data to participate in the federated learning, it is important to evaluate and reward the participant contribution in a reasonable, robust, and efficient manner. To achieve this goal, we propose a novel contribution estimation method: Intrinsic Performance Influence-based Contribution Estimation (IPICE). In particular, the class-level intrinsic performance influence is adopted as the contribution estimation criteria in IPICE, and a neural network is employed to exploit the non-linear relationship between the performance change and estimated contribution. Extensive experiments are conducted on various datasets, and the results demonstrate that IPICE is more accurate and stable than the counterpart in various data distribution settings. The computational complexity is significantly reduced in our IPICE, especially when a new party joins the federation. IPICE assigns small contributions to bad/garbage data and thus prevent them from participating and deteriorating the learning ecosystem.|10.1145/3523059|https://doi.org/10.1145/3523059|New York, NY, USA|Association for Computing Machinery||2022|Intrinsic Performance Influence-Based Participant Contribution Estimation for Horizontal Federated Learning|Zhang, Lin and Fan, Lixin and Luo, Yong and Duan, Ling-Yu|article|10.1145/3523059|88|sep|ACM Trans. Intell. Syst. Technol.|21576904|6|13|December 2022||||||||||||||||||||||1013846172|273436860
MSR '19|Montreal, Quebec, Canada|software supply chain, software mining, software ecosystem|12|143–154|Proceedings of the 16th International Conference on Mining Software Repositories|Open source software (OSS) is essential for modern society and, while substantial research has been done on individual (typically central) projects, only a limited understanding of the periphery of the entire OSS ecosystem exists. For example, how are tens of millions of projects in the periphery interconnected through technical dependencies, code sharing, or knowledge flows? To answer such questions we a) create a very large and frequently updated collection of version control data for FLOSS projects named World of Code (WoC) and b) provide basic tools for conducting research that depends on measuring interdependencies among all FLOSS projects. Our current WoC implementation is capable of being updated on a monthly basis and contains over 12B git objects. To evaluate its research potential and to create vignettes for its usage, we employ WoC in conducting several research tasks. In particular, we find that it is capable of supporting trend evaluation, ecosystem measurement, and the determination of package usage. We expect WoC to spur investigation into global properties of OSS development leading to increased resiliency of the entire OSS ecosystem. Our infrastructure facilitates the discovery of key technical dependencies, code flow, and social networks that provide the basis to determine the structure and evolution of the relationships that drive FLOSS activities and innovation.|10.1109/MSR.2019.00031|https://doi.org/10.1109/MSR.2019.00031||IEEE Press||2019|World of Code: An Infrastructure for Mining the Universe of Open Source VCS Data|Ma, Yuxing and Bogart, Chris and Amreen, Sadika and Zaretzki, Russell and Mockus, Audris|inproceedings|10.1109/MSR.2019.00031|||||||||||||||||||||||||||||1013986262|42
GeoHumanities'18|Seattle, WA, USA|Africa, hotspot detection, geospatial autocorrelation, political violence, knowledge discovery|10||Proceedings of the 2nd ACM SIGSPATIAL Workshop on Geospatial Humanities|With recent increases in incidences of political violence globally, the world has now become more uncertain and less predictable. Of particular concern is the case of violence against civilians, who are often caught in the crossfire between armed state or non-state actors. Classical methods of studying political violence and international relations need to be updated. Adopting the use of data analytic tools and techniques of studying big data would enable academics and policy makers to make sense of a rapidly changing world.|10.1145/3282933.3282935|https://doi.org/10.1145/3282933.3282935|New York, NY, USA|Association for Computing Machinery|9781450360326|2018|Is There Space for Violence? A Data-Driven Approach to the Exploration of Spatial-Temporal Dimensions of Conflict|Mack, Vincent Z. W. and Kam, Tin Seong|inproceedings|10.1145/3282933.3282935|1||||||||||||||||||||||||||||1014845692|42
||Big data, Data mining, Machine learning, Predictive modeling, Intensive care unit||300-304||The digitalization of the Intensive Care Unit (ICU) led to an increasing amount of clinical data being collected at the bedside. The term “Big Data” can be used to refer to the analysis of these datasets that collect enormous amount of data of different origin and format. Complexity and variety define the value of Big Data. In fact, the retrospective analysis of these datasets allows to generate new knowledge, with consequent potential improvements in the clinical practice. Despite the promising start of Big Data analysis in medical research, which has seen a rising number of peer-reviewed articles, very limited applications have been used in ICU clinical practice. A close future effort should be done to validate the knowledge extracted from clinical Big Data and implement it in the clinic. In this article, we provide an introduction to Big Data in the ICU, from data collection and data analysis, to the main successful examples of prognostic, predictive and classification models based on ICU data. In addition, we focus on the main challenges that these models face to reach the bedside and effectively improve ICU care.|https://doi.org/10.1016/j.jcrc.2020.09.002|https://www.sciencedirect.com/science/article/pii/S0883944120306791||||2020|Data-driven ICU management: Using Big Data and algorithms to improve outcomes|Giorgia Carra and Jorge I.F. Salluh and Fernando José {da Silva Ramos} and Geert Meyfroidt|article|CARRA2020300|||Journal of Critical Care|08839441||60|||||||||||||||||||||||1015818315|1415751128
||Blockchain, Data quality, Distributed ledger technology, Meat industry, Internet of things, Food supply chains||100261||The application of blockchain in food supply chains does not resolve conventional IoT data quality issues. Data on a blockchain may simply be immutable garbage. In response, this paper reports our observations and learnings from an ongoing beef supply chain project that integrates Blockchain and IoT for supply chain event tracking and beef provenance assurance and proposes two solutions for data integrity and trust in the Blockchain and IoT-enabled food supply chain. Rather than aiming for absolute truth, we explain how applying the notion of ‘common knowledge’ fundamentally changes oracle identity and data validity practices. Based on the learnings derived from leading an IoT supply chain project with a focus on beef exports from Australia to China, our findings unshackle IoT and Blockchain from being used merely to collect lag indicators of past states and liberate their potential as lead indicators of desired future states. This contributes: (a) to limit the possibility of capricious claims on IoT data performance, and; (b) to utilise mechanism design as an approach by which supply chain behaviours that increase the probability of desired future states being realised can be encouraged.|https://doi.org/10.1016/j.jii.2021.100261|https://www.sciencedirect.com/science/article/pii/S2452414X21000595||||2022|Garbage in garbage out: The precarious link between IoT and blockchain in food supply chains|Warwick Powell and Marcus Foth and Shoufeng Cao and Valéri Natanelov|article|POWELL2022100261|||Journal of Industrial Information Integration|2452414X||25|||||21100787106|2,042|Q1|24|28|86|2152|1361|83|12,26|76,86|Netherlands|Western Europe|2016-2020|Industrial and Manufacturing Engineering (Q1); Information Systems and Management (Q1)|1,149|10.063|0.00148|1018761893|121356201
ICGDA '18|Prague, Czech Republic|data science, geospatial big data, data mining, big data, machine learning|5|98–102|Proceedings of the International Conference on Geoinformatics and Data Analysis|Geospatial Big Data analytics are changing the way that businesses operate in many industries. Although a good number of research works have reported in the literature on geospatial data analytics and real-time data processing of large spatial data streams, only a few have addressed the full geospatial big data analytics project lifecycle and geospatial data science project lifecycle. Big data analysis differs from traditional data analysis primarily due to the volume, velocity and variety characteristics of the data being processed. One of a motivation of introducing new framework is to address these big data analysis challenges. Geospatial data science projects differ from most traditional data analysis projects because they could be complex and in need of advanced technologies in comparison to the traditional data analysis projects. For this reason, it is essential to have a process to govern the project and ensure that the project participants are competent enough to carry on the process. To this end, this paper presents, new geospatial big data mining and machine learning framework for geospatial data acquisition, data fusion, data storing, managing, processing, analysing, visualising and modelling and evaluation. Having a good process for data analysis and clear guidelines for comprehensive analysis is always a plus point for any data science project. It also helps to predict required time and resources early in the process to get a clear idea of the business problem to be solved.|10.1145/3220228.3220236|https://doi.org/10.1145/3220228.3220236|New York, NY, USA|Association for Computing Machinery|9781450364454|2018|A New Data Science Framework for Analysing and Mining Geospatial Big Data|Saraee, Mo and Silva, Charith|inproceedings|10.1145/3220228.3220236|||||||||||||||||||||||||||||1020335497|42
||Maintenance Management, Decision Support, Data Quality, Data-driven Decisions, Organisational Factors, Smart Maintenance||95-100||Technological development and innovations has been the focus of research in the field of smart maintenance, whereas there is less research regarding how maintenance organisations adapt the development. This case study focuses to understand what constraints maintenance organisations in the transition into applying more data-driven decisions in maintenance. This paper aims to emphasize the organisational challenges in data-driven maintenance, such as trustworthiness of data-driven decisions, data quality, management and competences. Through a case study at a global company in the automotive industry these challenges are highlighted and discussed through a questionnaire survey participated by 72 people and interviews with 7 people from the maintenance organisation.|https://doi.org/10.1016/j.ifacol.2020.11.015|https://www.sciencedirect.com/science/article/pii/S2405896320301592||||2020|Organisational Constraints in Data-driven Maintenance: a case study in the automotive industry|P Savolainen and J Magnusson and M. Gopalakrishnan and E. {Turanoglu Bekar} and A. Skoogh|article|SAVOLAINEN202095|||IFAC-PapersOnLine|24058963|3|53||4th IFAC Workshop on Advanced Maintenance Engineering, Services and Technologies - AMEST 2020|||21100456158|0,308|Q3|72|115|8407|1913|9863|8351|1,13|16,63|Austria|Western Europe|2002-2019|Control and Systems Engineering (Q3)||||1021266398|676980763
||consumer finance, P2P lending, user profiling, Personal credit scoring, features, social data|38|||With the booming popularity of online social networks like Twitter and Weibo, online user footprints are accumulating rapidly on the social web. Simultaneously, the question of how to leverage the large-scale user-generated social media data for personal credit scoring comes into the sight of both researchers and practitioners. It has also become a topic of great importance and growing interest in the P2P lending industry. However, compared with traditional financial data, heterogeneous social data presents both opportunities and challenges for personal credit scoring. In this article, we seek a deep understanding of how to learn users’ credit labels from social data in a comprehensive and efficient way. Particularly, we explore the social-data-based credit scoring problem under the micro-blogging setting for its open, simple, and real-time nature. To identify credit-related evidence hidden in social data, we choose to conduct an analytical and empirical study on a large-scale dataset from Weibo, the largest and most popular tweet-style website in China. Summarizing results from existing credit scoring literature, we first propose three social-data-based credit scoring principles as guidelines for in-depth exploration. In addition, we glean six credit-related insights arising from empirical observations of the testbed dataset. Based on the proposed principles and insights, we extract prediction features mainly from three categories of users’ social data, including demographics, tweets, and networks. To harness this broad range of features, we put forward a two-tier stacking and boosting enhanced ensemble learning framework. Quantitative investigation of the extracted features shows that online social media data does have good potential in discriminating good credit users from bad. Furthermore, we perform experiments on the real-world Weibo dataset consisting of more than 7.3 million tweets and 200,000 users whose credit labels are known through our third-party partner. Experimental results show that (i) our approach achieves a roughly 0.625 AUC value with all the proposed social features as input, and (ii) our learning algorithm can outperform traditional credit scoring methods by as much as 17% for social-data-based personal credit scoring.|10.1145/2996465|https://doi.org/10.1145/2996465|New York, NY, USA|Association for Computing Machinery||2016|From Footprint to Evidence: An Exploratory Study of Mining Social Data for Credit Scoring|Guo, Guangming and Zhu, Feida and Chen, Enhong and Liu, Qi and Wu, Le and Guan, Chu|article|10.1145/2996465|22|dec|ACM Trans. Web|15591131|4|10|December 2016||||5800207369|0,438|Q2|44|18|72|1309|250|71|4,46|72,72|United States|Northern America|2007-2020|Computer Networks and Communications (Q2)|782|2.043|7.7E-4|1021350292|1451030700
ICSET 2021|Taipei, Taiwan|Private information, Data protection, Public policy, E-society|6|171–176|2021 5th International Conference on E-Society, E-Education and E-Technology||10.1145/3485768.3485770|https://doi.org/10.1145/3485768.3485770|New York, NY, USA|Association for Computing Machinery|9781450390156|2021|Data Protection Measures in E-Society: Policy Implications of British Data Protection Act to China|Guo, Yuanyuan|inproceedings|10.1145/3485768.3485770|||||||||||||||||||||||||||||1022548953|42
||DATA CHECK;DATA QUALITY IDENTIFICATION;DECISION TREE;STATE ESTIMATION;BIG DATA||1-6|8th Renewable Power Generation Conference (RPG 2019)|Under the background of active distribution network, this paper proposes a state estimation algorithm of managing analysis data to solve the problem of big data, data missing and complex analysis. This paper proposes an active distribution network state estimation algorithm based on decision tree self-identification. Setting appropriate quality weight of big data based on the check rules different from traditional single-phase currents. Data in the input state estimation model is better compatible by estimating the pre-processed data, classifying and correcting the data including voltage and current. Moreover, on the premise of lacking of distributed energy measurement devices, this paper establishes a state estimation model for distributed power, which is used to correct the default data of distributed energy and improve the quality of input data in wind power and photovoltaic. The method can be verified in the actual example. Compared with the traditional state estimation, the active distribution network state estimation algorithm based on decision tree self-identification has better estimation effect and faster iteration speed. Therefore, the proposed algorithm can be effectively applied to the current state estimation of large-scale distributed energy access.|10.1049/cp.2019.0490|||||2019|Active distribution network state estimation algorithm based on decision tree of self-identification|Ding, Jian and Ma, Chunlei and Fu, Bin and Liu, Bing|inproceedings|9041478||Oct|||||||||||||||||||||||||||1025277771|42
||Internet of Things finance, Credit evaluation and early warning, Factor analysis, Particle swarm optimization, Big data driven||295-307||The big data technology framework has been successfully used in the Internet of Things, and the financial industry also hopes to use the advanced technology of big data to integrate and improve internal and external data related to credit risks. Relying on more efficient machine learning algorithms to get a reasonable prediction of credit risk can reduce the self-generated losses of the Internet of Things finance and increase profits. This article uses distributed search engine technology to customize web crawlers to obtain the required bank card and transaction data from the multi-source heterogeneous data of the Internet of Things financial industry, design the corresponding Spark parallel algorithm to preprocess the data, and establish an inverted table and two Level index file provides data source for big data analysis platform. After the data source is determined, the Mutually Exclusive Collectively Exhaustive (MECE) analysis method is combined with the scores of many financial business experts in the industry to obtain a set of candidate indicators and quantification methods for the financial credit risk evaluation of the Internet of Things, and analyze the correlation of indicators and risk grading. The random forest algorithm in the big data machine learning library is used to select the feature of the candidate index set, and a multi-level spatial association rule algorithm based on the Hash structure is designed to mine the financial risk information of the Internet of Things, and build a credit risk assessment and intelligent early warning model. This paper selects 26 indicators of Internet of Things finance as the research objects, uses SPSS26.0 software to perform sample Kaiser–Meyer–Olkin (KMO) test and Bartlett sphere test on the original data, and describes the results of factor analysis in detail. The particle swarm algorithm is introduced into the parameter optimization of random forest, and the financial credit risk assessment model of the Internet of Things is established. The results show that this method can significantly reduce the probability of banks making the first and second error rates when evaluating the credit risk of financing the Internet of Things Finance. This is conducive to the smooth development of the Internet of Things financial business for banks, which enables banks to enhance their own profitability while effectively reducing losses due to incorrect credit provision.|https://doi.org/10.1016/j.future.2021.06.003|https://www.sciencedirect.com/science/article/pii/S0167739X21001977||||2021|Big data driven Internet of Things for credit evaluation and early warning in finance|Chunhui Wen and Jinhai Yang and Liu Gan and Yang Pan|article|WEN2021295|||Future Generation Computer Systems|0167739X||124|||||12264|1,262|Q1|119|798|1935|36054|16877|1868|9,11|45,18|Netherlands|Western Europe|1984-2021|Computer Networks and Communications (Q1); Hardware and Architecture (Q1); Software (Q1)||||1025357312|562237118
||Filling;Power systems;Random forests;Interpolation;Data models;Big Data;Data mining;Big data cleaning;missing data filling;data preprocessing;random forest;data quality||33-39||Missing data filling is a key step in power big data preprocessing, which helps to improve the quality and the utilization of electric power data. Due to the limitations of the traditional methods of filling missing data, an improved random forest filling algorithm is proposed. As a result of the horizontal and vertical directions of the electric power data are based on the characteristics of time series. Therefore, the method of improved random forest filling missing data combines the methods of linear interpolation, matrix combination and matrix transposition to solve the problem of filling large amount of electric power missing data. The filling results show that the improved random forest filling algorithm is applicable to filling electric power data in various missing forms. What's more, the accuracy of the filling results is high and the stability of the model is strong, which is beneficial in improving the quality of electric power data.|10.23919/CJEE.2019.000025|||||2019|A missing power data filling method based on improved random forest algorithm|Deng, Wei and Guo, Yixiu and Liu, Jie and Li, Yong and Liu, Dingguo and Zhu, Liang|article|8950481||Dec|Chinese Journal of Electrical Engineering|20961529|4|5|||||21101018551||-|2|35|0|1113|0|0|0,00|31,80|United States|Northern America|2020|Control and Systems Engineering; Electrical and Electronic Engineering; Energy Engineering and Power Technology||||1029389302|1961283240
||Feature extraction;Predictive models;Data models;Quality assessment;Product design;Data mining;Analytical models;Industrial artificial intelligence (AI);industrial big data;Industrial Internet of Things;product quality prediction;wide-deep-sequence (WDS) model||3721-3731||Product quality prediction, as an important issue of industrial intelligence, is a typical task of industrial process analysis, in which product quality will be evaluated and improved as feedback for industrial process adjustment. Data-driven methods, with predictive model to analyze various industrial data, have been received considerable attention in recent years. However, to get an accurate prediction, it is an essential issue to extract quality features from industrial data, including several variables generated from supply chain and time-variant machining process. In this article, a data-driven method based on wide-deep-sequence (WDS) model is proposed to provide a reliable quality prediction for industrial process with different types of industrial data. To process industrial data of high redundancy, in this article, data reduction is first conducted on different variables by different techniques. Also, an improved wide-deep (WD) model is proposed to extract quality features from key time-invariant variables. Meanwhile, an long short-term memory (LSTM)-based sequence model is presented for exploring quality information from time-domain features. Under the joint training strategy, these models will be combined and optimized by a designed penalty mechanism for unreliable predictions, especially on reduction of defective products. Finally, experiments on a real-world manufacturing process data set are carried out to present the effectiveness of the proposed method in product quality prediction.|10.1109/TNNLS.2020.3001602|||||2020|A Wide-Deep-Sequence Model-Based Quality Prediction Method in Industrial Process Analysis|Ren, Lei and Meng, Zihao and Wang, Xiaokang and Lu, Renquan and Yang, Laurence T.|article|9126214||Sep.|IEEE Transactions on Neural Networks and Learning Systems|21622388|9|31|||||||||||||||||||||||1029974783|1596358290
||Big data ;Hadoop ;HDFS ;Security||596-601||Big data is the collection and analysis of large set of data which holds many intelligence and raw information based on user data, Sensor data, Medical and Enterprise data. The Hadoop platform is used to Store, Manage, and Distribute Big data across several server nodes. This paper shows the Big data issues and focused more on security issue arises in Hadoop Architecture base layer called Hadoop Distributed File System (HDFS). The HDFS security is enhanced by using three approaches like Kerberos, Algorithm and Name node.|https://doi.org/10.1016/j.procs.2015.04.091|https://www.sciencedirect.com/science/article/pii/S187705091500592X||||2015|Big Data and Hadoop-a Study in Security Perspective|B. Saraladevi and N. Pazhaniraja and P. Victer Paul and M.S. Saleem Basha and P. Dhavachelvan|article|SARALADEVI2015596|||Procedia Computer Science|18770509||50||Big Data, Cloud and Computing Challenges|||19700182801|0,334|-|76|1907|6483|38511|13722|6359|2,09|20,19|Netherlands|Western Europe|2010-2020|Computer Science (miscellaneous)||||1031401537|2108686752
https://doi.org/10.1016/j.sapharm.2018.02.009|https://www.sciencedirect.com/science/article/pii/S155174111830127X||||2018|Exhibiting caution with use of big data: The case of amphetamine in Iceland's prescription registry|Ingunn Björnsdottir and Guri Birgitte Verne|article|BJORNSDOTTIR20181195|||Research in Social and Administrative Pharmacy|15517411|12|14||||||||||||||||||||||||||||||1035256808|42
||Radiotherapy, Computer technology, Cloud computing, Machine learning, Big data||455-462||Recent rapid development of Internet-based computer technologies has made possible many novel applications in radiation dose delivery. However, translational speed of applying these new technologies in radiotherapy could hardly catch up due to the complex commissioning process and quality assurance protocol. Implementing novel Internet-based technology in radiotherapy requires corresponding design of algorithm and infrastructure of the application, set up of related clinical policies, purchase and development of software and hardware, computer programming and debugging, and national to international collaboration. Although such implementation processes are time consuming, some recent computer advancements in the radiation dose delivery are still noticeable. In this review, we will present the background and concept of some recent Internet-based computer technologies such as cloud computing, big data processing and machine learning, followed by their potential applications in radiotherapy, such as treatment planning and dose delivery. We will also discuss the current progress of these applications and their impacts on radiotherapy. We will explore and evaluate the expected benefits and challenges in implementation as well.|https://doi.org/10.1016/j.rpor.2017.08.005|https://www.sciencedirect.com/science/article/pii/S1507136716301602||||2017|Internet-based computer technology on radiotherapy|James C.L. Chow|article|CHOW2017455|||Reports of Practical Oncology & Radiotherapy|15071367|6|22|||||54509|0,367|Q3|23|166|252|5209|338|248|1,31|31,38|Poland|Eastern Europe|1998-2020|Oncology (Q3); Radiology, Nuclear Medicine and Imaging (Q3); Cancer Research (Q4)||||1037136277|229675888
Earth Observation||artificial intelligence, natural language processing, computer vision, machine learning, Big Data, Internet of Things, crowdsourcing, citizen science, surveillance video||295-304|Earth Observation for Flood Applications|Artificial intelligence (AI) is fundamentally changing our society, benefiting from the big data revolution and dramatical  declination in the Internet of Things (IoT) costs. Flood research and applications will progress with this emerging technology, as AI is creating new flood data sources, enhancing our capability to analyze the data, and improving our accuracy of flood predictions. This chapter introduces the basic concepts of AI and its technical frontier. Using the method of “review of the reviews” with example highlight the emerging AI applications in the field of flood hazards is summarized in terms of the data sources, including crowdsourcing and surveillance camera videos. The use of the AI-enabled big data is also discussed. The opportunities and barriers of this new technology are summarized. At the end of the chapter, the trend and the research gaps are identified in this field.|https://doi.org/10.1016/B978-0-12-819412-6.00013-4|https://www.sciencedirect.com/science/article/pii/B9780128194126000134||Elsevier|978-0-12-819412-6|2021|Chapter 13 - Artificial Intelligence for Flood Observation|Ruo-Qian Wang|incollection|WANG2021295||||||||||Guy J-P. Schumann|||||||||||||||||||1038696794|42
||Addiction, Big Data, Cognition, Genetics, Machine learning, Neuroimaging, Online methods||365-377|Cognition and Addiction|The etiology and trajectory of addictions is complex, caused and moderated by individual differences in cognition that are themselves a function of genetics and of environment. In this chapter, we discuss how “Big Data” can shed light on the cognitive correlates of addiction. Big Data is primarily data-driven, using algorithms that search for patterns in data, with accurate prediction on previously unseen data as the metric of success. In this chapter, we introduce and provide practical advice on Big Data approaches for addiction. In the first part of this chapter, we describe how online methods of data collection facilitate the collection of large datasets. In the second section, we outline some recent advances in neuroimaging, with a focus on prediction of substance use using machine learning methods. In the final section, we present advances in genetics—meta- and megaanalyses—which may provide breakthroughs in our understanding of the genetics of addiction.|https://doi.org/10.1016/B978-0-12-815298-0.00027-7|https://www.sciencedirect.com/science/article/pii/B9780128152980000277||Academic Press|978-0-12-815298-0|2020|Chapter 27 - Genetics, imaging, and cognition: big data approaches to addiction research|Robert Whelan and Zhipeng Cao and Laura O'Halloran and Brian Pennie|incollection|WHELAN2020365||||||||||Antonio Verdejo-Garcia|||||||||||||||||||1039551705|42
||Blockchain, Bigdata, Data analytics, Smart Cities applications, Healthcare||15-22||Blockchain and big data are the emerging technologies that are on the highest agendas of the firms. These are significantly expected to transform the ways in which the business as well as the firm run. These are on the verge of increasing the expectation of the distributed ledgers, which would keep the firms away from struggling challenges. The concept of big data and Blockchain has been used up by various other concepts that would help secure and interpret the information. The ideal solutions offered by these technologies shall address the challenges of big data management as well as for analytics. In addition to that, Blockchain provides its own consensus method, which is the primary means to create an audit trail. This enables users to verify all transactions. The audit trail is a means of verifying the correctness and integrity of every transaction, regardless of who owns the asset. The Blockchain can also verify that different parties of a transaction are following an agreement and not breaking the agreement. Moreover, there have been continuous arguments in the concept of Blockchain at which the bitcoin is fundamental and there are several popular blockchain approaches developed which would deliver performance, security as well as privacy. Apart from this, the use of Blockchain plays a major role in adding an extra data layer for the big data analytics process. Big data is considered secure which cannot be further forged with the network architecture. The current paper shall be discussing the emerging concepts that are using Blockchain as well as big data.|https://doi.org/10.1016/j.procs.2021.12.206|https://www.sciencedirect.com/science/article/pii/S1877050921024455||||2022|Emerging Concepts Using Blockchain and Big Data|Fadi Muheidat and Dhaval Patel and Spandana Tammisetty and Lo’ai A. Tawalbeh and Mais Tawalbeh|article|MUHEIDAT202215|||Procedia Computer Science|18770509||198||12th International Conference on Emerging Ubiquitous Systems and Pervasive Networks / 11th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare|||19700182801|0,334|-|76|1907|6483|38511|13722|6359|2,09|20,19|Netherlands|Western Europe|2010-2020|Computer Science (miscellaneous)||||1039873400|2108686752
||Measurement;Time series analysis;Data integrity;Decision support systems;Standards;Monitoring;Internet of Things;Data quality;streaming time series;decision support system||81458-81475||The Internet of Things (IoT) technologies plays a key role in the Fourth Industrial Revolution (Industry 4.0). This implies the digitisation of the industry and its services to improve productivity. To obtain the necessary information throughout the different processes, useful data streams are obtained to provide Artificial Intelligence and Big Data algorithms. However, strategic decision-making based on these algorithms may not be successful if they have been developed based on inadequate low-quality data. This research work proposes a set of metrics to measure Data Quality (DQ) in streaming time series, and implements and validates a set of techniques and tools that allow monitoring and improving the quality of the information. These techniques allow the early detection of problems that arise in relation to the quality of the data collected; and, in addition, they provide some mechanisms to solve these problems. Later, as part of the work, a use case related to industrial field is presented, where these techniques and tools have been deployed into a data management, monitoring and data analysis platform. This integration provides additional functionality to the platform, a Decision Support System (DSS) named DQ-REMAIN (Data Quality REport MAnagement and ImprovemeNt), for decision-making regarding the quality of data obtained from streaming time series.|10.1109/ACCESS.2022.3195338|||||2022|On the Evaluation, Management and Improvement of Data Quality in Streaming Time Series|Meritxell, Gómez-Omella and Sierra, Basilio and Ferreiro, Susana|article|9845398|||IEEE Access|21693536||10|||||21100374601|0,587|Q1|127|18036|24267|771081|116691|24200|4,48|42,75|United States|Northern America|2013-2020|Computer Science (miscellaneous) (Q1); Engineering (miscellaneous) (Q1); Materials Science (miscellaneous) (Q2)|105,968|3.367|0.15396|1042606253|1905633267
||||||"\"The report from the workshop, \"\"Big Data, Big Decisions for Government, Business and Society,\"\" makes a number of astute contributions. There is no need to replicate them in this foreword - they are in the report. What might be missed comes between the lines, where provocative points are made. Big Data means big opinions and big stakes. Those who think Big Data important want to be proven right, those who think Big Data a passing fad want Big Data to fade, and those who think Big Data will bring profound change hope for change. Big Data, like everything important, is political.\""|||USA|National Science Foundation||2015|Big Data, Big Decisions for Science, Society, and Business: Report on a Research Agenda Setting Workshop|Markus, M. Lynne and Topi, Heikki|techreport|10.5555/2849516|||||||||||||||||||||||||||||1042692978|42
||Standards organizations;Industries;Training;Companies;Decision making;Poor data quality;dirty data;poor data classification;data quality problems||179-188|2015 IEEE 21st Pacific Rim International Symposium on Dependable Computing (PRDC)|Data is part of our everyday life and an essential asset in numerous businesses and organizations. The quality of the data, i.e., the degree to which the data characteristics fulfill requirements, can have a tremendous impact on the businesses themselves, the companies, or even in human lives. In fact, research and industry reports show that huge amounts of capital are spent to improve the quality of the data being used in many systems, sometimes even only to understand the quality of the information in use. Considering the variety of dimensions, characteristics, business views, or simply the specificities of the systems being evaluated, understanding how to measure data quality can be an extremely difficult task. In this paper we survey the state of the art in classification of poor data, including the definition of dimensions and specific data problems, we identify frequently used dimensions and map data quality problems to the identified dimensions. The huge variety of terms and definitions found suggests that further standardization efforts are required. Also, data quality research on Big Data appears to be in its initial steps, leaving open space for further research.|10.1109/PRDC.2015.41|||||2015|A Survey on Data Quality: Classifying Poor Data|Laranjeiro, Nuno and Soydemir, Seyma Nur and Bernardino, Jorge|inproceedings|7371861||Nov|||||||||||||||||||||||||||1045705947|42
||Dilemma, Human rights, Jurisdiction, Law enforcement, Privacy, Purpose limitation||229-237|Application of Big Data for National Security|This chapter considers the specific issues that Big Data presents for law enforcement agencies (LEAs). In particular, it looks at the dilemmas created for LEAs seeking to use the advantages Big Data gives them while remaining compliant with the developing legal framework governing privacy and the protection of personal data, and how those very advantages can present challenges in law enforcement.|https://doi.org/10.1016/B978-0-12-801967-2.00015-X|https://www.sciencedirect.com/science/article/pii/B978012801967200015X||Butterworth-Heinemann|978-0-12-801967-2|2015|Chapter 15 - The Legal Challenges of Big Data Application in Law Enforcement|Fraser Sampson|incollection|SAMPSON2015229||||||||||Babak Akhgar and Gregory B. Saathoff and Hamid R. Arabnia and Richard Hill and Andrew Staniforth and Petra Saskia Bayerl|||||||||||||||||||1046616937|42
|||8|38–45||"\"This paper describes how machine learning works with \"\"coring matrix\"\", which is designed for measuring the similarity between heterogeneously structured references, to get a better performance in Entity Resolution (ER). In the scoring matrix, each entity reference is tokenized and all pairs of tokens between the references are scored by a similarity scoring function such as the Levenshtein edit distance. In so doing, a similarity score vector can measure the similarity between references. With the similarity score vector, machine learning is used to make the linking decision. Our experiments show that machine learning based on score vector outperforms TF-IDF and FuzzyWuzzy benchmarks. One possible explanation is that a similarity score vector conveys much more information than a single similarity score. Random forest and neural network even get better performance with raw score vector input than with the statistic characteristic input.\""|||Evansville, IN, USA|Consortium for Computing Sciences in Colleges||2019|Scoring Matrix Combined with Machine Learning for Heterogeneously Structured Entity Resolution|Li, Xinming and Talburt, John R. and Li, Ting and Liu, Xiangwen|article|10.5555/3344081.3344085||apr|J. Comput. Sci. Coll.|19374771|7|34|April 2019||||||||||||||||||||||1052179844|1795572528
||Social science, Big data, Crime, Social media, Data-driven social science||208-219||This review discusses practical benefits and limitations of novel data-driven research for social scientists in general and criminologists in particular by providing a comprehensive examination of the matter. Specifically, this study is an attempt to critically evaluate ‘big data’, data-driven perspectives, and their epistemological value for both scholars and practitioners, particularly those working on crime. It serves as guidance for those who are interested in data-driven research by pointing out new research avenues. In addition to the benefits, the drawbacks associated with data-driven approaches are also discussed. Finally, critical problems that are emerging in this era, such as privacy and ethical concerns are highlighted.|https://doi.org/10.1016/j.soscij.2018.10.010|https://www.sciencedirect.com/science/article/pii/S0362331918301514||||2019|Criminology in the age of data explosion: New directions|Turgut Ozkan|article|OZKAN2019208|||The Social Science Journal|03623319|2|56|||||26437|0,349|Q2|39|138|198|8250|340|194|1,61|59,78|United States|Northern America|1978, 1980, 1982-2020|Sociology and Political Science (Q2); Social Psychology (Q3)|1,817|2.376|0.00167|1053074629|1771390973
||Tensors;Big Data;Quality of service;Computational efficiency;Machine learning;Web services;Algorithm;big data;dynamics;high-dimensional and incomplete (HDI) data;machine learning;missing data estimation;multichannel data;nonnegative latent factorization of tensors (NLFT);temporal pattern;quality of service (QoS);web service||2142-2155||A nonnegative latent factorization of tensors (NLFT) model precisely represents the temporal patterns hidden in multichannel data emerging from various applications. It often adopts a single latent factor-dependent, nonnegative and multiplicative update on tensor (SLF-NMUT) algorithm. However, learning depth in this algorithm is not adjustable, resulting in frequent training fluctuation or poor model convergence caused by overshooting. To address this issue, this study carefully investigates the connections between the performance of an NLFT model and its learning depth via SLF-NMUT to present a joint learning-depth-adjusting scheme for it. Based on this scheme, a Depth-adjusted Multiplicative Update on tensor algorithm is innovatively proposed, thereby achieving a novel depth-adjusted nonnegative latent-factorization-of-tensors (DNL) model. Empirical studies on two industrial data sets demonstrate that compared with the state-of-the-art NLFT models, a DNL model achieves significant accuracy gain when performing missing data estimation on a high-dimensional and incomplete tensor with high efficiency. Note to Practitioners—Multichannel data are often encountered in various big-data-related applications. It is vital for a data analyzer to correctly capture the temporal patterns hidden in them for efficient knowledge acquisition and representation. This article focuses on analyzing temporal QoS data, which is a representative kind of multichannel data. To correctly extract their temporal patterns, an analyzer should correctly describe their nonnegativity. Such a purpose can be achieved by building a nonnegative latent factorization of tensors (NLFT) model relying on a single latent factor-dependent, nonnegative and multiplicative update on tensor (SLF-NMUT) algorithm. But its learning depth is not adjustable, making an NLFT model frequently suffer from severe fluctuations in its training error or even fail to converge. To address this issue, this study carefully investigates the learning rules for an NLFT model’s decision parameters using an SLF-NMUT and proposes a joint learning-depth-adjusting scheme. This scheme manipulates the multiplicative terms in SLF-NMUT-based learning rules linearly and exponentially, thereby making the learning depth adjustable. Based on it, this study builds a novel depth-adjusted nonnegative latent-factorization-of-tensors (DNL) model. Compared with the existing NLFT models, a DNL model better represents multichannel data. It meets industrial needs well and can be used to achieve high performance in data analysis tasks like temporal-aware missing data estimation|10.1109/TASE.2020.3040400|||||2021|Adjusting Learning Depth in Nonnegative Latent Factorization of Tensors for Accurately Modeling Temporal Patterns in Dynamic QoS Data|Luo, Xin and Chen, Minzhi and Wu, Hao and Liu, Zhigang and Yuan, Huaqiang and Zhou, MengChu|article|9320548||Oct|IEEE Transactions on Automation Science and Engineering|15583783|4|18|||||||||||||||||||||||1054458801|268137400
CCGRID '15|Shenzhen, China|parallel optimization, sequence alignment, SNP detection, TH-2 supercomputer, whole genome re-sequencing|6|823–828|Proceedings of the 15th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing|Whole genome re-sequencing plays a crucial role in biomedical studies. The emergence of genomic big data calls for an enormous amount of computing power. However, current computational methods are inefficient in utilizing available computational resources. In this paper, we address this challenge by optimizing the utilization of the fastest supercomputer in the world - TH-2 supercomputer. TH-2 is featured by its neo-heterogeneous architecture, in which each compute node is equipped with 2 Intel Xeon CPUs and 3 Intel Xeon Phi coprocessors. The heterogeneity and the massive amount of data to be processed pose great challenges for the deployment of the genome analysis software pipeline on TH-2. Runtime profiling shows that SOAP3-dp and SOAPsnp are the most time-consuming components (up to 70% of total runtime) in a typical genome-analyzing pipeline. To optimize the whole pipeline, we first devise a number of parallel and optimization strategies for SOAP3-dp and SOAPsnp, respectively targeting each node to fully utilize all sorts of hardware resources provided both by CPU and MIC. We also employ a few scaling methods to reduce communication between different nodes. We then scaled up our method on TH-2. With 8192 nodes, the whole analyzing procedure took 8.37 hours to finish the analysis of a 300 TB dataset of whole genome sequences from 2,000 human beings, which can take as long as 8 months on a commodity server. The speedup is about 700x.|10.1109/CCGrid.2015.46|https://doi.org/10.1109/CCGrid.2015.46||IEEE Press|9781479980062|2015|The Challenge of Scaling Genome Big Data Analysis Software on TH-2 Supercomputer|Peng, Shaoliang and Liao, Xiangke and Yang, Canqun and Lu, Yutong and Liu, Jie and Cui, Yingbo and Wang, Heng and Wu, Chengkun and Wang, Bingqiang|inproceedings|10.1109/CCGrid.2015.46|||||||||||||||||||||||||||||1054967294|42
||MaaS, Big data, Data mining, Support vector machine, Linear regression, Decision tree, Clustering, Bike-sharing, COVID-19||203-228|Big Data and Mobility as a Service|This chapter mainly introduces big data technologies for MaaS. Firstly, the development, definition, and purpose of MaaS and the significance of data mining technologies for MaaS are introduced briefly. Following this, the definition of data mining, its processing objects, classical steps and processes, and types of traffic big data are reviewed. Afterward, data mining technologies such as support vector machine, linear regression, decision tree, and clustering analysis are introduced. Finally, a case study of data mining technology used for bike-sharing in Beijing during the Covid-19 pandemic is presented to demonstrate the role of data mining technologies in travel behaviors. This chapter mainly provides a clue or reference for the exploration of big data analysis of MaaS.|https://doi.org/10.1016/B978-0-323-90169-7.00008-7|https://www.sciencedirect.com/science/article/pii/B9780323901697000087||Elsevier|978-0-323-90169-7|2022|Chapter 7 - Data mining technologies for Mobility-as-a-Service (MaaS)|Wen-Long Shang and Haoran Zhang and Yi Sui|incollection|SHANG2022203||||||||||Haoran Zhang and Xuan Song and Ryosuke Shibasaki|||||||||||||||||||1056767818|42
MK Series on Business Intelligence||Big Data, Big Data appliances, Hadoop, NoSQL, RDBMS, data virtualization, semantic framework||199-217|Data Warehousing in the Age of Big Data|The focus of this chapter is to discuss the integration of Big Data and the data warehouse, the possible techniques and pitfalls, and where we leverage a technology. How do we deal with complexity and heterogeneity of technologies? What are the performance and scalabilities of each technology, and how can we sustain performance for the new environment?|https://doi.org/10.1016/B978-0-12-405891-0.00010-6|https://www.sciencedirect.com/science/article/pii/B9780124058910000106|Boston|Morgan Kaufmann|978-0-12-405891-0|2013|Chapter 10 - Integration of Big Data and Data Warehousing|Krish Krishnan|incollection|KRISHNAN2013199||||||||||Krish Krishnan|||||||||||||||||||1061865388|42
||Metadata;Uncertainty;Geospatial analysis;Standards;Spatial databases;Analytical models;Geospatial Metadata;GIS;ontology;standard deviation;probability density function||1575-1579|2016 3rd International Conference on Computing for Sustainable Global Development (INDIACom)|The proliferation of Geospatial data analytics has greatly increased the usage of GIS in all smartphones and gadgets in today's Big Data environment. The footprints of GIS are found in all fields from government to business analytics. Therefore the error propagation in such a substantial need may lead to misunderstood decisions and confusions during emergency. In this paper we take up a chance to document the data quality assurance using the geospatial metadata based uncertainty analysis approach. The paper takes up an initial attempt to state that chances occur for existence of uncertainty in metadata. And it proposes a hybrid model combing ontology, standard deviation and probability density function for detecting the occurrence of uncertainties in geospatial metadata.||||||2016|Hybrid model based uncertainty analysis for geospatial metadata supporting decision making for spatial exploration|Manjula, K. R. and Gangothri, R.|inproceedings|7724532||March|||||||||||||||||||||||||||1062134539|42
||Data ecosystems, Data context, Data architectures, Data Integrity, Data Quality||81-92|Data-Centric Safety|Data is one component in a system. It has value. The economics of increasingly data-centric systems is explored. There is a growing reliance on data to create systems that are larger scale, have wider scope and are more complex. As reliance on the data component increases, a self-reinforcing problem of implementing checks and balances necessary to enforce appropriate levels of risk reduction arises. This chapter introduces data architecture elements of container and content. It places this architecture within an appropriate data context. It introduces the concepts of data quality and data integrity. Data integrity is placed within the Safety Management and Safety Assurance processes. Big data and machine learning are considered in this context.|https://doi.org/10.1016/B978-0-12-820790-1.00017-6|https://www.sciencedirect.com/science/article/pii/B9780128207901000176||Elsevier|978-0-12-820790-1|2020|4 - Data Fundamentals|Alastair Faulkner and Mark Nicholson|incollection|FAULKNER202081||||||||||Alastair Faulkner and Mark Nicholson|||||||||||||||||||1065645767|42
||public sector, workplace, future work, work, e-Government, digitalization, changing nature of work, digital sclerosis|14|||Contrasting the political ambitions on the next generation of government, the uptake of technology can lead to digital sclerosis characterized by stiffening of the governmental processes, failure to respond to changes in demand, and lowering innovation feedback from workers. In this conceptual article, we outline three early warnings of digital sclerosis: decreased bargaining and discretion power of governmental workers, enhanced agility and ability at shifting and extended proximities, and panopticonization. To respond proactively and take preventive care initiatives, policy makers and systems developers need to be sensitized about the digital sclerosis, prepare the technology, and design intelligent augmentations in a flexible and agile approach.|10.1145/3360000|https://doi.org/10.1145/3360000|New York, NY, USA|Association for Computing Machinery||2020|Digital Sclerosis? Wind of Change for Government and the Employees|Andersen, Kim Normann and Lee, Jungwoo and Henriksen, Helle Zinner|article|10.1145/3360000|9|feb|Digit. Gov.: Res. Pract.|2691199X|1|1|January 2020||||||||||||||||||||||1065645896|141116223
K-CAP 2015|Palisades, NY, USA|Knowledge base, Data Integration, Data Reconciliation, Smart City, Expo 2015, 3cixty|4||Proceedings of the 8th International Conference on Knowledge Capture|In this paper, we present the 3cixty Knowledge Base, which collects and harmonizes descriptions of events, places, transportation facilities and user-generated data such as reviews of the city and Expo site of Milan. This knowledge base is used by a set of web and mobile applications to guide Expo Milano 2015 visitors in the city and in the exhibit, allowing them to find places, satellite events and transportation facilities around Milan. As of July 24th, 2015 the knowledge base contains 18665 unique events, 225821 unique places, 94789 reviews, and 9343 transportation facilities, collected from several static, near- and real time local and global data providers, including Expo Milano 2015 official services and numerous social media platforms. The ontologies used as a backbone for structuring the knowledge base follow a rigorous development method where the design principle has generally been to re-use existing ontologies when they exist. We think that the lessons learned from this development will be useful for similar endeavors in other cities or large events around the world with a similar ecosystem of data provisioning services.|10.1145/2815833.2816944|https://doi.org/10.1145/2815833.2816944|New York, NY, USA|Association for Computing Machinery|9781450338493|2015|The 3cixty Knowledge Base for Expo Milano 2015: Enabling Visitors to Explore the City|"\"Rizzo, Giuseppe and Corcho, Oscar and Troncy, Rapha\"\"{e}l and Plu, Julien and Hermida, Juan Carlos Ballesteros and Assaf, Ahmad\""|inproceedings|10.1145/2815833.2816944|18||||||||||||||||||||||||||||1066834289|42
||Internet of Things, Deep Learning, Smart city, Big data analytics, Review||100303||The rapid growth of urban populations worldwide imposes new challenges on citizens’ daily lives, including environmental pollution, public security, road congestion, etc. New technologies have been developed to manage this rapid growth by developing smarter cities. Integrating the Internet of Things (IoT) in citizens’ lives enables the innovation of new intelligent services and applications that serve sectors around the city, including healthcare, surveillance, agriculture, etc. IoT devices and sensors generate large amounts of data that can be analyzed to gain valuable information and insights that help to enhance citizens’ quality of life. Deep Learning (DL), a new area of Artificial Intelligence (AI), has recently demonstrated the potential for increasing the efficiency and performance of IoT big data analytics. In this survey, we provide a review of the literature regarding the use of IoT and DL to develop smart cities. We begin by defining the IoT and listing the characteristics of IoT-generated big data. Then, we present the different computing infrastructures used for IoT big data analytics, which include cloud, fog, and edge computing. After that, we survey popular DL models and review the recent research that employs both IoT and DL to develop smart applications and services for smart cities. Finally, we outline the current challenges and issues faced during the development of smart city services.|https://doi.org/10.1016/j.cosrev.2020.100303|https://www.sciencedirect.com/science/article/pii/S1574013720304032||||2020|Leveraging Deep Learning and IoT big data analytics to support the smart cities development: Review and future directions|Safa Ben Atitallah and Maha Driss and Wadii Boulila and Henda Ben Ghézala|article|ATITALLAH2020100303|||Computer Science Review|15740137||38|||||8000153138|1,646|Q1|44|54|61|7789|891|61|12,49|144,24|Ireland|Western Europe|2007-2020|Computer Science (miscellaneous) (Q1); Theoretical Computer Science (Q1)|1,249|7.872|0.00195|1067586473|1526110236
||Urban big data infrastructure, Urban analytics, Spatial urban indicators, Small area assessment, Spatial big data||456-473||The Spatial Urban Data System (SUDS) is a spatial big data infrastructure to support UK-wide analytics of the social and economic aspects of cities and city-regions. It utilises data generated from traditional as well as new and emerging sources of urban data. The SUDS deploys geospatial technology, synthetic small area urban metrics, and cloud computing to enable urban analytics, and geovisualization with the goal of deriving actionable knowledge for better urban management and data-driven urban decision making. At the core of the system is a programme of urban indicators generated by using novel forms of data and urban modelling and simulation programme. SUDS differs from other similar systems by its emphasis on the generation and use of regularly updated spatially-activated urban area metrics from real or near-real time data sources, to enhance understanding of intra-city interactions and dynamics. By deploying public transport, labour market accessibility and housing advertisement data in the system, we were able to identify spatial variations of key urban services at intra-city levels as well as social and economically-marginalised output areas in major cities across the UK. This paper discusses the design and implementation of SUDS, the challenges and limitations encountered, and considerations made during its development. The innovative approach adopted in the design of SUDS will enable it to support research and analysis of urban areas, policy and city administration, business decision-making, private sector innovation, and public engagement. Having been tested with housing, transport and employment metrics, efforts are ongoing to integrate information from other sources such as IoT, and User Generated Content into the system to enable urban predictive analytics.|https://doi.org/10.1016/j.future.2019.03.052|https://www.sciencedirect.com/science/article/pii/S0167739X18319046||||2019|Spatial urban data system: A cloud-enabled big data infrastructure for social and economic urban analytics|Obinna C.D. Anejionu and Piyushimita (Vonu) Thakuriah and Andrew McHugh and Yeran Sun and David McArthur and Phil Mason and Rod Walpole|article|ANEJIONU2019456|||Future Generation Computer Systems|0167739X||98|||||12264|1,262|Q1|119|798|1935|36054|16877|1868|9,11|45,18|Netherlands|Western Europe|1984-2021|Computer Networks and Communications (Q1); Hardware and Architecture (Q1); Software (Q1)||||1069360888|562237118
dg.o '17|Staten Island, NY, USA|Administrative reform, E-government, Chinese government and reform|7|329–335|Proceedings of the 18th Annual International Conference on Digital Government Research|This1 paper summarizes the history of Chinese administrative modernization and reform and discusses the ways in which China's e-government development agenda supports reform in the areas of transforming functions, streamlining processes, and enhancing transparency and citizen engagement. It offers a conceptual model of how e-government supports reform through policies, technologies, management improvements, and data designed to overcome the barriers of technical capability, staff resistance, and lack of cross-boundary collaboration. The analysis also shows how this interaction has generated new issues regarding official corruption and public engagement. We conclude with a future research agenda.|10.1145/3085228.3085269|https://doi.org/10.1145/3085228.3085269|New York, NY, USA|Association for Computing Machinery|9781450353175|2017|E-Government Support for Administrative Reform in China|Chen, Yumei and Dawes, Sharon S. and Chen, Shanshan|inproceedings|10.1145/3085228.3085269|||||||||||||||||||||||||||||1073903924|42
||Big data;Cleaning;Computer science;Databases;Investment||1446-1447|2016 IEEE 32nd International Conference on Data Engineering (ICDE)|The increased reliance on data driven enterprise has seen an unprecedented investment in big data initiatives. Organizations averaged US$8M in investments in big data-related initiatives and programs in 2014, with 70% of large enterprises and 56% of small and medium enterprises (SMEs) having already deployed, or planning to deploy, big-data projects [1]. As companies intensify their efforts to get value from big data, the growth in the amount of data being managed continues at an exponential rate, leaving organizations with a massive footprint of unexplored, unfamiliar datasets. On February 8th, 2015, a group of global thought leaders from the database research community outlined the grand challenges in getting value from big data [2]. The key message was the need to develop the capacity to `understand how the quality of data affects the quality of the insight we derive from it'.|10.1109/ICDE.2016.7498367|||||2016|Big data quality - whose problem is it?|Sadiq, Shazia and Papotti, Paolo|inproceedings|7498367||May|||||||||||||||||||||||||||1079874863|42
||Big data collection, Trust evaluation, Trust model, True data discovery, Wireless sensor network||101937||Data collection is an important process in the life cycle of big data processing. It is the key part that must be completed first in all kinds of data applications, which determines the results of data analysis and application service quality. However, untrusted data sources and transmission links expose the data collection process to attacks and malicious threats such as counterfeiting, replay, and denial of service, and ultimately lead to untrustworthy data. In order to cope with the threat of data collection process and ensure data quality, this paper proposes trust evaluation scheme for data security collection based on wireless sensor network, one of the data collection applications, including direct trust, recommendation trust, link trust, and backhaul trust. Meanwhile, in order to realize the dynamic update of the trust of the data sources, a true data discovery and trust dynamic update mechanism based on ω-FCM (Weight Fuzzy C-Mean) algorithm is proposed. The results of a large number of simulation experiments show that the proposed scheme, model and algorithm can effectively evaluate the trust of data sources and ensure the authenticity of the collected data.|https://doi.org/10.1016/j.cose.2020.101937|https://www.sciencedirect.com/science/article/pii/S0167404820302133||||2020|Achieving secure big data collection based on trust evaluation and true data discovery|Denglong Lv and Shibing Zhu|article|LV2020101937|||Computers & Security|01674048||96|||||28898|0,861|Q1|92|321|559|17204|3843|550|6,75|53,60|United Kingdom|Western Europe|1982-2020|Computer Science (miscellaneous) (Q1); Law (Q1)||||1083691126|1687137310
CSCW '17|Portland, Oregon, USA|wikipedia, modeling, disease, forecasting, epidemiology|23|1812–1834|Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing|Effective disease monitoring provides a foundation for effective public health systems. This has historically been accomplished with patient contact and bureaucratic aggregation, which tends to be slow and expensive. Recent internet-based approaches promise to be real-time and cheap, with few parameters. However, the question of when and how these approaches work remains open. We addressed this question using Wikipedia access logs and category links. Our experiments, replicable and extensible using our open source code and data, test the effect of semantic article filtering, amount of training data, forecast horizon, and model staleness by comparing across 6 diseases and 4 countries using thousands of individual models. We found that our minimal-configuration, language-agnostic article selection process based on semantic relatedness is effective for improving predictions, and that our approach is relatively insensitive to the amount and age of training data. We also found, in contrast to prior work, very little forecasting value, and we argue that this is consistent with theoretical considerations about the nature of forecasting. These mixed results lead us to propose that the currently observational field of internet-based disease surveillance must pivot to include theoretical models of information flow as well as controlled experiments based on simulations of disease.|10.1145/2998181.2998183|https://doi.org/10.1145/2998181.2998183|New York, NY, USA|Association for Computing Machinery|9781450343350|2017|Measuring Global Disease with Wikipedia: Success, Failure, and a Research Agenda|Priedhorsky, Reid and Osthus, Dave and Daughton, Ashlynn R. and Moran, Kelly R. and Generous, Nicholas and Fairchild, Geoffrey and Deshpande, Alina and Del Valle, Sara Y.|inproceedings|10.1145/2998181.2998183|||||||||||||||||||||||||||||1085411668|42
||Big data, Blockchain, Agri-food supply chain, Investment decision, Coordination||123646||Researches about the fusion application of Big Data and blockchain have appeared for a long time, many information service providers have launched information service business based on Big Data and blockchain (hereafter, ISBD). However, in the green agri-food area, the ISBD application does not popularized. A vital reason is that many decision makers do not know how to make an optimal investment decision and coordinate chain members after adopting ISBD. The core of this problem is to study the issue of investment decision and coordination in a green agri-food supply chain. To solve this problem, firstly, combining with the status of Chinese agricultural development, we proposed a more suitable supply chain structure in the fusion application environment of Big Data and blockchain. Then, we chose a green agri-food supply chain with one producer and one retailer as research object and revised the demand function. Afterwards, considering the changes of agri-food freshness and greenness, we built and analysed the benefit models of producer and retailer before and after using ISBD, and then a cost-sharing and revenue-sharing contract was put forward to coordinate the supply chain. Findings: 1) When the total investment cost payed by producer and retailer is in a certain range, using ISBD will help chain members gain more benefits. 2) If chain members want to gain more benefits after using ISBD, they should try their best to optimize costs by extracting valuable information. Results can offer a theoretical guidance for producer and retailer in investing in ISBD, pricing decision and supply chain coordination after applying ISBD.|https://doi.org/10.1016/j.jclepro.2020.123646|https://www.sciencedirect.com/science/article/pii/S095965262033691X||||2020|Investment decision and coordination of green agri-food supply chain considering information service based on blockchain and big data|Pan Liu and Yue Long and Hai-Cao Song and Yan-Dong He|article|LIU2020123646|||Journal of Cleaner Production|09596526||277|||||19167|1,937|Q1|200|5126|10603|304498|103295|10577|9,56|59,40|United Kingdom|Western Europe|1993-2021|Environmental Science (miscellaneous) (Q1); Industrial and Manufacturing Engineering (Q1); Renewable Energy, Sustainability and the Environment (Q1); Strategy and Management (Q1)|170,352|9.297|0.18299|1085584555|1121054297
||general-purpose programming language, generality-power tradeoffs, open resource coalition, closed software system, domain-specific programming language, sufficient correctness, problem-setting design, problem-solving design, exploratory programming, vernacular software developer, fitness to task, software credentials, formal specifications|44|||Modern software does not stand alone; it is embedded in complex physical and sociotechnical systems. It relies on computational support from interdependent subsystems as well as non-code resources such as data, communications, sensors, and interactions with humans. Both general-purpose programming languages and mainstream programming language research focus on symbolic notations with well-defined abstractions that are intended for use by professionals to write programs that solve precisely specified problems. There is a strong emphasis on correctness of the resulting programs, preferably by formal reasoning. However, these languages, despite their careful design and formal foundations, address only a modest portion of modern software and only a minority of software developers. Several persistent myths reinforce this focus. These myths express an idealized model of software and software development. They provide a lens for examining modern software and software development practice: highly trained professionals are outnumbered by vernacular developers. Writing new code is dominated by composition of ill-specified software and non-software components. General-purpose languages may be less appropriate for a task than domain-specific languages, and functional correctness is often a less appropriate goal than overall fitness for task. Support for programming to meet a specification is of little help to people who are programming in order to understand their problems. Reasoning about software is challenged by uncertainty and nondeterminism in the execution environment and by the increasingly dominant role of data, especially with the advent of systems that rely on machine learning. The lens of our persistent myths illuminates the dissonance between our idealized view of software development and common practice, which enables us to identify emerging opportunities and challenges for programming language research.|10.1145/3480947|https://doi.org/10.1145/3480947|New York, NY, USA|Association for Computing Machinery||2022|Myths and Mythconceptions: What Does It Mean to Be a Programming Language, Anyhow?|Shaw, Mary|article|10.1145/3480947|234|apr|Proc. ACM Program. Lang.||HOPL|4|June 2020||||||||||||||||||||||1088264749|42
SAICSIT '17|Thaba 'Nchu, South Africa|business intelligence, systematic literature review, population health|9||Proceedings of the South African Institute of Computer Scientists and Information Technologists|"\"\"\"Business Intelligence\"\" is an area of Information Technology (IT) that involves the collection, analysis and presentation of large amounts of data. BI has been successfully applied to promote good decision making in a variety of environments, and has high potential to make a significant impact in the domain of population health. The promotion of population health is a key concern of government authorities and various health institutions and officials making decisions about interventions that may impact on population health would benefit from the use of information on population health. BI could clearly be a facilitator in this regard, but evidence of its current application and impact in this field is not easily accessible to policy makers. This systematic literature review explored the literature and provided a synthesis of information available on the current use of BI in this area, and evidence of the impact of its use on population health. An array of applications of BI for population health were found, including data warehouses, analytics, reports, data warehouse browsers, OLAP, GIS, Dashboards and Alerts. Evidence of the impact of these applications on population health was mainly anecdotal, with only one empirical study found. Issues and challenges encountered in the development and use of BI are Privacy and Security, Data Quality and Development and Maintenance of BI infrastructure\""|10.1145/3129416.3129441|https://doi.org/10.1145/3129416.3129441|New York, NY, USA|Association for Computing Machinery|9781450352505|2017|Impacts of Business Intelligence on Population Health: A Systematic Literature Review|Cohen, L.|inproceedings|10.1145/3129416.3129441|9||||||||||||||||||||||||||||1088923518|42
ICPE '18|Berlin, Germany|big data, data integration, data wrangling|4|25–28|Companion of the 2018 ACM/SPEC International Conference on Performance Engineering|As the generation of data becomes more prolific, the amount of time and resources necessary to perform analyses on these data increases. What is less understood, however, is the data preprocessing steps that must be applied before any meaningful analysis can begin. This problem of taking data in some initial form and transforming it into a desired one is known as data integration. Here, we introduce the Data Integration Benchmarking Suite (DIBS), a suite of applications that are representative of data integration workloads across many disciplines. We apply a comprehensive characterization to these applications to better understand the general behavior of data integration tasks. As a result of our benchmark suite and characterization methods, we offer insight regarding data integration tasks that will guide other researchers designing solutions in this area.|10.1145/3185768.3186307|https://doi.org/10.1145/3185768.3186307|New York, NY, USA|Association for Computing Machinery|9781450356299|2018|DIBS: A Data Integration Benchmark Suite|Cabrera, Anthony M. and Faber, Clayton J. and Cepeda, Kyle and Derber, Robert and Epstein, Cooper and Zheng, Jason and Cytron, Ron K. and Chamberlain, Roger D.|inproceedings|10.1145/3185768.3186307|||||||||||||||||||||||||||||1090373534|42
||Cyber–Physical Systems (CPS), Weibull Cumulative Distribution, Big data, Response time||235-244||Huge volumes of data are generated at rates faster than the speed of computing resources and executing processors available in market place. This anticipates a draft of information challenges associated with the performance capacity and the ability of big data processing systems to retort in real-time. Moreover, the elapsed time between probabilistic failures drops as the scale of information increases. An error occurred at a specific cluster node of a large Cyber–Physical System influences the overall computation requires to unfold big data transactions. Numerous failure characteristics, statistical response time and lifetime evaluation can be modeled through Weibull Distribution. In this paper, to scrutinize the latency for a data infrastructure, the three-parameter Weibull Cumulative Distribution is used through software defined networking in cyber–physical system. This speculation predicts that the shape of the response time distribution confide in the shape of the learning curve and depicts its parameters to the criterion of the input distribution.|https://doi.org/10.1016/j.comcom.2019.11.018|https://www.sciencedirect.com/science/article/pii/S0140366419311673||||2020|Weibull Cumulative Distribution based real-time response and performance capacity modeling of Cyber–Physical Systems through software defined networking|Gifty R. and Bharathi R.|article|R2020235|||Computer Communications|01403664||150|||||13681|0,627|Q1|105|616|599|24961|2424|591|4,08|40,52|Netherlands|Western Europe|1978-2020|Computer Networks and Communications (Q1)|6,725|3.167|0.00513|1092397724|550488617
||Automated decision making, Data ethics, Data quality, Data bias, Algorithm fairness, Digital policy, Digital governance||101619||Automated decision-making (ADM) systems may affect multiple aspects of our lives. In particular, they can result in systematic discrimination of specific population groups, in violation of the EU Charter of Fundamental Rights. One of the potential causes of discriminative behavior, i.e., unfairness, lies in the quality of the data used to train such ADM systems. Using a data quality measurement approach combined with risk management, both defined in ISO standards, we focus on balance characteristics and we aim to understand how balance indexes (Gini, Simpson, Shannon, Imbalance Ratio) identify discrimination risk in six large datasets containing the classification output of ADM systems. The best result is achieved using the Imbalance Ratio index. Gini and Shannon indexes tend to assume high values and for this reason they have modest results in both aspects: further experimentation with different thresholds is needed. In terms of policies, the risk-based approach is a core element of the EU approach to regulate algorithmic systems: in this context, balance measures can be easily assumed as risk indicators of propagation – or even amplification – of bias in the input data of ADM systems.|https://doi.org/10.1016/j.giq.2021.101619|https://www.sciencedirect.com/science/article/pii/S0740624X21000551||||2021|A data quality approach to the identification of discrimination risk in automated decision making systems|Antonio Vetrò and Marco Torchiano and Mariachiara Mecati|article|VETRO2021101619|||Government Information Quarterly|0740624X|4|38|||||14735|2,121|Q1|103|76|205|5894|1946|193|8,39|77,55|United Kingdom|Western Europe|1984-2020|E-learning (Q1); Law (Q1); Library and Information Sciences (Q1); Sociology and Political Science (Q1)|5,379|7.279|0.00502|1093025567|1582933551
||||243-269||Analysis of public transportation data in large cities is a challenging problem. Managing data ingestion, data storage, data quality enhancement, modelling and analysis requires intensive computing and a non-trivial amount of resources. In EUBra-BIGSEA (Europe–Brazil Collaboration of Big Data Scientific Research Through Cloud-Centric Applications) we address such problems in a comprehensive and integrated way. EUBra-BIGSEA provides a platform for building up data analytic workflows on top of elastic cloud services without requiring skills related to either programming or cloud services. The approach combines cloud orchestration, Quality of Service and automatic parallelisation on a platform that includes a toolbox for implementing privacy guarantees and data quality enhancement as well as advanced services for sentiment analysis, traffic jam estimation and trip recommendation based on estimated crowdedness. All developments are available under Open Source licenses (http://github.org/eubr-bigsea, https://hub.docker.com/u/eubrabigsea/).|https://doi.org/10.1016/j.future.2019.02.011|https://www.sciencedirect.com/science/article/pii/S0167739X18304448||||2019|BIGSEA: A Big Data analytics platform for public transportation information|Andy S. Alic and Jussara Almeida and Giovanni Aloisio and Nazareno Andrade and Nuno Antunes and Danilo Ardagna and Rosa M. Badia and Tania Basso and Ignacio Blanquer and Tarciso Braz and Andrey Brito and Donatello Elia and Sandro Fiore and Dorgival Guedes and Marco Lattuada and Daniele Lezzi and Matheus Maciel and Wagner Meira and Demetrio Mestre and Regina Moraes and Fabio Morais and Carlos Eduardo Pires and Nádia P. Kozievitch and Walter dos Santos and Paulo Silva and Marco Vieira|article|ALIC2019243|||Future Generation Computer Systems|0167739X||96|||||12264|1,262|Q1|119|798|1935|36054|16877|1868|9,11|45,18|Netherlands|Western Europe|1984-2021|Computer Networks and Communications (Q1); Hardware and Architecture (Q1); Software (Q1)||||1093332189|562237118
||Training;Data integrity;Tools;Strategic planning;Cleaning;Monitoring;Software engineering;Big Data Analytics;Data quality;Data cleaning;Software engineering||1028-1033|2020 43rd International Convention on Information, Communication and Electronic Technology (MIPRO)|The world of software engineering is dynamically changing over the last decade. Providing adequate university education is one of the key goals of the academic community for ensuring advanced and up-to-date students' training. One direction in achieving this goal is to constantly monitor the trends in the Information Technology (IT) sector. A reliable source of information is the data from the annual survey on technology and programming languages, as well as on preferred learning methods and ways to enhance competencies, conducted amongst Stack Overflow users since 2011. In processing the data from the survey, the authors have faced several problems that have provoked interest in a more general data problem - data quality and data cleaning.This paper looks into data quality, tools for data cleaning and the characteristics of high-quality data. A classification of data problems is proposed in the context of analyzing the information about software developers. Additionally, the proposed process of data cleaning in illustrated with data for 2018 and 2019.|10.23919/MIPRO48935.2020.9245416|||||2020|Data Cleaning Techniques in Detecting Tendencies in Software Engineering|Georgieva, P. and Nikolova, E. and Orozova, D.|inproceedings|9245416||Sep.||26238764|||||||||||||||||||||||||1094130619|656399508
|||2||||10.1145/2063504.2063505|https://doi.org/10.1145/2063504.2063505|New York, NY, USA|Association for Computing Machinery||2011|Editorial Notes Classification and Assessment of Large Amounts of Data: Examples in the Healthcare Industry and Collaborative Digital Libraries|Madnick, Stuart E. and Lee, Yang W.|article|10.1145/2063504.2063505|12|dec|J. Data and Information Quality|19361955|3|2|December 2011||||||||||||||||||||||1097057531|833754770
||Federal learning, Machine learning, Internet of things, Multi-center privacy protection||138-152||With the privacy protection increasingly being concerned, Data centralization often heavily causes a big risk of privacy protection, gradually, there is a prevailing trend to enhance the security performance by means of data decentralization, above all, for health care internet of things (IoT) data. Meanwhile, Federated learning has obvious privacy advantages compared to data center training on protecting privacy data. For this reason, a novel framework based on federated learning is presented in this paper, which is suitable for private and decentralized data sets, such as big data in healthy Internet of Things. Specifically, the main work of the puts forward framework includes: (1) Multi-center data collection of healthy Internet of Things. (2) healthy data analysis of Internet of Things. (3) privacy protection method for data of healthy Internet of Things. Finally, related experiments show that the proposed method is feasible, and compared with the traditional methods, it has significantly improved the performance in Quality of Service (QoS) and IoUs indicator.|https://doi.org/10.1016/j.ins.2022.10.011|https://www.sciencedirect.com/science/article/pii/S0020025522011306||||2022|An improved federated learning approach enhanced internet of health things framework for private decentralized distributed data|Chenxi Huang and Gengchen Xu and Sirui Chen and Wen Zhou and Eddie Y.K. Ng and Victor Hugo C. de Albuquerque|article|HUANG2022138|||Information Sciences|00200255||614|||||15134|1,524|Q1|184|928|2184|40007|17554|2168|7,89|43,11|United States|Northern America|1968-2021|Artificial Intelligence (Q1); Computer Science Applications (Q1); Control and Systems Engineering (Q1); Information Systems and Management (Q1); Software (Q1); Theoretical Computer Science (Q1)|44,038|6.795|0.04908|1097208371|1633962588
||Big data management, Dynamic capabilities, Service innovation, Knowledge creation, Customer generated online quality rating, Hospitality||106777||Despite the wide usage of big data in tourism and the hospitality sector, little research has been done to understand the role of organizations’ capability of managing big data in value creation. This study bridges this gap by investigating how big data management capabilities lead to service innovation and high online quality ratings. Instead of treating big data management as a whole, we access big data management capabilities at the strategic and operational level. Using a sample of 202 hotels in Pakistan, we collected the primary data for big data capabilities, knowledge creation and service innovation; the secondary data about quality rating were collected from Booking.com. Structural equation modelling through SmartPLS was used for data analysis. The results indicated that big data management capabilities lead to high online quality ratings through the mediation of knowledge creation and service innovation. We contribute to the current literature by empirically testing how strategic level big data capabilities enable the firm to add value in innovativeness and positive online quality ratings through acquiring, contextualizing, experimenting and applying big data.|https://doi.org/10.1016/j.chb.2021.106777|https://www.sciencedirect.com/science/article/pii/S074756322100100X||||2021|Big data management capabilities in the hospitality sector: Service innovation and customer generated online quality ratings|Saqib Shamim and Yumei Yang and Najam Ul Zia and Mahmood Hussain Shah|article|SHAMIM2021106777|||Computers in Human Behavior|07475632||121|||||19419|2,108|Q1|178|385|1597|26867|14501|1573|7,83|69,78|United Kingdom|Western Europe|1985-2021|Arts and Humanities (miscellaneous) (Q1); Human-Computer Interaction (Q1); Psychology (miscellaneous) (Q1)|45,035|6.829|0.05973|1102446986|265090421
||Urban studies, Deep learning, Socio-economics, Location encoder, Graph neural network||102936||Urban Geography studies forms, social fabrics, and economic structures of cities from a geographic perspective. Catalysed by the increasingly abundant spatial big data, Urban Geography seeks new models and research paradigms to explain urban phenomena and address urban issues. Recent years have witnessed significant advances in spatially-explicit geospatial artificial intelligence (GeoAI), which integrates spatial studies and AI, primarily focusing on incorporating spatial thinking and concept into deep learning models for urban studies. This paper provides an overview of techniques and applications of spatially-explicit GeoAI in Urban Geography based on 581 papers identified using a systematic review approach. We examined and screened papers in three scopes of Urban Geography (Urban Dynamics, Social Differentiation of Urban Areas, and Social Sensing) and found that although GeoAI is a trending topic in geography and the applications of deep neural network-based methods are proliferating, the development of spatially-explicit GeoAI models is still at their early phase. We identified three challenges of existing models and advised future research direction towards developing multi-scale explainable spatially-explicit GeoAI. This review paper acquaints beginners with the basics of GeoAI and state-of-the-art and serve as an inspiration to attract more research in exploring the potential of spatially-explicit GeoAI in studying the socio-economic dimension of the city and urban life.|https://doi.org/10.1016/j.jag.2022.102936|https://www.sciencedirect.com/science/article/pii/S1569843222001339||||2022|A review of spatially-explicit GeoAI applications in Urban Geography|Pengyuan Liu and Filip Biljecki|article|LIU2022102936|||International Journal of Applied Earth Observation and Geoinformation|15698432||112|||||39563|1,623|Q1|98|16|523|1076|3348|520|6,62|67,25|Netherlands|Western Europe|1998-2020|Computers in Earth Sciences (Q1); Earth-Surface Processes (Q1); Global and Planetary Change (Q1); Management, Monitoring, Policy and Law (Q1)|11,556|5.933|0.01275|1104902810|722670997
|||10|61–70||Every few years a group of database researchers meets to discuss the state of database research, its impact on practice, and important new directions. This report summarizes the discussion and conclusions of the eighth such meeting, held October 14- 15, 2013 in Irvine, California. It observes that Big Data has now become a defining challenge of our time, and that the database research community is uniquely positioned to address it, with enormous opportunities to make transformative impact. To do so, the report recommends significantly more attention to five research areas: scalable big/fast data infrastructures; coping with diversity in the data management landscape; end-to-end processing and understanding of data; cloud services; and managing the diverse roles of people in the data life cycle.|10.1145/2694428.2694441|https://doi.org/10.1145/2694428.2694441|New York, NY, USA|Association for Computing Machinery||2014|The Beckman Report on Database Research|Abadi, Daniel and Agrawal, Rakesh and Ailamaki, Anastasia and Balazinska, Magdalena and Bernstein, Philip A. and Carey, Michael J. and Chaudhuri, Surajit and Dean, Jeffrey and Doan, AnHai and Franklin, Michael J. and Gehrke, Johannes and Haas, Laura M. and Halevy, Alon Y. and Hellerstein, Joseph M. and Ioannidis, Yannis E. and Jagadish, H. V. and Kossmann, Donald and Madden, Samuel and Mehrotra, Sharad and Milo, Tova and Naughton, Jeffrey F. and Ramakrishnan, Raghu and Markl, Volker and Olston, Christopher and Ooi, Beng Chin and R\'{e}, Christopher and Suciu, Dan and Stonebraker, Michael and Walter, Todd and Widom, Jennifer|article|10.1145/2694428.2694441||dec|SIGMOD Rec.|01635808|3|43|September 2014||||13622|0,372|Q2|142|39|81|808|127|57|1,67|20,72|United States|Northern America|1969, 1973-1978, 1981-2020|Information Systems (Q2); Software (Q2)|1,819|0.775|7.5E-4|1106168267|962972343
|||14|752–765||"\"This paper proposes to deduce certain fixes to graphs G based on data quality rules Σ and ground truth Γ (i.e., validated attribute values and entity matches). We fix errors detected by Σ in G such that the fixes are assured correct as long as Σand Γ are correct. We deduce certain fixes in two paradigms. (a) We interact with users and \"\"incrementally\"\" fix errors online. Whenever users pick a small set V0 of nodes in G, we fix all errors pertaining to V0 and accumulate ground truth in the process. (b) Based on accumulated Γ, we repair the entire graph G offline; while this may not correct all errors in G, all fixes are guaranteed certain.We develop techniques for deducing certain fixes. (1) We define data quality rules to support conditional functional dependencies, recursively defined keys and negative rules on graphs, such that we can deduce fixes by combining data repairing and object identification. (2) We show that deducing certain fixes is Church-Rosser, i.e., the deduction converges at the same fixes regardless of the order of rules applied. (3) We establish the complexity of three fundamental problems associated with certain fixes. (4) We provide (parallel) algorithms for deducing certain fixes online and offline, and guarantee to reduce running time when given more processors. Using real-life and synthetic data, we experimentally verify the effectiveness and scalability of our methods.\""|10.14778/3317315.3317318|https://doi.org/10.14778/3317315.3317318||VLDB Endowment||2019|Deducing Certain Fixes to Graphs|Fan, Wenfei and Lu, Ping and Tian, Chao and Zhou, Jingren|article|10.14778/3317315.3317318||mar|Proc. VLDB Endow.|21508097|7|12|March 2019||||21100199855|0,946|Q1|134|246|598|12401|3759|566|5,50|50,41|United States|Northern America|2008-2020|Computer Science (miscellaneous) (Q1)|7,198|2.047|0.00891|1106876622|1216159931
CIAT 2020|Guangzhou, China|Multiple regression analysis, human resources, data quality|6|465–470|Proceedings of the 2020 International Conference on Cyberspace Innovation of Advanced Technologies|The essence of human resource management informatization is data. Firstly, human information is transformed into data, and then the data can be used. This requires that the comprehensive, complete, timely and effective human resource data is entered into the system, and then the data is analyzed by using the human resource management system to provide decision support for the management. Due to the complexity of the internal law of objective things and the limitation of people's cognition, it is impossible to analyze the internal causality of the actual object and establish a mathematical model in accordance with the mechanism law. Therefore, when some mathematical models cannot be established by mechanism analysis, we usually adopt the method of collecting a large amount of data, and establish the model based on the statistical analysis of the data. Among them, the most widely used random model is statistical regression model.|10.1145/3444370.3444614|https://doi.org/10.1145/3444370.3444614|New York, NY, USA|Association for Computing Machinery|9781450387828|2021|Human Resource Data Quality Management Based on Multiple Regression Analysis|Zhang, Yong|inproceedings|10.1145/3444370.3444614|||||||||||||||||||||||||||||1107166782|42
||Data quality dimensions, data completeness, data integrity, validity, data currency, metadata management, reference data management, data modeling||229-256|Meeting the Challenges of Data Quality Management|This chapter provides an in-depth discussion about a core concept in data quality management: data quality dimensions. Dimensions provide a framework through which we can understand the core capabilities. As the foundation for data quality rules and requirements, they play a critical role in helping answer the fundamental questions about data quality: “What do we mean by high-quality data?” “How do we detect low-quality data?” and “What action will we take when data does not meet quality standards?” This chapter will review a comprehensive set of dimensions (i.e., completeness, correctness, uniqueness, consistency, currency, validity, integrity, reasonability, precision, clarity, accessibility, timeliness, relevance, usability, trustworthiness) in the context of challenges associated with data structure and meaning, the processes for creating data, the influence of technology on quality, and the perceptions of data consumers.|https://doi.org/10.1016/B978-0-12-821737-5.00010-9|https://www.sciencedirect.com/science/article/pii/B9780128217375000109||Academic Press|978-0-12-821737-5|2022|Chapter 10 - Dimensions of Data Quality|Laura Sebastian-Coleman|incollection|SEBASTIANCOLEMAN2022229||||||||||Laura Sebastian-Coleman|||||||||||||||||||1108556385|42
||Big Data, Electronic medical record, Practice-based medicine, Learning health system, Semantic interoperability||121-131|Leveraging Biomedical and Healthcare Data|Any given health system needs to increase efficiency and effectiveness up to the point of requiring a transformation of their current model to ensure their sustainability and continuity. The electronic medical record (EMR) is the main source of knowledge to improve the quality of healthcare, clinical research, epidemiological surveillance, patient empowerment, personalized medicine, and clinical decision-making support systems. There is also a huge amount of available information related to diseases and other medical conditions, such as drugs and therapies, omics data (genetic and proteomic), social networks, and wearable devices. Big Data technologies allow the processing of this data to reach the final goal, which is a learning health system. The great diversity of data, sources, structures, and uses requires a data linkage procedure to integrate and harmonize these data. This generation of knowledge allows the transition from evidence-based medicine, which still prevails, to practice-based medicine. The key points for any Big Data project based on EMRs and other medical information sources are semantic interoperability, data structure and granularity, information quality, patient privacy, legal framework, and bioethics.|https://doi.org/10.1016/B978-0-12-809556-0.00008-3|https://www.sciencedirect.com/science/article/pii/B9780128095560000083||Academic Press|978-0-12-809556-0|2019|Chapter 8 - Healthcare Decision-Making Support Based on the Application of Big Data to Electronic Medical Records: A Knowledge Management Cycle|Javier Carnicero and David Rojas|incollection|CARNICERO2019121||||||||||Firas Kobeissy and Ali Alawieh and Fadi A. Zaraket and Kevin Wang|||||||||||||||||||1109159645|42
HPDC '22|Minneapolis, MN, USA|gpu, high-speed compressor, error-bounded lossy compression, scientific data|13|159–171|Proceedings of the 31st International Symposium on High-Performance Parallel and Distributed Computing|Today's scientific high-performance computing applications and advanced instruments are producing vast volumes of data across a wide range of domains, which impose a serious burden on data transfer and storage. Error-bounded lossy compression has been developed and widely used in the scientific community because it not only can significantly reduce the data volumes but also can strictly control the data distortion based on the user-specified error bound. Existing lossy compressors, however, cannot offer ultrafast compression speed, which is highly demanded by numerous applications or use cases (such as in-memory compression and online instrument data compression). In this paper, we propose a novel ultrafast error-bounded lossy compressor that can obtain fairly high compression performance on both CPUs and GPUs and with reasonably high compression ratios. The key contributions are threefold. (1) We propose a generic error-bounded lossy compression framework---called SZx---that achieves ultrafast performance through its novel design comprising only lightweight operations such as bitwise and addition/subtraction operations, while still keeping a high compression ratio. (2) We implement SZx on both CPUs and GPUs and optimize the performance according to their architectures. (3) We perform a comprehensive evaluation with six real-world production-level scientific datasets on both CPUs and GPUs. Experiments show that SZx is 2~16x faster than the second-fastest existing error-bounded lossy compressor (either SZ or ZFP) on CPUs and GPUs, with respect to both compression and decompression.|10.1145/3502181.3531473|https://doi.org/10.1145/3502181.3531473|New York, NY, USA|Association for Computing Machinery|9781450391993|2022|Ultrafast Error-Bounded Lossy Compression for Scientific Datasets|Yu, Xiaodong and Di, Sheng and Zhao, Kai and Tian, Jiannan and Tao, Dingwen and Liang, Xin and Cappello, Franck|inproceedings|10.1145/3502181.3531473|||||||||||||||||||||||||||||1110965865|42
SENSEMINE'13|Roma, Italy|Sensing Campaign, Participatory Sensing, Environmental Pollution Modeling|6|1–6|Proceedings of First International Workshop on Sensing and Big Data Mining|Environmental pollutants are an ever increasing problem in dense urban environments. To assess the effect of these pollutants, an unprecedented density of data is needed for large areas (cities, states, countries). In the past, participatory sensing has been proposed as a mean to acquire large sets of data. Since the smartphone is ubiquitous, scalability seems to be no problem anymore.In reality this far from the truth. Measuring their environment, people need to invest their time. For Android and iOS the application needs to compete with more than 700,000 other applications. Measuring large amounts of data is only possible, if we can attract large amounts of casual users.Since 2011, we have been working with and on Noisemap. Noisemap is one of many applications that uses the microphone to measure sound pressure. It then uploads the captured data to our backend, where the data is processed and visualized. Noisemap is officially available since February 2012, has been downloaded over 2,500 times, and has more than 1,000 registered users, which have collected over 500,000 unique data points in 39 countries and 58 cities. We want to share the current state of Noisemap as a multi-platform tool on Android and iOS, as well as our experience in scaling the application.|10.1145/2536714.2536720|https://doi.org/10.1145/2536714.2536720|New York, NY, USA|Association for Computing Machinery|9781450324304|2013|Noisemap: Discussing Scalability in Participatory Sensing|"\"Meurisch, Christian and Planz, Karsten and Sch\"\"{a}fer, Daniel and Schweizer, Immanuel\""|inproceedings|10.1145/2536714.2536720|||||||||||||||||||||||||||||1112448681|42
||Fuzzy classifier, MDBSCAN, MapReduce, Hadoop, Diabetes mellitus||107423||In the era of data deluge, the world is experiencing an intensive growth of Big data with complex structures. While processing of these data is a complex and labor-intensive process, a proper analysis of Big data leads to greater knowledge extraction. In this paper, Big data is used to predict high-risk factors of Diabetes Mellitus using a new integrated framework with four Hadoop clusters, which are developed to classify the data based on Multi-level MapReduce Fuzzy Classifier (MMR-FC) and MapReduce-Modified Density-Based Spatial Clustering of Applications with Noise (MR-MDBSCAN) algorithm. Big data concerning people’s food habits, physical activity are extracted from social media using the API’s provided. The MMR-FC takes place at three levels of index (Glycemic Index, Physical activity Index, Sleeping Pattern) values. The fuzzy rules are generated by the MMR-FC algorithm to predict the risk of Diabetes Mellitus using the data extracted. The result from MMR-FC is used as an input to the semantic location prediction framework to predict the high-risk zones of Diabetes Mellitus using the MR-MDBSCAN algorithm. The analysis shows that more than 55% of people are in a high-risk group with positive sentiments on the data extracted. More than 70% of food with a high Glycemic Index is usually consumed during Night and Early Evenings, which reveals that people consume food that has a high Glycemic Index during their sedentary slot and have irregular sleep practices. Around 70% of the unhealthiest dietary patterns are retrieved from urban hotspots such as Delhi, Cochin, Kolkata, and Chennai. From the results, it is evident that 55% of younger generations, users of social networking sites having high possibilities of Type II Diabetes Mellitus at large.|https://doi.org/10.1016/j.asoc.2021.107423|https://www.sciencedirect.com/science/article/pii/S156849462100346X||||2021|An integrated multi-node Hadoop framework to predict high-risk factors of Diabetes Mellitus using a Multilevel MapReduce based Fuzzy Classifier (MMR-FC) and Modified DBSCAN algorithm|J. Ramsingh and V. Bhuvaneswari|article|RAMSINGH2021107423|||Applied Soft Computing|15684946||108|||||18136|1,290|Q1|143|831|2023|45163|15975|2003|7,71|54,35|Netherlands|Western Europe|2001-2020|Software (Q1)||||1115574466|92497619
BDAW '16|Blagoevgrad, Bulgaria|Big Data characteristics, Data Quality, Big Data, Data Quality Dimensions|6||Proceedings of the International Conference on Big Data and Advanced Wireless Technologies|As Big Data becomes better understood, there is a need for a comprehensive definition of Big Data to support work in fields such as data quality for Big Data. Existing definitions of Big Data define Big Data by comparison with existing, usually relational, definitions, or define Big Data in terms of data characteristics or use an approach which combines data characteristics with the Big Data environment. In this paper we examine existing definitions of Big Data and discuss the strengths and limitations of the different approaches, with particular reference to issues related to data quality in Big Data. We identify the issues presented by incomplete or inconsistent definitions. We propose an alternative definition and relate this definition to our work on quality in Big Data.|10.1145/3010089.3010090|https://doi.org/10.1145/3010089.3010090|New York, NY, USA|Association for Computing Machinery|9781450347792|2016|Defining Big Data|Emmanuel, Isitor and Stanier, Clare|inproceedings|10.1145/3010089.3010090|5||||||||||||||||||||||||||||1116684662|42
||Industry 4.0, Performance management, Decision support, Big Data, Uncertainty modeling||103666||For the past few years, we have been hearing about Industry 4.0 (or the fourth industrial revolution), which promises to improve productivity, flexibility, quality, customer satisfaction and employee well-being. To assess whether these goals are achieved, it is necessary to implement a performance management system (PMS). However, a PMS must take into account the various challenges associated with Industry 4.0, including the availability of large amounts of data. While it represents an opportunity for companies to improve performance, big data does not necessarily mean good data. It can be uncertain, imprecise, ambiguous, etc. Uncertainty is one of the major challenges and it is essential to take it into account when computing performance indicators to increase confidence in decision making. To address this issue, we propose a method to model uncertainty in key performance indicators (KPIs). Our work allows associating with each indicator an uncertainty noted m, computed on the basis of the theory of belief functions. The KPI and its associated uncertainty form a pair (KP I, m). The method developed allows calculating this uncertainty m for the input data of the performance management system. We show how these modeled uncertainties should be propagated to the KPIs. For these KPI uncertainties, we have defined rules to support decision-making. The method developed, based on the theory of belief functions, is part of a methodology we propose to define and extract smart data from massive data. To our knowledge, this is the first attempt to use this theory to model uncertain performance indicators. Our work has shown its effectiveness and its applicability to a case of bottle filling line simulation. In addition to these results, this work opens up new perspectives, particularly for taking uncertainty into account in expert opinions and in industrial risk assessment.|https://doi.org/10.1016/j.compind.2022.103666|https://www.sciencedirect.com/science/article/pii/S016636152200063X||||2022|Uncertainty of key performance indicators for Industry 4.0: A methodology based on the theory of belief functions|Amel Souifi and Zohra Cherfi Boulanger and Marc Zolghadri and Maher Barkallah and Mohamed Haddar|article|SOUIFI2022103666|||Computers in Industry|01663615||140|||||19080|1,432|Q1|100|115|323|6926|3165|319|9,96|60,23|Netherlands|Western Europe|1979-2020|Computer Science (miscellaneous) (Q1); Engineering (miscellaneous) (Q1)|6,018|7.635|0.00584|1119203551|605146181
||Performance evaluation;Measurement uncertainty;Transforms;Medical services;Electrocardiography;Big Data;Monitoring;Transform coding;Lossy compression;IoT;Health care;R-peak;data fidelity||3494-3499|2020 IEEE International Conference on Big Data (Big Data)|The growing demand for recording longer ECG signals to improve the effectiveness of IoT-enabled remote clinical healthcare is contributing large amounts of ECG data. While lossy compression techniques have shown potential in significantly lowering the amount of data, investigation on how to trade-off between data reduction and data fidelity on ECG data received relatively less attention. This paper gives insight into the power of lossy compression to ECG signals by balancing between data quality and compression ratio. We evaluate the performance of transformed-based lossy compressions on the ECG datasets collected from the Biosemi ActiveTwo devices. Our experimental results indicate that ECG data exhibit high energy compaction property through transformations like DCT and DWT, thus could improve compression ratios significantly without hurting data fidelity much. More importantly, we evaluate the effect of lossy compression on ECG signals by validating the R-peak in the QRS complex. Our method can obtain low error rates measured in PRD (as low as 0.3) and PSNR (up to 67) using only 5% of the transform coefficients. Therefore, R-peaks in the reconstructed ECG signals are almost identical to ones in the original signals, thus facilitating extended ECG monitoring.|10.1109/BigData50022.2020.9378343|||||2020|Understanding Bit-Error Trade-off of Transform-based Lossy Compression on Electrocardiogram Signals|Moon, Aekyeung and Woo Son, Seung and Jung, Jiuk and Jeong Song, Yun|inproceedings|9378343||Dec|||||||||||||||||||||||||||1119489742|42
||Data cleansing, Data quality, Medical AI, Medical devices||105587||"\"Data quality is of paramount importance for the smooth functioning of modern data-driven AI applications with machine learning as a core technology. This is also true for medical AI, where malfunctions due to \"\"dirty data\"\" can have particularly dramatic harmful implications. Consequently, data cleansing is an important part in improving the usability of (Big) Data for medical AI systems. However, it should not be overlooked that data cleansing can also have negative effects on data quality if not performed carefully. This paper takes an interdisciplinary look at some of the technical and legal challenges of data cleansing against the background of European medical device law, with the key message that technical and legal aspects must always be considered together in such a sensitive context.\""|https://doi.org/10.1016/j.clsr.2021.105587|https://www.sciencedirect.com/science/article/pii/S0267364921000601||||2021|Legal aspects of data cleansing in medical AI|Karl Stöger and David Schneeberger and Peter Kieseberg and Andreas Holzinger|article|STOGER2021105587|||Computer Law & Security Review|02673649||42|||||28888|0,815|Q1|38|66|242|1245|707|223|3,39|18,86|United Kingdom|Western Europe|1985-2020|Business, Management and Accounting (miscellaneous) (Q1); Computer Networks and Communications (Q1); Law (Q1)||||1120849393|1769124433
||Data quality, Main path analysis, Knowledge diffusion, Citation analysis, Social network analysis, Big data||594-605||This study presents a unique approach in investigating the knowledge diffusion structure for the field of data quality through an analysis of the main paths. We study a dataset of 1880 papers to explore the knowledge diffusion path, using citation data to build the citation network. The main paths are then investigated and visualized via social network analysis. This paper takes three different main path analyses, namely local, global, and key-route, to depict the knowledge diffusion path and additionally implements the g-index and h-index to evaluate the most important journals and researchers in the data quality domain.|https://doi.org/10.1016/j.joi.2014.05.001|https://www.sciencedirect.com/science/article/pii/S1751157714000492||||2014|Knowledge diffusion path analysis of data quality literature: A main path analysis|Yu Xiao and Louis Y.Y. Lu and John S. Liu and Zhili Zhou|article|XIAO2014594|||Journal of Informetrics|17511577|3|8|||||5100155103|1,605|Q1|76|79|276|3786|1696|245|5,66|47,92|Netherlands|Western Europe|2007-2020|Applied Mathematics (Q1); Computer Science Applications (Q1); Library and Information Sciences (Q1); Management Science and Operations Research (Q1); Modeling and Simulation (Q1); Statistics and Probability (Q1)|4,326|5.107|0.00554|1121360195|682984487
||quality management, Quality assessment, big data|3||||10.1145/3449052|https://doi.org/10.1145/3449052|New York, NY, USA|Association for Computing Machinery||2021|Editorial: Special Issue on Quality Assessment and Management in Big Data—Part I|Aljawarneh, Shadi and Lara, Juan A.|article|10.1145/3449052|6|may|J. Data and Information Quality|19361955|2|13|June 2021||||||||||||||||||||||1122436189|833754770
||Fault diagnosis;Principal component analysis;Accuracy;Power grids;Correlation;Power transformers;Correlation coefficient;transformer fault diagnosis;PCC;PCA;BPNN||1737-1742|2015 IEEE 17th International Conference on High Performance Computing and Communications, 2015 IEEE 7th International Symposium on Cyberspace Safety and Security, and 2015 IEEE 12th International Conference on Embedded Software and Systems|In recent power grid systems, data-driven approach has been taken to grid condition evaluation and classification after successful adoption of big data techniques in internet applications. However, the raw training data from single monitoring system, e.g. dissolved gas analysis (DGA), are rarely sufficient for training in the form of valid instances and the data quality can rarely meet the requirement of precise data analytics since raw data set usually contains samples with noisy data. This paper proposes a machine learning scheme (PCA_IR) to improve the accuracy of fault diagnose, which combines dimension-increment procedure based on association analysis, dimension-reduction procedure based on principal component analysis and back propagation neural network (BPNN). First, the dimension of training data is increased by adding selected data which originates from different source such as production management system (PMS) to the original data obtained by DGA. The added data would also inevitably result in more noise. Thus, we then take advantage of the PCA method to reduce the noise in the training data as well as retaining significant information for classification. Finally, the new training data yielded after PCA procedure is inputted into BPNN for classification. We test the PCA_IR scheme on fault diagnosis of power transformers in power grid system. The experimental results show that the classifiers based on our scheme achieve higher accuracy than traditional ones. Therefore, the scheme PCA_IR would be successfully deployed for fault diagnosis in power grid system.|10.1109/HPCC-CSS-ICESS.2015.236|||||2015|An Improved Machine Learning Scheme for Data-Driven Fault Diagnosis of Power Grid Equipment|Zhang, Jinkui and Zhu, Yongxin and Shi, Weiwei and Sheng, Gehao and Chen, Yufeng|inproceedings|7336422||Aug|||||||||||||||||||||||||||1125230710|42
||Open data, Bibliometric analysis, Co-word analysis, Science map, Knowledge areas, Most-studied themes, Future trends||77-87||This paper aims to contribute to a better understanding of the literature on open data in three ways. The first is to develop a descriptive analysis of journals and authors to identify the knowledge areas in which open data are applied. The second is to analyse the conceptual structure of the field using a bibliometric technique. The co-word analysis enabled us to create a map of the main themes that have been studied, identifying their importance and relevance. These themes were analysed and grouped. The third is to propose future research trends. According to our results, the main knowledge areas are Engineering, Health, Public Administration, Management and Education. The main themes are big data, open-linked data and data reuse. Finally, several research questions are proposed according to knowledge area and theme.|https://doi.org/10.1016/j.giq.2018.10.008|https://www.sciencedirect.com/science/article/pii/S0740624X18303216||||2019|Knowledge areas, themes and future research on open data: A co-word analysis|Diego Corrales-Garay and Marta Ortiz-de-Urbina-Criado and Eva-María Mora-Valentín|article|CORRALESGARAY201977|||Government Information Quarterly|0740624X|1|36|||||14735|2,121|Q1|103|76|205|5894|1946|193|8,39|77,55|United Kingdom|Western Europe|1984-2020|E-learning (Q1); Law (Q1); Library and Information Sciences (Q1); Sociology and Political Science (Q1)|5,379|7.279|0.00502|1126283883|1582933551
||Cloud computing, service management, data quality, big data analytics|4||||10.1145/3138806|https://doi.org/10.1145/3138806|New York, NY, USA|Association for Computing Machinery||2018|Challenges in Enabling Quality of Analytics in the Cloud|Truong, Hong-Linh and Murguzur, Aitor and Yang, Erica|article|10.1145/3138806|9|jan|J. Data and Information Quality|19361955|2|9|June 2017||||||||||||||||||||||1128937154|833754770
||software project, big data analytics, anlytics tools.||295-300||Software project is collaborative enterprise of making a desired software for the client. Each software is unique and is delivered by following the process. The process includes understanding the requirement, planning, designing the software and implementation. Risk occurs in the software project which need attention by the managers and workers to make the project efficient. Big data analytics is commonly used in all fields. Big data deals with huge data which are unstructured. Using analytics tools, it can be chunked down and analyzed to provide valuable solutions. In this paper, a review of risk in software project and big data analytics are briefed out.|https://doi.org/10.1016/j.procs.2015.04.045|https://www.sciencedirect.com/science/article/pii/S1877050915005463||||2015|Survey on Software Project Risks and Big Data Analytics|J.H. Rekha and R. Parvathi|article|REKHA2015295|||Procedia Computer Science|18770509||50||Big Data, Cloud and Computing Challenges|||19700182801|0,334|-|76|1907|6483|38511|13722|6359|2,09|20,19|Netherlands|Western Europe|2010-2020|Computer Science (miscellaneous)||||1129249150|2108686752
SoCC '19|Santa Cruz, CA, USA|data provenance, big data systems, Performance debugging, fault localization, data intensive scalable computing|12|465–476|Proceedings of the ACM Symposium on Cloud Computing|Performance is a key factor for big data applications, and much research has been devoted to optimizing these applications. While prior work can diagnose and correct data skew, the problem of computation skew---abnormally high computation costs for a small subset of input data---has been largely overlooked. Computation skew commonly occurs in real-world applications and yet no tool is available for developers to pinpoint underlying causes.To enable a user to debug applications that exhibit computation skew, we develop a post-mortem performance debugging tool. PerfDebug automatically finds input records responsible for such abnormalities in a big data application by reasoning about deviations in performance metrics such as job execution time, garbage collection time, and serialization time. The key to PerfDebug's success is a data provenance-based technique that computes and propagates record-level computation latency to keep track of abnormally expensive records throughout the pipeline. Finally, the input records that have the largest latency contributions are presented to the user for bug fixing. We evaluate PerfDebug via in-depth case studies and observe that remediation such as removing the single most expensive record or simple code rewrite can achieve up to 16X performance improvement.|10.1145/3357223.3362727|https://doi.org/10.1145/3357223.3362727|New York, NY, USA|Association for Computing Machinery|9781450369732|2019|PerfDebug: Performance Debugging of Computation Skew in Dataflow Systems|Teoh, Jason and Gulzar, Muhammad Ali and Xu, Guoqing Harry and Kim, Miryung|inproceedings|10.1145/3357223.3362727|||||||||||||||||||||||||||||1130414451|42
||||113172|||https://doi.org/10.1016/j.dss.2019.113172|https://www.sciencedirect.com/science/article/pii/S0167923619302015||||2019|Perspectives on numerical data quality in IS research|James R. Marsden and David E. Pingry and Jason B. Thatcher|article|MARSDEN2019113172|||Decision Support Systems|01679236||126||Perspectives on Numerical Data Quality in IS Research|||21933|1,564|Q1|151|115|342|7152|2672|338|7,04|62,19|Netherlands|Western Europe|1985-2020|Arts and Humanities (miscellaneous) (Q1); Developmental and Educational Psychology (Q1); Information Systems (Q1); Information Systems and Management (Q1); Management Information Systems (Q1)|13,580|5.795|0.0081|1130773473|1234879127
||Big data, Sensor-based systems, Survey, Application, Challenges||1-12||The emergence of new technologies such as Internet/Web/Network-of-Things and large scale wireless sensor systems enables the collection of data from an increasing volume and variety of networked sensors for analysis. In this review article, we summarize the latest developments of big sensor data systems (a term to conceptualize the application of the big data model towards networked sensor systems) in various representative studies for urban environments, including for air pollution monitoring, assistive living, disaster management systems, and intelligent transportation. An important focus is the inclusion of how value is extracted from the big data system. We also discuss some recent techniques for big data acquisition, cleaning, aggregation, modeling, and interpretation in large scale sensor-based systems. We conclude the paper with a discussion on future perspectives and challenges of sensor-based data systems in the big data era.|https://doi.org/10.1016/j.bdr.2015.12.003|https://www.sciencedirect.com/science/article/pii/S2214579615300241||||2016|Big Sensor Data Applications in Urban Environments|Li-Minn Ang and Kah Phooi Seng|article|ANG20161|||Big Data Research|22145796||4|||||21100356018|0,565|Q2|25|11|76|547|324|69|4,06|49,73|United States|Northern America|2014-2020|Computer Science Applications (Q2); Information Systems (Q2); Information Systems and Management (Q2); Management Information Systems (Q2)|586|3.578|0.00126|1132460702|1627174784
||Quality management, Data mining, Machine Learning, Multi-class classification, Neural Network||107580||Root cause analysis for quality problem solving is critical to improve product quality performance and reduce the quality risk for manufacturers. Subjective conventional methods have been applied frequently in past decades. However, due to increasingly complex product and supply chain structures, diverse working conditions, and massive amounts of components, accuracy and efficiency of root cause analysis are progressively challenged in practice. Therefore, data-driven root cause analysis methods have attracted attention lately. In this paper, taking advantage of the availability of big operations data and the rapid development of data science, we design a big data-driven root cause analysis system utilizing Machine Learning techniques to improve the performance of root cause analysis. More specifically, we first propose a conceptual framework of the big data-driven root cause analysis system including three modules of Problem Identification, Root Cause Identification, and Permanent Corrective Action. Furthermore, in the Problem Identification Module, we construct a unified feature-based approach to describe multiple and different types of quality problems by applying a data mining method. In the Root Cause Identification Module, we use supervised Machine Learning (classification) methods to automatically predict the root causes of multiple quality problems. Finally, we illustrate the accuracy and efficiency of the proposed system and algorithms based on actual quality data from a case company. This study contributes to the literature from the following aspects: (i) the integrated system and algorithms can be used directly to develop a computer application to manage and solve quality problems with high concurrences and complexities in any manufacturing process; (ii) a general procedure and method are provided to formulate and describe a large quantity and different types of quality problems; (iii) compared with traditional methods, it is demonstrated using real case data that manufacturing companies can save significant time and cost with our proposed data-driven root cause analysis system; (iv) this study not only aims at improving the quality problem solving practices for a complex manufacturing process but also bridges a gap between the theoretical development of Machining Learning methods and their application in the operations management domain.|https://doi.org/10.1016/j.cie.2021.107580|https://www.sciencedirect.com/science/article/pii/S0360835221004848||||2021|A big data-driven root cause analysis system: Application of Machine Learning in quality problem solving|Qiuping Ma and Hongyan Li and Anders Thorstenson|article|MA2021107580|||Computers & Industrial Engineering|03608352||160|||||18164|1,315|Q1|128|641|1567|31833|9597|1557|5,97|49,66|United Kingdom|Western Europe|1976-2020|Computer Science (miscellaneous) (Q1); Engineering (miscellaneous) (Q1)||||1132463350|1798521593
||RDF, Web of Data, Semantic Web, Big Data, Data management||1-8|RDF Database Systems|This chapter motivates the importance of RDF data management through the Big Data and Web of Data/Semantic Web phenomena. It also provides some insights of existing RDF stores and presents the dimensions used in this book to compare these systems.|https://doi.org/10.1016/B978-0-12-799957-9.00001-8|https://www.sciencedirect.com/science/article/pii/B9780127999579000018|Boston|Morgan Kaufmann|978-0-12-799957-9|2015|Chapter One - Introduction||incollection|20151||||||||||Olivier Curé and Guillaume Blin|||||||||||||||||||1134310497|42
||Big data analytics usage, Data analytics capabilities, Decision-making quality, Agricultural firms||121355||Big data initiatives are critical for transforming traditional organizational decision making into data-driven decision making. However, prior information systems research has not paid enough attention to the impact of big data analytics usage on decision-making quality. Drawing on the dynamic capability theory, this study investigated the impact of big data analytics usage on decision-making quality and tested the mediating effect of data analytics capabilities. We collected data from 240 agricultural firms in China. The empirical results showed that big data analytics usage had a positive impact on decision-making quality and that data analytics capabilities played a mediating role in the relationship between big data analytics usage and decision-making quality. Hence, firms should not only popularize big data analytics usage in their business activities but also take measures to improve their data analytics capabilities, which will improve their decision-making quality toward competitive advantages.|https://doi.org/10.1016/j.techfore.2021.121355|https://www.sciencedirect.com/science/article/pii/S0040162521007861||||2022|Evaluating the impact of big data analytics usage on the decision-making quality of organizations|Lei Li and Jiabao Lin and Ye Ouyang and Xin (Robert) Luo|article|LI2022121355|||Technological Forecasting and Social Change|00401625||175|||||14704|2,226|Q1|117|448|1099|35581|10127|1061|9,01|79,42|United States|Northern America|1970-2020|Applied Psychology (Q1); Business and International Management (Q1); Management of Technology and Innovation (Q1)|21,116|8.593|0.02416|1135308467|1949868303
|||2|195–196|Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice|||https://doi.org/10.1145/3447404.3447415|New York, NY, USA|Association for Computing Machinery|9781450390293|2021|Ethical Issues in Machine Learning|McMenemy, David|inbook|10.1145/3447404.3447415|||||||||1||||||||||||||||||||1136043353|42
Next Gen Tech Driven Personalized Med&Smart Healthcare||Data mining, maritime healthcare, maritime risk estimation, automatic identification system (AIS), maritime safety, artificial intelligence||145-160|Artificial Intelligence and Big Data Analytics for Smart Healthcare|With the rapid development of maritime industries, the vessel traffic density has been gradually increased leading to increasing the potential risk of ship collision accidents in crowded inland waterways. It will bring negative effects on human life safety and global maritime economy. Therefore it is of vital significance to study the risk of ship collision using big data mining techniques. The big data–driven computational results are beneficial for guaranteeing smart maritime healthcare in the fields of ocean engineering and maritime management. This chapter proposes to quantitatively estimate the ship collision risk based on ship domain modeling and real-time vessel trajectory data. In particular, the trajectory data quality is first improved using the cubic spline interpolation method. We assume that the ship collision risk is highly related to the cross areas of ship domains between different ships, which are then computed using the Monte Carlo simulation strategy. For the sake of better understanding, the kernel density estimation method is finally adopted to visually generate the ship collision risk in maps. Experimental results on realistic spatiotemporal big data have illustrated the effectiveness of the proposed method in crowded inland waterways.|https://doi.org/10.1016/B978-0-12-822060-3.00006-1|https://www.sciencedirect.com/science/article/pii/B9780128220603000061||Academic Press|978-0-12-822060-3|2021|Chapter 10 - Spatiotemporal Big Data-Driven Vessel Traffic Risk Estimation for Promoting Maritime Healthcare: Lessons Learnt from Another Domain than Healthcare|Zikun Feng and Yan Li and Zhao Liu and Ryan Wen Liu|incollection|FENG2021145||||||||||Miltiadis D. Lytras and Akila Sarirete and Anna Visvizi and Kwok Tai Chui|||||||||||||||||||1141379780|42
IoT '17|Linz, Austria||8||Proceedings of the Seventh International Conference on the Internet of Things|Internet of Things (IoT) data are increasingly viewed as a new form of massively distributed and large scale digital assets, which are continuously generated by millions of connected devices. The real value of such assets can only be realized by allowing IoT data trading to occur on a marketplace that rewards every single producer and consumer, at a very granular level. Crucially, we believe that such a marketplace should not be owned by anybody, and should instead fairly and transparently self-enforce a well defined set of governance rules. In this paper we address some of the technical challenges involved in realizing such a marketplace. We leverage emerging blockchain technologies to build a decentralized, trusted, transparent and open architecture for IoT traffic metering and contract compliance, on top of the largely adopted IoT brokered data infrastructure. We discuss an Ethereum-based prototype implementation and experimentally evaluate the overhead cost associated with Smart Contract transactions, concluding that a viable business model can indeed be associated with our technical approach.|10.1145/3131542.3131564|https://doi.org/10.1145/3131542.3131564|New York, NY, USA|Association for Computing Machinery|9781450353182|2017|Mind My Value: A Decentralized Infrastructure for Fair and Trusted IoT Data Trading|Missier, Paolo and Bajoudah, Shaimaa and Capossele, Angelo and Gaglione, Andrea and Nati, Michele|inproceedings|10.1145/3131542.3131564|15||||||||||||||||||||||||||||1144285737|42
||network management, internet measurements, traffic engineering, quality of experience|8|32–39||This article summarises a 2.5 day long Dagstuhl seminar on Global Measurements: Practice and Experience held in January 2016. This seminar was a followup of the seminar on Global Measurement Frameworks held in 2013, which focused on the development of global Internet measurement platforms and associated metrics. The second seminar aimed at discussing the practical experience gained with building these global Internet measurement platforms. It brought together people who are actively involved in the design and maintenance of global Internet measurement platforms and who do research on the data delivered by such platforms. Researchers in this seminar have used data derived from global Internet measurement platforms in order to manage networks or services or as input for regulatory decisions. The entire set of presentations delivered during the seminar is made publicly available at [1].|10.1145/2935634.2935641|https://doi.org/10.1145/2935634.2935641|New York, NY, USA|Association for Computing Machinery||2016|Global Measurements: Practice and Experience (Report on Dagstuhl Seminar #16012)|"\"Bajpai, Vaibhav and Berger, Arthur W. and Eardley, Philip and Ott, J\"\"{o}rg and Sch\\\"\"{o}nw\\\"\"{a}lder, J\\\"\"{u}rgen\""|article|10.1145/2935634.2935641||may|SIGCOMM Comput. Commun. Rev.|01464833|2|46|April 2016||||||||||||||||||||||1144874819|121469385
SIGMOD '15|Melbourne, Victoria, Australia|data profiling, error diagnosis, data cleaning|15|1231–1245|Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data|A lot of systems and applications are data-driven, and the correctness of their operation relies heavily on the correctness of their data. While existing data cleaning techniques can be quite effective at purging datasets of errors, they disregard the fact that a lot of errors are systematic, inherent to the process that produces the data, and thus will keep occurring unless the problem is corrected at its source. In contrast to traditional data cleaning, in this paper we focus on data diagnosis: explaining where and how the errors happen in a data generative process.We develop a large-scale diagnostic framework called DATA X-RAY. Our contributions are three-fold. First, we transform the diagnosis problem to the problem of finding common properties among erroneous elements, with minimal domain-specific assumptions. Second, we use Bayesian analysis to derive a cost model that implements three intuitive principles of good diagnoses. Third, we design an efficient, highly-parallelizable algorithm for performing data diagnosis on large-scale data. We evaluate our cost model and algorithm using both real-world and synthetic data, and show that our diagnostic framework produces better diagnoses and is orders of magnitude more efficient than existing techniques.|10.1145/2723372.2750549|https://doi.org/10.1145/2723372.2750549|New York, NY, USA|Association for Computing Machinery|9781450327589|2015|Data X-Ray: A Diagnostic Tool for Data Errors|Wang, Xiaolan and Dong, Xin Luna and Meliou, Alexandra|inproceedings|10.1145/2723372.2750549|||||||||||||||||||||||||||||1145942539|42
|||4|1786–1789||The great amount of time series generated by machines has enormous value in intelligent industry. Knowledge can be discovered from high-quality time series, and used for production optimization and anomaly detection in industry. However, the original sensors data always contain many errors. This requires a sophisticated cleaning strategy and a well-designed system for industrial data cleaning. Motivated by this, we introduce Cleanits, a system for industrial time series cleaning. It implements an integrated cleaning strategy for detecting and repairing three kinds of errors in industrial time series. We develop reliable data cleaning algorithms, considering features of both industrial time series and domain knowledge. We demonstrate Cleanits with two real datasets from power plants. The system detects and repairs multiple dirty data precisely, and improves the quality of industrial time series effectively. Cleanits has a friendly interface for users, and result visualization along with logs are available during each cleaning process.|10.14778/3352063.3352066|https://doi.org/10.14778/3352063.3352066||VLDB Endowment||2019|Cleanits: A Data Cleaning System for Industrial Time Series|Ding, Xiaoou and Wang, Hongzhi and Su, Jiaxuan and Li, Zijue and Li, Jianzhong and Gao, Hong|article|10.14778/3352063.3352066||aug|Proc. VLDB Endow.|21508097|12|12|August 2019||||21100199855|0,946|Q1|134|246|598|12401|3759|566|5,50|50,41|United States|Northern America|2008-2020|Computer Science (miscellaneous) (Q1)|7,198|2.047|0.00891|1151974124|1216159931
CHI '21|Yokohama, Japan|Kenya, raters, USA, data cascades, Nigeria, data politics, application-domain experts, Uganda, data collectors, Data, developers, India, AI, data quality, high-stakes AI, ML, Ghana|15||Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems|AI models are increasingly applied in high-stakes domains like health and conservation. Data quality carries an elevated significance in high-stakes AI due to its heightened downstream impact, impacting predictions like cancer detection, wildlife poaching, and loan allocations. Paradoxically, data is the most under-valued and de-glamorised aspect of AI. In this paper, we report on data practices in high-stakes AI, from interviews with 53 AI practitioners in India, East and West African countries, and USA. We define, identify, and present empirical evidence on Data Cascades—compounding events causing negative, downstream effects from data issues—triggered by conventional AI/ML practices that undervalue data quality. Data cascades are pervasive (92% prevalence), invisible, delayed, but often avoidable. We discuss HCI opportunities in designing and incentivizing data excellence as a first-class citizen of AI, resulting in safer and more robust systems for all.|10.1145/3411764.3445518|https://doi.org/10.1145/3411764.3445518|New York, NY, USA|Association for Computing Machinery|9781450380966|2021|“Everyone Wants to Do the Model Work, Not the Data Work”: Data Cascades in High-Stakes AI|Sambasivan, Nithya and Kapania, Shivani and Highfill, Hannah and Akrong, Diana and Paritosh, Praveen and Aroyo, Lora M|inproceedings|10.1145/3411764.3445518|39||||||||||||||||||||||||||||1153531011|42
||Data, history of data, data quality, data management, data governance, data stewardship, data and technology, process improvement, technology strategy, culture/organization, data literacy||31-45|Meeting the Challenges of Data Quality Management|This chapter describes the five challenges in data quality management (data, process, technology, people, and culture/organization) and proposes that organizations that want to get more value and insight from their data should take a strategic approach to data quality management. This is because quality is not an accident, and it cannot be an afterthought, especially in today’s complex organizations. This chapter provides the context for Section 2 and introduces critical terminology used throughout the book.|https://doi.org/10.1016/B978-0-12-821737-5.00002-X|https://www.sciencedirect.com/science/article/pii/B978012821737500002X||Academic Press|978-0-12-821737-5|2022|Chapter 2 - Organizational Data and the Five Challenges of Managing Data Quality|Laura Sebastian-Coleman|incollection|SEBASTIANCOLEMAN202231||||||||||Laura Sebastian-Coleman|||||||||||||||||||1156449041|42
||Productivity;Circuits and systems;Big Data;Acceleration;Information technology;data quality;human judgments||1-4|2021 IEEE International Symposium on Circuits and Systems (ISCAS)|Accumulation of information is essential for human knowledge production, and information technology has accelerated the speed of data accumulation. The increase in quantity of information with high speed does not promise high-quality knowledge production and possibly does cause problems. One big problem is lack of storage for such big data. Another critical problem in information usage is information overload, that is, deterioration of productivity by too much information. Decision accuracy decrease with amount of information beyond a certain point while it increases at the beginning. We introduce an approach for solution of these problems with an example of research along the approach.|10.1109/ISCAS51556.2021.9401161|||||2021|Quali-Informatics in the Society with Yotta Scale Data|Shioiri, Satoshi and Sato, Yoshiyuki and Horaguchi, Yuta and Muraoka, Hiroaki and Nihei, Mariko|inproceedings|9401161||May||21581525|||||||||||||||||||||||||1156706975|1416780799
GeoHumanities '19|Chicago, Illinois|big data, points of interest, graph database, openstreetmap, ontology|6||Proceedings of the 3rd ACM SIGSPATIAL International Workshop on Geospatial Humanities|Scalability, standardization, and management are important issues when working with very large Volunteered Geographic Information (VGI). VGI is a rich and valuable source of Points of Interest (POI) information, but its inherent heterogeneity in content, structure, and scale across sources present major challenges for interlinking data sources for analysis. To be useful at scale, the raw information needs to be transformed into a standardized schema that can be easily and reliably used by data analysts. In this work, we tackle the problem of unifying POI categories (e.g. restaurants, temple, and hotel) across multiple data sources to aid in improving land use maps and population distribution estimation as well as support data analysts wishing to fuse multiple data sources with the OpenStreetMap (OSM) mapping platform or working with projects that are already configured in the OSM schema and wish to add additional sources of information. Graph theory and its implementation through the SONET graph database, provides a programmatic way to organize, store, and retrieve standardized POI categories at multiple levels of abstraction. Additionally, it addresses category heterogeneity across data sources by standardizing and managing categories in a way that makes cross-domain analysis possible.|10.1145/3356991.3365474|https://doi.org/10.1145/3356991.3365474|New York, NY, USA|Association for Computing Machinery|9781450369602|2019|SONET: A Semantic Ontological Network Graph for Managing Points of Interest Data Heterogeneity|Palumbo, Rachel and Thompson, Laura and Thakur, Gautam|inproceedings|10.1145/3356991.3365474|6||||||||||||||||||||||||||||1156843142|42
||Receivers;Mathematical models;Surface treatment;Sea surface;Indexes;Earth;Computational modeling;Closure phase;first arrivals;interferometry;statics||1-13||Recorded seismograms are usually distorted by statics owing to complex geological conditions, such as lateral variations in sediment thickness or complex topographies. These distorted and discontinuous signals usually exist in either arrival times or amplitudes of waves, and they are most likely to be smeared as velocity perturbations along their associated raypaths. Therefore, statics may blur images of the target bodies or, even worse, introduce unexpected and false anomalies into subsurface structures. To partly resolve this problem, we develop a weighted statics correction method to estimate unwanted temporal shifts of traces using the closure-phase technique, which is utilized in astronomical imaging. In the proposed method, the source and receiver statics are regarded as independent quantities contributing to the waveform shifts based on their acquisition geometries. Numerical tests on both the synthetic and field cases show noticeable, although gradual, improvements in data quality compared to the conventional plus–minus (PM) method. In general, this method provides a straightforward strategy to reedit the travel times in seismic profiles without inverting for a near-surface velocity model. Moreover, it can be extended to any interferometrical methods in seismic data processing that satisfies the closure-phase conditions.|10.1109/TGRS.2022.3169519|||||2022|A Weighted Closure-Phase Statics Correction Method: Synthetic and Field Data Examples|Yu, Han and Hanafy, Sherif M. and Liu, Lulu|article|9761878|||IEEE Transactions on Geoscience and Remote Sensing|15580644||60|||||17360|2,141|Q1|254|823|1908|30841|13865|1908|6,87|37,47|United States|Northern America|1980-2020|Earth and Planetary Sciences (miscellaneous) (Q1); Electrical and Electronic Engineering (Q1)|48,898|5.600|0.04379|1159650509|1478527554
FAccT '22|Seoul, Republic of Korea|Intelligence Analysis, Ethic Awareness, Critical Data Studies, Machine Learning, Critical Algorithm Studies, Visual Analytics, Science &amp; Technology Studies, Interdisciplinary Research, Communication Analysis|13|877–889|2022 ACM Conference on Fairness, Accountability, and Transparency|Digital systems for analyzing human communication data have become prevalent in recent years. This may be related to the increasing abundance of data that can be harnessed but can hardly be managed manually. Intelligence analysis of communications data in investigative journalism, criminal intelligence, and law present particularly interesting cases, as they must take into account the often highly sensitive properties of the underlying operations and data. At the same time, these are areas where increasingly automated, sophisticated approaches and tailored systems can be particularly useful and relevant, especially in terms of Big Data manageability. However, by the shifting of responsibilities, this also poses dangers. In addition to privacy concerns, these dangers relate to uncertain or poor data quality, leading to discrimination and potentially misleading insights. Other problems relate to a lack of transparency and traceability, making it difficult to accurately identify problems and determine appropriate remedial strategies. Visual analytics combines machine learning methods with interactive visual interfaces to enable human sense- and decision-making. This technique can be key for designing and operating meaningful interactive communication analysis systems that consider these ethical challenges. In this interdisciplinary work, a joint endeavor of computer scientists, ethicists, and scholars in Science &amp; Technology Studies, we investigate and evaluate opportunities and risks involved in using Visual analytics approaches for communication analysis in intelligence applications in particular. We introduce, at first, the common technological systems used in communication analysis, with a special focus on intelligence analysis in criminal investigations, further discussing the domain-specific ethical implications, tensions, and risks involved. We then make the case of how tailored Visual Analytics approaches may reduce and mitigate the described problems, both theoretically and through practical examples. Offering interactive analysis capabilities and what-if explorations while facilitating guidance, provenance generation, and bias awareness (through nudges, for example) can improve analysts’ understanding of their data, increasing trustworthiness, accountability, and generating knowledge. We show that finding Visual Analytics design solutions for ethical issues is not a mere optimization task with an ideal final solution. Design solutions for specific ethical problems (e.g., privacy) often trigger new ethical issues (e.g., accountability) in other areas. Balancing out and negotiating these trade-offs has, as we argue, to be an integral aspect of the system design process from the outset. Finally, our work identifies existing gaps and highlights research opportunities, further describing how our results can be transferred to other domains. With this contribution, we aim at informing more ethically-aware approaches to communication analysis in intelligence operations.|10.1145/3531146.3533151|https://doi.org/10.1145/3531146.3533151|New York, NY, USA|Association for Computing Machinery|9781450393522|2022|Promoting Ethical Awareness in Communication Analysis: Investigating Potentials and Limits of Visual Analytics for Intelligence Applications|Fischer, Maximilian T. and Hirsbrunner, Simon David and Jentner, Wolfgang and Miller, Matthias and Keim, Daniel A. and Helm, Paula|inproceedings|10.1145/3531146.3533151|||||||||||||||||||||||||||||1160566085|42
dg.o 2019|Dubai, United Arab Emirates|e-government, 311 data, big data analytics, information visualization|10|1–10|Proceedings of the 20th Annual International Conference on Digital Government Research|As part of the open government movement, an increasing number of 311 call centers have made their datasets available to the public. Studies have found that 311 request patterns are associated with personal attributes and living conditions. Most of these studies used New York City 311 data. In this study, we use 311 data from the City of Miami, a smaller local government, as a case study. This study contributes to digital government research and practices by making suggestions on best practices regarding the use of big data analytics on 311 data. In addition, we discuss limitations of 311 data and analytics results. Finally, we expect our results to inform decision making within the City of Miami government and other local governments.|10.1145/3325112.3325212|https://doi.org/10.1145/3325112.3325212|New York, NY, USA|Association for Computing Machinery|9781450372046|2019|Processes, Potential Benefits, and Limitations of Big Data Analytics: A Case Analysis of 311 Data from City of Miami|Hagen, Loni and Seon Yi, Hye and Pietri, Siana and E. Keller, Thomas|inproceedings|10.1145/3325112.3325212|||||||||||||||||||||||||||||1162848095|42
||Cities and towns;Decision making;IEEE 802.11 Standards;Time series analysis;Context;Urban planning;Data Driven Decision Making;Value Network Analysis;urban planning||998-1003|2013 International Conference on Signal-Image Technology & Internet-Based Systems|This article provides a methodology of assessing the (Big)/(Open) Data quality in Data Driven Decision Making with the Value Network Analysis approach discovering the value creation failure point(s) in the network and evaluating the impact of loss of vale of data in DDDM process.|10.1109/SITIS.2013.161|||||2013|Using Value Network Analysis to Support Data Driven Decision Making in Urban Planning|Martelli, Cristina and Bellini, Emanuele|inproceedings|6727311||Dec|||||||||||||||||||||||||||1163223863|42
||Big data, Informatics, High-dimensionality, Alzheimer's disease, Aging, Molecular signature, Transcriptomics, Metabolomics, Proteomics, Genomics||961-975||Biomedical data sets are becoming increasingly larger and a plethora of high-dimensionality data sets (“Big Data”) are now freely accessible for neurodegenerative diseases, such as Alzheimer's disease. It is thus important that new informatic analysis platforms are developed that allow the organization and interrogation of Big Data resources into a rational and actionable mechanism for advanced therapeutic development. This will entail the generation of systems and tools that allow the cross-platform correlation between data sets of distinct types, for example, transcriptomic, proteomic, and metabolomic. Here, we provide a comprehensive overview of the latest strategies, including latent semantic analytics, topological data investigation, and deep learning techniques that will drive the future development of diagnostic and therapeutic applications for Alzheimer's disease. We contend that diverse informatic “Big Data” platforms should be synergistically designed with more advanced chemical/drug and cellular/tissue-based phenotypic analytical predictive models to assist in either de novo drug design or effective drug repurposing.|https://doi.org/10.1016/j.jalz.2018.01.014|https://www.sciencedirect.com/science/article/pii/S1552526018300402||||2018|Intelligent and effective informatic deconvolution of “Big Data” and its future impact on the quantitative nature of neurodegenerative disease therapy|Stuart Maudsley and Viswanath Devanarayan and Bronwen Martin and Hugo Geerts|article|MAUDSLEY2018961|||Alzheimer's & Dementia|15525260|7|14|||||||||||||||||||||||1163691473|1942035071
ICARCSET '15|Unnao, India|exploration, Data analytics, monitor, application, Big data, infringement, unstructured data|5||Proceedings of the 2015 International Conference on Advanced Research in Computer Science Engineering &amp; Technology (ICARCSET 2015)|As today there is an enormous volume of data, examining these large sets contains structure and unstructured data of different types and sizes; big data analytics is used. Data Analytics allows the user to analyze the unusable data to make a faster and better decision. The Latest supply chain professionals are suffused with data, which provokes various new ways of thoughts regarding how the data are produced, ordered, controlled and analyzed. Data Quality in Supply-Chain Management is used to monitor and control the data. This paper presents the knowledge infrastructure about trends in big data analytics. Big Data can also be given as an all-encompassing term for any collection of data sets so large or complex that it becomes difficult to process using traditional data processing applications. The challenges include examination, confine, extent, exploration, sharing, storage, relocate, and visualization and privacy infringement. The Big Data have their application in various fields like in Tourism, Climate Research and many other fields.|10.1145/2743065.2743099|https://doi.org/10.1145/2743065.2743099|New York, NY, USA|Association for Computing Machinery|9781450334419|2015|Perspectives, Motivations and Implications Of Big Data Analytics|Amudhavel, J. and Padmapriya, V. and Gowri, V. and Lakshmipriya, K. and Kumar, K. Prem and Thiyagarajan, B.|inproceedings|10.1145/2743065.2743099|34||||||||||||||||||||||||||||1166395729|42
||Decision making;Solid modeling;Mechanical variables measurement;Electric variables measurement;Shape measurement;Data analysis;Analytical models;Data analytics;opportunity identification and prioritization;architecture;additive manufacturing||3458-3467|2019 IEEE International Conference on Big Data (Big Data)|Many industries, including manufacturing, are adopting data analytics (DA) in making decisions to improve quality, cost, and on-time delivery. In recent years, more research and development efforts have applied DA to additive manufacturing (AM) decision-making problems such as part design and process planning. Though there are many AM decision-making problems, not all benefit greatly from DA. This may be due to insufficient AM data, unreliable data quality, or the fact that DA is not cost effective when it is applied to some AM problems. This paper proposes a framework to investigate DA opportunities in a manufacturing operation, specifically AM. The proposed framework identifies and prioritizes AM potential opportunities where DA can make impact. The proposed framework is presented in a five-tier architecture, including value, decision-making, data analytics, data, and data source tiers. A case study is developed to illustrate how the proposed framework identifies DA opportunities in AM.|10.1109/BigData47090.2019.9006489|||||2019|A Framework for Identifying and Prioritizing Data Analytics opportunities in Additive Manufacturing|Park, Hyunseop and Ko, Hyunwoong and Lee, Yung-Tsun T. and Cho, Hyunbo and Witherell, Paul|inproceedings|9006489||Dec|||||||||||||||||||||||||||1166989000|42
|||10|985–994||Protein-protein interactions play essential roles in various biological progresses. Identifying protein interaction sites can facilitate researchers to understand life activities and therefore will be helpful for drug design. However, the number of experimental determined protein interaction sites is far less than that of protein sites in protein-protein interaction or protein complexes. Therefore, the negative and positive samples are usually imbalanced, which is common but bring result bias on the prediction of protein interaction sites by computational approaches. In this work, we presented three imbalance data processing strategies to reconstruct the original dataset, and then extracted protein features from the evolutionary conservation of amino acids to build a predictor for identification of protein interaction sites. On a dataset with 10,430 surface residues but only 2,299 interface residues, the imbalance dataset processing strategies can obviously reduce the prediction bias, and therefore improve the prediction performance of protein interaction sites. The experimental results show that our prediction models can achieve a better prediction performance, such as a prediction accuracy of 0.758, or a high F-measure of 0.737, which demonstrated the effectiveness of our method.|10.1109/TCBB.2019.2953908|https://doi.org/10.1109/TCBB.2019.2953908|Washington, DC, USA|IEEE Computer Society Press||2019|Imbalance Data Processing Strategy for Protein Interaction Sites Prediction|Wang, Bing and Mei, Changqing and Wang, Yuanyuan and Zhou, Yuming and Cheng, Mu-Tian and Zheng, Chun-Hou and Wang, Lei and Zhang, Jun and Chen, Peng and Xiong, Yan|article|10.1109/TCBB.2019.2953908||nov|IEEE/ACM Trans. Comput. Biol. Bioinformatics|15455963|3|18|May-June 2021||||||||||||||||||||||1167315074|1878427007
CIKM '14|Shanghai, China|big data, data cleaning, data quality|3|2024–2026|Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management|In this demo, we present Cleanix, a prototype system for cleaning relational Big Data. Cleanix takes data integrated from multiple data sources and cleans them on a shared-nothing machine cluster. The backend system is built on-top-of an extensible and flexible data-parallel substrate - the Hyracks framework. Cleanix supports various data cleaning tasks such as abnormal value detection and correction, incomplete data filling, de-duplication, and conflict resolution. We demonstrate that Cleanix is a practical tool that supports effective and efficient data cleaning at the large scale.|10.1145/2661829.2661837|https://doi.org/10.1145/2661829.2661837|New York, NY, USA|Association for Computing Machinery|9781450325981|2014|Cleanix: A Big Data Cleaning Parfait|Wang, Hongzhi and Li, Mingda and Bu, Yingyi and Li, Jianzhong and Gao, Hong and Zhang, Jiacheng|inproceedings|10.1145/2661829.2661837|||||||||||||||||||||||||||||1168109644|42
SIGMETRICS '16|Antibes Juan-les-Pins, France|mechanism design, game theory, strategic data subjects, incentive mechanism, differential privacy|12|249–260|Proceedings of the 2016 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Science|We study the value of data privacy in a game-theoretic model of trading private data, where a data collector purchases private data from strategic data subjects (individuals) through an incentive mechanism. The private data of each individual represents her knowledge about an underlying state, which is the information that the data collector desires to learn. Different from most of the existing work on privacy-aware surveys, our model does not assume the data collector to be trustworthy. Then, an individual takes full control of its own data privacy and reports only a privacy-preserving version of her data. In this paper, the value of ε units of privacy is measured by the minimum payment of all nonnegative payment mechanisms, under which an individual's best response at a Nash equilibrium is to report the data with a privacy level of ε. The higher ε is, the less private the reported data is. We derive lower and upper bounds on the value of privacy which are asymptotically tight as the number of data subjects becomes large. Specifically, the lower bound assures that it is impossible to use less amount of payment to buy ε units of privacy, and the upper bound is given by an achievable payment mechanism that we designed. Based on these fundamental limits, we further derive lower and upper bounds on the minimum total payment for the data collector to achieve a given learning accuracy target, and show that the total payment of the designed mechanism is at most one individual's payment away from the minimum.|10.1145/2896377.2901461|https://doi.org/10.1145/2896377.2901461|New York, NY, USA|Association for Computing Machinery|9781450342667|2016|The Value of Privacy: Strategic Data Subjects, Incentive Mechanisms and Fundamental Limits|Wang, Weina and Ying, Lei and Zhang, Junshan|inproceedings|10.1145/2896377.2901461|||||||||||||||||||||||||||||1168511172|42
||game theory, differential privacy, incentive mechanism, mechanism design, strategic data subjects|12|249–260||We study the value of data privacy in a game-theoretic model of trading private data, where a data collector purchases private data from strategic data subjects (individuals) through an incentive mechanism. The private data of each individual represents her knowledge about an underlying state, which is the information that the data collector desires to learn. Different from most of the existing work on privacy-aware surveys, our model does not assume the data collector to be trustworthy. Then, an individual takes full control of its own data privacy and reports only a privacy-preserving version of her data. In this paper, the value of ε units of privacy is measured by the minimum payment of all nonnegative payment mechanisms, under which an individual's best response at a Nash equilibrium is to report the data with a privacy level of ε. The higher ε is, the less private the reported data is. We derive lower and upper bounds on the value of privacy which are asymptotically tight as the number of data subjects becomes large. Specifically, the lower bound assures that it is impossible to use less amount of payment to buy ε units of privacy, and the upper bound is given by an achievable payment mechanism that we designed. Based on these fundamental limits, we further derive lower and upper bounds on the minimum total payment for the data collector to achieve a given learning accuracy target, and show that the total payment of the designed mechanism is at most one individual's payment away from the minimum.|10.1145/2964791.2901461|https://doi.org/10.1145/2964791.2901461|New York, NY, USA|Association for Computing Machinery||2016|The Value of Privacy: Strategic Data Subjects, Incentive Mechanisms and Fundamental Limits|Wang, Weina and Ying, Lei and Zhang, Junshan|article|10.1145/2964791.2901461||jun|SIGMETRICS Perform. Eval. Rev.|01635999|1|44|June 2016||||26742|0,223|Q3|80|72|323|565|337|301|1,04|7,85|United States|Northern America|1980, 1982, 1984, 1986-1989, 1994, 1996-2020|Computer Networks and Communications (Q3); Software (Q3); Hardware and Architecture (Q4)||||1168511172|302815259
iiWAS '20|Chiang Mai, Thailand|persistent messaging, e-government, data exchange layers, consent management, X-Road, GAIA-X, e-governance, public key infrastructures, digital transformation, Fiware, data governance|8|3–10|Proceedings of the 22nd International Conference on Information Integration and Web-Based Applications &amp; Services|"\"The \"\"digital transformation\"\" is perceived as the key enabler for increasing wealth and well-being by many in politics, media and among the citizens alike. In the same vein, e-Government steadily received and receives more and more attention. e-Government gives rise to complex, large-scale system landscapes consisting of many players and technological systems - and we call such system landscapes e-Government ecosystems. In this talk, we are interested in the architecture of e-Government ecosystems. \"\"Form ever follows function.\"\" Now, what is the function that determines e-Government? And what is the form in which it manifests? After briefly reviewing the purpose of e-Government from a democratic as well as a technocratic viewpoint, we will discover the primacy of the state's institutional design in the architecture of e-Government ecosystems. From there, we will arrive at the notion of data governance architecture, which provides the core of all system design efforts in e-Government. A data governance architecture maps data assets to accountable legal entities and represents the essence of co-designing institutions and technological systems. Against the background of what has been achieved, we review a series of established and emerging technologies that have been explicitly designed for or are otherwise relevant for building e-Government systems.\""|10.1145/3428757.3429972|https://doi.org/10.1145/3428757.3429972|New York, NY, USA|Association for Computing Machinery|9781450389228|2021|On Architecture of E-Government Ecosystems: From e-Services to e-Participation: [IiWAS'2020 Keynote]|Draheim, Dirk|inproceedings|10.1145/3428757.3429972|||||||||||||||||||||||||||||1171487429|42
||Case retrieval, Big data, BN model, Hadoop platform||1-13||Although case retrieval of Bayesian network has greatly promoted the application of CBR technique in engineering fields, it is facing huge challenges with the arrival of the era of big data. First, huge computation task of BN learning caused by big data seriously hampers the efficiency of case retrieval; Second, with the increasing data size, the accuracy of case retrieval becomes poorer and poorer because existing methods of improving probability learning become unfit for new situation. Aiming at the first problem, this paper proposes Within-Cross algorithm to assign computation task to improve the result of parallel data processing and gain better efficiency of case retrieval. For the second problem, this paper proposes a new method called Weighted Index Coefficient of Dirichlet Distribution (WICDD) algorithm, which first measures the influence of different factors on probability learning and then gives a weight to each super parameter of Dirichlet Distribution to adjust the result of probability learning. Thus with WICDD algorithm, the effect of probability learning is greatly improved, which then further enhances the accuracy of case retrieval. Finally, lots of experiments are executed to validate the effectiveness of the proposed method.|https://doi.org/10.1016/j.datak.2018.08.002|https://www.sciencedirect.com/science/article/pii/S0169023X18300624||||2018|Research on case retrieval of Bayesian network under big data|Yuan Guo and Yuan Guo and K. Wu|article|GUO20181|||Data & Knowledge Engineering|0169023X||118|||||24437|0,480|Q2|87|37|155|1512|440|152|2,55|40,86|Netherlands|Western Europe|1985, 1987-2020|Information Systems and Management (Q2)||||1173219115|1516868485
||Big data, Datasets, Cardiovascular disease, National platform||1467-1476||Cardiovascular diseases (CVD) are leading causes of death and morbidity in Australia and worldwide. Despite improvements in treatment, there remain large gaps in our understanding to prevent, treat and manage CVD events and associated morbidities. This article lays out a vision for enhancing CVD research in Australia through the development of a Big Data system, bringing together the multitude of rich administrative and health datasets available. The article describes the different types of Big Data available for CVD research in Australia and presents an overview of the potential benefits of a Big Data system for CVD research and some of the major challenges in establishing the system for Australia. The steps for progressing this vision are outlined.|https://doi.org/10.1016/j.hlc.2021.04.023|https://www.sciencedirect.com/science/article/pii/S1443950621005175||||2021|A Versatile Big Data Health System for Australia: Driving Improvements in Cardiovascular Health|Ellie Paige and Kerry Doyle and Louisa Jorm and Emily Banks and Meng-Ping Hsu and Lee Nedkoff and Tom Briffa and Dominique A. Cadilhac and Ray Mahoney and Johan W. Verjans and Girish Dwivedi and Michael Inouye and Gemma A. Figtree|article|PAIGE20211467|||Heart, Lung and Circulation|14439506|10|30|||||||||||||||||||||||1175450390|1343245790
||Crowdsourcing;Training;Annotations;Big Data;Tools;Inference algorithms;Task analysis;label;truth inference;learning model;crowdsourcing||201-208|2020 IEEE Intl Conf on Dependable, Autonomic and Secure Computing, Intl Conf on Pervasive Intelligence and Computing, Intl Conf on Cloud and Big Data Computing, Intl Conf on Cyber Science and Technology Congress (DASC/PiCom/CBDCom/CyberSciTech)|It is well known that many intelligent and computer-hard tasks cannot be effectively addressed by existing machine-based approaches, so that it is nature to think of utilizing the intelligence of human being. With the popularization of crowdsourcing concepts as well as the development of crowdsourcing platforms, as a new way of human intelligence to participate in machine computing, crowdsourcing annotation helps more and more supervised-learning-based approaches easily obtain enormous labeled data with relatively low cost. However, because of the diversity of the crowd employed by crowdsourcing platforms, how to control qualities of labels coming from the crowd plays a key role in crowdsourcing annotation. In this survey, we first present basic concepts and definitions of crowdsourcing annotation. Then, we review existing ground truth inference algorithms and learning models. After that, the advantages and distinctions among these algorithms and learning models as well as the levels of study progresses will be reported. And finally, we summarize realworld datasets widely utilized in the field of crowdsourcing annotation as well as available open source tools.|10.1109/DASC-PICom-CBDCom-CyberSciTech49142.2020.00044|||||2020|Research on Data Quality Control of Crowdsourcing Annotation: A Survey|Lu, Jian and Li, Wei and Wang, Qingren and Zhang, Yiwen|inproceedings|9251151||Aug|||||||||||||||||||||||||||1176291192|42
IDEAS '21|Montreal, QC, Canada|Representational Theory of Measurement,, Quality Characteristics (V's), Validity, Measurement Hierarchical Model, Big Data|7|285–291|Proceedings of the 25th International Database Engineering &amp; Applications Symposium|Big Data is becoming a substantial part of the decision-making processes in both industry and academia, especially in areas where Big Data may have a profound impact on businesses and society. However, as more data is being processed, data quality is becoming a genuine issue that negatively affects credibility of the systems we build because of the lack of visibility and transparency of the underlying data. Therefore, Big Data quality measurement is becoming increasingly necessary in assessing whether data can serve its purpose in a particular context (such as Big Data analytics, for example). This research addresses Big Data quality measurement modelling and automation by proposing a novel quality measurement framework for Big Data (MEGA) that objectively assesses the underlying quality characteristics of Big Data (also known as the V's of Big Data) at each step of the Big Data Pipelines. Five of the Big Data V's (Volume, Variety, Velocity, Veracity and Validity) are currently automated by the MEGA framework. In this paper, a new theoretically valid quality measurement model is proposed for an essential quality characteristic of Big Data, called Validity. The proposed measurement information model for Validity of Big Data is a hierarchy of 4 derived measures / indicators and 5 based measures. Validity measurement is illustrated on a running example.|10.1145/3472163.3472171|https://doi.org/10.1145/3472163.3472171|New York, NY, USA|Association for Computing Machinery|9781450389914|2021|Rigorous Measurement Model for Validity of Big Data: MEGA Approach|Bhardwaj, Dave and Ormandjieva, Olga|inproceedings|10.1145/3472163.3472171|||||||||||||||||||||||||||||1176953436|42
|||10|40–49||Data profiling comprises a broad range of methods to efficiently analyze a given data set. In a typical scenario, which mirrors the capabilities of commercial data profiling tools, tables of a relational database are scanned to derive metadata, such as data types and value patterns, completeness and uniqueness of columns, keys and foreign keys, and occasionally functional dependencies and association rules. Individual research projects have proposed several additional profiling tasks, such as the discovery of inclusion dependencies or conditional functional dependencies.Data profiling deserves a fresh look for two reasons: First, the area itself is neither established nor defined in any principled way, despite significant research activity on individual parts in the past. Second, more and more data beyond the traditional relational databases are being created and beg to be profiled. The article proposes new research directions and challenges, including interactive and incremental profiling and profiling heterogeneous and non-relational data.|10.1145/2590989.2590995|https://doi.org/10.1145/2590989.2590995|New York, NY, USA|Association for Computing Machinery||2014|Data Profiling Revisited|Naumann, Felix|article|10.1145/2590989.2590995||feb|SIGMOD Rec.|01635808|4|42|December 2013||||13622|0,372|Q2|142|39|81|808|127|57|1,67|20,72|United States|Northern America|1969, 1973-1978, 1981-2020|Information Systems (Q2); Software (Q2)|1,819|0.775|7.5E-4|1177947272|962972343
ICBDC '22|Shenzhen, China|Relation Extraction, Entity Extraction, Equipment Maintenance, Knowledge Graph, Knowledge Extraction|7|102–108|Proceedings of the 7th International Conference on Big Data and Computing|The service data of equipment maintenance is scattered, complex and uncorrelated, and depends on the experience of experts. This paper focuses on the knowledge extraction technology of equipment maintenance documents. However, equipment maintenance documents are characterized by no obvious boundary symbols, many professional terms, and rich context information. Therefore, Convolution Neural Network - Bi-directional Long Short-Term Memory - Conditional Random Field(CNN-BiLSTM-CRF) entity extraction model is designed in this paper on the basis of Bi-directional Long Short-Term Memory – Conditional Random Field(BiLSTM-CRF) model. Aiming at the normative and implicit characteristics of equipment documents, this paper designed a relationship extraction method based on the fusion of pattern and CNN. Experimental results show that both models have good results.|10.1145/3545801.3545816|https://doi.org/10.1145/3545801.3545816|New York, NY, USA|Association for Computing Machinery|9781450396097|2022|Supervised Information Extraction of Chinese Equipment Maintenance Documents|Bei, Yijun and Gao, Kewei and Wang, Linxin|inproceedings|10.1145/3545801.3545816|||||||||||||||||||||||||||||1178631018|42
||Big data system, Empirical study, Stack Overflow||111488||Apache Spark is one of the most popular big data frameworks that abstract the underlying distributed computation details. However, even though Spark provides various abstractions, developers may still encounter challenges related to the peculiarity of distributed computation and environment. To understand the challenges that developers encounter, and provide insight for future studies, in this paper, we conduct an empirical study on the questions that developers encounter. We manually analyze 1,000 randomly selected questions that we collected from Stack Overflow. We find that: 1) questions related to data processing (e.g., transforming data format) are the most common among the 11 types of questions that we uncovered. 2) Even though data processing questions are the most common ones, they require the least amount of time to receive an answer. Questions related to configuration and performance require the most time to receive an answer. 3) Most of the issues are caused by developers’ insufficient knowledge in API usages, data conversation across frameworks, and environment-related configurations. We also discuss the implication of our findings for researchers and practitioners. In summary, our work provides insights for future research directions and highlight the need for more software engineering research in this area.|https://doi.org/10.1016/j.jss.2022.111488|https://www.sciencedirect.com/science/article/pii/S0164121222001674||||2022|An empirical study on the challenges that developers encounter when developing Apache Spark applications|Zehao Wang and Tse-Hsun (Peter) Chen and Haoxiang Zhang and Shaowei Wang|article|WANG2022111488|||Journal of Systems and Software|01641212||194|||||19309|0,642|Q1|109|183|619|11845|3058|590|4,94|64,73|United States|Northern America|1979, 1981-2021|Hardware and Architecture (Q1); Information Systems (Q2); Software (Q2)|6,579|2.829|0.00727|1179735620|1111852116
||Data science, Big data, Data science methodology, Project life-cycle, Organizational impacts, Knowledge management||100183||Data science has employed great research efforts in developing advanced analytics, improving data models and cultivating new algorithms. However, not many authors have come across the organizational and socio-technical challenges that arise when executing a data science project: lack of vision and clear objectives, a biased emphasis on technical issues, a low level of maturity for ad-hoc projects and the ambiguity of roles in data science are among these challenges. Few methodologies have been proposed on the literature that tackle these type of challenges, some of them date back to the mid-1990, and consequently they are not updated to the current paradigm and the latest developments in big data and machine learning technologies. In addition, fewer methodologies offer a complete guideline across team, project and data & information management. In this article we would like to explore the necessity of developing a more holistic approach for carrying out data science projects. We first review methodologies that have been presented on the literature to work on data science projects and classify them according to the their focus: project, team, data and information management. Finally, we propose a conceptual framework containing general characteristics that a methodology for managing data science projects with a holistic point of view should have. This framework can be used by other researchers as a roadmap for the design of new data science methodologies or the updating of existing ones.|https://doi.org/10.1016/j.bdr.2020.100183|https://www.sciencedirect.com/science/article/pii/S2214579620300514||||2021|Data Science Methodologies: Current Challenges and Future Approaches|Iñigo Martinez and Elisabeth Viles and Igor {G. Olaizola}|article|MARTINEZ2021100183|||Big Data Research|22145796||24|||||21100356018|0,565|Q2|25|11|76|547|324|69|4,06|49,73|United States|Northern America|2014-2020|Computer Science Applications (Q2); Information Systems (Q2); Information Systems and Management (Q2); Management Information Systems (Q2)|586|3.578|0.00126|1187394244|1627174784
||Urban fire risk, Fire risk management, Big data technologies, Data governance, Socio-economic factors, City-wide analysis||103138||The effects of data governance (as a means to maximize big data value creation in fire risk management) performance on fire risk was analyzed based on multi-source statistical data of 105 cities in China from 2016 to 2018. Specifically, data governance was first quantified with ten detailed indicators, which were then selected for explaining urban fire risk through correlation analysis. Next, the sample cities were clustered in terms of major socio-economic characteristics, and then the effects of data governance were examined by constructing multivariate regression models for each city cluster with ordinary least squares (OLS). The results showed that the constructed regression models produced good interpretation of fire risk in different types of cities, with coefficient of determination (R2) in each model exceeding 0.65. Among the indicators, the development of infrastructures (e.g. data collection devices and data analysis platforms), the level of data use, and the updating of fire risk related data were proved to produce significant effects on the reduction of fire frequency and fire consequence. Moreover, the organizational maturity of data governance was proved to be helpful in reducing fire frequency. For the cities with large population, the cross-department sharing of high-value data was found to be another important determinant of urban fire frequency. In comparison with existing statistical models which interpreted fire risk with general social factors (with the highest R2 = 0.60), these new regression models presented a better statistical performance (with the average R2 = 0.72). These findings are expected to provide decision support for the local governments of China and other jurisdictions to facilitate big data projects in improving fire risk management.|https://doi.org/10.1016/j.ijdrr.2022.103138|https://www.sciencedirect.com/science/article/pii/S2212420922003570||||2022|Effects of governmental data governance on urban fire risk: A city-wide analysis in China|Zhao-Ge Liu and Xiang-Yang Li and Grunde Jomaas|article|LIU2022103138|||International Journal of Disaster Risk Reduction|22124209||78|||||21100228018|1,161|Q1|45|562|856|35838|4363|844|4,78|63,77|United Kingdom|Western Europe|2012-2020|Geology (Q1); Geotechnical Engineering and Engineering Geology (Q1); Safety Research (Q1)|6,931|4.320|0.01003|1187829619|1258726689
||Data quality, Decisive action, Smart water management, Water quality, South Africa, Upper vaal catchment||100100||The need for informed management of water resources has been continuously highlighted worldwide. Societies are increasingly faced with water quality challenges globally which directly translate into multifaceted challenges. South Africa has acknowledged that water is not receiving the attention and status it deserves. Wastage is rife and degradation widespread. The sustainability of South Africa's freshwater resources has reached a critical point and requires decisive action. Vast amounts of water quality data, varying in quality, is available however the seemingly lack of integrative data management has led to reactive planning and questionable decisions. The paper highlights the necessity for making water smart through a case study of the Upper Vaal catchment. The quality of available government data is mostly of an acceptable standard according to the evaluated data dimensions and elements. The practical application of determining hydrological responses to predict possible water quality changes towards land cover change in the Vaal river catchment emphasises that there is suitable data available and highlights the value of Smart Water Management (SWM). SWM can enable improved integrated water resource management by increasing sharing and effective use of real-time data of acceptable quality to promote proactive unambiguous strategies and decisions focused on overall improved water management and the evasion of a future water predicament.|https://doi.org/10.1016/j.envc.2021.100100|https://www.sciencedirect.com/science/article/pii/S2667010021000792||||2021|Necessity of making water smart for proactive informed decisive actions: A case study of the upper vaal catchment, South Africa|Anja {du Plessis}|article|DUPLESSIS2021100100|||Environmental Challenges|26670100||4|||||||||||||||||||||||1188402122|1554333338
||Organizations;Patents;Databases;Systematics;Pragmatics;data quality improvement;hot deck imputation;record completion;gender name mapping;patenting||2628-2636|2017 IEEE International Conference on Big Data (Big Data)|First name to gender mappings have been widely recognized as a critical tool to complete, study and validate data records in a range of different areas. In this study, we investigate how organizations with large databases of existing entities can create their own mappings between first names and gender and how these mappings can be improved and utilized. Therefore, we first explore a dataset with demographic information on more than 6 million people, provided by a car insurance. We then study how naming conventions have changed over time and how they differ by nationality. Second, we build a probabilistic first name to gender mapping and augment the mapping by adding nationality and decade of birth to improve the mapping's performance. We test our mapping in a two label and three label setting and further validate our mapping by categorizing patent filings by gender of the inventor. We compare the results with previous studies' outcomes and find that our mapping produces high precision results. We validate that the additional information of nationality and year of birth improve the recall scores of name to gender mappings. Therefore, it constitutes an efficient process to improve data quality of organizations' records, whenever the attribute gender is missing or unreliable.|10.1109/BigData.2017.8258223|||||2017|Improving data quality through high precision gender categorization|Müller, Daniel and Te, Yiea-Funk and Jain, Pratiksha|inproceedings|8258223||Dec|||||||||||||||||||||||||||1189541510|42
|||11|16–26||To contain the COVID-19 epidemic, one of the non-pharmacological epidemic control measures is reducing the transmission rate of SARS-COV-2 in the population through social distancing. An interactive web-based mapping platform that provides timely quantitative information on how people in different counties and states reacted to the social distancing guidelines was developed by the GeoDS Lab @UW-Madison with the support of the National Science Foundation RAPID program. The web portal integrates geographic information systems (GIS) and daily updated human mobility statistical patterns (median travel distance and stay-at-home dwell time) derived from large-scale anonymized and aggregated smartphone location big data at the county-level in the United States, and aims to increase risk awareness of the public, support data-driven public health and governmental decision-making, and help enhance community responses to the COVID-19 pandemic.|10.1145/3404820.3404824|https://doi.org/10.1145/3404820.3404824|New York, NY, USA|Association for Computing Machinery||2020|Mapping County-Level Mobility Pattern Changes in the United States in Response to COVID-19|Gao, Song and Rao, Jinmeng and Kang, Yuhao and Liang, Yunlei and Kruse, Jake|article|10.1145/3404820.3404824||jul|SIGSPATIAL Special||1|12|March 2020||||||||||||||||||||||1191289232|42
AIAM2021|Manchester, United Kingdom||5|1107–1111|2021 3rd International Conference on Artificial Intelligence and Advanced Manufacture|The era of big data has quietly arrived, which is a revolution that determines the development and future destiny of enterprises. Any enterprises that are not ready for this revolution will be eliminated by the era. This paper mainly studies the construction, analysis and management of human resource system in the era of big data. Based on the actual needs, this paper analyzes the business process and functional requirements of human resource management, completes the system architecture design, function module design, database design, realizes the system function module, and completes the test of the system function. The functional modules realized in this paper include: core personnel management, salary management and comprehensive inquiry. The human resource information system designed in this paper ensures the scientific nature, security, availability and portability of the system, meets the demand of data sharing, and plays a positive role in the whole human resource management cycle.|10.1145/3495018.3495345|https://doi.org/10.1145/3495018.3495345|New York, NY, USA|Association for Computing Machinery|9781450385046|2022|Human Resource Information System Performance Test under Big Data Technology|Chen, Xin and Yang, Lirong and Sun, Yanzhi|inproceedings|10.1145/3495018.3495345|||||||||||||||||||||||||||||1193661829|42
IUI '22|Helsinki, Finland|After-Action Review, Explainable AI|21|191–211|27th International Conference on Intelligent User Interfaces|Faced with several AI-powered sequential decision-making systems, how might someone choose on which to rely? For example, imagine car buyer Blair shopping for a self-driving car, or developer Dillon trying to choose an appropriate ML model to use in their application. Their first choice might be infeasible (i.e., too expensive in money or execution time), so they may need to select their second or third choice. To address this question, this paper presents: 1) Explanation Resolution, a quantifiable direct measurement concept; 2) a new XAI empirical task to measure explanations: “the Ranking Task”; and 3) a new strategy for inducing controllable agent variations—Mutant Agent Generation. In support of those main contributions, it also presents 4) novel explanations for sequential decision-making agents; 5) an adaptation to the AAR/AI assessment process; and 6) a qualitative study around these devices with 10 participants to investigate how they performed the Ranking Task on our mutant agents, using our explanations, and structured by AAR/AI. From an XAI researcher perspective, just as mutation testing can be applied to any code, mutant agent generation can be applied to essentially any neural network for which one wants to evaluate an assessment process or explanation type. As to an XAI user’s perspective, the participants ranked the agents well overall, but showed the importance of high explanation resolution for close differences between agents. The participants also revealed the importance of supporting a wide diversity of explanation diets and agent “test selection” strategies.|10.1145/3490099.3511115|https://doi.org/10.1145/3490099.3511115|New York, NY, USA|Association for Computing Machinery|9781450391443|2022|How Do People Rank Multiple Mutant Agents?|Dodge, Jonathan and Anderson, Andrew A. and Olson, Matthew and Dikkala, Rupika and Burnett, Margaret|inproceedings|10.1145/3490099.3511115|||||||||||||||||||||||||||||1193670297|42
||||100123|||https://doi.org/10.1016/j.bdr.2019.100123|https://www.sciencedirect.com/science/article/pii/S2214579619302254||||2019|Big Data Exploration, Visualization and Analytics|Nikos Bikakis and George Papastefanatos and Olga Papaemmanouil|article|BIKAKIS2019100123|||Big Data Research|22145796||18|||||21100356018|0,565|Q2|25|11|76|547|324|69|4,06|49,73|United States|Northern America|2014-2020|Computer Science Applications (Q2); Information Systems (Q2); Information Systems and Management (Q2); Management Information Systems (Q2)|586|3.578|0.00126|1194816921|1627174784
ICMI '13|Sydney, Australia|data mining, conversation analysis, multimodality, interaction studies|8|165–172|Proceedings of the 15th ACM on International Conference on Multimodal Interaction|Multimodal research in human interaction has to consider a variety of factors, ranging from local short-time phenomena to complex interaction patterns. As of today, no single discipline engaged in communication research offers the methods and tools to investigate the full complexity continuum in a time-efficient way. A synthesis of qualitative and quantitative analysis is required to merge insights about micro-sequential structures with big data patterns. Using the example of a co-present dyadic negotiation analysis to combine methods offered by Conversation Analysis and Data Mining, we show how such a partnership can benefit each discipline and lead to insights as well as new hypotheses evaluation opportunities.|10.1145/2522848.2522892|https://doi.org/10.1145/2522848.2522892|New York, NY, USA|Association for Computing Machinery|9781450321297|2013|Interaction Analysis and Joint Attention Tracking in Augmented Reality|Neumann, Alexander and Schnier, Christian and Hermann, Thomas and Pitsch, Karola|inproceedings|10.1145/2522848.2522892|||||||||||||||||||||||||||||1197680129|42
WebMedia '18|Salvador, BA, Brazil|Internet of things, Wireless sensor networks, Visual sensors, Camera, Raspberry Pi|4|19–22|Proceedings of the 24th Brazilian Symposium on Multimedia and the Web|The increasing interest for Internet of Things (IoT) technologies has brought a lot of attention to microelectronics and sensors development. With the availability of affordable embedded platforms for countless applications, it is possible to develop low-cost programmable sensors to provide different types of data, benefiting applications in the IoT world. When cameras can be integrated to such development platforms, visual sensors can be easily created, supporting monitoring and controls functions based on the processing of images and videos. In this context, some of the most relevant details concerning the development of visual sensors with the Raspberry Pi platform are described herein, bringing fundamentals for the creation of highly programmable visual sensors.|10.1145/3243082.3264607|https://doi.org/10.1145/3243082.3264607|New York, NY, USA|Association for Computing Machinery|9781450358675|2018|On the Development of Visual Sensors with Raspberry Pi|Costa, Daniel G.|inproceedings|10.1145/3243082.3264607|||||||||||||||||||||||||||||1197743072|42
MSIE 2019|Phuket, Thailand|Selection Strategy Analysis, Full cost, Appropriate products, Crowdsourcing platform|6|92–97|Proceedings of the 2019 International Conference on Management Science and Industrial Engineering|From the perspective of full cost, this paper uses Coase's transaction cost theory to analyze the causes of crowdsourcing, and on this basis to analyze the applicability of crowdsourcing platform products. At the same time, based on the crowdsourcing platform--zbj.com, we use the big data technology to grasp and analyze the related data of the crowdsourcing platform's successful cases in the past five months, and use the relevant statistical analysis method to categorize and analyze the industry attributes of the top five orders of the success cases of the zbj.com, in order to verify the theory mentioned in the article.|10.1145/3335550.3335577|https://doi.org/10.1145/3335550.3335577|New York, NY, USA|Association for Computing Machinery|9781450362641|2019|Product Selection Strategy Analysis of Crowdsourcing Platform from the Full Cost Perspective|Li, Ruixue and Peng, Can and Sun, Huiliang|inproceedings|10.1145/3335550.3335577|||||||||||||||||||||||||||||1197861513|42
||Data privacy;Privacy;Numerical models;Big Data;Measurement;Data models;Greedy algorithms;privacy preserving;k-anonymity;de-identification;data quality;information loss||1610-1614|2018 17th IEEE International Conference On Trust, Security And Privacy In Computing And Communications/ 12th IEEE International Conference On Big Data Science And Engineering (TrustCom/BigDataSE)|In the globalized knowledge economy, big data analytics have been widely applied in diverse areas. A critical issue in big data analysis on personal information is the possible leak of personal privacy. Therefore, it is necessary to have an anonymization-based de-identification method to avoid undesirable privacy leak. Such method can prevent published data form being traced back to personal privacy. Prior empirical researches have provided approaches to reduce privacy leak risk, e.g. Maximum Distance to Average Vector (MDAV), Condensation Approach and Differential Privacy. However, previous methods inevitably generate synthetic data of different sizes and is thus unsuitable for general use. To satisfy the need of general use, k-anonymity can be chosen as a privacy protection mechanism in the de-identification process to ensure the data not to be distorted, because k-anonymity is strong in both protecting privacy and preserving data authenticity. Accordingly, this study proposes an optimized multidimensional method for anonymizing data based on both the priority weight-adjusted method and the mean difference recommending tree method (MDR tree method). The results of this study reveal that this new method generate more reliable anonymous data and reduce the information loss rate.|10.1109/TrustCom/BigDataSE.2018.00235|||||2018|Optimized Data de-Identification Using Multidimensional k-Anonymity|Liu, Kai-Cheng and Kuo, Chuan-Wei and Liao, Wen-Chiuan and Wang, Pang-Chieh|inproceedings|8456103||Aug||23249013|||||||||||||||||||||||||1198209679|1841690667
||Wireless communication;Low voltage;Data integrity;Urban areas;Clustering algorithms;Collaboration;Big Data;Smart grid big data;Data quality;Curve mean clustering Algorithm||395-398|2021 International Conference on Wireless Communications and Smart Grid (ICWCSG)|As the demand for smart grid construction increases, advanced power applications based on edge-cloud collaboration continue to increase. Among them, there are many data-driven artificial intelligence calculations and analyses, all of which are calculated and analyzed based on electric power big data. However, for the massive electric power big data, it is impossible to obtain more internally related information only by observing the data from the surface. To a certain extent, it directly affects the upper-level advanced applications. To solve this problem, this paper studies and proposes a curve-mean clustering algorithm for load big data, which is the most widely used load data in smart grid. By analyzing the advanced measurement infrastructure, the matrix low-rank property of load big data and the calculation of singular value, the curve mean clustering of load big data is realized, and the optimal determination method of cluster number is expounded. Experiments are conducted based on actual resident user load data and compared with the classic mean shift clustering algorithm. By calculating the average distance within the cluster, the average distance between clusters and the DI index, it is verified that the proposed method clustering is more accurate and the selection of cluster number is optimal. The research plays a very good role in basic analysis for improving the big data analysis capability and data quality of smart grid.|10.1109/ICWCSG53609.2021.00085|||||2021|Research on smart grid big data’s curve mean clustering algorithm for edge-cloud collaborative application|Li, Congli and Yang, Bin and Chen, Xuezhen and Zhang, Enjie and Huang, Hongjun and Li, Da|inproceedings|9616552||Aug|||||||||||||||||||||||||||1200925519|42
|||2||||10.1145/2579167|https://doi.org/10.1145/2579167|New York, NY, USA|Association for Computing Machinery||2014|Editorial|Raschid, Louiqa|article|10.1145/2579167|14|may|J. Data and Information Quality|19361955|4|4|May 2014||||||||||||||||||||||1202426680|833754770
||Big data, Cloud computing, Hadoop||98-115||Cloud computing is a powerful technology to perform massive-scale and complex computing. It eliminates the need to maintain expensive computing hardware, dedicated space, and software. Massive growth in the scale of data or big data generated through cloud computing has been observed. Addressing big data is a challenging and time-demanding task that requires a large computational infrastructure to ensure successful data processing and analysis. The rise of big data in cloud computing is reviewed in this study. The definition, characteristics, and classification of big data along with some discussions on cloud computing are introduced. The relationship between big data and cloud computing, big data storage systems, and Hadoop technology are also discussed. Furthermore, research challenges are investigated, with focus on scalability, availability, data integrity, data transformation, data quality, data heterogeneity, privacy, legal and regulatory issues, and governance. Lastly, open research issues that require substantial research efforts are summarized.|https://doi.org/10.1016/j.is.2014.07.006|https://www.sciencedirect.com/science/article/pii/S0306437914001288||||2015|The rise of “big data” on cloud computing: Review and open research issues|Ibrahim Abaker Targio Hashem and Ibrar Yaqoob and Nor Badrul Anuar and Salimah Mokhtar and Abdullah Gani and Samee {Ullah Khan}|article|HASHEM201598|||Information Systems|03064379||47|||||12305|0,547|Q2|85|100|271|4739|1027|255|3,39|47,39|United Kingdom|Western Europe|1975-2021|Hardware and Architecture (Q2); Information Systems (Q2); Software (Q2)|2,604|2.309|0.00331|1204317446|303930735
||||e55-e56|||https://doi.org/10.1016/j.jtho.2022.02.010|https://www.sciencedirect.com/science/article/pii/S1556086422001381||||2022|Unaccounted Confounders Limit the Ability to Draw Conclusions From Big Data Analysis Comparing Radiotherapy Fractionation Regimens in NSCLC|Ahmed Salem and Kevin Franks and Alastair Greystoke and Gerard G. Hanna and Stephen Harrow and Matthew Hatton and Crispin Hiley and Fiona McDonald and Corinne Faivre-Finn|article|SALEM2022e55|||Journal of Thoracic Oncology|15560864|6|17|||||||||||||||||||||||1206710371|379964659
CHI '16|San Jose, California, USA|clinical decision making, self-tracking, quantified self|13|3066–3078|Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems|While the Quantified Self and personal informatics fields have focused on the individual's use of self-logged data about themselves, the same kinds of data could, in theory, be used to improve diagnosis and care planning. In this paper, we seek to understand both the opportunities and bottlenecks in the use of self-logged data for differential diagnosis and care planning during patient visits to both primary and secondary care. We first conducted a literature review to identify potential factors influencing the use of self-logged data in clinical settings. This informed the design of our experiment, in which we applied a vignette-based role-play approach with general practitioners and hospital specialists in the US and UK, to elicit reflections on and insights about using patient self-logged data. Our analysis reveals multiple opportunities for the use of self-logged data in the differential diagnosis workflow, identifying capture, representational, and interpretational challenges that are potentially preventing self-logged data from being effectively interpreted and applied by clinicians to derive a patient's prognosis and plan of care.|10.1145/2858036.2858445|https://doi.org/10.1145/2858036.2858445|New York, NY, USA|Association for Computing Machinery|9781450333627|2016|The Quantified Patient in the Doctor's Office: Challenges &amp; Opportunities|West, Peter and Giordano, Richard and Van Kleek, Max and Shadbolt, Nigel|inproceedings|10.1145/2858036.2858445|||||||||||||||||||||||||||||1211602527|42
||Servitization, Big Data, Manufacturing, Competitive advantage, Value, Information||174-184||Servitization has become a pervasive business strategy among manufacturers, enabling them to undergird their competitive advantage. However, it has at least one weakness. While it is used worldwide also in economies with lower production costs, services in manufacturing are slowly becoming commoditized and will become a necessary, though not sufficient, condition for reaching an above average competitive advantage. Consequently, in this article we propose a new basis for competitive advantage for manufacturing enterprises called a Big Data Strategy in servitization. We scrutinize how manufacturers can exploit the opportunity arising from combined Big Data and servitization. Therefore, the concept of a Big Data Strategy framework in servitization is proposed. The findings are benchmarked against established frameworks in the Big Data and servitization literature. Its impact on competitive advantage is assessed through three theoretical perspectives that increase the validity of the results. The main finding is that, through the proposed strategy, new revenue streams can be created, while opening the possibility to decrease prices for product–services. Through the proposed strategy manufacturers can differentiate themselves from the ones that are already servitizing. This article introduces the possibility of influencing the most important of the five “Vs” in Big Data–Value, in addition to the other four “Vs”—Volume, Variety, Velocity and Verification. As in regards to servitization, the article adds a third layer of added value— “information”, beside the two existing ones: product and service. The results have strategic implications for managers.|https://doi.org/10.1016/j.ijpe.2014.12.036|https://www.sciencedirect.com/science/article/pii/S0925527314004307||||2015|The value of Big Data in servitization|David Opresnik and Marco Taisch|article|OPRESNIK2015174|||International Journal of Production Economics|09255273||165|||||19165|2,406|Q1|185|327|891|21712|8124|881|8,31|66,40|Netherlands|Western Europe|1991-2021|Business, Management and Accounting (miscellaneous) (Q1); Economics and Econometrics (Q1); Industrial and Manufacturing Engineering (Q1); Management Science and Operations Research (Q1)|32,606|7.885|0.0228|1213390075|850534974
||Disruptive innovation, Data protection regulation, Big data, Governance, Inter-organizational collaboration||330-338||Big data is an important driver of disruptive innovation that may increase organizations' competitive advantage. To create innovative data combinations and decrease investments, big data is often shared among organizations, crossing organizational boundaries. However, these big data collaborations need to balance disruptive innovation and compliance to a strict data protection regime in the EU. This paper investigates how inter-organizational big data collaborations arrange and govern their activities in the context of this dilemma. We conceptualize big data as inter-organizational systems and build on IS and Organization Theory literature to develop four archetypical governance arrangements: Market, Hierarchy, Bazaar and Network. Subsequently, these arrangements are investigated in four big data collaboration use cases. The contributions of this study to literature are threefold. First, we conceptualize the organization behind big data collaborations as IOS governance. Second, we show that the choice for an inter-organizational governance arrangement highly depends on the institutional pressure from regulation and the type of data that is shared. In this way, we contribute to the limited body of research on the antecedents of IOS governance. Last, we highlight with four use cases how the principles of big data, specifically data maximization, clash with the principles of EU data protection regulation. Practically, our study provides guidelines for IT and innovation managers how to arrange and govern the sharing of data among multiple organizations.|https://doi.org/10.1016/j.techfore.2017.09.040|https://www.sciencedirect.com/science/article/pii/S0040162517314695||||2018|Governance of big data collaborations: How to balance regulatory compliance and disruptive innovation|Tijs {van den Broek} and Anne Fleur {van Veenstra}|article|VANDENBROEK2018330|||Technological Forecasting and Social Change|00401625||129|||||14704|2,226|Q1|117|448|1099|35581|10127|1061|9,01|79,42|United States|Northern America|1970-2020|Applied Psychology (Q1); Business and International Management (Q1); Management of Technology and Innovation (Q1)|21,116|8.593|0.02416|1214312483|1949868303
||Cloud computing;Power demand;Collaboration;Power transmission;Signal processing algorithms;Computer architecture;Signal processing;cloud computing;edge computing;cloud edge collaboration;transmission assets;state assessment||458-461|2022 7th International Conference on Intelligent Computing and Signal Processing (ICSP)|In the process of asset status assessment, the power transmission intelligent Internet of Things (IoT) with smart towers as the core IoT nodes faces many problems such as large workload of physical information, poor data quality, large data processing delay and heavy cloud computing pressure. At the same time, traditional front-end sensing equipment is limited by the actual hardware computing power level and low power consumption requirements, which makes the front-end algorithm low in intelligence and consumes a lot of manual data verification. In view of the above problems, this paper proposes a cloud-edge collaborative data processing architecture suitable for transmission asset status assessment by combining big data framework, deep learning and edge computing technology. The architecture clearly divides the functions of the cloud, edge and data terminals based on the status assessment requirements of power transmission assets, and then divides a part of the data processing and analysis operations in the cloud to the edge, which reduces the computing pressure on the cloud and enhances resources utilization rate.|10.1109/ICSP54964.2022.9778298|||||2022|Cloud-edge collaborative data processing architecture for state assessment of transmission equipments|Chen, Honghu and Zhou, Te and Yang, Chao and Li, Qiang and Peng, Bo and Cheng, Qing|inproceedings|9778298||April|||||||||||||||||||||||||||1216179852|42
||Big data, Data protection, Council of Europe, Risk assessment, Data protection by design, Consent, Data anonymization, Open data, Algorithms||584-602||In January 2017 the Consultative Committee of Convention 108 adopted its Guidelines on the Protection of Individuals with Regard to the Processing of Personal Data in a World of Big Data. These are the first guidelines on data protection provided by an international body which specifically address the issues surrounding big data applications. This article examines the main provisions of these Guidelines and highlights the approach adopted by the Consultative Committee, which contextualises the traditional principles of data protection in the big data scenario and also takes into account the challenges of the big data paradigm. The analysis of the different provisions adopted focuses primarily on the core of the Guidelines namely the risk assessment procedure. Moreover, the article discusses the novel solutions provided by the Guidelines with regard to the data subject's informed consent, the by-design approach, anonymization, and the role of the human factor in big data-supported decisions. This critical analysis of the Guidelines introduces a broader reflection on the divergent approaches of the Council of Europe and the European Union to regulating data processing. Where the principle-based model of the Council of Europe differs from the approach adopted by the EU legislator in the detailed Regulation (EU) 2016/679. In the light of this, the provisions of the Guidelines and their attempt to address the major challenges of the new big data paradigm set the stage for concluding remarks about the most suitable regulatory model to deal with the different issues posed by the development of technology.|https://doi.org/10.1016/j.clsr.2017.05.011|https://www.sciencedirect.com/science/article/pii/S0267364917301644||||2017|Regulating big data. The guidelines of the Council of Europe in the context of the European data protection framework|Alessandro Mantelero|article|MANTELERO2017584|||Computer Law & Security Review|02673649|5|33|||||28888|0,815|Q1|38|66|242|1245|707|223|3,39|18,86|United Kingdom|Western Europe|1985-2020|Business, Management and Accounting (miscellaneous) (Q1); Computer Networks and Communications (Q1); Law (Q1)||||1217117427|1769124433
||Credit card, Validation, LUHN, Big data, Bank||532-537||The period of Big Data examination has started in many businesses inside created nations. With expanding headway of Internet technology, expanding measures of data are spilling into contemporary associations. Data are getting bigger and more muddled because of the nonstop age of data from numerous gadgets and sources. In this investigation, we have examined the banking area's inconsistencies because of big data technology, inconsistencies in credit card and afterwards the path how to remove these inconsistencies.|https://doi.org/10.1016/j.matpr.2021.05.597|https://www.sciencedirect.com/science/article/pii/S2214785321042243||||2022|Perspective of anomaly detection in big data for data quality improvement|Vinaya Keskar and Jyoti Yadav and Ajay Kumar|article|KESKAR2022532|||Materials Today: Proceedings|22147853||51||CMAE'21|||21100370037|0,341|-|47|3996|10585|88521|14096|10409|1,24|22,15|United Kingdom|Western Europe|2005, 2014-2020|Materials Science (miscellaneous)||||1217226364|400517803
||Feeds;Clouds;Wireless sensor networks;Computer architecture;Fluid flow measurement;Markup languages;Data models;Internet of Things;Federated Sensor Clouds;Data Quality;Sensor Virtualization||86-93|2013 IEEE International Congress on Big Data|As the Internet of Things (IoT) paradigm gains popularity, the next few years will likely witness 'servitization' of domain sensing functionalities. We envision a cloud-based eco-system in which high quality data from large numbers of independently-managed sensors is shared or even traded in real-time. Such an eco-system will necessarily have multiple stakeholders such as sensor data providers, domain applications that utilize sensor data (data consumers), and cloud infrastructure providers who may collaborate as well as compete. While there has been considerable research on wireless sensor networks, the challenges involved in building cloud-based platforms for hosting sensor services are largely unexplored. In this paper, we present our vision for data quality (DQ)-centric big data infrastructure for federated sensor service clouds. We first motivate our work by providing real-world examples. We outline the key features that federated sensor service clouds need to possess. This paper proposes a big data architecture in which DQ is pervasive throughout the platform. Our architecture includes a markup language called SDQ-ML for describing sensor services as well as for domain applications to express their sensor feed requirements. The paper explores the advantages and limitations of current big data technologies in building various components of the platform. We also outline our initial ideas towards addressing the limitations.|10.1109/BigData.Congress.2013.21|||||2013|Towards a Quality-centric Big Data Architecture for Federated Sensor Services|Ramaswamy, Lakshmish and Lawson, Victor and Gogineni, Siva Venkat|inproceedings|6597123||June||23797703|||||||||||||||||||||||||1218179992|1226158248
||Real-world fuel consumption rate, Energy consumption, Private passenger vehicles, Big data, China||116388||Measuring real-world fuel consumption of light duty vehicles can be challenging due to the limited collection of actual data. In this paper, we use big data retrieved from the record of real-world fuel consumptions of different brands of vehicles in different areas (n = 106,809 samples from 201 brands of vehicles and 34 cities) in China to build up a real-world fuel consumption rate (RFCR) model to estimate the fuel consumption given the driving conditions and figure out the main factors that affect actual fuel consumption in the real world. We find the average deviation of actual fuel consumptions and the fitting results of RFCR model is 4.22% , which does not significantly differ from zero, and the fuel consumptions calculated by RFCR model tend to be 1.40 L/100 km (about 25%) higher than the official reported data. Furthermore, we find that annual average temperature and altitude factors significantly influence the fuel consumption rate. The results indicate that there is a real world performance discrepancy between the theoretical fuel consumption released by authorities and that in the real world, and some green behaviors (choose light duty vehicles, reduce the use of air conditioning and change to manual transmission type) can reduce energy consumption of vehicles.|https://doi.org/10.1016/j.energy.2019.116388|https://www.sciencedirect.com/science/article/pii/S0360544219320833||||2020|Impact factors of the real-world fuel consumption rate of light duty vehicles in China|Tian Wu and Xiao Han and M. Mocarlo Zheng and Xunmin Ou and Hongbo Sun and Xiong Zhang|article|WU2020116388|||Energy|03605442||190|||||||||||||||||||||||1219871265|969898259
aiDM'18|Houston, TX, USA|Data Governance, Analytics, Graph, Context|9||Proceedings of the First International Workshop on Exploiting Artificial Intelligence Techniques for Data Management|Current data governance techniques are very labor-intensive, as teams of data stewards typically rely on best practices to transform business policies into governance rules. As data plays an increasingly key role in today's data-driven enterprises, current approaches do not scale to the complexity and variety present in the data ecosystem of an enterprise as an increasing number of data requirements, use cases, applications, tools and systems come into play. We believe techniques from artificial intelligence and machine learning have potential to improve discoverability, quality and compliance in data governance. In this paper, we propose a framework for 'contextual intelligence', where we argue for (1) collecting and integrating contextual metadata from variety of sources to establish a trusted unified repository of contextual data use across users and applications, and (2) applying machine learning and artificial intelligence techniques over this rich contextual metadata to improve discoverability, quality and compliance in governance practices. We propose an architecture that unifies governance across several systems, with a graph serving as a core repository of contextual metadata, accurately representing data usage across the enterprise and facilitating machine learning, We demonstrate how our approach can enable ML-based recommendations in support of governance best practices.|10.1145/3211954.3211955|https://doi.org/10.1145/3211954.3211955|New York, NY, USA|Association for Computing Machinery|9781450358514|2018|Contextual Intelligence for Unified Data Governance|Seabolt, Ed and Kandogan, Eser and Roth, Mary|inproceedings|10.1145/3211954.3211955|2||||||||||||||||||||||||||||1221490031|42
||Subjective well-being, Air pollution, Big data, Sentiment analysis||134380||Self-reported life satisfaction of China's population has not improved as much as expected during the economic boom, which was accompanied by a significant decline in environmental performance. Is environmental pollution the culprit for the lagging subjective well-being? To explore this issue, this paper adopts sentiment analysis to construct a real-time daily subjective well-being metric at the city level based on the big data of online search traces. Using daily data from 13 Chinese cities centred on Beijing between August 2014 and December 2019, we look at the corelation between subjective well-being and air pollution and the heterogeneity in this relationship based on two separate identification strategies. We find that air pollutants are negatively correlated with subjective well-being, and well-being tends to decline more from pollution during hot seasons. In addition, residents in wealthier regions tend to be more sensitive to air pollution. This result may be explained by the differences in the subjective perception of air pollution and personal preferences at different levels of income. These findings provide information about concerns of the public, thereby helping the government to take appropriate actions to respond to the dynamics of subjective well-being.|https://doi.org/10.1016/j.jclepro.2022.134380|https://www.sciencedirect.com/science/article/pii/S095965262203952X||||2022|Dirty skies lower subjective well-being|Lu Cheng and Zhifu Mi and Yi-Ming Wei and Shidong Wang and Klaus Hubacek|article|CHENG2022134380|||Journal of Cleaner Production|09596526||378|||||19167|1,937|Q1|200|5126|10603|304498|103295|10577|9,56|59,40|United Kingdom|Western Europe|1993-2021|Environmental Science (miscellaneous) (Q1); Industrial and Manufacturing Engineering (Q1); Renewable Energy, Sustainability and the Environment (Q1); Strategy and Management (Q1)|170,352|9.297|0.18299|1223126316|1121054297
DOLAP '15|Melbourne, Australia|nosql, big data, database design|4|35–38|Proceedings of the ACM Eighteenth International Workshop on Data Warehousing and OLAP|It is widely accepted today that Relational databases are not appropriate in highly distributed shared-nothing architectures of commodity hardware, that need to handle poorly structured heterogeneous data. This has brought the blooming of NoSQL systems with the purpose of mitigating such problem, specially in the presence of analytical workloads. Thus, the change in the data model and the new analytical needs beyond OLAP take us to rethink methods and models to design and manage these newborn repositories. In this paper, we will analyze state of the art and future research directions.|10.1145/2811222.2811235|https://doi.org/10.1145/2811222.2811235|New York, NY, USA|Association for Computing Machinery|9781450337854|2015|Big Data Design|Abell\'{o}, Alberto|inproceedings|10.1145/2811222.2811235|||||||||||||||||||||||||||||1224001053|42
||Quality of data, Storage management, Internet of Things (IoT), Cloud computing, Quality of service, Data summarisation||196-208||The proliferation of Internet of Things (IoT) has led to the emergence of enabling many interesting applications within the realm of several domains including smart cities. However, the accumulation of data from smart IoT devices poses significant challenges for data storage while there are needs to deliver relevant and high quality services to consumers. In this paper, we propose QDaS, a novel domain agnostic framework as a solution for effective data storage and management of IoT applications. The framework incorporates a novel data summarisation mechanism that uses an innovative data quality estimation technique. This proposed data quality estimation technique computes the quality of data (based on their utility) without requiring any feedback from users of this IoT data or domain awareness of the data. We evaluate the effectiveness of the proposed QDaS framework using real world datasets.|https://doi.org/10.1016/j.jpdc.2018.03.013|https://www.sciencedirect.com/science/article/pii/S074373151830220X||||2019|QDaS: Quality driven data summarisation for effective storage management in Internet of Things|Jonathan Liono and Prem Prakash Jayaraman and A.K. Qin and Thuong Nguyen and Flora D. Salim|article|LIONO2019196|||Journal of Parallel and Distributed Computing|07437315||127|||||25621|0,638|Q1|87|169|592|7166|2473|568|4,72|42,40|United States|Northern America|1984-2021|Computer Networks and Communications (Q1); Hardware and Architecture (Q1); Artificial Intelligence (Q2); Software (Q2); Theoretical Computer Science (Q2)|4,371|3.734|0.00477|1225027705|1083163244
IAIT2021|Bangkok, Thailand|Developing Nations, Health records, Visualisation|9||The 12th International Conference on Advances in Information Technology|The benefits of effectively visualizing health records in huge volumes has resulted in health organizations, insurance companies, policy and decision makers, governments and drug manufactures’ transformation in the way research is conducted. This has also played a key role in determining investment of resources. Health records contain highly valuable information; processing these records in large volumes is now possible due to technological advancement which allows for the extraction of highly valuable knowledge that has resulted in breakthroughs in scientific communities. To visualize health records in large volumes, the records need to be stored in electronic forms, properly documented, processed, and analyzed. A good visualization technique is used to present the analyzed information, allowing for effective knowledge extraction which is done in a secured manner protecting the privacy of the patients whose health records were used. As research and technological advancement have improved, the quality of knowledge extracted from health records have also improved; unfortunately, the numerous benefits of visualizing health records have only been felt in developed nations, unlike other sectors where technological advancement in developed nations have had similar impact in developing nations. This paper identifies the characteristics of health records and the challenges involved in processing large volumes of health records. This is to identify possible steps that could be taken for developing nations to benefit from visualizing health records in huge volumes.|10.1145/3468784.3471607|https://doi.org/10.1145/3468784.3471607|New York, NY, USA|Association for Computing Machinery|9781450390125|2021|Visualising Developing Nations Health Records: Opportunities, Challenges and Research Agenda|Umejiaku, Afamefuna and Dang, Tommy|inproceedings|10.1145/3468784.3471607|38||||||||||||||||||||||||||||1225815894|42
PETRA '22|Corfu, Greece|user survey, duplicate data, data utility, data quality metrics, incorrect data, data annotation, incomplete data, data quality, datasets, inconsistent data|7|118–124|Proceedings of the 15th International Conference on PErvasive Technologies Related to Assistive Environments|As large-scale machine learning models become more prevalent in assistive and pervasive technologies, the research community has started examining limitations and challenges that arise from training data, e.g., fairness, bias, and interpretability issues. To this end, data-centric approaches are increasingly prevailing over time, showing that high-quality data is a critical component in many applications. Several studies explore methods to define and improve data quality, however, no uniform definition exists. In this work, we present an empirical analysis of the multifaceted problem of evaluating data quality. Our work aims at identifying data quality challenges that are most commonly observed by data users and practitioners. Inspired by the need for generally applicable methods, we select a representative set of quality indicators, that covers a broad spectrum of issues, and investigate the utility of these indicators on a broad range of datasets through inter-annotator agreement analysis. Our work provides insights and presents open challenges in designing improved data life cycles.|10.1145/3529190.3529222|https://doi.org/10.1145/3529190.3529222|New York, NY, USA|Association for Computing Machinery|9781450396318|2022|[Data] Quality Lies In The Eyes Of The Beholder|Pleimling, Xavier and Shah, Vedant and Lourentzou, Ismini|inproceedings|10.1145/3529190.3529222|||||||||||||||||||||||||||||1226448067|42
ICCBDC 2017|London, United Kingdom|big data, NoSQL, MapReduce, data warehouse, databases, NewSQL|5|6–10|Proceedings of the 2017 International Conference on Cloud and Big Data Computing|Big data are a data trend present around us mainly through Internet -- social networks and smart devices and meters -- mostly without us being aware of them. Also they are a fact that both industry and scientific research needs to deal with. They are interesting from analytical point of view, for they contain knowledge that cannot be ignored and left unused. Traditional system that supports the advanced analytics and knowledge extraction -- data warehouse -- is not able to cope with large amounts of fast incoming various and unstructured data, and may be facing a paradigm shift in terms of utilized concepts, technologies and methodologies, which have become a very active research area in the last few years. This paper provides an overview of research trends important for the big data warehousing, concepts and technologies used for data storage and (ETL) processing, and research approaches done in attempts to empower traditional data warehouses for handling big data.|10.1145/3141128.3141139|https://doi.org/10.1145/3141128.3141139|New York, NY, USA|Association for Computing Machinery|9781450353434|2017|Big Data and New Data Warehousing Approaches|Pti\v{c}ek, Marina and Vrdoljak, Boris|inproceedings|10.1145/3141128.3141139|||||||||||||||||||||||||||||1230675087|42
||human-in-the-loop decision support system, Data entry, online learning|19|||This article studies the problem of automated information processing from large volumes of unstructured, heterogeneous, and sometimes untrustworthy data sources. The main contribution is a novel framework called Machine Assisted Record Selection (MARS). Instead of today’s standard practice of relying on human experts to manually decide the order of records for processing, MARS learns the optimal record selection via an online learning algorithm. It further integrates algorithm-based record selection and processing with human-based error resolution to achieve a balanced task allocation between machine and human. Both fixed and adaptive MARS algorithms are proposed, leveraging different statistical knowledge about the existence, quality, and cost associated with the records. Experiments using semi-synthetic data that are generated from real-world patients record processing in the UK national cancer registry are carried out, which demonstrate significant (3 to 4 fold) performance gain over the fixed-order processing. MARS represents one of the few examples demonstrating that machine learning can assist humans with complex jobs by automating complex triaging tasks.|10.1145/3494582|https://doi.org/10.1145/3494582|New York, NY, USA|Association for Computing Machinery||2022|MARS: Assisting Human with Information Processing Tasks Using Machine Learning|Shen, Cong and Qian, Zhaozhi and Huyuk, Alihan and Van Der Schaar, Mihaela|article|10.1145/3494582|21|mar|ACM Trans. Comput. Healthcare|26911957|2|3|April 2022||||||||||||||||||||||1231667468|1983512862
||Data integrity;Manufacturing;Measurement;Decision making;Production facilities;Data models;manufacturing test;data quality;test data quality;cost of data quality||1-6|2018 IEEE AUTOTESTCON|Manufacturing test data volumes are constantly increasing. While there has been extensive focus in the literature on big data processing, less focus has existed on data quality, and considerably less focus has been placed specifically on manufacturing test data quality. This paper presents a fully automated test data quality measurement developed by the authors to facilitate analysis of manufacturing test operations, resulting in a single number used to compare manufacturing test data quality across programs and factories, and focusing effort cost-effectively. The automation enables program and factory users to see, understand, and improve their test data quality directly. Immediate improvements in test data quality speed manufacturing test operation analysis, reducing elapsed time and overall spend in test operations. Data quality has significant financial impacts to businesses [1]. While manufacturing cost models are well understood, data quality cost models are less well understood (see Eppler & Helfert [2] who review manufacturing cost models and create a taxonomy for data quality costs). Kim & Choi [3] discuss measuring data quality costs, and a rudimentary data quality cost calculation is described in [4]. Haug et al. [5] describe a classification of costs for poor data quality, and while they do not provide a cost calculation, they do define optimality for data quality. Laranjeiro et al. [6] have a recent survey of poor data quality classification. Ge & Helfert [7] extend the work in [2], and provide an updated review of data quality costs. Test data is specifically addressed in the context of data processing in [8]. Big data quality efforts are reviewed in [9, 10]. Data quality metrics are discussed in [11], and requirements for data quality metrics are identified in [12]. Data inconsistencies are detailed in [13], while categorical data inconsistencies are explained in [14]. In the current work, manufacturing test data quality is directly correlated to the speed of manufacturing test operations analysis. A measurement for manufacturing test data quality indicates the speed at which analysis can be performed, and increases in the test data quality score have precipitated increases in the speed of analysis, described herein.|10.1109/AUTEST.2018.8532518|||||2018|Measuring Manufacturing Test Data Analysis Quality|Burkhardt, Andrew and Berryman, Sheila and Brio, Ashley and Ferkau, Susan and Hubner, Gloria and Lynch, Kevin and Mittman, Susan and Sonderer, Kathy|inproceedings|8532518||Sep.||15584550|||||||||||||||||||||||||1232502538|1729021450
|||||||||New York, NY, USA|Association for Computing Machinery|9781450364164|2017|Information Technology Curricula 2017: Curriculum Guidelines for Baccalaureate Degree Programs in Information Technology|Task Group on Information Technology Curricula|book|10.1145/3173161|||||||||||||||||||||||||||||1238585862|42
ICBDC '21|Shenzhen, China|online course, education platform, recommendation system, Collaborative filtering algorithm|6|142–147|Proceedings of the 6th International Conference on Big Data and Computing|Aiming at the problem that the overload of online education platform course resources leads to the difficulty of user selection, this paper mainly studies the improvement and application of collaborative filtering algorithm based on online course recommendation system, which organically combines personalized recommendation technology and online course system to meet the needs of users and online education platform. In the process of recommendation, firstly, user preferences are collected to establish a data model, and user login information and learning behavior information are used as implicit characteristics of user preferences. The loss rate of users in the computing platform is defined, the popularity of each course is calculated, and the relationship between users and courses is constructed, and the correlation and comparative analysis are carried out, Then, the traditional collaborative filtering algorithm is improved by introducing the implicit features after analysis, and the cosine similarity method is used to calculate the course similarity. Finally, the topN recommendation list is generated to get the recommendation results. Based on the desensitization data of an education platform, the experimental results show that the improved recommendation model can improve the precision of recommendation by introducing implicit features.|10.1145/3469968.3469992|https://doi.org/10.1145/3469968.3469992|New York, NY, USA|Association for Computing Machinery|9781450389808|2021|Application of Collaborative Filtering Recommendation Algorithm in Internet Online Courses|Pan, Zhengjun and Zhao, Lianfen and Zhong, Xingyu and Xia, Zitong|inproceedings|10.1145/3469968.3469992|||||||||||||||||||||||||||||1239445680|42
||Resource description framework;Cleaning;Ontologies;Data mining;Knowledge based systems;Conferences;Databases||77-79|2015 31st IEEE International Conference on Data Engineering Workshops|Without a shadow of a doubt, data cleaning has played an important part in the history of data management and data analytics. Possessing high quality data has been proven to be crucial for businesses to do data driven decision making, especially within the information age and the era of big data. Resource Description Framework (RDF) is a standard model for data interchange on the semantic web. However, it is known that RDF data is dirty, since many of them are automatically extracted from the web. In this paper, we will first revisit data quality problems appeared in RDF data. Although many efforts have been put to clean RDF data, unfortunately, most of them are based on laborious manual evaluation. We will also describe possible solutions that shed lights on (semi-)automatically cleaning (big) RDF data.|10.1109/ICDEW.2015.7129549|||||2015|Big RDF data cleaning|Tang, Nan|inproceedings|7129549||April|||||||||||||||||||||||||||1239775700|42
||Manufacturing processes;Production;Quality control;Product design;Real-time systems;Manufacturing;Quality assessment;Quality management;production control;prediction methods||208506-208517||Quality prediction is one of the key links of quality control. Benefitting from the development of digital manufacturing, manufacturing process data have grown rapidly, which allows product quality predictions to be made based on a real-time manufacturing process. A real-time quality control system (RTQCS) based on manufacturing process data is presented in this paper. In this study, the relationship between the product real-time quality status and processing task process was established by analyzing the relationship between the product manufacturing resources and the quality status. The key quality characteristics of the product were identified by analyzing the similarity of the product quality characteristic variations in the manufacturing process based on the big data technology, and a quality-resource matrix was constructed. Based on the quality-resource matrix, the RTQCS was established by introducing an association-rule incremental-update algorithm. Finally, the RTQCS was applied in actual production, and the performance of RTQCS was verified by experiments. The experiments showed that the RTQCS can effectively guarantee the quality of product manufacturing and improve the manufacturing efficiency during production.|10.1109/ACCESS.2020.3038394|||||2020|A Real-Time Quality Control System Based on Manufacturing Process Data|Duan, Gui-Jiang and Yan, Xin|article|9261414|||IEEE Access|21693536||8|||||21100374601|0,587|Q1|127|18036|24267|771081|116691|24200|4,48|42,75|United States|Northern America|2013-2020|Computer Science (miscellaneous) (Q1); Engineering (miscellaneous) (Q1); Materials Science (miscellaneous) (Q2)|105,968|3.367|0.15396|1241343147|1905633267
||Open data, data quality assurance, risk identification, risk mitigation, fake news, post-truth|13|||Curated, labeled, high-quality data is a valuable commodity for tasks such as business analytics and machine learning. Open data is a common source of such data—for example, retail analytics draws on open demographic data, and weather forecast systems draw on open atmospheric and ocean data. Open data is released openly by governments to achieve various objectives, such as transparency, informing citizen engagement, or supporting private enterprise. Critical examination of ongoing social changes, including the post-truth phenomenon, suggests the quality, integrity, and authenticity of open data may be at risk. We introduce this risk through various lenses, describe some of the types of risk we expect using a threat model approach, identify approaches to mitigate each risk, and present real-world examples of cases where the risk has already caused harm. As an initial assessment of awareness of this disinformation risk, we compare our analysis to perspectives captured during open data stakeholder consultations in Canada.|10.1145/3328747|https://doi.org/10.1145/3328747|New York, NY, USA|Association for Computing Machinery||2020|Characterizing Disinformation Risk to Open Data in the Post-Truth Era|Colborne, Adrienne and Smit, Michael|article|10.1145/3328747|13|jun|J. Data and Information Quality|19361955|3|12|September 2020||||||||||||||||||||||1244149681|833754770
||smart manufacturing;big data;quality prediction;BP neural network;AdaBoost algorithm;BP-AdsysBoost model||1-3|2019 IEEE International Conference on Smart Manufacturing, Industrial & Logistics Engineering (SMILE)|To accurately predict the product quality in smart manufacturing, this paper designs the BP-AdsysBoost model on the basis of BP neural network and AdaBoost algorithm. The BP-AdsysBoost model considers both the data characteristics and the technology advantages, which pays more attentions to the unqualified products wrongly predicted. To further examine the model, the 110560 data of smart manufacturing from German BOSCH company is used for this research. The proposed BP-AdsysBoost model is compared with the BP neural network and the unmodified BP-AdaBoost model according to prediction performance. The results show that the BP-AdsysBoost model has significant advantages in prediction accuracy and FDR, which proves its satisfied prediction ability for product quality in smart manufacturing.|10.1109/SMILE45626.2019.8965303|||||2019|A Study on Quality Prediction for Smart Manufacturing Based on the Optimized BP-AdaBoost Model|Caihong, Zhou and Zengyuan, Wu and Chang, Liu|inproceedings|8965303||April|||||||||||||||||||||||||||1244397630|42
||Task analysis;Transmitters;Signal to noise ratio;Data integrity;Sensors;IEEE transactions;Big Data;Crowdsensing;truthful incentive mechanism;data quality||1959-1972||Mobile crowdsensing has found a variety of applications (e.g., spectrum sensing, environmental monitoring) by leveraging the “wisdom” of a potentially large crowd of mobile users. An important metric of a crowdsensing task is data accuracy, which relies on the data quality of the participating users' data (e.g., users' received SNRs for measuring a transmitter's transmit signal strength). However, the quality of a user can be its private information (which, e.g., may depend on the user's location) that it can manipulate to its own advantage, which can mislead the crowdsensing requester about the knowledge of the data's accuracy. This issue is exacerbated by the fact that the user can also manipulate its effort made in the crowdsensing task, which is a hidden action that could result in the requester having incorrect knowledge of the data's accuracy. In this paper, we devise truthful crowdsensing mechanisms for Quality and Effort Elicitation (QEE), which incentivize strategic users to truthfully reveal their private quality and truthfully make efforts as desired by the requester. The QEE mechanisms achieve the truthful design by overcoming the intricate dependency of a user's data on its private quality and hidden effort. Under the QEE mechanisms, we show that the crowdsensing requester's optimal (RO) effort assignment assigns effort only to the best user that has the smallest “virtual valuation”, which depends on the user's quality and the quality's distribution. We also show that, as the number of users increases, the performance gap between the RO effort assignment and the socially optimal effort assignment decreases, and converges to 0 asymptotically. We further discuss some extensions of the QEE mechanisms. Simulation results demonstrate the truthfulness of the QEE mechanisms and the system efficiency of the RO effort assignment.|10.1109/TNET.2019.2934026|||||2019|Truthful Mobile Crowdsensing for Strategic Users With Private Data Quality|Gong, Xiaowen and Shroff, Ness B.|article|8820081||Oct|IEEE/ACM Transactions on Networking|15582566|5|27|||||||||||||||||||||||1244704899|1652846677
|||37|197–233|Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice|||https://doi.org/10.1145/3447404.3447416|New York, NY, USA|Association for Computing Machinery|9781450390293|2021|Combining Infrastructure Sensor and Tourism Market Data in a Smart City Project—Case Study 1|Komninos, Andreas and Dunlop, Mark D. and Wilson, John N.|inbook|10.1145/3447404.3447416|||||||||1||||||||||||||||||||1248283505|42
||Logic gates;Big data;Measurement;Heuristic algorithms;Complexity theory;Standards;Optimization;M2M;IoT;configuration;gateway;autonomic;self-management||490-497|2014 IEEE International Congress on Big Data|Internet of Things (IoT) platforms that handle Big Data might perform poorly or not according to the goals of their operator (in terms of costs, database utilization, data quality, energy-efficiency, throughput) if they are not configured properly. The latter configuration refers mainly to system parameters of the data-collecting gateways, e.g., polling intervals, capture intervals, encryption schemes, used protocols etc. However, re-configuring the platform appropriately upon changes of the system context or the operator targets is currently not taking place. This happens because of the complexity or unawareness of the synergies between system configurations and various aspects of the Big Data-handling IoT platform, but also because of the human resources that an efficient re-configuration would require. This paper presents an auto-configuration solution based on interpretable configuration suggestions, focusing on the algorithms for computing the mentioned suggested configurations. Five such algorithms are contributed, while a thorough evaluation reveals which of these algorithms should be used in different operation scenarios in order to achieve high fulfillment of the operator's targets.|10.1109/BigData.Congress.2014.78|||||2014|Auto-configuration System and Algorithms for Big Data-Enabled Internet-of-Things Platforms|Papageorgiou, Apostolos and Zahn, Manuel and Kovacs, Ernö|inproceedings|6906820||June||23797703|||||||||||||||||||||||||1251993665|1226158248
||Smart health care, Personalized remote physical therapy, Synchronous Big Data, Gigabit networking app||3-20||With gigabit networking becoming economically feasible and widely installed at homes, there are new opportunities to revisit in-home, personalized telehealth services. In this paper, we describe a novel telehealth eldercare service that we developed viz., “PhysicalTherapy-as-a-Service” (PTaaS) that connects a remote physical therapist at a clinic to a senior at home. The service leverages a high-speed, low-latency network connection through an interactive interface built on top of Microsoft Kinect motion sensing capabilities. The interface that is built using user-centered design principles for wellness coaching exercises is essentially a ‘Synchronous Big Data’ application due to its: (i) high data-in-motion velocity (i.e., peak data rate is ≈400 Mbps), (ii) considerable variety (i.e., measurements include 3D sensing, network health, user opinion surveys and video clips of RGB, skeletal and depth data), and (iii) large volume (i.e., several GB of measurement data for a simple exercise activity). The successful PTaaS delivery through this interface is dependent on the veracity analytics needed for correlation of the real-time Big Data streams within a session, in order to assess exercise balance of the senior without any bias due to network quality effects. Our experiments with PTaaS in an actual testbed involving senior homes in Kansas City with Google Fiber connections and our university clinic demonstrate the network configuration and time synchronization related challenges in order to perform online analytics. Our findings provide insights on how to: (a) enable suitable resource calibration and perform network troubleshooting for high user experience for both the therapist and the senior, and (b) realize a Big Data architecture for PTaaS and other similar personalized healthcare services to be remotely delivered at a large-scale in a reliable, secure and cost-effective manner.|https://doi.org/10.1016/j.pmcj.2015.09.004|https://www.sciencedirect.com/science/article/pii/S1574119215001704||||2016|Synchronous Big Data analytics for personalized and remote physical therapy|Prasad Calyam and Anup Mishra and Ronny Bazan Antequera and Dmitrii Chemodanov and Alex Berryman and Kunpeng Zhu and Carmen Abbott and Marjorie Skubic|article|CALYAM20163|||Pervasive and Mobile Computing|15741192||28||Special Issue on Big Data for Healthcare; Guest Editors: Sriram Chellappan, Nirmalya Roy, Sajal K. Das and Special Issue on Security and Privacy in Mobile Clouds Guest; Editors: Sherman S.M. Chow, Urs Hengartner, Joseph K. Liu, Kui Ren|||3200147819|0,687|Q1|64|68|350|2760|1558|342|4,67|40,59|Netherlands|Western Europe|2005-2020|Computer Networks and Communications (Q1); Computer Science (miscellaneous) (Q1); Hardware and Architecture (Q1); Information Systems (Q1); Software (Q1); Applied Mathematics (Q2); Computer Science Applications (Q2)|2,540|3.453|0.00335|1252415201|1910356919
||Digital health, Secondary use of data, Health analytics, Predictive modeling, Health forecasting||36-42||Recent advances on prospective monitoring and retrospective analysis of health information at national or regional level are generating high expectations for the application of Big Data technologies that aim to analyze at real time high-volumes and/or complex of data from healthcare delivery (e.g., electronic health records, laboratory and radiology information, electronic prescriptions, etc.) and citizens' lifestyles (e.g., personal health records, personal monitoring devices, social networks, etc.). Along these same lines, advances in the field of genomics are revolutionizing biomedical research, both in terms of data volume and prospects, as well as in terms of the social impact it entails. The potential of Big Data applications that consider all of the above levels of health information lies in the possibility of combining and integrating de-identified health information to allow secondary uses of data. This is the use and re-use of various sources of health information for purposes in addition to the direct clinical care of specific patients or the direct investigation of specific biomedical research hypotheses. Current applications include: epidemiological and pharmacovigilance studies, facilitating recruitment to randomized controlled trials, carrying out audits and benchmarking studies, financial and service planning, and ultimately supporting the generation of novel biomedical research outcomes.|https://doi.org/10.1016/j.coisb.2017.04.012|https://www.sciencedirect.com/science/article/pii/S2452310017300409||||2017|Perspectives on Big Data applications of health information|Isaac Cano and Akos Tenyi and Emili Vela and Felip Miralles and Josep Roca|article|CANO201736|||Current Opinion in Systems Biology|24523100||3||• Mathematical modelling • Mathematical modelling, Dynamics of brain activity at the systems level • Clinical and translational systems biology|||21100857212|1,576|Q1|21|43|269|2736|800|239|2,90|63,63|United Kingdom|Western Europe|2017-2020|Applied Mathematics (Q1); Biochemistry, Genetics and Molecular Biology (miscellaneous) (Q1); Computer Science Applications (Q1); Drug Discovery (Q1); Modeling and Simulation (Q1)||||1252866990|1614330484
|||41|59–99|Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice|||https://doi.org/10.1145/3447404.3447410|New York, NY, USA|Association for Computing Machinery|9781450390293|2021|Statistical Grounding|Arif, Ahmed Sabbir|inbook|10.1145/3447404.3447410|||||||||1||||||||||||||||||||1252903329|42
|||12|37–48||"\"We are often thrilled by the abundance of information surrounding us and wish to integrate data from as many sources as possible. However, understanding, analyzing, and using these data are often hard. Too much data can introduce a huge integration cost, such as expenses for purchasing data and resources for integration and cleaning. Furthermore, including low-quality data can even deteriorate the quality of integration results instead of bringing the desired quality gain. Thus, \"\"the more the better\"\" does not always hold for data integration and often \"\"less is more\"\".In this paper, we study how to select a subset of sources before integration such that we can balance the quality of integrated data and integration cost. Inspired by the Marginalism principle in economic theory, we wish to integrate a new source only if its marginal gain, often a function of improved integration quality, is higher than the marginal cost, associated with data-purchase expense and integration resources. As a first step towards this goal, we focus on data fusion tasks, where the goal is to resolve conflicts from different sources. We propose a randomized solution for selecting sources for fusion and show empirically its effectiveness and scalability on both real-world data and synthetic data.\""|10.14778/2535568.2448938|https://doi.org/10.14778/2535568.2448938||VLDB Endowment||2012|Less is More: Selecting Sources Wisely for Integration|Dong, Xin Luna and Saha, Barna and Srivastava, Divesh|article|10.14778/2535568.2448938||dec|Proc. VLDB Endow.|21508097|2|6|December 2012||||21100199855|0,946|Q1|134|246|598|12401|3759|566|5,50|50,41|United States|Northern America|2008-2020|Computer Science (miscellaneous) (Q1)|7,198|2.047|0.00891|1255382952|1216159931
WWW '21|Ljubljana, Slovenia|binary coding, graph regularization, multi-view clustering, parameter selection|10|3269–3278|Proceedings of the Web Conference 2021|Multi-view clustering has been widely studied in machine learning, which uses complementary information to improve clustering performance. However, challenges remain when handling large-scale multi-view data due to the traditional approaches’ high time complexity. Besides, the existing approaches suffer from parameter selection. Due to the lack of labeled data, parameter selection in practical clustering applications is difficult, especially in big data. In this paper, we propose a novel approach for large-scale multi-view clustering to overcome the above challenges. Our approach focuses on learning the low-dimensional binary embedding of multi-view data, preserving the samples’ local structure during binary embedding, and optimizing the embedding and clustering in a unified framework. Furthermore, we proposed to learn the parameters using a combination of data-driven and heuristic approaches. Experiments on five large-scale multi-view datasets show that the proposed method is superior to the state-of-the-art in terms of clustering quality and running time.|10.1145/3442381.3449956|https://doi.org/10.1145/3442381.3449956|New York, NY, USA|Association for Computing Machinery|9781450383127|2021|Scalable Auto-Weighted Discrete Multi-View Clustering|Yang, Longqi and Zhang, Liangliang and Tang, Yuhua|inproceedings|10.1145/3442381.3449956|||||||||||||||||||||||||||||1256209944|42
ICSCA 2020|Langkawi, Malaysia|Recommendations, Data, COBIT 4.1, Maturity, DS-11|5|57–61|Proceedings of the 2020 9th International Conference on Software and Computer Applications|Data is an valuable asset that potentially provides substantial benefits for the government and society. To make the performance of local government apparatus runs optimally and the public gets the best service, the government of Bandung Regency strives to improve data management. The initial stage of optimizing data management is the assessment of the maturity level in managing data (DS-11) using COBIT 4.1. Base on the assessment maturity level for DS-11, the government of Bandung Regency needs to raise the level from 2.46 (Repeatable but Intuitive) to 3.0 (Defined). Recommendations given to improve data management in Government with focuses on maintaining the completeness, accuracy, availability, and protection of data.|10.1145/3384544.3384588|https://doi.org/10.1145/3384544.3384588|New York, NY, USA|Association for Computing Machinery|9781450376655|2020|Recommendations for Improving Data Management Process in Government of Bandung Regency Using COBIT 4.1 Framework|Nugroho, Heru and Gumilang, Soni Fajar Surya|inproceedings|10.1145/3384544.3384588|||||||||||||||||||||||||||||1256380144|42
||data quality assessment, system identification, big data, Industry 4.0, soft sensors||104-113||As the amount of data stored from industrial processes increases with the demands of Industry 4.0, there is an increasing interest in finding uses for the stored data. However, before the data can be used its quality must be determined and appropriate regions extracted. Initially, such testing was done manually using graphs or basic rules, such as the value of a variable. With large data sets, such an approach will not work, since the amount of data to tested and the number of potential rules is too large. Therefore, there is a need for automated segmentation of the data set into different components. Such an approach has recently been proposed and tested using various types of industrial data. Although the industrial results are promising, there still remain many unanswered questions including how to handle a priori knowledge, over- or undersegmentation of the data set, and setting the appropriate thresholds for a given application. Solving these problems will provide a robust and reliable method for determining the data quality of a given data set.|https://doi.org/10.1016/j.ifacol.2020.12.103|https://www.sciencedirect.com/science/article/pii/S2405896320303591||||2020|Data Quality Assessment for System Identification in the Age of Big Data and Industry 4.0|Yuri A.W. Shardt and Xu Yang and Kevin Brooks and Andrei Torgashov|article|SHARDT2020104|||IFAC-PapersOnLine|24058963|2|53||21st IFAC World Congress|||21100456158|0,308|Q3|72|115|8407|1913|9863|8351|1,13|16,63|Austria|Western Europe|2002-2019|Control and Systems Engineering (Q3)||||1257083583|676980763
||Cloud computing;Collaboration;Monitoring;Security;Big Data;Quality of service;Computer architecture;Cloud computing;service behavior monitoring;trust computing;big data analysis||1917-1931||Providing high trustworthy service is the most fundamental task for any cloud computing platform. Users are willing to deliver their computing tasks and the most sensitive data to cloud data centers, which is based on the trust relationship established between users and cloud service providers. However, with the development of collaboration cloud computing, how to provider fast response for a large number of users' service requests becomes a challenging problem. In order to quickly provide highly trustworthy services, the service platform must efficiently and quickly reply tens of millions of service requests, and automatically match-make tens of thousands of service resources. In this context, lightweight and fast (high-speed, low-overhead) trust computing schemes become the fundamental demand for implementing a trustworthy and collaborative cloud service. In this paper, we propose an innovative and parallel trust computing scheme based on big data analysis for the trustworthy cloud service environment. First, a distributed and modular perceiving architecture for large-scale virtual machines' service behavior is proposed relying on distributed monitoring agents. Then, an adaptive, lightweight, and parallel trust computing scheme is proposed for big monitored data. To the best of our knowledge, this paper is the first to use a blocked and parallel computing mechanism, the speed of trust calculation is greatly accelerated, which makes this trust computing scheme very suitable for a large-scale cloud computing environment. Performance analysis and experimental results verify feasibility and effectiveness of the proposed scheme.|10.1109/TIFS.2018.2806925|||||2018|Fast and Parallel Trust Computing Scheme Based on Big Data Analysis for Collaboration Cloud Service|Li, Xiaoyong and Yuan, Jie and Ma, Huadong and Yao, Wenbin|article|8293821||Aug|IEEE Transactions on Information Forensics and Security|15566021|8|13|||||||||||||||||||||||1257504765|1313956176
SIGMOD '17|Chicago, Illinois, USA|data exploration, data profiling, dependency discovery|5|1747–1751|Proceedings of the 2017 ACM International Conference on Management of Data|is to understand the dataset at hand and its metadata. The process of metadata discovery is known as data profiling. Profiling activities range from ad-hoc approaches, such as eye-balling random subsets of the data or formulating aggregation queries, to systematic inference of structural information and statistics of a dataset using dedicated profiling tools. In this tutorial, we highlight the importance of data profiling as part of any data-related use-case, and we discuss the area of data profiling by classifying data profiling tasks and reviewing the state-of-the-art data profiling systems and techniques. In particular, we discuss hard problems in data profiling, such as algorithms for dependency discovery and profiling algorithms for dynamic data and streams. We also pay special attention to visualizing and interpreting the results of data profiling. We conclude with directions for future research in the area of data profiling. This tutorial is based on our survey on profiling relational data [2].|10.1145/3035918.3054772|https://doi.org/10.1145/3035918.3054772|New York, NY, USA|Association for Computing Machinery|9781450341974|2017|Data Profiling: A Tutorial|Abedjan, Ziawasch and Golab, Lukasz and Naumann, Felix|inproceedings|10.1145/3035918.3054772|||||||||||||||||||||||||||||1259747587|42
||Big data, Business continuity, Research project, Software, Storyboard, User interface||175-197|Building Big Data Applications|This chapter will discuss the building and delivery of the application. We will look at all aspects of what needs to be done to complete the process from requirements to outcomes, program management, testing, methodology, and all risks and pitfalls. We will discuss KANBAN, budgets and finances, governance, timeline, increase of efficiency, maintenance, support, and application implementation.|https://doi.org/10.1016/B978-0-12-815746-6.00010-7|https://www.sciencedirect.com/science/article/pii/B9780128157466000107||Academic Press|978-0-12-815746-6|2020|10 - Building the big data application|Krish Krishnan|incollection|KRISHNAN2020175||||||||||Krish Krishnan|||||||||||||||||||1259790479|42
||Big data analytics, Persuasion, Practice, Capabilities, Value||120300||This paper draws on data from three organisational case studies and expert interviews to propose that persuasive practices are the precursors and enablers of analytical capability development. A bundle of seven practices was identified and observed to bridge multiple gaps between technical and non-technical colleagues on big data analytics (BDA) projects. The deployment of these practices varied according to the level of BDA maturity and featured a host of socio-material elements. This paper complements existing technical case studies with a fine-grained qualitative account of the managerial and human elements of BDA implementation. Effective deployment of persuasive practices potentially both embeds the benefits and mitigates the risks of BDA, sowing the seeds of many different forms of value.|https://doi.org/10.1016/j.techfore.2020.120300|https://www.sciencedirect.com/science/article/pii/S0040162520311264||||2020|Sowing the seeds of value? Persuasive practices and the embedding of big data analytics|Jeffrey Hughes and Kirstie Ball|article|HUGHES2020120300|||Technological Forecasting and Social Change|00401625||161|||||14704|2,226|Q1|117|448|1099|35581|10127|1061|9,01|79,42|United States|Northern America|1970-2020|Applied Psychology (Q1); Business and International Management (Q1); Management of Technology and Innovation (Q1)|21,116|8.593|0.02416|1260453947|1949868303
||Data integrity;Renewable energy sources;Data models;Analytical models;Production;Monitoring;Clouds;renewable energy cloud;data quality analysis;AHP;evaluation metrics||687-692|2020 IEEE 7th International Conference on Industrial Engineering and Applications (ICIEA)|Global climate crisis in 21st century pushed countries to move towards energy transformation in generation and consumption. To achieve green and low-carbon energy transformation goals, it is necessary that a large number of renewable energy resources such as wind and solar to be consumed. Renewable energy with intermittent fluctuations in time dimension and agglomerations in spatial dimension increases the complexity of green energy consumption friendly. Therefore, comprehensive data and advanced predictive analysis methods are required to guarantee safety of operation and transactions for renewable energy plants and stations. We can even say that quality of renewable energy data determines the accuracy of prediction and analysis. Firstly, this article analyzes the operation and transaction characteristics of distributed renewable energy plants, and data quality analysis framework for distributed renewable energy operations and transactions was built on the new energy cloud platform. Data information were classified into model parameter and status instance, which are related to dispatching and energy power transaction businesses such as equipment model management, operation monitoring and security analysis, measurement statistics etc. The importance between them is determined according to pairwise comparison. Finally, analytic hierarchy process (AHP) theory was applied to calculate weights for data integrity, accuracy, consistency and timeliness, data quality assessment process and calculation methods were designed, and load series data was used to verify its correctness.|10.1109/ICIEA49774.2020.9102068|||||2020|Data Quality Analysis Framework and Evaluation Methods for Power System Operation with High Proportion of Renewable Energy Penetration|Wang, Jiye and Li, Yang and Guo, Jian and Cao, Junwei and Hua, Haochen and Xing, Chunxiao and Qi, Caijuan and Pi, Zhixian|inproceedings|9102068||April|||||||||||||||||||||||||||1261238006|42
||citizen science, distributed collaboration, collaboratory, sustainability|22|||Citizen science project leaders collecting field data in a hyperlocal community often face common socio-technical challenges, which can potentially be addressed by sharing innovations across different groups through peer-to-peer collaboration. However, most citizen science groups practice in isolation, and end up re-inventing the wheel when it comes to addressing these common challenges. This study seeks to investigate distributed collaboration between different water monitoring citizen science groups. We discovered a unique social network application called Water Reporter that mediated distributed collaboration by creating more visibility and transparency between groups using the app. We interviewed 8 citizen science project leaders who were users of this app, and 6 other citizen science project leaders to understand how distributed collaboration mediated by this app differed from collaborative practices of Non Water Reporter users. We found that distributed collaboration was an important goal for both user groups, however, the tasks that support these collaboration activities differed for the two user groups.|10.1145/3512944|https://doi.org/10.1145/3512944|New York, NY, USA|Association for Computing Machinery||2022|Instagram of Rivers: Facilitating Distributed Collaboration in Hyperlocal Citizen Science|Gupta, Srishti and Jablonski, Julia and Tsai, Chun-Hua and Carroll, John M.|article|10.1145/3512944|97|apr|Proc. ACM Hum.-Comput. Interact.||CSCW1|6|April 2022||||||||||||||||||||||1261800552|42
||Companies;Information management;Data handling;Data storage systems;Computer architecture;Information Governance;Big Data;Data Quality||1142-1143|2013 IEEE 16th International Conference on Computational Science and Engineering|The value of information as a competitive differential has been taken into consideration in companies all over the world for some time already. In recent years, there has been heated debate about some terms originated from new concepts related to information, such as big data, due to the promise that such topic might revolutionise world trade. Hence, data and information governance and quality have been increasingly discussed in the business world.|10.1109/CSE.2013.168|||||2013|Information Governance, Big Data and Data Quality|Freitas, Patrícia Alves de and Reis, Everson Andrade dos and Michel, Wanderson Senra and Gronovicz, Mauro Edson and Rodrigues, Márcio Alexandre de Macedo|inproceedings|6755349||Dec|||||||||||||||||||||||||||1263479671|42
Middleware '19|Davis, CA, USA||5|6–10|Proceedings of the 20th International Middleware Conference Tutorials|Since the introduction of Bitcoin---the first wide-spread application driven by blockchains---the interest of the public and private sector in blockchains has skyrocketed. At the core of this interest are the ways in which blockchains can be used to improve data management, e.g., by enabling federated data management via decentralization, resilience against failure and malicious actors via replication and consensus, and strong data provenance via a secured immutable ledger.In practice, high-performance blockchains for data management are usually built in permissioned environments in which the participants are vetted and can be identified. In this setting, blockchains are typically powered by Byzantine fault-tolerant consensus protocols. These consensus protocols are used to provide full replication among all honest blockchain participants by enforcing an unique order of processing incoming requests among the participants.In this tutorial, we take an in-depth look at Byzantine fault-tolerant consensus. First, we take a look at the theory behind replicated computing and consensus. Then, we delve into how common consensus protocols operate. Finally, we take a look at current developments and briefly look at our vision moving forward.|10.1145/3366625.3369437|https://doi.org/10.1145/3366625.3369437|New York, NY, USA|Association for Computing Machinery|9781450370400|2019|An In-Depth Look of BFT Consensus in Blockchain: Challenges and Opportunities|Gupta, Suyash and Hellings, Jelle and Rahnama, Sajjad and Sadoghi, Mohammad|inproceedings|10.1145/3366625.3369437|||||||||||||||||||||||||||||1265239033|42
||Big data analytics, Supply chain management, Logistics||343-349||This special issue explores big data analytics and applications for logistics and supply chain management by examining novel methods, practices, and opportunities. The articles present and analyse a variety of opportunities to improve big data analytics and applications for logistics and supply chain management, such as those through exploring technology-driven tracking strategies, financial performance relations with data driven supply chains, and implementation issues and supply chain capability maturity with big data. This editorial note summarizes the discussions on the big data attributes, on effective practices for implementation, and on evaluation and implementation methods.|https://doi.org/10.1016/j.tre.2018.03.011|https://www.sciencedirect.com/science/article/pii/S1366554518302606||||2018|Big data analytics and application for logistics and supply chain management|Kannan Govindan and T.C.E. Cheng and Nishikant Mishra and Nagesh Shukla|article|GOVINDAN2018343|||Transportation Research Part E: Logistics and Transportation Review|13665545||114|||||20909|2,042|Q1|110|233|507|13126|4018|499|7,63|56,33|United Kingdom|Western Europe|1997-2020|Business and International Management (Q1); Civil and Structural Engineering (Q1); Management Science and Operations Research (Q1); Transportation (Q1)||||1265301053|1729083653
ICSE '16|Austin, Texas||12|96–107|Proceedings of the 38th International Conference on Software Engineering|Creating and running software produces large amounts of raw data about the development process and the customer usage, which can be turned into actionable insight with the help of skilled data scientists. Unfortunately, data scientists with the analytical and software engineering skills to analyze these large data sets have been hard to come by; only recently have software companies started to develop competencies in software-oriented data analytics. To understand this emerging role, we interviewed data scientists across several product groups at Microsoft. In this paper, we describe their education and training background, their missions in software engineering contexts, and the type of problems on which they work. We identify five distinct working styles of data scientists: (1) Insight Providers, who work with engineers to collect the data needed to inform decisions that managers make; (2) Modeling Specialists, who use their machine learning expertise to build predictive models; (3) Platform Builders, who create data platforms, balancing both engineering and data analysis concerns; (4) Polymaths, who do all data science activities themselves; and (5) Team Leaders, who run teams of data scientists and spread best practices. We further describe a set of strategies that they employ to increase the impact and actionability of their work.|10.1145/2884781.2884783|https://doi.org/10.1145/2884781.2884783|New York, NY, USA|Association for Computing Machinery|9781450339001|2016|The Emerging Role of Data Scientists on Software Development Teams|Kim, Miryung and Zimmermann, Thomas and DeLine, Robert and Begel, Andrew|inproceedings|10.1145/2884781.2884783|||||||||||||||||||||||||||||1265420107|42
||Dictionaries;Software architecture;Databases;Soft sensors;Data visualization;Big Data;Natural language processing||2801-2812|2021 IEEE International Conference on Big Data (Big Data)|Retrospective data harmonization across multiple research cohorts and studies is frequently done to increase statistical power, provide comparison analysis, and create a richer data source for data mining. However, when combining disparate data sources, harmonization projects face data management and analysis challenges. These include differences in the data dictionaries and variable definitions, privacy concerns surrounding health data representing sensitive populations, and lack of properly defined data models. With the availability of mature open-source web-based database technologies, developing a complete software architecture to overcome the challenges associated with the harmonization process can alleviate many roadblocks. By leveraging state-of-the-art software engineering and database principles, we can ensure data quality and enable cross-center online access and collaboration.This paper outlines a complete software architecture developed and customized using the Django web framework, leveraged to harmonize sensitive data collected from three NIH-support birth cohorts. We describe our framework and show how we successfully overcame challenges faced when harmonizing data from these cohorts. We discuss our efforts in data cleaning, data sharing, data transformation, data visualization, and analytics, while reflecting on what we have learned to date from these harmonized datasets.|10.1109/BigData52589.2021.9671538|||||2021|A Secure and Reusable Software Architecture for Supporting Online Data Harmonization|Feric, Zlatan and Agostini, Nicolas Bohm and Beene, Daniel and Signes-Pastor, Antonio J. and Halchenko, Yuliya and Watkins, Deborah and MacKenzie, Debra and Karagas, Margaret and Manjourides, Justin and Alshawabkeh, Akram and Kaeli, David|inproceedings|9671538||Dec|||||||||||||||||||||||||||1268802455|42
||Big data analytics, Data protection, Data protection directive, General data protection regulation, Governance, Information security, Privacy, Privacy impact assessment, Systematic literature review||105640||Big Data Analytics enables today's businesses and organisations to process and utilise the raw data that is generated on a daily basis. While Big Data Analytics has improved efficiency and created many opportunities, it has also increased the risk of personal data being compromised or breached. The General Data Protection Regulation (GDPR) mandates Data Protection Impact Assessment (DPIA) as a means of identifying appropriate controls to mitigate risks associated with the protection of personal data. However, little is currently known about how to conduct such a DPIA in a Big Data Analytics context. To this end, we conducted a systematic literature review with the aim of identifying privacy and data protection risks specific to the Big Data Analytics context that could negatively impact individuals' rights and freedoms when they occur. Based on a sample of 159 articles, we applied a thematic analysis to all identified risks which resulted in the definition of nine Privacy Touch Points that summarise the identified risks. The coverage of these Privacy Touch Points was then analysed for ten Privacy Impact Assessment (PIA) methodologies. The insights gained from our analysis will inform the next phase of our research, in which we aim to develop a comprehensive DPIA methodology that will enable data processors and data controllers to identify, analyse and mitigate privacy and data protection risks when storing and processing data involving Big Data Analytics.|https://doi.org/10.1016/j.clsr.2021.105640|https://www.sciencedirect.com/science/article/pii/S0267364921001138||||2022|Towards a privacy impact assessment methodology to support the requirements of the general data protection regulation in a big data analytics context: A systematic literature review|Georgios Georgiadis and Geert Poels|article|GEORGIADIS2022105640|||Computer Law & Security Review|02673649||44|||||28888|0,815|Q1|38|66|242|1245|707|223|3,39|18,86|United Kingdom|Western Europe|1985-2020|Business, Management and Accounting (miscellaneous) (Q1); Computer Networks and Communications (Q1); Law (Q1)||||1269716468|1769124433
||Oscillation identification, Big data, Evidence theory, Support vector machine||663-671||With the development of big data technology, power system has entered the era of data analysis. With the help of the massive data provided by the wide area measurement system, the power system can be easily evaluated, and the abnormal operation status can be detected and positioned. As the increase of renewable energy permeability, more new abnormal operating status have appeared in the system. Aimed at the abnormal operation state in the development of new energy, this paper proposes an oscillation location scheme based on evidence theory and support vector machine, which makes up for the limitation of single oscillation location method. The result of location analysis of oscillation energy method, oscillation phase difference method and forced oscillation phase difference location method is fused by evidence theory.|https://doi.org/10.1016/j.egyr.2022.02.022|https://www.sciencedirect.com/science/article/pii/S2352484722002682||||2022|A novel oscillation identification method for grid-connected renewable energy based on big data technology|Jian Wang|article|WANG2022663|||Energy Reports|23524847||8||2021 International Conference on New Energy and Power Engineering|||21100389511|1,199|Q1|33|983|239|28463|1788|239|7,37|28,96|United Kingdom|Western Europe|2015-2020|Energy (miscellaneous) (Q1)|2,964|6.870|0.00318|1270253441|1436431897
||Big Data, Big Data Analytics (BDA), Cloud Computing, Cloud based Big Data Enterprise Solutions, Big Data Storage, Big Data Warehouse, Streaming Data, Amazon Web Services (AWS), Google Cloud Platform (GCP), IBM Cloud, Microsoft Azure||118-127||A cloud framework refers to the aggregation of components like development tools, middleware and database services, needed for cloud computing, which aids in developing, deploying and managing cloud based applications strenuously, consequently making it an efficacious paradigm for massive scaling of dynamically allocated resources and their complex computing. Big Data Analytics (BDA) delivers data management solutions in the cloud architecture for storing, analysing and processing a huge volume of data. This paper presents a survey for performance based comparative analysis of cloud-based big data frameworks from leading enterprises like Amazon, Google, IBM, and Microsoft, which will assist researcher, IT analysts, reader and business user in picking the framework best suited for their work ensuring success in terms of favourable outcomes.|https://doi.org/10.1016/j.procs.2018.05.172|https://www.sciencedirect.com/science/article/pii/S1877050918309062||||2018|Performance Analysis of Big Data and Cloud Computing Techniques: A Survey|Subia Saif and Samar Wazir|article|SAIF2018118|||Procedia Computer Science|18770509||132||International Conference on Computational Intelligence and Data Science|||19700182801|0,334|-|76|1907|6483|38511|13722|6359|2,09|20,19|Netherlands|Western Europe|2010-2020|Computer Science (miscellaneous)||||1270727170|2108686752
||Emission inventory, Guangdong Province, Ship emissions, Big data, VOCs speciation||144535||An accurate characterization of spatial-temporal emission patterns and speciation of volatile organic compounds (VOCs) for multiple chemical mechanisms is important to improving the air quality ensemble modeling. In this study, we developed a 2017-based high-resolution (3 km × 3 km) model-ready emission inventory for Guangdong Province (GD) by updating estimation methods, emission factors, activity data, and allocation profiles. In particular, a full-localized speciation profile dataset mapped to five chemical mechanisms was developed to promote the determination of VOC speciation, and two dynamic approaches based on big data were used to improve the estimation of ship emissions and open fire biomass burning (OFBB). Compared with previous emissions, more VOC emissions were classified as oxygenated volatile organic compound (OVOC) species, and their contributions to the total ozone formation potential (OFP) in the Pearl River Delta (PRD) region increased by 17%. Formaldehyde became the largest OFP species in GD, accounting for 11.6% of the total OFP, indicating that the model-ready emission inventory developed in this study is more reactive. The high spatial-temporal variability of ship sources and OFBB, which were previously underestimated, was also captured by using big data. Ship emissions during typhoon days and holidays decreased by 23–55%. 95% of OFBB emissions were concentrated in 9% of the GD area and 31% of the days in 2017, demonstrating their strong spatial-temporal variability. In addition, this study revealed that GD emissions have changed rapidly in recent years due to the leap-forward control measures implemented, and thus, they needed to be updated regularly. All of these updates led to a 5–17% decrease in the emission uncertainty for most pollutants. The results of this study provide a reference for how to reduce uncertainties in developing model-ready emission inventories.|https://doi.org/10.1016/j.scitotenv.2020.144535|https://www.sciencedirect.com/science/article/pii/S0048969720380669||||2021|An updated model-ready emission inventory for Guangdong Province by incorporating big data and mapping onto multiple chemical mechanisms|Zhijiong Huang and Zhuangmin Zhong and Qinge Sha and Yuanqian Xu and Zhiwei Zhang and Lili Wu and Yuzheng Wang and Lihang Zhang and Xiaozhen Cui and MingShuang Tang and Bowen Shi and Chuanzeng Zheng and Zhen Li and Mingming Hu and Linlin Bi and Junyu Zheng and Min Yan|article|HUANG2021144535|||Science of The Total Environment|00489697||769|||||25349|1,795|Q1|244|6929|13278|455970|106837|13125|7,96|65,81|Netherlands|Western Europe|1970, 1972-2021|Environmental Chemistry (Q1); Environmental Engineering (Q1); Pollution (Q1); Waste Management and Disposal (Q1)|210,143|7.963|0.23082|1271988332|2019676356
||Big Data, Cloud computing, Analytics, Data management||3-15||This paper discusses approaches and environments for carrying out analytics on Clouds for Big Data applications. It revolves around four important areas of analytics and Big Data, namely (i) data management and supporting architectures; (ii) model development and scoring; (iii) visualisation and user interaction; and (iv) business models. Through a detailed survey, we identify possible gaps in technology and provide recommendations for the research community on future directions on Cloud-supported Big Data computing and analytics solutions.|https://doi.org/10.1016/j.jpdc.2014.08.003|https://www.sciencedirect.com/science/article/pii/S0743731514001452||||2015|Big Data computing and clouds: Trends and future directions|Marcos D. Assunção and Rodrigo N. Calheiros and Silvia Bianchi and Marco A.S. Netto and Rajkumar Buyya|article|ASSUNCAO20153|||Journal of Parallel and Distributed Computing|07437315||79-80||Special Issue on Scalable Systems for Big Data Management and Analytics|||25621|0,638|Q1|87|169|592|7166|2473|568|4,72|42,40|United States|Northern America|1984-2021|Computer Networks and Communications (Q1); Hardware and Architecture (Q1); Artificial Intelligence (Q2); Software (Q2); Theoretical Computer Science (Q2)|4,371|3.734|0.00477|1274345593|1083163244
||Big Data, healthcare, Big Data Analytics, Hadoop||45-67|Applications of Big Data in Healthcare|In the past 10 years, the healthcare industry is growing at a remarkable rate. The healthcare industry is generating enormous amounts of data in terms of volume, velocity, and variety. Big Data methodologies in healthcare can not only increase the business value but will also add to the improvement of healthcare services. Several techniques can be implemented to develop early disease diagnose systems and improve treatment procedures using detailed analysis over time. In such a situation, Big data Analytics proposes to connect intricate databases to achieve more useful results. In this chapter, we will discuss the procedure of big data analytics in the healthcare sector with some practical applications along with its challenges. We will also have a look at the various big data techniques and their tools for implementation. We conclude this chapter with a discussion on potential opportunities for analytics in the healthcare sector.|https://doi.org/10.1016/B978-0-12-820203-6.00008-4|https://www.sciencedirect.com/science/article/pii/B9780128202036000084||Academic Press|978-0-12-820203-6|2021|2 - Big Data Analytics for healthcare: theory and applications|Shivam Bachhety and Shivani Kapania and Rachna Jain|incollection|BACHHETY202145||||||||||Ashish Khanna and Deepak Gupta and Nilanjan Dey|||||||||||||||||||1274622196|42
||M2M, Big Data, Divide-and-conquer, Data fusion domain||439-453||Machine-to-Machine (M2M) communication relies on the physical objects (e.g., satellites, sensors, and so forth) interconnected with each other, creating mesh of machines producing massive volume of data about large geographical area (e.g., living and non-living environment). Thus, the M2M is an ideal example of Big Data. On the contrary, the M2M platforms that handle Big Data might perform poorly or not according to the goals of their operator (in term of cost, database utilization, data quality, processing and computational efficiency, analysis and feature extraction applications). Therefore, to address the aforementioned needs, we propose a new effective, memory and processing efficient system architecture for Big Data in M2M, which, unlike other previous proposals, does not require whole set of data to be processed (including raw data sets), and to be kept in the main memory. Our designed system architecture exploits divide-and-conquer approach and data block-wise vertical representation of the database follows a particular petitionary strategy, which formalizes the problem of feature extraction applications. The architecture goes from physical objects to the processing servers, where Big Data set is first transformed into a several data blocks that can be quickly processed, then it classifies and reorganizes these data blocks from the same source. In addition, the data blocks are aggregated in a sequential manner based on a machine ID, and equally partitions the data using fusion algorithm. Finally, the results are stored in a server that helps the users in making decision. The feasibility and efficiency of the proposed system architecture are implemented on Hadoop single node setup on UBUNTU 14.04 LTS core™i5 machine with 3.2GHz processor and 4GB memory. The results show that the proposed system architecture efficiently extract various features (such as River) from the massive volume of satellite data.|https://doi.org/10.1016/j.neucom.2015.04.109|https://www.sciencedirect.com/science/article/pii/S0925231215012369||||2016|An efficient divide-and-conquer approach for big data analytics in machine-to-machine communication|Awais Ahmad and Anand Paul and M. Mazhar Rathore|article|AHMAD2016439|||Neurocomputing|09252312||174|||||24807|1,085|Q1|143|1653|3586|78823|25554|3535|7,08|47,68|Netherlands|Western Europe|1989-2020|Artificial Intelligence (Q1); Computer Science Applications (Q1); Cognitive Neuroscience (Q2)|46,751|5.719|0.06669|1274972034|411767096
ICMLC 2018|Macau, China|maturity model, Unstructured data, Data management, Measurement and evaluation|4|157–160|Proceedings of the 2018 10th International Conference on Machine Learning and Computing|Through the analysis and contrast of the different Data Management Maturity Model, such as DCAM, DMM, DCMM and the model of IBM, we try to make empirical research under the framework of data management maturity model. This article take a project whose main research object is about the academic career of scientists and with massive unstructured data for example, through analysis of the goal, management processes and influence factors of this project in detail, we built up an evaluation system for data management for such projects under the framework of DCMM. It is expected to have a positive significance to the evaluation of similar data management capability.|10.1145/3195106.3195177|https://doi.org/10.1145/3195106.3195177|New York, NY, USA|Association for Computing Machinery|9781450363532|2018|Research and Application of Data Management Based on Data Management Maturity Model (DMM)|Baolong, Yang and Hong, Wu and Haodong, Zhang|inproceedings|10.1145/3195106.3195177|||||||||||||||||||||||||||||1275371611|42
||genetic programming, visual analytics, data mining, interactive evolutionary computation, Interactive evolutionary algorithms, information visualization.|32|55–86||We evaluate and analyse a framework for evolutionary visual exploration EVE that guides users in exploring large search spaces. EVE uses an interactive evolutionary algorithm to steer the exploration of multidimensional data sets toward two-dimensional projections that are interesting to the analyst. Our method smoothly combines automatically calculated metrics and user input in order to propose pertinent views to the user. In this article, we revisit this framework and a prototype application that was developed as a demonstrator, and summarise our previous study with domain experts and its main findings. We then report on results from a new user study with a clearly predefined task, which examines how users leverage the system and how the system evolves to match their needs. While we previously showed that using EVE, domain experts were able to formulate interesting hypotheses and reach new insights when exploring freely, our new findings indicate that users, guided by the interactive evolutionary algorithm, are able to converge quickly to an interesting view of their data when a clear task is specified. We provide a detailed analysis of how users interact with an evolutionary algorithm and how the system responds to their exploration strategies and evaluation patterns. Our work aims at building a bridge between the domains of visual analytics and interactive evolution. The benefits are numerous, in particular for evaluating interactive evolutionary computation IEC techniques based on user study methodologies.|10.1162/EVCO_a_00161|https://doi.org/10.1162/EVCO_a_00161|Cambridge, MA, USA|MIT Press||2017|Evolutionary Visual Exploration: Evaluation of an Iec Framework for Guided Visual Search|Boukhelifa, N. and Bezerianos, A. and Cancino, W. and Lutton, E.|article|10.1162/EVCO_a_00161||mar|Evol. Comput.|10636560|1|25|Spring 2017||||||||||||||||||||||1275458386|93550055
||Big data, Computational cost, Knowledge hiding techniques, MapReduce, Privacy preservation, Scalability||538-550||Border based Knowledge hiding techniques (BB-KHT) are widely adopted form of privacy preservation techniques of data mining. These approaches are used to hide sensitive knowledge (confidential information) present in a dataset before sharing or analyzing it. BB-KHT primarily rely on border theory and maximum criterion method for preserving privacy and perpetuating good data quality of sanitized dataset but costs high computational complexity. Further, due to sequential nature, these approaches are particularly felicitous for small datasets and become infeasible while dealing with large scale datasets. Therefore, to subjugate the identified challenges of infeasibility and high computational complexity, a scalable two-phase improved MaxMin BB-KHT using MapReduce framework (MR-I MaxMin) is proposed. The proposed scheme requires only two database scans throughout the hiding process and hence, is computationally inexpensive. Moreover, the scheme also commits to preserve good data quality of sanitized dataset. The MapReduce version of proposed approach helps in achieving the feasibility by processing large voluminous data in a parallel fashion. Quantitative experiments and evaluations have been performed over a number of real and synthetically generated large-scale datasets. It is shown that the proposed MR-I MaxMin technique outperforms the similar existing approaches and vanquishes the identified challenges along with much-needed privacy preservation.|https://doi.org/10.1016/j.future.2018.05.063|https://www.sciencedirect.com/science/article/pii/S0167739X17322173||||2020|MR-I MaxMin-scalable two-phase border based knowledge hiding technique using MapReduce|Shivani Sharma and Durga Toshniwal|article|SHARMA2020538|||Future Generation Computer Systems|0167739X||109|||||12264|1,262|Q1|119|798|1935|36054|16877|1868|9,11|45,18|Netherlands|Western Europe|1984-2021|Computer Networks and Communications (Q1); Hardware and Architecture (Q1); Software (Q1)||||1278459291|562237118
ICCIP 2020|Tokyo, Japan|Data Quality Assessment, Accuracy, Information Management Systems, Human Centered Design|5|148–152|2020 the 6th International Conference on Communication and Information Processing|Developing countries are increasingly taking advantage of the rapid advancement in ICT to replace paper-based operations with Information Management Systems (IMS) such as District Health Information Software (DHIS). While the adoption of IMS presents significant benefits, challenges exist in the quality of IMS data. Inaccurate, incomplete or redundant data in IMS has misled organisations into making incorrect decisions leading to customer dissatisfaction and high cost implications. There is an urgent need for IMS stakeholders to have mechanisms of assessing the quality of data prior to data analysis, data sharing or decision making. Data Accuracy Assessment Tool (DAAT) assesses and identifies errors in a pair of context related datasets. DAAT provides ability for Data Managers to easily compare datasets by choosing attributes of their interest from a pool of diverse attributes that define the data. Through reports and visualization, the tool reveals the accuracy of data in real time using metrics such as validity, completeness and duplication of data. DAAT is scalable since it can be integrated with any IMS such as DHIS. The tool has been tested using four years Voluntary Medical Male Circumcision (VMMC) program data from JHPIEGO's AIDSFree project in Tanzania.|10.1145/3442555.3442579|https://doi.org/10.1145/3442555.3442579|New York, NY, USA|Association for Computing Machinery|9781450388092|2021|Improved Data Accuracy Assessment Tool for Information Management Systems|Maziku, Hellen|inproceedings|10.1145/3442555.3442579|||||||||||||||||||||||||||||1278497318|42
||Big data, Data analytics, Machine learning, Big data visualization, Decision-making, Smart agriculture, Smart city application, Value-creation, Value-discover, Value-realization||758-790||Big Data Analytics (BDA) is increasingly becoming a trending practice that generates an enormous amount of data and provides a new opportunity that is helpful in relevant decision-making. The developments in Big Data Analytics provide a new paradigm and solutions for big data sources, storage, and advanced analytics. The BDA provide a nuanced view of big data development, and insights on how it can truly create value for firm and customer. This article presents a comprehensive, well-informed examination, and realistic analysis of deploying big data analytics successfully in companies. It provides an overview of the architecture of BDA including six components, namely: (i) data generation, (ii) data acquisition, (iii) data storage, (iv) advanced data analytics, (v) data visualization, and (vi) decision-making for value-creation. In this paper, seven V's characteristics of BDA namely Volume, Velocity, Variety, Valence, Veracity, Variability, and Value are explored. The various big data analytics tools, techniques and technologies have been described. Furthermore, it presents a methodical analysis for the usage of Big Data Analytics in various applications such as agriculture, healthcare, cyber security, and smart city. This paper also highlights the previous research, challenges, current status, and future directions of big data analytics for various application platforms. This overview highlights three issues, namely (i) concepts, characteristics and processing paradigms of Big Data Analytics; (ii) the state-of-the-art framework for decision-making in BDA for companies to insight value-creation; and (iii) the current challenges of Big Data Analytics as well as possible future directions.|https://doi.org/10.1016/j.ipm.2018.01.010|https://www.sciencedirect.com/science/article/pii/S0306457316307178||||2018|A survey towards an integration of big data analytics to big insights for value-creation|Mandeep Kaur Saggi and Sushma Jain|article|SAGGI2018758|||Information Processing & Management|03064573|5|54||In (Big) Data we trust: Value creation in knowledge organizations|||||||||||||||||||||1279129037|1769516999
||Big data, Data analytics and Data predective, Data modeling, Data warehousing||59-86|Knowledge is Power in Four Dimensions: Models to Forecast Future Paradigm|Data warehouses are constantly evolving to support new technologies and business requirements—and remain relevant when it comes to big data and analytics. Regardless of how new or sophisticated your data warehouse is, it likely needs modernization. Data warehousing, along with data modeling, and side by side with data analytic capability gives us the upper hand with our knowledge by collecting the right information at the right time with the right data coming from all directions, whether or not these data are structured or unstructured. We should be able to have proper tools in hand to be able to take this information and knowledge to be in a position of resilience based on predictive analysis driven by data. This chapter will discuss data warehousing, data modeling, and consequently, data analytics where, in combination, they all are variables functioning within the process of predictive analytic modeling. This process allows us to have the knowledge we are looking for. Getting reliable information from data warehouses is resource-intensive; missing one step can result in wasted processing time and/or bad data.|https://doi.org/10.1016/B978-0-323-95112-8.00001-5|https://www.sciencedirect.com/science/article/pii/B9780323951128000015||Academic Press|978-0-323-95112-8|2022|Chapter 3 - Data warehousing, data mining, data modeling, and data analytics|Bahman Zohuri and Farhang Mossavar-Rahmani and Farahnaz Behgounia|incollection|ZOHURI202259||||||||||Bahman Zohuri and Farhang Mossavar-Rahmani and Farahnaz Behgounia|||||||||||||||||||1279630774|42
||Sensors;Containers;Roads;Data integrity;Vehicle dynamics;Data mining;Memory;V2X wireless communication;vehicular edge computing;data storage;data quality;dimension reduction;data sampling||7-12|2017 6th IEEE International Conference on Advanced Logistics and Transport (ICALT)|Future vehicles will be equipped with advanced communication capabilities and a multitude of sensing devices. Vehicle-to-vehicle and to Infrastructure (V2X) is one of these future technologies. V2X-technology-enabled vehicles are expected to become a great source of big data. This data, if gathered in the right time and processed in the right way, can enable an interesting number of existing and new applications. This can be a challenging task, taken into account the considerable size of the data that will be gathered. One of the challenges is to find a good balance between the number of data to filter out and the quality of the end data. This contribution tackles this specific challenge, by studying data storage cost reduction and evaluating its impact on the data quality. The proposed solution compares three approaches of treating the collected data at the road-side unit after taking out unnecessary information details. This solution has been tested and validated through simulations that show promising results.|10.1109/ICAdLT.2017.8547038|||||2017|Optimizing V2X Data Collection and Storage for a Better Cost and Quality Trade-off|Brahim, Mohamed Ben and Menouar, Hamid|inproceedings|8547038||July|||||||||||||||||||||||||||1279944428|42
SAC '15|Salamanca, Spain|data quality, machine learning, metaheuristic, provisioning, cloud computing|8|1696–1703|Proceedings of the 30th Annual ACM Symposium on Applied Computing|Cloud Computing as a service has become a topic of increasing interest. The outsourcing of duties and infrastructure to external parties became a crucial concept for many business models. In this paper we discuss the design and experimental evaluation of provisioning algorithms, in a Data Quality-aware Service (DQaS) context, that enables dynamic Data Quality Service Level Agreements (DQSLA) management and optimization of cloud resources. The DQaS has been designed to respond effectively to the DQSLA requirements of the service customers, by minimizing SLA penalties and provisioning the cloud infrastructure for the execution of data quality algorithms. An experimental evaluation of the proposed provisioning algorithms, carried out through simulation, has provided very encouraging results that confirm the adequacy of these algorithms in the DQaS context.|10.1145/2695664.2695753|https://doi.org/10.1145/2695664.2695753|New York, NY, USA|Association for Computing Machinery|9781450331968|2015|A Data Quality-Aware Cloud Service Based on Metaheuristic and Machine Learning Provisioning Algorithms|Nascimento, Dimas C. and Pires, Carlos Eduardo and Mestre, Demetrio Gomes|inproceedings|10.1145/2695664.2695753|||||||||||||||||||||||||||||1280028522|42
https://doi.org/10.1016/j.wneu.2022.02.113|https://www.sciencedirect.com/science/article/pii/S1878875022002601||||2022|The National Inpatient Sample: A Primer for Neurosurgical Big Data Research and Systematic Review|Oliver Y. Tang and Alisa Pugacheva and Ankush I. Bajaj and Krissia M. {Rivera Perla} and Robert J. Weil and Steven A. Toms|article|TANG2022e198|||World Neurosurgery|18788750||162||||||||||||||||||||||||||||||1281117349|42
||Biomarkers, EEG, Epilepsy, Epileptogenesis, Informatics, MRI, Neuroimaging, TBI||127-136||We describe the infrastructure and functionality for a centralized preclinical and clinical data repository and analytic platform to support importing heterogeneous multi-modal data, automatically and manually linking data across modalities and sites, and searching content. We have developed and applied innovative image and electrophysiology processing methods to identify candidate biomarkers from MRI, EEG, and multi-modal data. Based on heterogeneous biomarkers, we present novel analytic tools designed to study epileptogenesis in animal model and human with the goal of tracking the probability of developing epilepsy over time.|https://doi.org/10.1016/j.nbd.2018.05.026|https://www.sciencedirect.com/science/article/pii/S0969996118301700||||2019|Big data sharing and analysis to advance research in post-traumatic epilepsy|Dominique Duncan and Paul Vespa and Asla Pitkänen and Adebayo Braimah and Niina Lapinlampi and Arthur W. Toga|article|DUNCAN2019127|||Neurobiology of Disease|09699961||123||Antiepileptogenesis following Traumatic Brain Injury|||||||||||||||||||||1283044215|1276434044
||||97-104||Systemic phenotyping of mutant mice has been established at large scale in the last decade as a new tool to uncover the relations between genotype, phenotype and environment. Recent advances in that field led to the generation of a valuable open access data resource that can be used to better understanding the underlying causes for human diseases. From an ethical perspective, systemic phenotyping significantly contributes to the reduction of experimental animals and the refinement of animal experiments by enforcing standardisation efforts. There are particular logistical, experimental and analytical challenges of systemic large-scale mouse phenotyping. On all levels, IT solutions are critical to implement and efficiently support breeding, phenotyping and data analysis processes that lead to the generation of high-quality systemic phenotyping data accessible for the scientific community.|https://doi.org/10.1016/j.coisb.2017.07.012|https://www.sciencedirect.com/science/article/pii/S2452310017300525||||2017|Big data in large-scale systemic mouse phenotyping|Holger Maier and Stefanie Leuchtenberger and Helmut Fuchs and Valerie Gailus-Durner and Martin {Hrabe de Angelis}|article|MAIER201797|||Current Opinion in Systems Biology|24523100||4||Big data acquisition and analysis • Pharmacology and drug discovery|||21100857212|1,576|Q1|21|43|269|2736|800|239|2,90|63,63|United Kingdom|Western Europe|2017-2020|Applied Mathematics (Q1); Biochemistry, Genetics and Molecular Biology (miscellaneous) (Q1); Computer Science Applications (Q1); Drug Discovery (Q1); Modeling and Simulation (Q1)||||1283700245|1614330484
||Sociology;Statistics;Servers;Testing;Finance;Google;Measurement;online experimentation;quality assurance;bucket size gap;dark matter;loss of traffic;data quality||1620-1626|2017 IEEE International Conference on Big Data (Big Data)|The rise of online controlled experimentation, a.k.a. A/B testing began around the turn of the millennium with the emergence of internet giants like Amazon, Bing, Facebook, Google, LinkedIn, and Yahoo. A step towards good experimental design includes the planning for sample size, confidence level, metrics to be measured and test duration. Generally, these factors impact the quality and validity of an experiment. In practice, additional factors may also impact the validity of an experiment. One such critical factor is the discrepancy between the planned bucket size and the actual bucket size. We call this hidden gap “Experimentation Dark Matter”. Experimentation dark matter is invisible to A/A or A/B validation of experimental analysis but can impact the validity of an experiment. In this paper, we have demonstrated in detail, this gap that may cause the loss of statistical power as well as the loss of representativeness and generalizability of an experiment. We have proposed a framework to monitor experimentation dark matter that may go unnoticed in a balanced AB test. We have further discussed the remediation of a recent dark matter issue using our framework. This scalable, low-latency framework is effective and applicable to similar online controlled experimentation systems.|10.1109/BigData.2017.8258096|||||2017|Demystifying dark matter for online experimentation|Appiktala, Nirupama and Chen, Miao and Natkovich, Michael and Walters, Joshua|inproceedings|8258096||Dec|||||||||||||||||||||||||||1284854875|42
||Data governance, Information governance, Conceptual framework, Literature review, Research agenda||424-438||Data governance refers to the exercise of authority and control over the management of data. The purpose of data governance is to increase the value of data and minimize data-related cost and risk. Despite data governance gaining in importance in recent years, a holistic view on data governance, which could guide both practitioners and researchers, is missing. In this review paper, we aim to close this gap and develop a conceptual framework for data governance, synthesize the literature, and provide a research agenda. We base our work on a structured literature review including 145 research papers and practitioner publications published during 2001-2019. We identify the major building blocks of data governance and decompose them along six dimensions. The paper supports future research on data governance by identifying five research areas and displaying a total of 15 research questions. Furthermore, the conceptual framework provides an overview of antecedents, scoping parameters, and governance mechanisms to assist practitioners in approaching data governance in a structured manner.|https://doi.org/10.1016/j.ijinfomgt.2019.07.008|https://www.sciencedirect.com/science/article/pii/S0268401219300787||||2019|Data governance: A conceptual framework, structured review, and research agenda|Rene Abraham and Johannes Schneider and Jan {vom Brocke}|article|ABRAHAM2019424|||International Journal of Information Management|02684012||49|||||15631|2,770|Q1|114|224|382|20318|5853|373|16,16|90,71|United Kingdom|Western Europe|1970, 1986-2021|Artificial Intelligence (Q1); Computer Networks and Communications (Q1); Information Systems (Q1); Information Systems and Management (Q1); Library and Information Sciences (Q1); Management Information Systems (Q1); Marketing (Q1)|12,245|14.098|0.01167|1285126090|747927863
||mobile multimedia, data mining, indexing, multimedia analysis, multimedia databases, survey, retrieval, 5V challenges, Big data analytics, machine learning|34|||With the proliferation of online services and mobile technologies, the world has stepped into a multimedia big data era. A vast amount of research work has been done in the multimedia area, targeting different aspects of big data analytics, such as the capture, storage, indexing, mining, and retrieval of multimedia big data. However, very few research work provides a complete survey of the whole pine-line of the multimedia big data analytics, including the management and analysis of the large amount of data, the challenges and opportunities, and the promising research directions. To serve this purpose, we present this survey, which conducts a comprehensive overview of the state-of-the-art research work on multimedia big data analytics. It also aims to bridge the gap between multimedia challenges and big data solutions by providing the current big data frameworks, their applications in multimedia analyses, the strengths and limitations of the existing methods, and the potential future directions in multimedia big data analytics. To the best of our knowledge, this is the first survey that targets the most recent multimedia management techniques for very large-scale data and also provides the research studies and technologies advancing the multimedia analyses in this big data era.|10.1145/3150226|https://doi.org/10.1145/3150226|New York, NY, USA|Association for Computing Machinery||2018|Multimedia Big Data Analytics: A Survey|Pouyanfar, Samira and Yang, Yimin and Chen, Shu-Ching and Shyu, Mei-Ling and Iyengar, S. S.|article|10.1145/3150226|10|jan|ACM Comput. Surv.|03600300|1|51|January 2019||||23038|2,079|Q1|163|112|365|15859|6978|365|19,16|141,60|United States|Northern America|1969-2020|Computer Science (miscellaneous) (Q1); Theoretical Computer Science (Q1)|10,790|10.282|0.01438|1285442823|1517405264
CIKM '22|Atlanta, GA, USA||2|5161–5162|Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management|Although data quality is a long-standing and enduring problem, it has recently received a resurgence of attention due to the fast proliferation of data analytics, machine learning, and decision-support applications built upon the wide-scale availability and accessibility of (big) data. The success of such applications heavily relies on not only the quantity, but also the quality of data. Data curation, which may include annotation, cleaning, transformation, integration, etc., is a critical step to provide adequate assurances on the quality of analytics and machine learning results. Such data preparation activities are recognised as time and resource intensive for data scientists as data often comes with a number of challenges that need to be tackled before it can be used in practice. Data re-purposing and the resulting distance between design and use intentions of the data, is a fundamental issue behind many of these challenges. These challenges include a variety of data issues such as noise and outliers, incompleteness, representativeness or biases, heterogeneity of format or semantics, etc. Mishandling these challenges can lead to negative and sometimes damaging effects, especially in critical domains like healthcare, transport, and finance. An observable distinct feature of data quality in these contexts is the increasingly important role played by humans, being often the source of data generation and the active players in data curation. This workshop will provide an opportunity to explore the interdisciplinary overlap between manual, automated, and hybrid human-machine methods of data curation.|10.1145/3511808.3557498|https://doi.org/10.1145/3511808.3557498|New York, NY, USA|Association for Computing Machinery|9781450392365|2022|Workshop on Human-in-the-Loop Data Curation|Demartini, Gianluca and Yang, Jie and Sadiq, Shazia|inproceedings|10.1145/3511808.3557498|||||||||||||||||||||||||||||1286462091|42
ICEGOV '17|New Delhi AA, India|e-Commerce ontology, open data-driven organization, Open government data, formal conceptualization, e-Business ontology, open data business model, and business model ontology|9|195–203|Proceedings of the 10th International Conference on Theory and Practice of Electronic Governance|Despite the existence of number of well-known conceptualization in e-Business and e-Commerce, there have been no efforts so far to develop a detailed, comprehensive conceptualization for business model. Current business literature is replete with fragmented conceptualizations, which only partially describe aspects of a business model. In addition, the existing conceptualizations do not explicitly support the emerging phenomenon of open government data -- an increasingly valuable economic and strategic resource. Consequently, no comprehensive, formal, executable open government data business model ontology exists, that could be directly leveraged to facilitate the design, development of an operational open data business model. This paper bridges this gap by providing a parsimonious yet sufficiently detailed, conceptualization and formal ontology of open government data business model for open data-driven organizations. Following the design science approach, we developed the ontology as a 'design artefact' and validate the ontology by using it to describe an open data business model of an open data-driven organization.|10.1145/3047273.3047327|https://doi.org/10.1145/3047273.3047327|New York, NY, USA|Association for Computing Machinery|9781450348256|2017|An Ontology for Open Government Data Business Model|Zeleti, Fatemeh Ahmadi and Ojo, Adegboyega|inproceedings|10.1145/3047273.3047327|||||||||||||||||||||||||||||1287462001|42
ICISE 2021|Shanghai, China|Real-time, Sea Battlefield, Big Data, Data Warehouse|8|13–20|2021 the 6th International Conference on Information Systems Engineering|Aiming at the data governance problems in the sea battlefield, this paper proposes a real-time data warehouse construction method for naval battlefields, which realizes the functions of storage, analysis and mining of battle data. This paper completes the construction of the data warehouse from the aspects of real-time data life cycle, real-time data application scenarios, data warehouse real-time safeguard measures, and data warehouse theme design. It can effectively provide data support for naval combat forces and provide auxiliary decision-making for commanders.|10.1145/3503928.3503930|https://doi.org/10.1145/3503928.3503930|New York, NY, USA|Association for Computing Machinery|9781450385220|2022|Research on Real-Time Data Warehouse Technology for Sea Battlefield|Zeng, Xian and Han, Minglei and Li, Ning and Liu, Peng|inproceedings|10.1145/3503928.3503930|||||||||||||||||||||||||||||1288428320|42
||Retail pharmacy, Data mining, Time series, Forecasting, Big data, Demand uncertainty||343-354||Data management tools and analytics have provided managers with the opportunity to contemplate inventory performance as an ongoing activity by no longer examining only data agglomerated from ERP systems, but also, considering internet information derived from customers’ online buying behaviour. The realisation of this complex relationship has increased interest in business intelligence through data and text mining of structured, semi-structured and unstructured data, commonly referred to as “big data” to uncover underlying patterns which might explain customer behaviour and improve the response to demand volatility. This paper explores how sales structured data can be used in conjunction with non-structured customer data to improve inventory management either in terms of forecasting or treating some inventory as “top-selling” based on specific customer tendency to acquire more information through the internet. A medical condition is considered - namely pain - by examining 129 weeks of sales data regarding analgesics and information seeking data by customers through Google, online newspapers and YouTube. In order to facilitate our study we consider a VARX model with non-structured data as exogenous to obtain the best estimation and we perform tests against several univariate models in terms of best fit performance and forecasting.|https://doi.org/10.1016/j.cor.2017.08.009|https://www.sciencedirect.com/science/article/pii/S0305054817302162||||2018|Coping with demand volatility in retail pharmacies with the aid of big data exploration|Christos I. Papanagnou and Omeiza Matthews-Amune|article|PAPANAGNOU2018343|||Computers & Operations Research|03050548||98|||||24355|1,506|Q1|152|210|755|9383|3837|746|4,85|44,68|United Kingdom|Western Europe|1974-2021|Computer Science (miscellaneous) (Q1); Management Science and Operations Research (Q1); Modeling and Simulation (Q1)||||1290708832|1185516425
KDD '20|Virtual Event, CA, USA|auctions, bundling, data pricing, privacy, data products, digital products, revenue maximization, subscription, fairness, trustfulness, arbitrage, information goods|2|3553–3554|Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining|Data are invaluable. How can we assess the value of data objectively and quantitatively? Pricing data, or information goods in general, has been studied and practiced in dispersed areas and principles, such as economics, data management, data mining, electronic commerce, and marketing. In this tutorial, we present a unified and comprehensive overview of this important direction. We examine various motivations behind data pricing, understand the economics of data pricing, review the development and evolution of pricing models, and compare the proposals of marketplaces of data. We cover both digital products, such as ebooks and MP3 music, and data products, such as data sets, data queries and machine learning models. We also connect data pricing with the highly related areas, such as cloud service pricing, privacy pricing, and decentralized privacy preserving infrastructure like blockchains.|10.1145/3394486.3406473|https://doi.org/10.1145/3394486.3406473|New York, NY, USA|Association for Computing Machinery|9781450379984|2020|Data Pricing -- From Economics to Data Science|Pei, Jian|inproceedings|10.1145/3394486.3406473|||||||||||||||||||||||||||||1292685592|42
ICSSP 2014|Nanjing, China|cost drivers, Parametric cost estimation, incremental development, scale factors, IDPD|5|20–24|Proceedings of the 2014 International Conference on Software and System Process|The phenomenon called Incremental Development Productivity Decline (IDPD) is presumed to be present in all incremental soft-ware projects to some extent. COCOMO II is a popular parametric cost estimation model that has not yet been adapted to account for the challenges that IDPD poses to cost estimation. Instead, its cost driver and scale factors stay constant throughout the increments of a project. While a simple response could be to make these parameters variable per increment, questions are raised as to whether the existing parameters are enough to predict the behavior of an incrementally developed project even in that case. Individual COCOMO II parameters are evaluated with regard to their development over the course of increments and how they influence IDPD. The reverse is also done. In light of data collected in recent experimental projects, additional new variable parameters that either extend COCOMO II or could stand on their own are proposed.|10.1145/2600821.2600847|https://doi.org/10.1145/2600821.2600847|New York, NY, USA|Association for Computing Machinery|9781450327541|2014|COCOMO II Parameters and IDPD: Bilateral Relevances|Moazeni, Ramin and Link, Daniel and Boehm, Barry|inproceedings|10.1145/2600821.2600847|||||||||||||||||||||||||||||1295395500|42
||Big data, Mobility, Linear and logistic regression, Support vector machine (SVM), Traffic management||4980-4984||Mobility is the main key for smart living, where navigation and automatic suggestions are also a strategy for a successful life in smart cities. Big Data analytics are behind urban changes in the mobility of smart cities to bring sustainable life. By the year 2025 all over Indian states can reach the expected lifestyle by providing high security and mobility which can grow the opportunities also high. As the population is rapidly increasing, the needs of people are also increasing such that necessitating real-time apps for daily needs, communication devices, and so on. We focus our idea on the benefits of traffic and safety measures which are becoming a huge challenge nowadays. Many are preferred with sophistication when traveling for short distances. In such a way the big data analytics tools R studio and weka are used on the dataset smart city from the Kaggle website for traffic patterns during the high traffic duration. Using the dataset, the data are classified using a Support vector machine (SVM) and applied with regression such as linear, logistic regression to find the accuracy of traffic peak situations. The proposed work aims to compare the efficiency of big data technologies which can be applied using various classification and regression that can be shown on various tools such as R, Weka, map-reduce which can produce accurate results to visualize the smart cities and their traffic analysis.|https://doi.org/10.1016/j.matpr.2022.03.722|https://www.sciencedirect.com/science/article/pii/S2214785322022131||||2022|Traffic management in smart cities using support vector machine for predicting the accuracy during peak traffic conditions|T. Devi and K. Alice and N. Deepa|article|DEVI20224980|||Materials Today: Proceedings|22147853||62||International Conference on Innovative Technology for Sustainable Development|||21100370037|0,341|-|47|3996|10585|88521|14096|10409|1,24|22,15|United Kingdom|Western Europe|2005, 2014-2020|Materials Science (miscellaneous)||||1298431014|400517803
||Polycentricity, Urban centers, Multi-scale, Street blocks, Geospatial big data, Chinese cities||103298||With current decentralization trends and polycentric planning efforts, the urban spatial structures of Chinese cities have been changing tremendously. To detect the true urban polycentric pattern of Chinese cities, this article analyzed the urban polycentricity characteristics of 294 cities. The natural cities were delineated by points of interest (POIs), and road networks constituted street blocks. Based on check-in data and new spatial units, centers within both metropolitan areas and central cities were identified and examined. We discovered that all Chinese cities have at least one natural city in their metropolitan areas because of rapid urban sprawl. Although a monocentric structure is still the most common urban spatial structure, 110 Chinese cities displayed different degrees of polycentricity at the metropolitan level. Many natural cities beyond central cities contribute to polycentric development at the metropolitan level. Central cities have maintained their original vitality and importance, most Chinese cities have dispersed urban structures in central cities, and 45 central cities are polycentric. The spatial structures in metropolitan areas are more polycentric than those in central cities. The only 36 cities with polycentric urban structures at both the metropolitan and central city levels are all national or regional central cities in eastern China.|https://doi.org/10.1016/j.cities.2021.103298|https://www.sciencedirect.com/science/article/pii/S0264275121001980||||2021|Detecting the true urban polycentric pattern of Chinese cities in morphological dimensions: A multiscale analysis based on geospatial big data|Yongqiang Lv and Lin Zhou and Guobiao Yao and Xinqi Zheng|article|LV2021103298|||Cities|02642751||116|||||16956|1,771|Q1|90|419|732|28409|4866|723|6,19|67,80|United Kingdom|Western Europe|1983-2020|Development (Q1); Sociology and Political Science (Q1); Tourism, Leisure and Hospitality Management (Q1); Urban Studies (Q1)|11,076|5.835|0.01251|1298793228|2090576639
||Sustainable enterprises, Value creation, Big data analytics, Data reduction, Business model||917-928||Value creation is a major sustainability factor for enterprises, in addition to profit maximization and revenue generation. Modern enterprises collect big data from various inbound and outbound data sources. The inbound data sources handle data generated from the results of business operations, such as manufacturing, supply chain management, marketing, and human resource management, among others. Outbound data sources handle customer-generated data which are acquired directly or indirectly from customers, market analysis, surveys, product reviews, and transactional histories. However, cloud service utilization costs increase because of big data analytics and value creation activities for enterprises and customers. This article presents a novel concept of big data reduction at the customer end in which early data reduction operations are performed to achieve multiple objectives, such as (a) lowering the service utilization cost, (b) enhancing the trust between customers and enterprises, (c) preserving privacy of customers, (d) enabling secure data sharing, and (e) delegating data sharing control to customers. We also propose a framework for early data reduction at customer end and present a business model for end-to-end data reduction in enterprise applications. The article further presents a business model canvas and maps the future application areas with its nine components. Finally, the article discusses the technology adoption challenges for value creation through big data reduction in enterprise applications.|https://doi.org/10.1016/j.ijinfomgt.2016.05.013|https://www.sciencedirect.com/science/article/pii/S0268401216303097||||2016|Big data reduction framework for value creation in sustainable enterprises|Muhammad Habib ur Rehman and Victor Chang and Aisha Batool and Teh Ying Wah|article|REHMAN2016917|||International Journal of Information Management|02684012|6, Part A|36|||||15631|2,770|Q1|114|224|382|20318|5853|373|16,16|90,71|United Kingdom|Western Europe|1970, 1986-2021|Artificial Intelligence (Q1); Computer Networks and Communications (Q1); Information Systems (Q1); Information Systems and Management (Q1); Library and Information Sciences (Q1); Management Information Systems (Q1); Marketing (Q1)|12,245|14.098|0.01167|1300213642|747927863
BDC '14||Scientific workflow, Bayesian network, Hadoop, Kepler, Ensemble learning, Big Data, Distributed computing|10|16–25|Proceedings of the 2014 IEEE/ACM International Symposium on Big Data Computing|In the Big Data era, machine learning has more potential to discover valuable insights from the data. As an important machine learning technique, Bayesian Network (BN) has been widely used to model probabilistic relationships among variables. To deal with the challenges of Big Data PN learning, we apply the techniques in distributed data-parallelism (DDP) and scientific workflow to the BN learning process. We first propose an intelligent Big Data pre-processing approach and a data quality score to measure and ensure the data quality and data faithfulness. Then, a new weight based ensemble algorithm is proposed to learn a BN structure from an ensemble of local results. To easily integrate the algorithm with DDP engines, such as Hadoop, we employ Kepler scientific workflow to build the whole learning process. We demonstrate how Kepler can facilitate building and running our Big Data BN learning application. Our experiments show good scalability and learning accuracy when running the application in real distributed environments.|10.1109/BDC.2014.10|https://doi.org/10.1109/BDC.2014.10|USA|IEEE Computer Society|9781479918973|2014|A Scalable Data Science Workflow Approach for Big Data Bayesian Network Learning|Wang, Jianwu and Tang, Yan and Nguyen, Mai and Altintas, Ilkay|inproceedings|10.1109/BDC.2014.10|||||||||||||||||||||||||||||1301253449|42
||Data privacy;Itemsets;Heuristic algorithms;Scalability;Big Data;Parallel processing;Sparks;Privacy Preserving Data Mining;Data Sanitisation;Reduced Sensitive itemset;Data Quality||4026-4035|2020 IEEE International Conference on Big Data (Big Data)|Frequent itemset mining is used to extract interesting associations and correlations between the itemsets present in transactional datasets. The frequently appearing patterns are used for various business decision making policies, for instance to increase co-purchase of products, price optimization, cross promotion etc. However, there are some sensitive patterns present in datasets that can reveal individual or organisation's specific confidential information that they would not prefer to be known since it can cause them huge social and monetary loss. Privacy Preserving Data Mining (PPDM) approaches are used to hide these sensitive patterns with maintaining the utility of the data. Heuristics-based PPDM approaches are widely adopted sensitive pattern hiding approaches due to their simplicity and lesser computational time as compared to the border-based and exact approaches. However, these approaches causes high side effects concerning the quality of datasets. In this paper, two heuristics-based algorithms, Removal of Closed Sensitive Itemsets with Maximum Support (MaxRCSI) and Removal of Closed Sensitive Itemsets with Minimum Support (MinRCSI), are proposed. In these algorithms, data sanitization is performed over closed sensitive itemsets to improve the utility of sanitized data. The proposed algorithms are parallelized on Spark parallel computing framework to deal with the massive amount of data i.e. big data. Experiments performed on real and synthetic datasets show that the proposed algorithms preserve the privacy of datasets with substantially better utility as compared to the traditional algorithms with less execution time.|10.1109/BigData50022.2020.9378401|||||2020|Closed Itemset based Sensitive Pattern Hiding for Improved Data Utility and Scalability|Makkar, Himanshu and Toshniwal, Durga and Jangra, Shalini|inproceedings|9378401||Dec|||||||||||||||||||||||||||1302130172|42
ISAIMS 2021|Beijing, China|Medical treatment, User portrait, Review|5|500–504|Proceedings of the 2nd International Symposium on Artificial Intelligence for Medicine Sciences|In recent years, due to the rise of big data mining and intelligent recommendation, user portrait technology has gradually become a hot topic. As a new data analysis method, user portrait technology aims to mine user characteristics from a large number of user behavior data, and complete the user information panorama of the monomer or group through information mining, so as to prepare for the realization of precision service and personalized recommendation in various industries. It has been widely used in personalized recommendation and precision recommendation, personalized service and intelligent service, group characteristics analysis, prediction analysis and auxiliary decision-making. User portrait technology in the domestic research started late, currently in e-commerce, commercial precision marketing, book recommendation management, network personalized search, video entertainment and other fields of application is relatively mature. However, the application of user portrait technology in the field of medical and health is still in the preliminary exploration stage, and the combination of medical big data and user portrait technology can vividly depict the portraits of patients, doctors, residents and other different groups in promoting health and disease prevention. According to the different characteristics extracted, it can provide reference for meeting the needs of patients and achieving precision medicine. Therefore, this study reviews the concept, elements, implementation process and application of user portrait technology in the medical field, and provides reference for the subsequent application research of user portrait technology in the medical field|10.1145/3500931.3501016|https://doi.org/10.1145/3500931.3501016|New York, NY, USA|Association for Computing Machinery|9781450395588|2021|Research Progress of User Portrait Technology in Medical Field|Gao, Mengke and Zhang, Yan and Gao, Yue|inproceedings|10.1145/3500931.3501016|||||||||||||||||||||||||||||1306923752|42
CIKM '15|Melbourne, Australia|trajectory mining, spatiotemporal database, trajectory data management|2|671–672|Proceedings of the 24th ACM International on Conference on Information and Knowledge Management|Spatial trajectory data is widely available today. Over a sustained period of time, trajectory data has been collected from numerous GPS devices, smartphones, sensors and social media applications. Daily increases of real-time trajectory data have also been phenomenal in recent years. More and more new applications have emerged to derive business values from both trajectory data warehouses and real-time trajectory data. Due to their very large volumes, their nature of streaming, their highly variable levels of data quality, as well as many possible links with other types of data, making sense of spatial trajectory data becomes one of the crucial areas for big data analytics. In this paper we will present a review of the extensive work in spatiotemporal data management and trajectory mining, and discuss new challenges and new opportunities in the context of new applications, focusing on recent advances in trajectory data management and trajectory mining from their foundations to high performance processing with modern computing infrastructure.|10.1145/2806416.2806418|https://doi.org/10.1145/2806416.2806418|New York, NY, USA|Association for Computing Machinery|9781450337946|2015|Making Sense of Spatial Trajectories|Zhou, Xiaofang and Zheng, Kai and Jueng, Hoyoung and Xu, Jiajie and Sadiq, Shazia|inproceedings|10.1145/2806416.2806418|||||||||||||||||||||||||||||1309064309|42
EGOSE '16|St. Petersburg, Russia|smart city, open innovations, open data, government 2.0, civic issue tracker, e-government, crowdsourcing|7|171–177|Proceedings of the International Conference on Electronic Governance and Open Society: Challenges in Eurasia|In this research paper we describe the transformation of open data strategy and implementation of crowdsourcing technologies for the city E-government services. Analysis of smart city projects provides the role of open data and crowdsourcing for smart city vision in United States and Russia. We define challenges and perspectives for collaboration of open data and crowdsourcing in smart city projects.|10.1145/3014087.3014112|https://doi.org/10.1145/3014087.3014112|New York, NY, USA|Association for Computing Machinery|9781450348591|2016|Open Data and Crowdsourcing Perspectives for Smart City in the United States and Russia|Nikiforov, Alexander and Singireja, Anastasija|inproceedings|10.1145/3014087.3014112|||||||||||||||||||||||||||||1309894152|42
||Organizations;Standards organizations;Information architecture;Fuels;Information management;Big data||1-3|2014 IT Professional Conference|Organizations have understood the value of their structured data — mostly financial transactions — since the first mainframes were developed in the 40's. Data quality issues have always been a challenge and the increasing numbers of applications consuming and producing structured transactional data has grown exponentially. Unstructured information has been given less significance and strategic importance and therefore fewer resources and less attention on the part of leadership. All of that has changed and is changing faster than anyone imagined. Unstructured content is what humans produce. They create the documents: strategies, proposals, support documents, marketing content, white papers, engineering specifications, etc. that form the intelligence and core knowledge capital of the enterprise. Many organizations have left business units to fend for themselves and “go figure it out” with little guidance or support. These has led to terabytes of content that people cannot find their way through and that leaves the organization open to risks, liabilities and costs of e discovery. According to the Minnesota Journal of Law, Science & Technology, a gig of data costs $30,000 in e discovery costs. The cost of storage is ten cents. The problem is that the enterprise does not understand the hidden costs of not making data accessible and usable — lost time, lost IP, inefficiency, poor customer service that can lead to lost customers, slower growth, etc. As newer collaboration technologies are deployed, they expose the bad habits and sins of the past. Deploying a new search engine shows that the content is not curated. Standing up a new content management application like SharePoint reveals the haphazard shanty town of an information architecture with inconsistencies in models, terminology and applications. Today's landscape of marketing and customer experience technologies is complex and interconnected and requires those upstream knowledge processes that produce unstructured content as the fuel. Customer experience entails everything that happens before you purchase (marketing, education and outreach), when you purchase (e commerce with product content and data), and after you purchase (self-service systems and knowledge bases that support the call center). This is the customer lifecycle and at each step in the process systems and tools need to be harmonized as they gather information about the users using attribute models that are consistent and that serve the business and the customer. They take content and data as input and then output more data. One applications exhaust is another applications fuel. Organizations are also purchasing data streams to enrich their internal information sources. Social media is an enormous virtually untapped reservoir of data about customers and what they think about organizations. This can be mined for sentiment and to gauge marketing effectiveness. Increasingly, much of this is being placed into the hands of the marketing organization. In fact, a study by Gartner Group said that by 2017 the CMO will spend more money on IT than the CIO. What all of this means is that information, content and data governance need to be considered as part of a whole and not as separate initiatives. Elements of good information governance include: • Deployment and Operationalization • Alignment with User Needs • Business Value • Buy-In and Change Management • Sponsorship and Accountability Leads to the following outcomes: • Manages conflicts in business priorities (between initiatives, business units, drivers, etc) • Allows for ongoing input from various stakeholders and constituencies in order to evolve capabilities with the needs of the business • Prioritizes efforts and allocation of resources • Assigns roles and responsibilities with accountability to critical functions • Takes into consideration various levels of maturity in the organization — no one size fits all • Ensures that investments in systems, processes and tools are providing sufficient return to the business • Balances centralized standards with decentralized decision making • Aligns incentives to use a system with business goals This session will review governance concepts, discuss how they apply to various types of data and content and provide a framework for developing governance processes and structures.|10.1109/ITPRO.2014.7029280|||||2014|Presentation 1. Information governance in the age of big data|Earley, Seth|inproceedings|7029280||May|||||||||||||||||||||||||||1310680455|42
||Spatiotemporal optimization, Big Data, Cloud computing, Global operation, GEOSS||191-203||Based on our GEOSS Clearinghouse operating experience, we summarized the three Earth Observation (EO) Big Data access challenges, namely, fast access, accurate service estimation and global access, and two essential research questions: are there any spatiotemporal patterns when end users access EO data, and how can these spatiotemporal patterns be utilized to better facilitate EO Big Data access? To tackle these two research questions, we conducted a two-year pattern analysis with 2+ million user access records. The spatial pattern, temporal pattern and spatiotemporal pattern of user-data interactions were explored. For the second research question, we developed three spatiotemporal optimization strategies to respond to the three access challenges: a) spatiotemporal indexing to accelerate data access, b) spatiotemporal service modeling to improve data access accuracy and c) spatiotemporal cloud computing to enhance global access. This research is a pioneering framework for spatiotemporal optimization of EO Big Data access and valuable for other multidisciplinary geographic data and information research.|https://doi.org/10.1016/j.compenvurbsys.2018.06.010|https://www.sciencedirect.com/science/article/pii/S0198971518300401||||2018|Using spatiotemporal patterns to optimize Earth Observation Big Data access: Novel approaches of indexing, service modeling and cloud computing|Jizhe Xia and Chaowei Yang and Qingquan Li|article|XIA2018191|||Computers, Environment and Urban Systems|01989715||72|||||23269|1,549|Q1|92|85|327|4842|2135|324|6,11|56,96|United Kingdom|Western Europe|1980-1986, 1988-2020|Ecological Modeling (Q1); Environmental Science (miscellaneous) (Q1); Geography, Planning and Development (Q1); Urban Studies (Q1)||||1313063914|1571752529
PCI 2021|Volos, Greece||6|419–424|25th Pan-Hellenic Conference on Informatics|The COVID-19 pandemic brought many changes in society, with one of the most important being an explosion of software development concerning technological solutions for combatting its crippling effects. In this global crisis, many software enthusiasts, combined with seasoned developers and specialists turned their attention to Questions and Answers platforms such as Stack Overflow to expand their knowledge and ask questions regarding their COVID-19 related solutions. This paper examines the different characteristics of these users, dividing them into Newcomers and Oldcomers and pinpoints popularity differences, scientific and technological backgrounds by analyzing key technologies, as well as the role of gender in their participation.|10.1145/3503823.3503900|https://doi.org/10.1145/3503823.3503900|New York, NY, USA|Association for Computing Machinery|9781450395557|2022|An Analysis of User Profiles from Covid-19 Questions in Stack Overflow|Stakoulas, Konstantinos and Georgiou, Konstantinos and Mittas, Nikolaos and Angelis, Lefteris|inproceedings|10.1145/3503823.3503900|||||||||||||||||||||||||||||1314838603|42
||Strategic asset management (SAM), Building information modeling (BIM), Portfolio management, Data quality management||104070||A building's strategic asset management (SAM) capability has traditionally been limited by its site-based management. With the emergence of needs from clients about delivering a long-term portfolio-based building asset management plan that minimizes the asset risk and optimizes the value of their asset portfolios, SAM Units have emerged as a new business form to provide various SAM services to their clients. However, the quality of their current data model is still hindered by many issues, such as missing important attributes and the lack of customized information flow guidance. In addition, there is a gap in integrating their existing data collection with various data sources and Building Information Modeling (BIM) to enhance their data quality. By evaluating a SAM Unit's portfolio case study, this paper identifies the factors limiting the quality of SAM Units' data model and develops a guide to integrating various data sources better. We develop a BIM-integrated portfolio-based SAM information flow framework and a detailed hierarchical portfolio-based non-geometric data structure. The proposed framework and data structure will help SAM professionals, building asset owners, and other facilities management professionals embrace the benefits of managing the portfolio-based SAM data.|https://doi.org/10.1016/j.autcon.2021.104070|https://www.sciencedirect.com/science/article/pii/S0926580521005215||||2022|BIM-integrated portfolio-based strategic asset data quality management|Zigeng Fang and Yan Liu and Qiuchen Lu and Michael Pitt and Sean Hanna and Zhichao Tian|article|FANG2022104070|||Automation in Construction|09265805||134|||||24931|1,837|Q1|121|364|779|20472|7433|778|9,16|56,24|Netherlands|Western Europe|1992-2020|Building and Construction (Q1); Civil and Structural Engineering (Q1); Control and Systems Engineering (Q1)|16,738|7.700|0.01309|1315706039|1751532024
||Electronic healthcare records||105-110||The transferring of medical records into huge electronic databases has opened up opportunities for research but requires attention to data quality, study design and issues of bias and confounding.|https://doi.org/10.1016/j.spl.2018.02.044|https://www.sciencedirect.com/science/article/pii/S0167715218300890||||2018|The role of statistics in the era of big data: Electronic health records for healthcare research|Linda D. Sharples|article|SHARPLES2018105|||Statistics & Probability Letters|01677152||136||The role of Statistics in the era of big data|||14794|0,576|Q2|66|241|862|4066|1014|860|1,17|16,87|Netherlands|Western Europe|1982-2021|Statistics and Probability (Q2); Statistics, Probability and Uncertainty (Q2)||||1317644115|1688735168
||Smoothing methods;Data analysis;Conferences;Time series analysis;Big Data;Cleaning;Spatiotemporal phenomena;collision detect;location;spatiotemporal data;data clean||128-134|2021 IEEE 15th International Conference on Big Data Science and Engineering (BigDataSE)|In the era of big data, data resources are becoming more and more abundant, and the quality of data is getting more and more attention. Data cleaning is the process of identifying and processing dirty data in order to improve the quality of data, which is conducive to make full use of the collected data and ensure the effectiveness and accuracy of the follow-up data analysis. Spatiotemporal data is a kind of time series data, and its cleaning has been widely studied. However, existing methods based on smoothing and statistics are often only suitable for dense spatiotemporal datasets. In this paper, we propose a general spatiotemporal data cleaning method based on collision detection (SDCCD), which is suitable for both dense and sparse spatiotemporal datasets. Experiments on real spatiotemporal datasets show that SDCCD can effectively detect and process spatiotemporal collision records in spatiotemporal datasets.|10.1109/BigDataSE53435.2021.00027|||||2021|SDCCD: Spatiotemporal Data Cleaning based on Collision Detection|Xue, Hui and Sun, Bo and Si, Chengxiang and Zhang, Wei and Fang, Jing|inproceedings|9724540||Oct|||||||||||||||||||||||||||1317912299|42
||Synthetic data, long short term memory network (LSTM), privacy, location sequences|27|||People’s location data are continuously tracked from various devices and sensors, enabling an ongoing analysis of sensitive information that can violate people’s privacy and reveal confidential information. Synthetic data have been used to generate representative location sequences yet to maintain the users’ privacy. Nonetheless, the privacy-accuracy tradeoff between these two measures has not been addressed systematically. In this article, we analyze the use of different synthetic data generation models for long location sequences, including extended short-term memory networks (LSTMs), Markov Chains (MC), and variable-order Markov models (VMMs). We employ different performance measures, such as data similarity and privacy, and discuss the inherent tradeoff. Furthermore, we introduce other measurements to quantify each of these measures. Based on the anonymous data of 300 thousand cellular-phone users, our work offers a road map for developing policies for synthetic data generation processes. We propose a framework for building data generation models and evaluating their effectiveness regarding those accuracy and privacy measures.|10.1145/3529260|https://doi.org/10.1145/3529260|New York, NY, USA|Association for Computing Machinery||2022|Synthesis of Longitudinal Human Location Sequences: Balancing Utility and Privacy|Benarous, Maya and Toch, Eran and Ben-gal, Irad|article|10.1145/3529260|118|jul|ACM Trans. Knowl. Discov. Data|15564681|6|16|December 2022||||5800173377|0,728|Q1|59|71|169|3729|816|168|4,54|52,52|United States|Northern America|2007-2020|Computer Science (miscellaneous) (Q1)|1,740|2.713|0.00224|1319724718|1302859451
||Linked Open Data, Quality assessment, Quality improvement, Synonym predicates, Profiling statistics, DBpedia||5552-5563||For many years, data quality is among the most commonly discussed issue in Linked Open Data (LOD) due to the huge volume of integrated datasets that are usually heterogeneous. Several ontology-based approaches dealing with quality problems have been proposed. However, when datasets lack a well-defined schema, these approaches become ineffective because of the lack of metadata. Moreover, the detection of quality problems based on an analysis between RDF (Resource Description Framework) triples without requiring ontology statistical and semantical information is not addressed. Keeping in mind that ontologies are not always available and they may be incomplete or misused. In this paper, a novel free-ontology process called LODQuMa is proposed to assess and improve the quality of LOD. It is mainly based on profiling statistics, synonym relationships between predicates, QVCs (Quality Verification Cases), and SPARQL (SPARQL Protocol and RDF Query Language) query templates. Experiments on the DBpedia dataset demonstrate that the proposed process is effective for increasing the intrinsic quality dimensions, resulting in correct and compact datasets.|https://doi.org/10.1016/j.jksuci.2021.06.001|https://www.sciencedirect.com/science/article/pii/S1319157821001348||||2022|LODQuMa: A Free-ontology process for Linked (Open) Data quality management|Samah Salem and Fouzia Benchikha|article|SALEM20225552|||Journal of King Saud University - Computer and Information Sciences|13191578|8, Part A|34|||||||||||||||||||||||1319754491|1257083324
CLIHC '19|Panama City, Panama|big data, urban agriculture, precision agriculture, focus groups|4||Proceedings of the IX Latin American Conference on Human Computer Interaction|As big data has become increasingly necessary in modern farming techniques, the dependence on high quality and quantity of ground truthing data has risen. Collecting ground truthing data is one of the most labor-intensive aspects of the research process. A crowdsourcing platform application to aid laypeople in completing ground truthing data can improve the quality and quantity of data for growers and agricultural researchers. Focus groups were conducted to gauge opinions on crowdsourcing initiatives in agriculture to inform the design of the platform. Preliminary results demonstrate that the greatest motivation for the participants was having opportunities to develop their skills and access to educational resources. They also discussed having a finite timeframe for collecting the data, feeling appreciated by the researchers, and being informed on the context and next steps of the research. The results of these focus groups will be used to develop design prototypes for the crowdsourcing platform.|10.1145/3358961.3358970|https://doi.org/10.1145/3358961.3358970|New York, NY, USA|Association for Computing Machinery|9781450376792|2020|Opinions Concerning Crowdsourcing Applications in Agriculture in D.C.|Posadas, Brianna B. and Hanumappa, Mamatha and Gilbert, Juan E.|inproceedings|10.1145/3358961.3358970|3||||||||||||||||||||||||||||1323025687|42
|||44|277–320|Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice|||https://doi.org/10.1145/3447404.3447420|New York, NY, USA|Association for Computing Machinery|9781450390293|2021|Probabilistic Text Entry—Case Study 3|Vertanen, Keith|inbook|10.1145/3447404.3447420|||||||||1||||||||||||||||||||1324771252|42
||Feeds;Big data;Computer architecture;Clouds;Wireless sensor networks;Software;Conferences;data quality;cloud service;energy management;sensor network||749-752|2015 IEEE International Congress on Big Data|"\"Cloud-based sensor data collection services are becoming an essential part of the Internet of Things (IoT). As the consumer demand grows for these services, the data quality (DQ) of the stream becomes an increasingly vital issue. Of particular interest is the inherent tradeoff between the DQ and the energy consumption of the sensor. Unfortunately, there has been very little research on the management of this tradeoff that allows data consumers to receive high quality data while simultaneously conserving energy. Our work seeks to explore this tradeoff in detail by combining DQ services for the data stream consumer with customizable energy efficient \"\"EE\"\" throttling algorithms for the data feed producers. These energy management services provide cost reduction rewards for consumers who would otherwise make poor DQ/EE decisions. Our primary contributions include cloud-based services for monitoring the tradeoff, an architecture that adjusts to DQ needs and a producer/consumer data stream best matching cloud service. We envision that our services architecture will reward energy efficiency decisions and profoundly affect consumer choices.\""|10.1109/BigDataCongress.2015.124|||||2015|Data Quality and Energy Management Tradeoffs in Sensor Service Clouds|Lawson, Victor and Ramaswamy, Lakshmish|inproceedings|7207308||June||23797703|||||||||||||||||||||||||1325361371|1226158248
||Costs;Data integrity;Education;Distributed databases;Big Data;Maintenance engineering;Real-time systems;financial big data;full data reconciliation;incremental data reconciliation;quasi real time data reconciliation||260-263|2021 International Symposium on Advances in Informatics, Electronics and Education (ISAIEE)|For data errors in distributed financial system caused by multi-system interaction, asynchronous processing and system bug, this paper proposes offline and quasi real-time data reconciliation methods based on the combination of Alibaba big data processing platform and accounting theory. In offline data reconciliation, Full data reconciliation and hour level incremental data reconciliation are introduced. And in quasi real-time data reconciliation, single system and distributed multi-system reconciliation models are introduced. These data reconciliation methods are then verified against 7 million pieces of daily data of the distributed loan system in a financial company. Results show that these methods can complete the financial big data processing, discover the data quality problems timely, and minimize the financial system capital loss.|10.1109/ISAIEE55071.2021.00071|||||2021|Financial Big Data Reconciliation Method|Xing, Xiaobo|inproceedings|9727023||Dec|||||||||||||||||||||||||||1326557360|42
||synthesis, missing value, database design, redundancy, key, updates, functional dependency, Boyce-Codd normal form, third normal form, normal form, decomposition|46|||We establish a principled schema design framework for data with missing values. The framework is based on the new notion of an embedded functional dependency, which is independent of the interpretation of missing values, able to express completeness and integrity requirements on application data, and capable of capturing redundant data value occurrences that may cause problems with processing data that meets the requirements. We establish axiomatic, algorithmic, and logical foundations for reasoning about embedded functional dependencies. These foundations enable us to introduce generalizations of Boyce-Codd and Third normal forms that avoid processing difficulties of any application data, or minimize these difficulties across dependency-preserving decompositions, respectively. We show how to transform any given schema into application schemata that meet given completeness and integrity requirements, and the conditions of the generalized normal forms. Data over those application schemata are therefore fit for purpose by design. Extensive experiments with benchmark schemata and data illustrate the effectiveness of our framework for the acquisition of the constraints, the schema design process, and the performance of the schema designs in terms of updates and join queries.|10.1145/3450518|https://doi.org/10.1145/3450518|New York, NY, USA|Association for Computing Machinery||2021|Embedded Functional Dependencies and Data-Completeness Tailored Database Design|Wei, Ziheng and Link, Sebastian|article|10.1145/3450518|7|may|ACM Trans. Database Syst.|03625915|2|46|June 2021||||||||||||||||||||||1326566704|1726010902
|||4|1986–1989||The ubiquity of data lakes has created fascinating new challenges for data management research. In this tutorial, we review the state-of-the-art in data management for data lakes. We consider how data lakes are introducing new problems including dataset discovery and how they are changing the requirements for classic problems including data extraction, data cleaning, data integration, data versioning, and metadata management.|10.14778/3352063.3352116|https://doi.org/10.14778/3352063.3352116||VLDB Endowment||2019|Data Lake Management: Challenges and Opportunities|Nargesian, Fatemeh and Zhu, Erkang and Miller, Ren\'{e}e J. and Pu, Ken Q. and Arocena, Patricia C.|article|10.14778/3352063.3352116||aug|Proc. VLDB Endow.|21508097|12|12|August 2019||||21100199855|0,946|Q1|134|246|598|12401|3759|566|5,50|50,41|United States|Northern America|2008-2020|Computer Science (miscellaneous) (Q1)|7,198|2.047|0.00891|1327026809|1216159931
||Big data, Electronic health records, Epidemiology, Metrology, Precision||111-115||A broad review is given of the impact of big data on various aspects of investigation. There is some but not total emphasis on issues in epidemiological research.|https://doi.org/10.1016/j.spl.2018.02.015|https://www.sciencedirect.com/science/article/pii/S0167715218300609||||2018|Big data: Some statistical issues|D.R. Cox and Christiana Kartsonaki and Ruth H. Keogh|article|COX2018111|||Statistics & Probability Letters|01677152||136||The role of Statistics in the era of big data|||14794|0,576|Q2|66|241|862|4066|1014|860|1,17|16,87|Netherlands|Western Europe|1982-2021|Statistics and Probability (Q2); Statistics, Probability and Uncertainty (Q2)||||1327597678|1688735168
|||24|126–149||Every fiscal quarter automated writing algorithms churn out thousands of corporate earnings articles for the AP (Associated Press) based on little more than structured data. Companies such as Automated Insights, which produces the articles for AP, and Narrative Science can now write straight news articles in almost any domain that has clean and well-structured data: finance, sure, but also sports, weather, and education, among others. The articles aren’t cardboard either; they have variability, tone, and style, and in some cases readers even have difficulty distinguishing the machine-produced articles from human-written ones.|10.1145/2857274.2886105|https://doi.org/10.1145/2857274.2886105|New York, NY, USA|Association for Computing Machinery||2015|Accountability in Algorithmic Decision-Making: A View from Computational Journalism|Diakopoulos, Nicholas|article|10.1145/2857274.2886105||nov|Queue|15427730|9|13|November-December 2015||||19700186817|0,457|Q2|44|39|131|267|265|117|2,89|6,85|United States|Northern America|2003-2020|Computer Science (miscellaneous) (Q2)||||1332582184|925254269
EM-GIS '19|Chicago, Illinois|risk assessment, big data, machine learning, foodborne disease|6||Proceedings of the 5th ACM SIGSPATIAL International Workshop on the Use of GIS in Emergency Management|In recent years, the outbreak of foodborne diseases has been on an upward trend clearly. It is of great significance for us to predict the outbreak of foodborne diseases accurately and conduct quantitative risk assessment timely. Traditional prediction methods based on a single data source have drawbacks such as complex prediction processes and inaccurate prediction results. In this article, we figure out the scientific issues of how to improve the temporal and spatial accuracy of foodborne disease outbreak risk prediction in Beijing. Firstly, we analyze the different foodborne disease risk factors caused by the spread of water pollution in Beijing, and study the methods of collecting and preprocessing multi-source data. Then, through the comparison of different regression models and parameters tuning, we propose the use of Gradient Boosting Regression (GBR) model as a multi-source data fusion model to predict the outbreak of foodborne disease. Finally, we use the risk map to detect and predict foodborne disease outbreak in different business districts of Beijing based on visualization techniques, aiming to provide prevention and control assessment for decision-makers quickly and precisely.|10.1145/3356998.3365776|https://doi.org/10.1145/3356998.3365776|New York, NY, USA|Association for Computing Machinery|9781450369657|2020|Risk Prediction and Assessment of Foodborne Disease Based on Big Data|Zhang, Mingke and Guo, Danhuai and Hu, Jinyong and Jin, Wei|inproceedings|10.1145/3356998.3365776|8||||||||||||||||||||||||||||1334013202|42
SSDBM '15|La Jolla, California||11||Proceedings of the 27th International Conference on Scientific and Statistical Database Management|To ease the proliferation of big data, it frequently is transformed, be it by compression, be it by anonymization. Such transformations however modify characteristics of the data, such as changes in the case of time series. Changes however are important for subsequent analyses. The impact of those modifications depends on the application scenario, and quantifying it is far from trivial. This is because a transformation can shift or modify existing changes or introduce new ones. In this paper, we propose MILTON, a flexible and robust Measure for quantifying the Impact of Lossy Transformations on subsequent change detectiON. MILTON is applicable to any lossy transformation technique on time-series data and to any general-purpose change-detection approach. We have evaluated it with three real-world use cases. Our evaluation shows that MILTON allows to quantify the impact of lossy transformations and to choose the best one from a class of transformation techniques for a given application scenario.|10.1145/2791347.2791371|https://doi.org/10.1145/2791347.2791371|New York, NY, USA|Association for Computing Machinery|9781450337090|2015|How to Quantify the Impact of Lossy Transformations on Change Detection|"\"Efros, Pavel and Buchmann, Erik and Englhardt, Adrian and B\"\"{o}hm, Klemens\""|inproceedings|10.1145/2791347.2791371|17||||||||||||||||||||||||||||1336519762|42
|||2|11–12||How customer insights keep one company agile, and challenge these data scientist to stay ahead in an ever-changing world.|10.1145/2983463|https://doi.org/10.1145/2983463|New York, NY, USA|Association for Computing Machinery||2016|One Thousand Interviews|Peek, Geerten and Taspinar, Ahmet|article|10.1145/2983463||sep|XRDS|15284972|1|23|Fall 2016||||||||||||||||||||||1336788405|613180419
CCGRID '16|Cartagena, Columbia|data warehouse, ETL design, ontologies, data quality, semantic database sources|8|631–638|Proceedings of the 16th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing|Data Warehouse (DW) is a collection of data, consolidated from several heterogeneous sources, used to perform data analysis and support decision-making in an organization. Extract-Transform-Load (ETL) phase plays a crucial role in designing DW. To overcome the complexity of the ETL phase, different studies have recently proposed the use of ontologies. Ontology-based ETL approaches have been used to reduce heterogeneity between data sources and ensure automation of the ETL process. Existing studies in semantic ETL have largely focused on fulfilling functional requirements. However, the ETL process quality dimension has not been sufficiently considered by these studies. As the amount of data has exploded with the advent of big data era, dealing with quality challenges in the early stages of designing the process become more important than ever. To address this issue, we propose to keep data quality requirements at the center of the ETL phase design. We present in this paper an approach, defining the ETL process at the ontological level. We define a set of quality indicators and quantitative measures that can anticipate data quality problems and identify causes of deficiencies. Our approach checks the quality of data before loading them into the target data warehouse to avoid the propagation of corrupted data. Finally, our proposal is validated through a case study, using Oracle Semantic DataBase sources (SDBs), where each source references the Lehigh University BenchMark ontology (LUBM).|10.1109/CCGrid.2016.79|https://doi.org/10.1109/CCGrid.2016.79||IEEE Press|9781509024520|2016|A Quality-Driven Approach for Building Heterogeneous Distributed Databases: The Case of Data Warehouses|Abdellaoui, Sabrina and Bellatreche, Ladjel and Nader, Fahima|inproceedings|10.1109/CCGrid.2016.79|||||||||||||||||||||||||||||1337298958|42
||Sustainable urban forms, Smart sustainable cities, Big data analytics, Context-aware computing, Typologies and design concepts, Technologies and applications, ICT of the new wave of computing||449-474||Undoubtedly, sustainable development has inspired a generation of scholars and practitioners in different disciplines into a quest for the immense opportunities created by the development of sustainable urban forms for human settlements that will enable built environments to function in a more constructive and efficient way. However, there are still significant challenges that need to be addressed and overcome. The issue of such forms has been problematic and difficult to deal with, particularly in relation to the evaluation and improvement of their contribution to the goals of sustainable development. As it is an urban world where the informational and physical landscapes are increasingly being merged, sustainable urban forms need to embrace and leverage what current and future ICT has to offer as innovative solutions and sophisticated methods so as to thrive—i.e. advance their contribution to sustainability. The need for ICT of the new wave of computing to be embedded in such forms is underpinned by the recognition that urban sustainability applications are deemed of high relevance to the contemporary research agenda of computing and ICT. To unlock and exploit the underlying potential, the field of sustainable urban planning is required to extend its boundaries and broaden its horizons beyond the ambit of the built form of cities to include technological innovation opportunities. This paper explores and substantiates the real potential of ICT of the new wave of computing to evaluate and improve the contribution of sustainable urban forms to the goals of sustainable development. This entails merging big data and context-aware technologies and their applications with the typologies and design concepts of sustainable urban forms to achieve multiple hitherto unrealized goals. In doing so, this paper identifies models of smart sustainable city and their technologies and applications and models of sustainable urban form and their design concepts and typologies. In addition, it addresses the question of how these technologies and applications can be amalgamated with these design concepts and typologies in ways that ultimately evaluate and improve the contribution of sustainable urban forms to the goals of sustainable development. The overall aim of this paper suits a mix of three methodologies: literature review, thematic analysis, and secondary (qualitative) data analysis to achieve different but related objectives. The study identifies four technologies and two classes of applications pertaining to models of smart sustainable city as well as three design concepts and four typologies related to models of sustainable urban form. Finally, this paper proposes a Matrix to help scholars and planners in understanding and analyzing how and to what extent the contribution of sustainable urban forms to sustainability can be improved through ICT of the new wave of computing as to the underlying novel technologies and their applications, as well as a data-centric approach into investigating and evaluating this contribution and a simulation method for strategically optimizing it.|https://doi.org/10.1016/j.scs.2017.04.012|https://www.sciencedirect.com/science/article/pii/S2210670716302475||||2017|ICT of the new wave of computing for sustainable urban forms: Their big data and context-aware augmented typologies and design concepts|Simon Elias Bibri and John Krogstie|article|BIBRI2017449|||Sustainable Cities and Society|22106707||32|||||19700194105|1,645|Q1|61|705|1286|43818|10974|1284|8,53|62,15|Netherlands|Western Europe|2011-2020|Civil and Structural Engineering (Q1); Geography, Planning and Development (Q1); Renewable Energy, Sustainability and the Environment (Q1); Transportation (Q1)|14,373|7.587|0.01684|1340074155|1912866754
SenSys '17|Delft, Netherlands|software architecture, mobile sensor big data, mHealth|14||Proceedings of the 15th ACM Conference on Embedded Network Sensor Systems|The development and validation studies of new multisensory biomarkers and sensor-triggered interventions requires collecting raw sensor data with associated labels in the natural field environment. Unlike platforms for traditional mHealth apps, a software platform for such studies needs to not only support high-rate data ingestion, but also share raw high-rate sensor data with researchers, while supporting high-rate sense-analyze-act functionality in real-time. We present mCerebrum, a realization of such a platform, which supports high-rate data collections from multiple sensors with realtime assessment of data quality. A scalable storage architecture (with near optimal performance) ensures quick response despite rapidly growing data volume. Micro-batching and efficient sharing of data among multiple source and sink apps allows reuse of computations to enable real-time computation of multiple biomarkers without saturating the CPU or memory. Finally, it has a reconfigurable scheduler which manages all prompts to participants that is burden- and context-aware. With a modular design currently spanning 23+ apps, mCerebrum provides a comprehensive ecosystem of system services and utility apps. The design of mCerebrum has evolved during its concurrent use in scientific field studies at ten sites spanning 106,806 person days. Evaluations show that compared with other platforms, mCerebrum's architecture and design choices support 1.5 times higher data rates and 4.3 times higher storage throughput, while causing 8.4 times lower CPU usage.|10.1145/3131672.3131694|https://doi.org/10.1145/3131672.3131694|New York, NY, USA|Association for Computing Machinery|9781450354592|2017|MCerebrum: A Mobile Sensing Software Platform for Development and Validation of Digital Biomarkers and Interventions|Hossain, Syed Monowar and Hnat, Timothy and Saleheen, Nazir and Nasrin, Nusrat Jahan and Noor, Joseph and Ho, Bo-Jhang and Condie, Tyson and Srivastava, Mani and Kumar, Santosh|inproceedings|10.1145/3131672.3131694|7||||||||||||||||||||||||||||1340364257|42
DIVANet '14|Montreal, QC, Canada|cloud computing, big data|6|139–144|Proceedings of the Fourth ACM International Symposium on Development and Analysis of Intelligent Vehicular Networks and Applications|This paper aims at developing the Big Data Architecture, and its relation with Analytics, Cloud Services as well as Business Intelligence. The chief aim from all mentioned is to enable the Enterprise Architecture and the Vision of an Organizational target to utilize all the data they are ingesting and regressing data for their short-term or long-terms analytical needs, while making sure that they are addressing during the design phase of such data architecture for both directly and indirectly related stakeholder. Since all stakeholders have their relative interests to utilize the transformed data-sets. This paper also identifies most of the Big Data Architecture, threat analysis within a Big Data System and Big Data Analytic Roadmaps, in terms of smaller components by conducting a gap-analysis that has significant importance as Baseline Big Data Architecture, targeting the end resultant Architectures, once the distillation process of main Big Data Architecture is completed by the Data Architects.|10.1145/2656346.2656358|https://doi.org/10.1145/2656346.2656358|New York, NY, USA|Association for Computing Machinery|9781450330282|2014|Big Data Architecture Evolution: 2014 and Beyond|Mohammad, Atif and Mcheick, Hamid and Grant, Emanuel|inproceedings|10.1145/2656346.2656358|||||||||||||||||||||||||||||1343467160|42
||Data quality, Open data, IoT, Machine learning||328-333||This work assesses the quality of Internet of Things data not only as an intrinsic quality on how well it represents the related phenomenon but also, on how much information it contains to educate an artificial entity. The quality metrics here proposed are tested with real datasets. Also, they are implemented on OpenCPU, so the open data repositories can use them off-the-shelf to rate their datasets without computational cost and minimum human intervention, making them more attractive to potential users and gaining visibility and impact.|https://doi.org/10.1016/j.icte.2022.06.001|https://www.sciencedirect.com/science/article/pii/S240595952200090X||||2022|Intrinsic and extrinsic quality of data for open data repositories|Aurora González-Vidal and Alfonso P. Ramallo-González and Antonio F. Skarmeta|article|GONZALEZVIDAL2022328|||ICT Express|24059595|3|8|||||21100836194|0,733|Q1|22|85|141|1553|813|140|6,30|18,27|South Korea|Asiatic Region|2015-2020|Computer Networks and Communications (Q1); Hardware and Architecture (Q1); Information Systems (Q1); Software (Q1); Artificial Intelligence (Q2)|789|4.317|0.0014|1343591152|1436534730
||||S36|||https://doi.org/10.1016/j.toxlet.2015.08.097|https://www.sciencedirect.com/science/article/pii/S0378427415020378||||2015|The importance of data quality to enhance the impact of omics sciences|T. Gant|article|GANT2015S36|||Toxicology Letters|03784274|2, Supplement|238||ABSTRACTS OF THE 51st Congress of the European Societies of Toxicology (EUROTOX)|||25240|1,007|Q1|145|303|840|14151|3425|821|3,93|46,70|Netherlands|Western Europe|1977-2020|Medicine (miscellaneous) (Q1); Toxicology (Q1)|18,729|4.372|0.01073|1343847149|229996272
ICBDE '18|Honolulu, HI, USA|Big Data, Data Management|5|52–56|Proceedings of the 2018 International Conference on Big Data and Education|In this emerging computing and digital globe, information and Knowledge are created and then collected with a rapid approach by wide range of applications through scientific computing and commercial workloads. Over 3.8 billion people out of 7.6 billion population of the world are connected to the internet. Out of 13.4 billion devices, 8.06 billion devices have a mobile connection. In 2020, 38.5 billion devices will be connected and globally internet traffic will be 92 times greater than it was in 2005. The use of such devices and internet not only increase the data volume but the velocity of market brings in fast-track and accelerates as information is transferred and shared with light speed on optic fiber and wireless networks. This fast generation of huge data creates numerous challenges. The existing approaches addressing issues such as, Volume, Variety, Velocity and Value in big data research perspective. The objectives of the paper are to investigate and analyze the current status of Big Data and furthermore a comprehensive overview of various aspects has discussed, and additionally has been described all 10 Vs' (Issues) of Big Data.|10.1145/3206157.3206166|https://doi.org/10.1145/3206157.3206166|New York, NY, USA|Association for Computing Machinery|9781450363587|2018|The 10 Vs, Issues and Challenges of Big Data|Khan, Nawsher and Alsaqer, Mohammed and Shah, Habib and Badsha, Gran and Abbasi, Aftab Ahmad and Salehian, Soulmaz|inproceedings|10.1145/3206157.3206166|||||||||||||||||||||||||||||1344217413|42
||Oil and Gas 4.0, Big data, Digitization, IIoT, Intelligentization||68-90||Recently, with the development of “Industry 4.0”, “Oil and Gas 4.0” has also been put on the agenda in the past two years. Some companies and experts believe that “Oil and Gas 4.0” can completely change the status quo of the oil and gas industry, which can bring huge benefits because it accelerates the digitization and intelligentization of the oil and gas industry. However, the “Oil and Gas 4.0” is still in its infancy. Therefore, this paper systematically introduces the concept and core technologies of “Oil and Gas 4.0”, such as big data and the industrial Internet of Things (IIoT). Moreover, this paper analyzes typical application scenarios of the oil and gas industry chain (upstream, midstream and downstream) through examples, such as intelligent oilfield, intelligent pipeline, and intelligent refinery. It is concluded that the essence of “Oil and Gas 4.0” is a data-driven intelligence system based on the highly digitization. To the best of our knowledge, this is the first academic peer-reviewed paper on the “Oil and Gas 4.0” era, aiming to let more oil and gas industry personnel understand its benefits and application scenarios, so as to better apply it to practical engineering in the future. In the discussion section, this paper also analyzes the opportunities and difficulties that may be brought about by the “Oil and Gas 4.0” era. Finally, relevant policy recommendations are proposed.|https://doi.org/10.1016/j.compind.2019.06.007|https://www.sciencedirect.com/science/article/pii/S0166361519302064||||2019|Oil and Gas 4.0 era: A systematic review and outlook|Hongfang Lu and Lijun Guo and Mohammadamin Azimi and Kun Huang|article|LU201968|||Computers in Industry|01663615||111|||||19080|1,432|Q1|100|115|323|6926|3165|319|9,96|60,23|Netherlands|Western Europe|1979-2020|Computer Science (miscellaneous) (Q1); Engineering (miscellaneous) (Q1)|6,018|7.635|0.00584|1344348820|605146181
MK Series on Business Intelligence||sensor data, machine data, social media, compliance, safety||101-123|Data Warehousing in the Age of Big Data|The first four chapters provided you an introduction to Big Data, the complexities associated with Big Data, and the processing techniques and technologies for Big Data. This chapter will focus on use cases of Big Data and how real-world companies are implementing Big Data.|https://doi.org/10.1016/B978-0-12-405891-0.00005-2|https://www.sciencedirect.com/science/article/pii/B9780124058910000052|Boston|Morgan Kaufmann|978-0-12-405891-0|2013|Chapter 5 - Big Data Driving Business Value|Krish Krishnan|incollection|KRISHNAN2013101||||||||||Krish Krishnan|||||||||||||||||||1345084350|42
||Detectors;Image edge detection;Radiography;Standards;Image quality;X-ray imaging;Indexes;image quality;in-service;radiographic product;routine data;quality control||341-346|2016 7th International Conference on Cloud Computing and Big Data (CCBD)|Mass data generated from in-service radiographic product contain assignable information on Image Quality (IQ). Analyzing data from routine work might supplement the time-consuming Image Quality Assurance Test Procedure (IQATP) to evaluate IQ and to know product type performance on site, which can also locate risks and give manufacturer directions for the further actions as well. This article illustrates methodologies of extracting IQ information from mass data and visual quality track, analysis, control, and risk mitigation in Big Data environments.|10.1109/CCBD.2016.073|||||2016|Big Data Analysis on Radiographic Image Quality|Wen, Hongsheng and Chen, Zhiqiang and Gu, Jianping and Zhu, Qiangqiang|inproceedings|7979931||Nov|||||||||||||||||||||||||||1350038490|42
CAIH2020|Taiyuan, China|SOA, Big Data, Healthcare, EMR, Electronic Health Record|7|170–176|Proceedings of the 2020 Conference on Artificial Intelligence and Healthcare|"\"Healthcare Big Data Platform is the important content in the process of medical information industry. To a certain extent, it represents the overall level of the regional informatization. It is also a data exchange and sharing platform connecting the basic systems of local various medical and health institutions, and it is also the base and carrier to integrate the regional information system. This paper introduces the local regional medical informatization construction, planning architecture, data center construction mode and technical realization methods. Through this project, the informatization level of basic health agencies and all hospitals will have been greatly improved. It can provide more convenient and high-quality medical service for patients, alleviates \"\"difficulty and expensive\"\" problem effectively.\""|10.1145/3433996.3434027|https://doi.org/10.1145/3433996.3434027|New York, NY, USA|Association for Computing Machinery|9781450388641|2020|The Planning and Construction of Healthcare Big Data Platform|Ding, Shifu and Liu, Yan and Zhang, Jianjun and Tan, Yaqi and Li, Xiaoxia and Tang, RuiChun|inproceedings|10.1145/3433996.3434027|||||||||||||||||||||||||||||1350898909|42
||Internet population, Population distribution, Zipf's law, Public resource distortions, Big data||101808||Based on mobile internet user data, we construct an “Internet population” measure and reexamine spatial population distribution in China. The location based service (LBS) data of mobile internet uses is able to capture the accurate location of users' residence and solve the underestimation problem of missing migrants. We have three main findings. First, contrary to previous studies based on traditional population statistics, city size distribution of Internet population fits well into Zipf's law with a R2 of 90.7%. Second, the Internet population indicator is superior to traditional population statistics in explaining inelastic household consumption such as water consumption, electricity consumption, and garbage disposal. It suggests that the “Internet population” is a better proxy of actual city population. Third, the traditional population statistics systematically overestimate population in small cities and underestimate population in large cities. It indicates that the public resource distortions will continue to exist or even worsen off in China if the allocation process relies greatly on traditional population statistics. Although no measures are perfect, our new population measure provides important incremental information for future discussion.|https://doi.org/10.1016/j.chieco.2022.101808|https://www.sciencedirect.com/science/article/pii/S1043951X22000669||||2022|Urban population distribution in China: Evidence from internet population|Huixuan Li and Jing Chen and Zihao Chen and Jianguo Xu|article|LI2022101808|||China Economic Review|1043951X||74|||||20566|1,361|Q1|76|184|319|8316|1318|311|3,96|45,20|Netherlands|Western Europe|1989-2020|Economics and Econometrics (Q1); Finance (Q1)|5,722|4.227|0.00605|1353583540|837851341
|||13|3319–3331||We live in the gilded age of data-driven computing. With public clouds offering virtually unlimited amounts of compute and storage, enterprises collecting data about every aspect of their businesses, and advances in analytics and machine learning technologies, data driven decision making is now timely, cost-effective, and therefore, pervasive. Alas, only a handful of power users can wield today's powerful data engineering tools. For one thing, most solutions require knowledge of specific programming interfaces or libraries. Furthermore, running them requires complex configurations and knowledge of the underlying cloud for cost-effectiveness.We decided that a fundamental redesign is in order to democratize data engineering for the masses at cloud scale. The result is Informatica Cloud Data Integration - Elastic (CDI-E). Since the early 1990s, Informatica has been a pioneer and industry leader in building no-code data engineering tools. Non-experts can express complex data engineering tasks using a graphical user interface (GUI). Informatica CDI-E is built to incorporate the simplicity of GUI in the design layer with an elastic and highly scalable run time to handle data in any format without little to no user input using automated optimizations. Users upload their data to the cloud in any format and can immediately use them in conjunction with their data management and analytic tools of choice using CDI-E GUI. Implementation began in the Spring of 2017, and Informatica CDI-E has been generally available since the Summer of 2019. Today, CDI-E is used in production by a growing number of small and large enterprises to make sense of data in arbitrary formats.In this paper, we describe the architecture of Informatica CDI-E and its novel no-code data engineering interface. The paper highlights some of the key features of CDI-E: simplicity without loss in productivity and extreme elasticity. It concludes with lessons we learned and an outlook of the future.|10.14778/3554821.3554825|https://doi.org/10.14778/3554821.3554825||VLDB Endowment||2022|CDI-E: An Elastic Cloud Service for Data Engineering|Das, Prakash and Srivastava, Shivangi and Moskovich, Valentin and Chaturvedi, Anmol and Mittal, Anant and Xiao, Yongqin and Chowdhury, Mosharaf|article|10.14778/3554821.3554825||sep|Proc. VLDB Endow.|21508097|12|15|August 2022||||21100199855|0,946|Q1|134|246|598|12401|3759|566|5,50|50,41|United States|Northern America|2008-2020|Computer Science (miscellaneous) (Q1)|7,198|2.047|0.00891|1353742544|1216159931
||Accidents, Traffic congestion, Big data, Highways, England||301-314||This paper aims to estimate the causal effect of accidents on traffic congestion and vice versa. In order to identify both effects of this two-way relationship, I use dynamic panel data techniques and open access ‘big data’ of highway traffic and accidents in England for the period 2012–2014. The research design is based on the daily-and-hourly specific mean reversion pattern of highway traffic, which can be used to define a recurrent congestion benchmark. Using this benchmark, I am able to identify the causal effect of accidents on non-recurrent traffic congestion. A positive relationship between traffic congestion and road accidents would yield multiplicative benefits for policies that aim at reducing either of these issues. Additionally, I explore the duration of the effect of an accident on congestion, the ‘rubbernecking’ effect, as well as heterogeneous effects in the most congested highway segments. Then, I test the use of methods which employ the bulk of information in big data and other methods using a very reduced sample. In my application, both approaches produce similar results. Finally, I find a non-linear negative effect of traffic congestion on the probability of an accident.|https://doi.org/10.1016/j.jtrangeo.2017.10.006|https://www.sciencedirect.com/science/article/pii/S0966692317300704||||2019|Congestion by accident? A two-way relationship for highways in England|Ilias Pasidis|article|PASIDIS2019301|||Journal of Transport Geography|09666923||76|||||29295|1,809|Q1|108|264|515|15444|2931|514|5,21|58,50|United Kingdom|Western Europe|1993-2020|Environmental Science (miscellaneous) (Q1); Geography, Planning and Development (Q1); Transportation (Q1)|11,719|4.986|0.01053|1354697979|745814393
||Privacy;Crowdsensing;Degradation;Optimization;Perturbation methods;Sensors;Approximation algorithms;Crowdsensing;location data quality;location privacy;k-anonymity||3535-3544||Crowdsensing enables a wide range of data collection, where the data are usually tagged with private locations. Protecting users' location privacy has been a central issue. The study of various location perturbation techniques, e.g., k-anonymity, for location privacy has received widespread attention. Despite the huge promise and considerable attention, provable good algorithms considering the tradeoff between location privacy and location information quality from the optimization perspective in crowdsensing are lacking in the literature. In this article, we study two related optimization problems from two different perspectives. The first problem is to minimize the location quality degradation caused by the protection of users' location privacy. We present an efficient optimal algorithm OLoQ for this problem. The second problem is to maximize the number of protected users, subject to a location quality degradation constraint. To satisfy the different requirements of the platform, we consider two cases for this problem: 1) overlapping and 2) nonoverlapping perturbations. For the former case, we give an efficient optimal algorithm OPUMO. For the latter case, we first prove its NP-hardness. We then design a (1-E)-approximation algorithm NPUMN and a fast and effective heuristic algorithm HPUMN. Extensive simulations demonstrate that OLoQ, OPUMO, and HPUMN significantly outperform an existing algorithm.|10.1109/JIOT.2020.2972555|||||2020|Tradeoff Between Location Quality and Privacy in Crowdsensing: An Optimization Perspective|Zhang, Yuhui and Li, Ming and Yang, Dejun and Tang, Jian and Xue, Guoliang and Xu, Jia|article|8988265||April|IEEE Internet of Things Journal|23274662|4|7|||||21100338350|2,075|Q1|97|1163|1594|40380|20461|1555|12,37|34,72|United States|Northern America|2014-2020|Computer Networks and Communications (Q1); Computer Science Applications (Q1); Hardware and Architecture (Q1); Information Systems (Q1); Information Systems and Management (Q1); Signal Processing (Q1)|21,151|9.471|0.03208|1357531808|1395601152
|Portland, Oregon, USA|||||"\"Welcome to CSCW 2017, the ACM 2017 Conference on Computer Supported Cooperative Work and Social Computing! We are excited to welcome the CSCW community back to Portland, Oregon, where the second CSCW conference was held in 1988. Both Portland and CSCW have matured a great deal during the intervening 29 years. We hope that you will find that Portland provides a stimulating environment for our conference.CSCW is the premier venue for presenting research in the design and use of technologies that affect groups, organizations, communities, and networks. Bringing together top researchers and practitioners from academia and industry, CSCW explores the technical, social, material, and theoretical challenges of designing technology to support collaborative work and life activities. CSCW welcomes a diverse range of topics and research methodologies. Studies often involve the development and application of novel technologies and/or ethnographic studies that inform design practice or theory. The mission of the conference is to share research that advances the state of human knowledge and improves both the design of systems and the ways they are used. The diversity of work in our conference program reflects the diversity of technology use in people's work, social, and civic lives as well as the geographic and cultural diversity of contributors.As many of you know, CSCW follows a rigorous \"\"revise and resubmit\"\" review process that uses peer review to improve submitted papers while maintaining a high-quality threshold for final acceptance. We also help prepare the next generation of reviewers with a mentorship program in which students review papers under the guidance of an experienced reviewer. This year we have the largest CSCW program ever. We had 530 submitted papers and 183 were accepted for presentation at the conference. The program also includes 4 papers published in ACM Transactions on Human- Computer Interaction (TOCHI). In addition, we will feature 14 workshops, 56 posters, 12 demos, and 3 panels.Lili Cheng of Microsoft Research will open the conference, speaking on \"\"Conversational AI &amp; Lessons Learned.\"\" Our closing plenary will feature Jorge Cham, the creator of PhD Comics, who will talk about, \"\"The Science Gap.\"\" We also welcome Paul Luff and Christian Heath from King's College as the recipients of this year's CSCW Lasting Impact award for their influential 1998 paper, \"\"Mobility in Collaboration.\"\"\""|||New York, NY, USA|Association for Computing Machinery|9781450343350|2017|CSCW '17: Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing||proceedings|10.1145/2998181|||||||||||||||||||||||||||||1358169899|42
ASIST '16|Copenhagen, Denmark|biocollections, biology, data curation, data provenance, data curation profiles|9||Proceedings of the 79th ASIS&amp;T Annual Meeting: Creating Knowledge, Enhancing Lives through Information &amp; Technology|In the contexts of the data deluge and open data, scientists studying biodiversity benefit from online access to global datasets of existing vouchered biological and paleontological collections. Using biocollections collected over time across the world allows for the advancement of scientific knowledge concerning evolution in process as well as species poleward migrations, an indicator of climate change. This study's purpose was to validate and expand the Data Curation Profiles (DCP) to digital biocollections and inform a DCP framework for worldwide biota. Ten biocollection producers, curating various types of specimens affiliated with the project building the United States' national biodiversity infrastructure, were interviewed using the DCP questionnaire. Results indicate there is extreme diversity in the curation of biocollections and additional DCP questions should be added to reflect the complicated approaches to biological data curation. Although discipline specific metadata creation tools, standards, and practices enable long-term sustainability of the U.S. digitization effort, some scientists would benefit from further clarification and guidance on the information needs of consumers beyond designated communities of expert users, and the long-term preservation of biocollections.|||USA|American Society for Information Science||2016|Data Curation Profiling of Biocollections|Bishop, Bradley Wade and Hank, Carolyn|inproceedings|10.5555/3017447.3017493|46||||||||||||||||||||||||||||1359933931|42
||Big data analytics, Complexity theory, fsQCA, Business value, Mixed-method, Environmental uncertainty||261-276||Big data analytics has been widely regarded as a breakthrough technological development in academic and business communities. Despite the growing number of firms that are launching big data initiatives, there is still limited understanding on how firms translate the potential of such technologies into business value. The literature argues that to leverage big data analytics and realize performance gains, firms must develop strong big data analytics capabilities. Nevertheless, most studies operate under the assumption that there is limited heterogeneity in the way firms build their big data analytics capabilities and that related resources are of similar importance regardless of context. This paper draws on complexity theory and investigates the configurations of resources and contextual factors that lead to performance gains from big data analytics investments. Our empirical investigation followed a mixed methods approach using survey data from 175 chief information officers and IT managers working in Greek firms, and three case studies to show that depending on the context, big data analytics resources differ in significance when considering performance gains. Applying a fuzzy-set qualitative comparative analysis (fsQCA) method on the quantitative data, we show that there are four different patterns of elements surrounding big data analytics that lead to high performance. Outcomes of the three case studies highlight the inter-relationships between these elements and outline challenges that organizations face when orchestrating big data analytics resources.|https://doi.org/10.1016/j.jbusres.2019.01.044|https://www.sciencedirect.com/science/article/pii/S014829631930061X||||2019|Big data analytics and firm performance: Findings from a mixed-method approach|Patrick Mikalef and Maria Boura and George Lekakos and John Krogstie|article|MIKALEF2019261|||Journal of Business Research|01482963||98|||||20550|2,049|Q1|195|878|1342|73221|11507|1315|7,38|83,40|United States|Northern America|1973-2021|Marketing (Q1)|46,935|7.550|0.03523|1361668145|1502892296
DATeCH2017|"\"G\"\"{o}ttingen, Germany\""|metadata quality, REST API, design patterns, big data|5|111–115|Proceedings of the 2nd International Conference on Digital Access to Textual Cultural Heritage|This paper describes the structure of an extensible metadata quality assessment framework, which supports multiple metadata schemas, and is flexible enough to work with new schemas. The software has to be scalable to be able to process huge amount of metadata records within a reasonable time. Fundamental requirements that need to be considered during the design of such a software are i) the abstraction of the metadata schema (in the context of the measurement process), ii) how to address distinct parts within metadata records, iii) the workflow of the measurement, iv) a common and powerful interface for the individual metrics, and v) interoperability with Java and REST APIs.|10.1145/3078081.3078109|https://doi.org/10.1145/3078081.3078109|New York, NY, USA|Association for Computing Machinery|9781450352659|2017|Towards an Extensible Measurement of Metadata Quality|Kir\'{a}ly, P\'{e}ter|inproceedings|10.1145/3078081.3078109|||||||||||||||||||||||||||||1365277513|42
DSMM'19|Amsterdam, Netherlands|Entity Matching, RDF, Record Linkage, Company data|6||Proceedings of the 5th Workshop on Data Science for Macro-Modeling with Financial and Economic Datasets|In the business environment, knowledge of company data is essential for a variety of tasks. The European funded project euBusinessGraph enables the establishment of a company data platform where data providers and consumers can publish and access company data. The core of the platform is the semantic data model that is the conceptual representation of company data in a common way so that it is easier to share and interlink company data. In this paper we show how the unified model and Grafterizer, a tool for manipulating and transforming raw data into Linked Data, support the linking challenge proposed in FEIII 2019. Results show that geographical enrichment of RDF data supports the interlinking process between company entities in different datasets.|10.1145/3336499.3338012|https://doi.org/10.1145/3336499.3338012|New York, NY, USA|Association for Computing Machinery|9781450368230|2019|Modelling and Linking Company Data in the EuBusinessGraph Platform|Maurino, Andrea and Rula, Anisa and von, Bj\o{}rn Marius and Gomez, Mauricio Soto and Elves\ae{}ter, Brian and Roman, Dumitru|inproceedings|10.1145/3336499.3338012|12||||||||||||||||||||||||||||1368124815|42
||Energy internet, Renewable energy, Smart grid, Big data analytics, Machine learning, Predictive models||100363||The application of big data in the energy sector is considered as one of the main elements of Energy Internet. Crucial and promising challenges exist especially with the integration of renewable energy sources and smart grids. The ability to collect data and to properly use it for better decision-making is a key feature; in this work, the benefits and challenges of implementing big data analytics for renewable energy power stations are addressed. A framework was developed for the potential implementation of big data analytics for smart grids and renewable energy power utilities. A five-step approach is proposed for predicting the smart grid stability by using five different machine learning methods. Data from a decentralized smart grid data system consisting of 60,000 instances and 12 attributes was used to predict the stability of the system through three different machine learning methods. The results of fitting the penalized linear regression model show an accuracy of 96% for the model implemented using 70% of the data as a training set. Using the random forest tree model has shown 84% accuracy, and the decision tree model has shown 78% accuracy. Both the convolutional neural network model and the gradient boosted decision tree model yielded 87% for the classification model. The main limitation of this work is that the amount of data available in the dataset is considered relatively small for big data analytics; however the cloud computing and real-time event analysis provided was suitable for big data analytics framework. Future research should include bigger datasets with variety of renewable energy sources and demand across more countries.|https://doi.org/10.1016/j.mlwa.2022.100363|https://www.sciencedirect.com/science/article/pii/S2666827022000597||||2022|Renewable energy management in smart grids by using big data analytics and machine learning|Noha Mostafa and Haitham Saad Mohamed Ramadan and Omar Elfarouk|article|MOSTAFA2022100363|||Machine Learning with Applications|26668270||9|||||||||||||||||||||||1368198036|139872947
||IoT, Data visualization, Business intelligence, Corporate finance||102736||Business intelligence (BI) incorporates business research, data mining, data visualization, data tools,infrastructure, and best practices to help businesses make more data-driven choices.Business intelligence's challenging characteristics include data breaches, difficulty in analyzing different data sources, and poor data quality is consideredessential factors. In this paper, IoT-based Efficient Data Visualization Framework (IoT- EDVF) has been proposed to strengthen leaks' risk, analyze multiple data sources, and data quality management for business intelligence in corporate finance.Corporate analytics management is introduced to enhance the data analysis system's risk, and the complexity of different sources can allow accessing Business Intelligence. Financial risk analysis is implemented to improve data quality management initiative helps use main metrics of success, which are essential to the individual needs and objectives. The statistical outcomes of the simulation analysis show the increasedperformance with a lower delay response of 5ms and improved revenue analysis with the improvement of 29.42% over existing models proving the proposed framework's reliability.|https://doi.org/10.1016/j.ipm.2021.102736|https://www.sciencedirect.com/science/article/pii/S0306457321002181||||2022|IoT data visualization for business intelligence in corporate finance|Cuili Shao and Yonggang Yang and Sapna Juneja and Tamizharasi GSeetharam|article|SHAO2022102736|||Information Processing & Management|03064573|1|59|||||||||||||||||||||||1372661345|1769516999
||smart grid, Privacy|28|||The Internet of Things (IoT) promises many advantages in the control and monitoring of physical systems from both efficacy and efficiency perspectives. However, in the wrong hands, the data might pose a privacy threat. In this article, we consider the tradeoff between the operational value of data collected in the IoT and the privacy of consumers. We present a general framework for quantifying this tradeoff in the IoT, and focus on a smart grid application for a proof of concept. In particular, we analyze the tradeoff between smart grid operations and how often data are collected by considering a realistic direct-load control example using thermostatically controlled loads, and we give simulation results to show how its performance degrades as the sampling frequency decreases. Additionally, we introduce a new privacy metric, which we call inferential privacy. This privacy metric assumes a strong adversary model and provides an upper bound on the adversary’s ability to infer a private parameter, independent of the algorithm he uses. Combining these two results allows us to directly consider the tradeoff between better operational performance and consumer privacy.|10.1145/3185511|https://doi.org/10.1145/3185511|New York, NY, USA|Association for Computing Machinery||2018|Quantifying the Utility--Privacy Tradeoff in the Internet of Things|Dong, Roy and Ratliff, Lillian J. and C\'{a}rdenas, Alvaro A. and Ohlsson, Henrik and Sastry, S. Shankar|article|10.1145/3185511|8|may|ACM Trans. Cyber-Phys. Syst.|2378962X|2|2|April 2018||||||||||||||||||||||1372874758|1535435511
||Web services;Tagging;Search engines;Clustering algorithms;Task analysis;Feature extraction;Web service;WSDL documents clustering;co-clustering;tag recommendation||168981-168993||We propose a novel Web services clustering framework by considering the word distribution of WSDL documents and tags. Typically, tags are annotated to Web services by users for organization. In this paper, four strategies are proposed to integrate the tagging data and WSDL documents in the process of service clustering. Tagging data is inherently uncontrolled, ambiguous, and overly personalized. Two tag recommendation approaches are proposed to improve the tagging data quality and service clustering performance. Comprehensive experiments demonstrate the effectiveness of the proposed framework using a real-world dataset.|10.1109/ACCESS.2019.2950355|||||2019|Exploiting User Tagging for Web Service Co-Clustering|Liang, Tingting and Chen, Yishan and Gao, Wei and Chen, Ming and Zheng, Meilian and Wu, Jian|article|8892516|||IEEE Access|21693536||7|||||21100374601|0,587|Q1|127|18036|24267|771081|116691|24200|4,48|42,75|United States|Northern America|2013-2020|Computer Science (miscellaneous) (Q1); Engineering (miscellaneous) (Q1); Materials Science (miscellaneous) (Q2)|105,968|3.367|0.15396|1372911403|1905633267
|||||Data Cleaning|Data quality is one of the most important problems in data management, since dirty data often leads to inaccurate data analytics results and incorrect business decisions. Poor data across businesses and the U.S. government are reported to cost trillions of dollars a year. Multiple surveys show that dirty data is the most common barrier faced by data scientists. Not surprisingly, developing effective and efficient data cleaning solutions is challenging and is rife with deep theoretical and engineering problems.This book is about data cleaning, which is used to refer to all kinds of tasks and activities to detect and repair errors in the data. Rather than focus on a particular data cleaning task, we give an overview of the endto- end data cleaning process, describing various error detection and repair methods, and attempt to anchor these proposals with multiple taxonomies and views. Specifically, we cover four of the most common and important data cleaning tasks, namely, outlier detection, data transformation, error repair (including imputing missing values), and data deduplication. Furthermore, due to the increasing popularity and applicability of machine learning techniques, we include a chapter that specifically explores how machine learning techniques are used for data cleaning, and how data cleaning is used to improve machine learning models.This book is intended to serve as a useful reference for researchers and practitioners who are interested in the area of data quality and data cleaning. It can also be used as a textbook for a graduate course. Although we aim at covering state-of-the-art algorithms and techniques, we recognize that data cleaning is still an active field of research and therefore provide future directions of research whenever appropriate.||https://doi.org/10.1145/3310205.3310209|New York, NY, USA|Association for Computing Machinery|9781450371520|2019|Data Deduplication||inbook|10.1145/3310205.3310209|||||||||||||||||||||||||||||1373075563|42
||Indexes;Mobile communication;Mobile computing;Big data;Internet;Delays;big data;terminal;QoE;variable coefficient method||241-245|2016 16th International Symposium on Communications and Information Technologies (ISCIT)|In this paper, we proposes a method to analyze and evaluate the quality of experience (QoE) in mobile terminals using “big data”. The feature parameters of key quality indicator (KQI) are obtained from operators and quantized through the use of a scoring system. Then the scores of customer experience indicator (CEI) and QoE are calculated based on our proposed analytical model. In combination with the data of market operation, the terminal QoE evaluation scores contribute to offer effective suggestions on the promotion of mobile terminal.|10.1109/ISCIT.2016.7751629|||||2016|Mobile terminal quality of experience analysis based on big data|Li, Mingxin and Wei, Heng and Liao, Hongxi|inproceedings|7751629||Sep.|||||||||||||||||||||||||||1375889101|42
||External data, Internal data, Abnormality, Missing data, Outliers, Randomness, Multivariate analysis, Data integration, Clustering||29-38||Typically, only a smaller portion of the monitorable operational data (e.g. from sensors and environment) from Offshore Support Vessels (OSVs) are used at present. Operational data, in addition to equipment performance data, design and construction data, creates large volumes of data with high veracity and variety. In most cases, such data richness is not well understood as to how to utilize it better during design and operation. It is, very often, too time consuming and resource demanding to estimate the final operational performance of vessel concept design solution in early design by applying simulations and model tests. This paper argues that there is a significant potential to integrate ship lifecycle data from different phases of its operation in large data repository for deliberate aims and evaluations. It is disputed discretely in the paper, evaluating performance of real similar type vessels during early stages of the design process, helps substantially improving and fine-tuning the performance criterion of the next generations of vessel design solutions. Producing learning from such a ship lifecycle data repository to find useful patterns and relationships among design parameters and existing fleet real performance data, requires the implementation of modern data mining techniques, such as big data and clustering concepts, which are introduced and applied in this paper. The analytics model introduced suggests and reviews all relevant steps of data knowledge discovery, including pre-processing (integration, feature selection and cleaning), processing (data analyzing) and post processing (evaluating and validating results) in this context.|https://doi.org/10.1016/j.jii.2018.02.002|https://www.sciencedirect.com/science/article/pii/S2452414X17300869||||2018|Improving early OSV design robustness by applying ‘Multivariate Big Data Analytics’ on a ship's life cycle|Niki Sadat Abbasian and Afshin Salajegheh and Henrique Gaspar and Per Olaf Brett|article|ABBASIAN201829|||Journal of Industrial Information Integration|2452414X||10|||||21100787106|2,042|Q1|24|28|86|2152|1361|83|12,26|76,86|Netherlands|Western Europe|2016-2020|Industrial and Manufacturing Engineering (Q1); Information Systems and Management (Q1)|1,149|10.063|0.00148|1378796761|121356201
||Intelligent manufacturing, Artificial intelligence, Industrial big data, Big data-driven technology, Decision-making||101021||Under the trend of economic globalization, intelligent manufacturing has attracted a lot of attention from academic and industry. Related enabling technologies make manufacturing industry more intelligent. As one of the key technologies in artificial intelligence, big data driven analysis improves the market competitiveness of manufacturing industry by mining the hidden knowledge value and potential ability of industrial big data, and helps enterprise leaders make wise decisions in various complex manufacturing environments. This paper provides a theoretical analysis basis for big data-driven technology to guide decision-making in intelligent manufacturing, fully demonstrating the practicability of big data-driven technology in the intelligent manufacturing industry, including key advantages and internal motivation. A conceptual framework of intelligent decision-making based on industrial big data-driven technology is proposed in this study, which provides valuable insights and thoughts for the severe challenges and future research directions in this field.|https://doi.org/10.1016/j.jestch.2021.06.001|https://www.sciencedirect.com/science/article/pii/S2215098621001336||||2022|A review of industrial big data for decision making in intelligent manufacturing|Chunquan Li and Yaqiong Chen and Yuling Shang|article|LI2022101021|||Engineering Science and Technology, an International Journal|22150986||29|||||21100806003|0,803|Q1|50|132|408|5391|2346|407|5,09|40,84|Netherlands|Western Europe|2014-2020|Civil and Structural Engineering (Q1); Computer Networks and Communications (Q1); Electronic, Optical and Magnetic Materials (Q1); Fluid Flow and Transfer Processes (Q1); Hardware and Architecture (Q1); Mechanical Engineering (Q1); Metals and Alloys (Q1); Biomaterials (Q2)||||1379280878|1203331061
||Biological system modeling;Economic indicators;Data models;Educational institutions;Accuracy;Mathematical model;Analytical models;data quality;gray interval number;ecological civilization evaluation||1-5|2014 11th International Conference on Service Systems and Service Management (ICSSSM)|In the age of Big Data, we must do best to economically extract value from very large volumes of a wide variety of statistics. However, because of subjective and objective reasons, it is becoming increasing clear that much data is of poor quality, which has serious effects on the research results. With the analysis on the cause and process of the low quality data, this paper introduces the concept of gray system and proposes a data-gray-correction model, which could change the original data into gray interval number and reduce the influence from the former. Assessing the quality of data with classical econometric model and correcting the data by error correction model, then find the reasonable range of the real data and instead the crisp data by interval number, which contain much more information. An example is provided to illustrate the ecological civilization evaluation process under gray interval number.|10.1109/ICSSSM.2014.6943406|||||2014|Research on data gray correction model based on grey interval number — A case study of Chinese ecological civilization evaluation|Zhi, Yanling and Liu, Gang and Wang, Huimin|inproceedings|6943406||June||21611904|||||||||||||||||||||||||1380138899|1477259195
||decision support systems, clinical, electronic health records, integrated advanced information management systems, medical informatics||350-356||Advances in computer technology, patient monitoring systems, and electronic health record systems have enabled rapid accumulation of patient data in electronic form (i.e. big data). Organizations such as the Anesthesia Quality Institute and Multicenter Perioperative Outcomes Group have spearheaded large-scale efforts to collect anaesthesia big data for outcomes research and quality improvement. Analytics—the systematic use of data combined with quantitative and qualitative analysis to make decisions—can be applied to big data for quality and performance improvements, such as predictive risk assessment, clinical decision support, and resource management. Visual analytics is the science of analytical reasoning facilitated by interactive visual interfaces, and it can facilitate performance of cognitive activities involving big data. Ongoing integration of big data and analytics within anaesthesia and health care will increase demand for anaesthesia professionals who are well versed in both the medical and the information sciences.|https://doi.org/10.1093/bja/aeu552|https://www.sciencedirect.com/science/article/pii/S0007091217311479||||2015|Big data and visual analytics in anaesthesia and health care†|A.F. Simpao and L.M. Ahumada and M.A. Rehman|article|SIMPAO2015350|||British Journal of Anaesthesia|00070912|3|115|||||21858|2,589|Q1|181|486|1260|12638|5321|684|4,33|26,00|United Kingdom|Western Europe|1923-2020|Anesthesiology and Pain Medicine (Q1)|27,510|9.166|0.02901|1381308474|728858816
WSSE 2021|Xiamen, China|Blockchain, Collaborative Production, Complex product assembly|5|205–209|2021 The 3rd World Symposium on Software Engineering|The collaborative development model of complex products brings the challenge to the data interaction management. There are many manufacturers and suppliers involved in the whole life cycle of the assembly process, which makes it difficult to ensure the data security and traceability. The Hyperledger-Fabric architecture in blockchain technology has modular design, pluggable architecture, complete authority control and security, which can well solve the data security and traceability management in the collaborative development of complex products. Therefore, this paper proposes a framework based on the Hyperledger-Fabric architecture of blockchain for the whole life cycle data management of complex products. We also demonstrate the effectiveness of our proposed new framework integrating blockchain technology through the case of quality data control during the aircraft final assembly collaborative process.|10.1145/3488838.3488873|https://doi.org/10.1145/3488838.3488873|New York, NY, USA|Association for Computing Machinery|9781450384094|2022|Blockchain-Based Data Control for Complex Product Assembly Collaboration Process|Cai, Hongxia and Tan, Qiqi|inproceedings|10.1145/3488838.3488873|||||||||||||||||||||||||||||1383134140|42
||Sensors;Calibration;Internet of Things;Data integration;Conferences;Interoperability;Standards;Internet of Things;Big Sensed Data;Next Generation Networks;Quality of Data;Quality of Information||207-208|2017 13th International Conference on Distributed Computing in Sensor Systems (DCOSS)|Internet of Things (IoT) systems are inherently built on data gathered from heterogeneous sources. In the quest to gather more data for better analytics, many IoT systems are instigating significant challenges. First, the sheer volume and velocity of data generated by IoT systems are burdening our networking infrastructure, especially at the edge. The mobility and intermittent connectivity of edge IoT nodes are further hampering real-time access and reporting of IoT data. As we attempt to synergize IoT systems to leverage resource discovery and remedy some of these challenges, the rising challenges of Quality of Information (QoI) and Quality of Resource (QoR) calibration, render many IoT interoperability attempts far-fetched. We survey a number of challenges in realizing IoT interoperability, and advocate for a uniform view of data management in IoT systems. We delve into three planes that encompass Big Sensed Data (BSD) research directions, presenting a building block for future research efforts in IoT data management.|10.1109/DCOSS.2017.35|||||2017|Big Sensed Data Challenges in the Internet of Things|Hassanein, Hossam S. and Oteafy, Sharief M. A.|inproceedings|8271966||June||23252944|||||||||||||||||||||||||1384056286|1898617345
||Simulation, Supply Chain, Big Data, Data issues, Industry 4.0||132-139||Supply Chains (SCs) are complex and dynamic networks, where certain events may cause severe problems. To avoid them, simulation can be used, allowing the uncertainty of these systems to be considered. Furthermore, the data that is generated at increasingly high volumes, velocities and varieties by relevant data sources allow, on one hand, the simulation model to capture all the relevant elements. While developing such solution, due to the inherent use of simulation, several data issues were identified and bypassed, so that the incorporated elements comprise a coherent SC simulation model. Thus, the purpose of this paper is to present the main issues that were faced, and discuss how these were bypassed, while working on a SC simulation model in a Big Data context and using real industrial data from an automotive electronics SC. This paper highlights the role of simulation in this task, since it worked as a semantic validator of the data. Moreover, this paper also presents the results that can be obtained from the developed model.|https://doi.org/10.1016/j.promfg.2020.02.033|https://www.sciencedirect.com/science/article/pii/S2351978920305825||||2020|Bypassing Data Issues of a Supply Chain Simulation Model in a Big Data Context|António A.C. Vieira and Luís Dias and Maribel Y. Santos and Guilherme A.B. Pereira and José Oliveira|article|VIEIRA2020132|||Procedia Manufacturing|23519789||42||International Conference on Industry 4.0 and Smart Manufacturing (ISM 2019)|||21100792109|0,504|Q2|43|1390|3628|27760|8346|3572|1,79|19,97|Netherlands|Western Europe|2015-2020|Artificial Intelligence (Q2); Industrial and Manufacturing Engineering (Q2)||||1385891163|896540749
Progress in Molecular Biology and Translational Science||p-Health, Data integration, Clinical informatics, Genomics, Machine learning and artificial intelligence, Biomedical big data analytics||1-37|Precision Medicine|Achieving predictive, precise, participatory, preventive, and personalized health (abbreviated as p-Health) requires comprehensive evaluations of an individual's conditions captured by various measurement technologies. Since the 1950s, analysis of care providers' and physicians' notes and measurement data by computers to improve healthcare delivery has been termed clinical informatics. Since the 2010s, wide adoptions of Electronic Health Records (EHRs) have greatly improved clinical informatics development with fast growing pervasive wearable technologies that continuously capture the human physiological profile in-clinic (EHRs) and out-of-clinic (PHRs or Personal Health Records) to bolster mobile health (mHealth). In addition, after the Human Genome Project in the 1990s, medical genomics has emerged to capture the high-throughput molecular profile of a person. As a result, integrated data analytics is becoming one of the fast-growing areas under Biomedical Big Data to improve human healthcare outcomes. In this chapter, we first introduce the scope of data integration and review applications, data sources, and tools for clinical informatics and medical genomics. We then describe the data integration analytics at the raw data level, feature level, and decision level with case studies, and the opportunity for research and translation using advanced artificial intelligence (AI), such as deep learning. Lastly, we summarize the opportunities in biomedical big data integration that can reshape healthcare toward p-health.|https://doi.org/10.1016/bs.pmbts.2022.05.002|https://www.sciencedirect.com/science/article/pii/S187711732200062X||Academic Press||2022|Chapter One - Introduction of medical genomics and clinical informatics integration for p-Health care|Li Tong and Hang Wu and May D. Wang and Geoffrey Wang|incollection|TONG20221||||18771173|1|190||||David B. Teplow|||||||||||||||||||1386008903|1841513576
||Intelligent data, tourism industry|20|||Data analysis involves the deployment of sophisticated approaches from data mining methods, information theory, and artificial intelligence in various fields like tourism, hospitality, and so on for the extraction of knowledge from the gathered and preprocessed data. In tourism, pattern analysis or data analysis using classification is significant for finding the patterns that represent new and potentially useful information or knowledge about the destination and other data. Several data mining techniques are introduced for the classification of data or patterns. However, overfitting, less accuracy, local minima, sensitive to noise are the drawbacks in some existing data mining classification methods. To overcome these challenges, Support vector machine with Red deer optimization (SVM-RDO) based data mining strategy is proposed in this article. Extended Kalman filter (EKF) is utilized in the first phase, i.e., data cleaning to remove the noise and missing values from the input data. Mantaray foraging algorithm (MaFA) is used in the data selection phase, in which the significant data are selected for the further process to reduce the computational complexity. The final phase is the classification, in which SVM-RDO is proposed to access the useful pattern from the selected data. PYTHON is the implementation tool used for the experiment of the proposed model. The experimental analysis is done to show the efficacy of the proposed work. From the experimental results, the proposed SVM-RDO achieved better accuracy, precision, recall, and F1 score than the existing methods for the tourism dataset. Thus, it is showed the effectiveness of the proposed SVM-RDO for pattern analysis.|10.1145/3494566|https://doi.org/10.1145/3494566|New York, NY, USA|Association for Computing Machinery||2022|Intelligent Data Analysis Using Optimized Support Vector Machine Based Data Mining Approach for Tourism Industry|Sharma, Ms Promila and Meena, Uma and Sharma, Girish Kumar|article|10.1145/3494566|94|mar|ACM Trans. Knowl. Discov. Data|15564681|5|16|October 2022||||5800173377|0,728|Q1|59|71|169|3729|816|168|4,54|52,52|United States|Northern America|2007-2020|Computer Science (miscellaneous) (Q1)|1,740|2.713|0.00224|1387156068|1302859451
ICEGOV 2020|Athens, Greece|data justice, Data production, Open government data|9|485–493|Proceedings of the 13th International Conference on Theory and Practice of Electronic Governance|The increased digitisation of government information systems, as well as emerging data analytics and visualization techniques, have led lately to a surge in interest in the role of data in governance and development. The latest buzzwords in governance now include data-driven governance, data-for-development, evidence-based policy-making, and open government data. However, not much attention has been paid to understand the process of the production of data in government information systems. Our findings are based on six months of an ethnographic study of India's livelihood program- Mahatma Gandhi National Rural Employment Guarantee Act (MGNREGA) in a rural district of Karnataka. We argue that the practice of data production is carefully managed and controlled by local power elites providing an illusion of transparency in a digital information system. Understanding and recognizing the political nature of data production can help in better evaluation of development interventions, policy-making as well as in the design of more just information systems.|10.1145/3428502.3428576|https://doi.org/10.1145/3428502.3428576|New York, NY, USA|Association for Computing Machinery|9781450376747|2020|Who Drives Data in Data-Driven Governance? The Politics of Data Production in India's Livelihood Program|Hanbal, Rajesh Dinesh and Prakash, Amit and Srinivasan, Janaki|inproceedings|10.1145/3428502.3428576|||||||||||||||||||||||||||||1387754793|42
||Big data, Microgrid||100945||The prospering Big data era is emerging in the power grid. Multiple world-wide studies are emphasizing the big data applications in the microgrid due to the huge amount of produced data. Big data analytics can impact the design and applications towards safer, better, more profitable, and effective power grid. This paper presents the recognition and challenges of the big data and the microgrid. The construction of big data analytics is introduced. The data sources, big data opportunities, and enhancement areas in the microgrid like stability improvement, asset management, renewable energy prediction, and decision-making support are summarized. Diverse case studies are presented including different planning, operation control, decision making, load forecasting, data attacks detection, and maintenance aspects of the microgrid. Finally, the open challenges of big data in the microgrid are discussed.|https://doi.org/10.1016/j.aei.2019.100945|https://www.sciencedirect.com/science/article/pii/S147403461830702X||||2019|State of the art in big data applications in microgrid: A review|Karim Moharm|article|MOHARM2019100945|||Advanced Engineering Informatics|14740346||42|||||23640|1,107|Q1|81|146|295|8184|1973|289|6,41|56,05|United Kingdom|Western Europe|2002-2020|Artificial Intelligence (Q1); Information Systems (Q1)|4,432|5.603|0.00428|1388873589|876312848
MK Series on Business Intelligence||Hadoop, RDBMS, NoSQL, transformation, architecture||257-265|Data Warehousing in the Age of Big Data|This chapter discusses the real-life implementation of the next-generation platform by three different companies and the direction they each have chosen from a technology and architecture perspective.|https://doi.org/10.1016/B978-0-12-405891-0.00014-3|https://www.sciencedirect.com/science/article/pii/B9780124058910000143|Boston|Morgan Kaufmann|978-0-12-405891-0|2013|Chapter 14 - Implementing the Big Data – Data Warehouse – Real-Life Situations|Krish Krishnan|incollection|KRISHNAN2013257||||||||||Krish Krishnan|||||||||||||||||||1392101674|42
||Ship energy efficiency, Speed optimization, Big data analysis, Parallel k-means algorithm, Hadoop||457-468||Energy efficiency of inland ships is significantly influenced by navigational environment, including wind speed and direction as well as water depth and speed. The complexity of the inland navigational environment makes it rather difficult to determine the optimal speeds under different environmental conditions to achieve the best energy efficiency. Route division according to the characteristics of these environmental factors could provide a good solution for the optimization of ship engine speed under different navigational environments. In this paper, the distributed parallel k-means clustering algorithm is adopted to achieve an elaborate route division by analyzing the corresponding environmental factors based on a self-developed big data analytics platform. Subsequently, a ship energy efficiency optimization model considering multiple environmental factors is established through analyzing the energy transfer among hull, propeller and main engine. Then, decisions are made concerning the optimal engine speeds in different segments along the path. Finally, a case study on the Yangtze River is performed to validate the present optimization method. The results show that the proposed method can effectively reduce energy consumption and CO2 emissions of ships.|https://doi.org/10.1016/j.oceaneng.2018.08.050|https://www.sciencedirect.com/science/article/pii/S0029801818316421||||2018|Energy-efficient shipping: An application of big data analysis for optimizing engine speed of inland ships considering multiple environmental factors|Xinping Yan and Kai Wang and Yupeng Yuan and Xiaoli Jiang and Rudy R. Negenborn|article|YAN2018457|||Ocean Engineering|00298018||169|||||28339|1,321|Q1|100|1216|2475|51177|10979|2469|4,31|42,09|United Kingdom|Western Europe|1968-2020|Environmental Engineering (Q1); Ocean Engineering (Q1)|23,463|3.795|0.0247|1392639589|1406779183
CHI '21|Yokohama, Japan|digital contact tracing, privacy, COVID-19|22||Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems|The COVID-19 pandemic has fueled the development of smartphone applications to assist disease management. Many “corona apps” require widespread adoption to be effective, which has sparked public debates about the privacy, security, and societal implications of government-backed health applications. We conducted a representative online study in Germany (n = 1003), the US (n = 1003), and China (n = 1019) to investigate user acceptance of corona apps, using a vignette design based on the contextual integrity framework. We explored apps for contact tracing, symptom checks, quarantine enforcement, health certificates, and mere information. Our results provide insights into data processing practices that foster adoption and reveal significant differences between countries, with user acceptance being highest in China and lowest in the US. Chinese participants prefer the collection of personalized data, while German and US participants favor anonymity. Across countries, contact tracing is viewed more positively than quarantine enforcement, and technical malfunctions negatively impact user acceptance.|10.1145/3411764.3445517|https://doi.org/10.1145/3411764.3445517|New York, NY, USA|Association for Computing Machinery|9781450380966|2021|Apps Against the Spread: Privacy Implications and User Acceptance of COVID-19-Related Smartphone Apps on Three Continents|"\"Utz, Christine and Becker, Steffen and Schnitzler, Theodor and Farke, Florian M. and Herbert, Franziska and Schaewitz, Leonie and Degeling, Martin and D\"\"{u}rmuth, Markus\""|inproceedings|10.1145/3411764.3445517|70||||||||||||||||||||||||||||1392784780|42
PMBS '15|Austin, Texas|performance, accelerators, benchmarking|12||Proceedings of the 6th International Workshop on Performance Modeling, Benchmarking, and Simulation of High Performance Computing Systems|"\"The Scalable HeterOgeneous Computing (SHOC) benchmark suite was released in 2010 as a tool to evaluate the stability and performance of emerging heterogeneous architectures and to compare different programming models for compute devices used in those architectures. Since then, high-performance computing (HPC) system architectures have increasingly incorporated both discrete and fused multi-core and many-core processors. The TOP500 list illustrates this trend: heterogeneous systems grew from a 3.4% to 18.0% share of the list between June 2010 and June 2015. Not only are there more heterogeneous systems on the TOP500 list today, those machines are responsible for a disproportionately large percentage of list's aggregate performance: as of June 2015, the performance share for heterogeneous systems has grown to 33.7%.Part of this shift toward heterogeneous architectures has stemmed from new products in the hardware accelerator market, such as Intel's Xeon Phi coprocessor, and improvements in the approaches for programming such accelerators. Existing approaches such as CUDA and OpenCL have become more powerful and easy to use, and directive-based programming models such as OpenACC, OpenMP 4.0, and Intel's Language Extensions for Offload (LEO) are rapidly gaining user acceptance. The benefits of these hardware and software advances are not limited to HPC; other problem domains such as \"\"big data\"\" are reaping the rewards also.The original SHOC benchmarks had adequate support for CUDA and OpenCL for graphics processing units, but did not support more recent programming models and devices. We extended SHOC to support evaluation of recent heterogeneous architectures and programming models such as OpenACC and LEO, and we added new benchmarks to increase SHOC's application domain coverage. In this paper, we describe our modifications to the stock SHOC distribution and present several examples of using our augmented version of SHOC for evaluation of recent heterogeneous architectures and programming models.\""|10.1145/2832087.2832090|https://doi.org/10.1145/2832087.2832090|New York, NY, USA|Association for Computing Machinery|9781450340090|2015|Examining Recent Many-Core Architectures and Programming Models Using SHOC|Lopez, M. Graham and Young, Jeffrey and Meredith, Jeremy S. and Roth, Philip C. and Horton, Mitchel and Vetter, Jeffrey S.|inproceedings|10.1145/2832087.2832090|3||||||||||||||||||||||||||||1393252561|42
||Motion pictures;Data visualization;Visualization;Prediction algorithms;Market research;Arrays;Electronic commerce;recommended algorithm;Dataset;data processing;data visualization||946-952|2020 IEEE International Conference on Information Technology,Big Data and Artificial Intelligence (ICIBA)|Data is the core of information, and good data quality is a prerequisite for many data analysis. Data cleaning is to increase the fault tolerance rate by correcting the error value of detected data. This paper aims to solve the problem of data set processing and visualization in the recommendation algorithm, so as to better apply in the field of recommendation algorithm. The recommendation algorithm and data sets Movielens and IMDB are analyzed theoretically. First, data set A was processed from data reading and movie score calculation; Again, the IMDB is processed in four steps to make it more suitable for the recommendation algorithm field; Finally, the plot function is used to visualize the key information. experiment shows: The data set sorted out by the above methods can effectively improve the quality and availability of data and provide relevant basis for better application in the algorithm.|10.1109/ICIBA50161.2020.9276830|||||2020|Research on Film Data Preprocessing and Visualization|Zhang, Huaxin and Liu, Yu and Wang, Zituo and Li, Tiansong and Cao, Keyin|inproceedings|9276830||Nov||||1|||||||||||||||||||||||1394444445|42
||Data integrity;Big Data;Semantics;Industries;Bibliographies;Libraries;Production;Big Data;data quality;health data;data quality dimensions;latent semantic analysis||1-6|2018 International Conference on Advances in Big Data, Computing and Data Communication Systems (icABCD)|Big Data quality is a field which is emerging. Many authors nowadays agree that data quality is still very relevant, even for Big Data uses. However, there is a lack of frameworks or guidelines about how to carry out those big data quality initiatives. The starting point of any data quality work is to determine the properties of data quality, termed as data quality dimensions (DQDs). Even those dimensions lack precise rigour in terms of definition from existing literature. This current research aims to contribute towards identifying the most important DQDs for big data in the health industry. It is a continuation of a previous work, which already identified five most important DQDs, using a human judgement based technique known as inner hermeneutic cycle. To remove potential bias coming from the human judgement aspect, this research uses the same set of literature but applies a statistical technique known to extract knowledge from a set of documents known as latent semantic analysis. The results confirm only 2 similar most important DQDs, namely accuracy and completeness.|10.1109/ICABCD.2018.8465129|||||2018|Discovering Most Important Data Quality Dimensions Using Latent Semantic Analysis|Juddoo, Suraj and George, Carlisle|inproceedings|8465129||Aug|||||||||||||||||||||||||||1394589728|42
||Parallel data quality algorithms, Distributed system, Data quality management system, Multi-tasks scheduling, Big data||132-147||In the big data era, large amounts of data are under generation and accumulation in various industries. However, users usually feel hindered by the data quality issues when extracting values from the big data. Thus, data quality issues are gaining more and more attention from data quality management analysts. Cutting-edge solutions like data ETL, data cleaning, and data quality monitoring systems have many deficiencies in capability and efficiency, making it difficult to cope with complicated situations on big data. These problems inspire us to build SparkDQ, a generic distributed data quality management model and framework that provides a series of data quality detection and repair interfaces. Users can quickly build custom tasks of data quality computing for various needs by utilizing these interfaces. In addition, SparkDQ implements a set of algorithms that in a parallel manner with optimizations. These algorithms aim at various data quality goals. We also propose several system-level optimizations, including the job-level optimization with multi-task execution scheduling and the data-level optimization with data state caching. The experimental evaluation shows that the proposed distributed algorithms in SparkDQ run up to 12 times faster compared to the corresponding stand-alone serial and multi-thread algorithms. Compared with the cutting-edge distributed data quality solution Apache Griffin, SparkDQ has more features, and its execution time is only around half of Apache Griffin on average. SparkDQ achieves near-linear data and node scalability.|https://doi.org/10.1016/j.jpdc.2021.05.012|https://www.sciencedirect.com/science/article/pii/S0743731521001246||||2021|SparkDQ: Efficient generic big data quality management on distributed data-parallel computation|Rong Gu and Yang Qi and Tongyu Wu and Zhaokang Wang and Xiaolong Xu and Chunfeng Yuan and Yihua Huang|article|GU2021132|||Journal of Parallel and Distributed Computing|07437315||156|||||25621|0,638|Q1|87|169|592|7166|2473|568|4,72|42,40|United States|Northern America|1984-2021|Computer Networks and Communications (Q1); Hardware and Architecture (Q1); Artificial Intelligence (Q2); Software (Q2); Theoretical Computer Science (Q2)|4,371|3.734|0.00477|1394895984|1083163244
||Big Data, Security, Data protection, Privacy, Regulation, Fraud, Policing, Surveillance, Algorithmic accountability, the Netherlands||309-323||Big Data analytics in national security, law enforcement and the fight against fraud have the potential to reap great benefits for states, citizens and society but require extra safeguards to protect citizens' fundamental rights. This involves a crucial shift in emphasis from regulating Big Data collection to regulating the phases of analysis and use. In order to benefit from the use of Big Data analytics in the field of security, a framework has to be developed that adds new layers of protection for fundamental rights and safeguards against erroneous and malicious use. Additional regulation is needed at the levels of analysis and use, and the oversight regime is in need of strengthening. At the level of analysis – the algorithmic heart of Big Data processes – a duty of care should be introduced that is part of an internal audit and external review procedure. Big Data projects should also be subject to a sunset clause. At the level of use, profiles and (semi-) automated decision-making should be regulated more tightly. Moreover, the responsibility of the data processing party for accuracy of analysis – and decisions taken on its basis – should be anchored in legislation. The general and security-specific oversight functions should be strengthened in terms of technological expertise, access and resources. The possibilities for judicial review should be expanded to stimulate the development of case law.|https://doi.org/10.1016/j.clsr.2017.03.002|https://www.sciencedirect.com/science/article/pii/S0267364917300675||||2017|Big Data and security policies: Towards a framework for regulating the phases of analytics and use of Big Data|Dennis Broeders and Erik Schrijvers and Bart {van der Sloot} and Rosamunde {van Brakel} and Josta {de Hoog} and Ernst {Hirsch Ballin}|article|BROEDERS2017309|||Computer Law & Security Review|02673649|3|33|||||28888|0,815|Q1|38|66|242|1245|707|223|3,39|18,86|United Kingdom|Western Europe|1985-2020|Business, Management and Accounting (miscellaneous) (Q1); Computer Networks and Communications (Q1); Law (Q1)||||1399488472|1769124433
||private data, quality of private data, data trust, Responsible data science|9|||High-quality data is critical for effective data science. As the use of data science has grown, so too have concerns that individuals’ rights to privacy will be violated. This has led to the development of data protection regulations around the globe and the use of sophisticated anonymization techniques to protect privacy. Such measures make it more challenging for the data scientist to understand the data, exacerbating issues of data quality. Responsible data science aims to develop useful insights from the data while fully embracing these considerations.We pose the high-level problem in this article, “How can a data scientist develop the needed trust that private data has high quality?” We then identify a series of challenges for various data-centric communities and outline research questions for data quality and privacy researchers, which would need to be addressed to effectively answer the problem posed in this article.|10.1145/3287168|https://doi.org/10.1145/3287168|New York, NY, USA|Association for Computing Machinery||2019|Ensuring High-Quality Private Data for Responsible Data Science: Vision and Challenges|Srivastava, Divesh and Scannapieco, Monica and Redman, Thomas C.|article|10.1145/3287168|1|jan|J. Data and Information Quality|19361955|1|11|March 2019||||||||||||||||||||||1399921112|833754770
||Hazard assessment, Deep learning, Big data predictive analytics, Power infrastructure, Zero count data||100158||Although advanced machine learning algorithms are predominantly used for predicting outcomes in many fields, their utilisation in predicting incident outcome in construction safety is still relatively new. This study harnesses Big Data with Deep Learning to develop a robust safety management system by analysing unstructured incident datasets consisting of 168,574 data points from power transmission and distribution projects delivered across the UK from 2004 to 2016. This study compared Deep Learning performance with popular machine learning algorithms (support vector machine, random forests, multivariate adaptive regression splines, generalised linear model, and their ensembles) concerning lost time injury and risk assessment in power utility projects. Deep Learning gave the best prediction for safety outcomes with high skills (AUC = 0.95, R2 = 0.88, and multi-class ROC = 0.93), thus outperforming the other algorithms. The results from this study also highlight the significance of quantitative analysis of empirical data in safety science and contribute to an enhanced understanding of injury patterns using predictive analytics in conjunction with safety experts’ perspectives. Additionally, the results will enhance the skills of safety managers in the power utility domain to advance safety intervention efforts.|https://doi.org/10.1016/j.mlwa.2021.100158|https://www.sciencedirect.com/science/article/pii/S2666827021000797||||2021|Machine learning predictions for lost time injuries in power transmission and distribution projects|Ahmed O. Oyedele and Anuoluwapo O. Ajayi and Lukumon O. Oyedele|article|OYEDELE2021100158|||Machine Learning with Applications|26668270||6|||||||||||||||||||||||1403659517|139872947
UbiComp '18|Singapore, Singapore|Data interpretability, Air quality, Machine learning, Short-term happiness, Subjective well-being prediction|4|702–705|Proceedings of the 2018 ACM International Joint Conference and 2018 International Symposium on Pervasive and Ubiquitous Computing and Wearable Computers|Subjective well-being (SWB) refers to people's subjective evaluation of their own quality of life. Previous studies show that environmental pollution, such as air pollution, has generated significant negative impacts on one's SWB. However, such works are often constrained by the lack of appropriate representation of SWB specifically related to air quality. In this study, we develop UMeAir, which collects one's real-time SWB, specifically, one's momentary happiness at a given air quality, pre-processes input data and detects outliers via Isolation Forests, trains and selects the best model via Support Vector Machine and Random Forests, and predicts the momentary happiness towards any air quality one experienced. Unlike traditional representation of air quality by pollution concentration/Air Pollution Index, UMeAir intends to represent air quality in a more user-comprehensible way, by connecting the air quality experienced at a particular time and location with the corresponding momentary happiness perceived towards the air. The higher the momentary happiness, the better the air quality one experienced. Our work is the first attempt to predict momentary happiness towards air quality in real-time, with the development of the-first-of-its-kind UMeAir Happiness Index (HAPI) towards air quality via machine learning.|10.1145/3267305.3267694|https://doi.org/10.1145/3267305.3267694|New York, NY, USA|Association for Computing Machinery|9781450359665|2018|UMeAir: Predicting Momentary Happiness Towards Air Quality via Machine Learning|Han, Yang and Li, Victor O.K. and Lam, Jacqueline C.K. and Lu, Zhiyi|inproceedings|10.1145/3267305.3267694|||||||||||||||||||||||||||||1404180682|42
LAK22|Online, USA|co-design, higher education, educational quality, implementation strategy, academic analytics|11|381–391|LAK22: 12th International Learning Analytics and Knowledge Conference|Academic analytics focuses on collecting, analysing and visualising educational data to generate institutional insights and improve decision-making for academic purposes. However, challenges that arise from navigating a complex organisational structure when introducing analytics systems have called for the need to engage key stakeholders widely to cultivate a shared vision and ensure that implemented systems create desired value. This paper presents a study that takes co-design steps to identify design needs and strategic approaches for the adoption of academic analytics, which serves the purpose of enhancing the measurement of educational quality utilising institutional data. Through semi-structured interviews with 54 educational stakeholders at a large research university, we identified particular interest in measuring student engagement and the performance of courses and programmes. Based on the observed perceptions and concerns regarding data use to measure or evaluate these areas, implications for adoption strategy of academic analytics, such as leadership involvement, communication, and training, are discussed.|10.1145/3506860.3506939|https://doi.org/10.1145/3506860.3506939|New York, NY, USA|Association for Computing Machinery|9781450395731|2022|Charting Design Needs and Strategic Approaches for Academic Analytics Systems through Co-Design|Tsai, Yi-Shan and Singh, Shaveen and Rakovic, Mladen and Lim, Lisa-Angelique and Roychoudhury, Anushka and Gasevic, Dragan|inproceedings|10.1145/3506860.3506939|||||||||||||||||||||||||||||1404571078|42
||Machine learning algorithms;Merging;Clustering algorithms;Data structures;Partitioning algorithms;Sparks;Data mining;clustering analysis;data partition;k-dimensional tree;adaptive computing;Spark framework||249-256|2020 2nd International Conference on Machine Learning, Big Data and Business Intelligence (MLBDBI)|The existing parallel DBSCAN (density based spatial clustering of applications with noise) algorithm needs to determine the parameter settings manually, and the datasets will be repeatedly accessed in the process of data partitioning and data merging, which reduces the efficiency of the algorithm excuting. Therefore, this paper proposes a parallel adaptive DBSCAN algorithm based on k-dimensional tree partition. It divides the dataset into several balanced data partitions by using k-dimensional tree, and carries out parallel computing in spark distributed computing framework, thus increasing the concurrent processing ability of the algorithm program and improving the I/O access speed. In addition, the improved adaptive DBSCAN parameter method is applied to each data partition for clustering analysis to obtain local clusters, which solves the random problem of manual setting parameters in the clustering process, and ensures the data quality of clustering mining. At the same time of creating local clusters, this algorithm also puts the mapping relationship between data points and adjacent points into the HashMap data structure of the master node, and uses it to merge local clusters into whole clusters, which can reduce the time cost of data merging. The experimental results show that the proposed algorithm can save about 18% running time compared with RDD-DBSCAN algorithm without reducing the clustering quality. With the increase of the number of cluster nodes, the running efficiency of the algorithm can be further improved, so it is suitable for processing massive data clustering analysis.|10.1109/MLBDBI51377.2020.00053|||||2020|A Parallel Adaptive DBSCAN Algorithm Based on k-Dimensional Tree Partition|Lu, Xin and Wang, Yu and Yuan, Jiao and Wang, Xun and Fu, Kun and Yang, Ke|inproceedings|9361005||Oct|||||||||||||||||||||||||||1407009263|42
||CPSS, Big data, Cloud computing, Privacy preserving, Clustering||132-155||Clustering technique plays a critical role in data mining, and has received great success to solve application problems like community analysis, image retrieval, personalized recommendation, activity prediction, etc. This paper first reviews the traditional clustering and the emerging multiple clustering methods, respectively. Although the existing methods have superior performance on some small or certain datasets, they fall short when clustering is performed on CPSS big data because of the high cost of computation and storage. With the powerful cloud computing, this challenge can be effectively addressed, but it brings enormous threat to individual or company’s privacy. Currently, privacy preserving data mining has attracted widespread attention in academia. Compared to other reviews, this paper focuses on privacy preserving clustering technique, guiding a detailed overview and discussion. Specifically, we introduce a novel privacy-preserving tensor-based multiple clustering, propose a privacy-preserving tensor-based multiple clustering analytic and service framework, and give an illustrated case study on the public transportation dataset. Furthermore, we indicate the remaining challenges of privacy preserving clustering and discuss the future significant research in this area.|https://doi.org/10.1016/j.ins.2019.10.019|https://www.sciencedirect.com/science/article/pii/S0020025519309764||||2020|Privacy-preserving clustering for big data in cyber-physical-social systems: Survey and perspectives|Yaliang Zhao and Samwel K. Tarus and Laurence T. Yang and Jiayu Sun and Yunfei Ge and Jinke Wang|article|ZHAO2020132|||Information Sciences|00200255||515|||||15134|1,524|Q1|184|928|2184|40007|17554|2168|7,89|43,11|United States|Northern America|1968-2021|Artificial Intelligence (Q1); Computer Science Applications (Q1); Control and Systems Engineering (Q1); Information Systems and Management (Q1); Software (Q1); Theoretical Computer Science (Q1)|44,038|6.795|0.04908|1408072589|1633962588
||Data Quality, Quality Assessment, Collaborative Open Data, Wikipedia, Quality Model||1883-1892||Nowadays, the development of data sharing technologies allows to involve more people to collaboratively contribute knowledge on the Web. The shared knowledge is usually represented as Collaborative Open Data (COD), for example, Wikipedia is one of the well-known sources for COD. The Wikipedia articles can be written in different languages, updated in real time, and originated from a vast variety of editors. However, COD also bring different data quality problems such as data inconsistency and low data objectiveness due to the crowd-based and dynamic nature. These data quality problems such as biased information may lead to sentimental changes or social impacts. This paper therefore proposes a new measurement model to assess the quality of COD. In order to evaluate the proposed model, A preliminary experiment is conducted with a large scale of Wikipedia articles to validate the applicability and efficiency of this proposed quality model in the real-world scenario.|https://doi.org/10.1016/j.procs.2020.09.228|https://www.sciencedirect.com/science/article/pii/S187705092032130X||||2020|Developing the Quality Model for Collaborative Open Data|Mouzhi Ge and Włodzimierz Lewoniewski|article|GE20201883|||Procedia Computer Science|18770509||176||Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 24th International Conference KES2020|||19700182801|0,334|-|76|1907|6483|38511|13722|6359|2,09|20,19|Netherlands|Western Europe|2010-2020|Computer Science (miscellaneous)||||1414529901|2108686752
||||582-585|||https://doi.org/10.1053/j.jvca.2019.11.012|https://www.sciencedirect.com/science/article/pii/S1053077019311590||||2020|Making Sense of Big Data to Improve Perioperative Care: Learning Health Systems and the Multicenter Perioperative Outcomes Group|Michael R. Mathis and Timur Z. Dubovoy and Matthew D. Caldwell and Milo C. Engoren|article|MATHIS2020582|||Journal of Cardiothoracic and Vascular Anesthesia|10530770|3|34|||||23831|0,678|Q2|82|770|1616|22269|2501|1041|1,58|28,92|United Kingdom|Western Europe|1991-2020|Anesthesiology and Pain Medicine (Q2); Cardiology and Cardiovascular Medicine (Q2)|7,080|2.628|0.00838|1415759541|1680334070
||Delays;Quality of service;Prediction algorithms;Resource management;Estimation;Probability;Big Data;channel state prediction;reconfiguration;delivery pattern;martingales;VoD;spectrum efficiency;delay-QoS||1-7|ICC 2020 - 2020 IEEE International Conference on Communications (ICC)|With the help of big data analytics, predictive resource allocation (PRA) techniques for video on demand (VoD) have been recognized as promising methods to save time-frequency resources, for a number of VoD packets can be transmitted in good channels in advance to avoid the predicted transmissions in bad channel conditions. With the increasing demands on a fantastic user quality of experience, a smooth playback and a low start-up delay are of equal importance to the emerging VoD with high fidelity, which inevitably leads to a critical delay requirement of VoD packets. However, the issue of resource estimation with quality of service (QoS) requirements is still an unsolved puzzle in PRA. In this paper, we propose a martingales-based physical resource block (PRB) abstraction method, where the random characteristics of the service process are embedded in the minimum PRB consumption. Based on the method, a proactive QoS-guaranteed reconfiguration algorithm is developed to optimize the multi-user delivery pattern applied in the prediction window, aiming to maximize spectrum efficiency. In this algorithm, since the delay sensitivity of VoD content transmitted in advance is dulled compared with the original VoD stream, we divide the original VoD slice into two sub-slices and derive a three-dimensional delivery pattern. The gain of resource saving and the capability of QoS guarantee brought by the reconfiguration have been demonstrated by the simulation results.|10.1109/ICC40277.2020.9149070|||||2020|Proactive VoD delivery pattern reconfiguration based on temporal-spatial channel prediction|Yang, Wanting and Chi, Xuefen and Zhao, Linlin|inproceedings|9149070||June||19381883|||||||||||||||||||||||||1417574471|276554849
||edge computing, ubiquitous data processing, resource management, architecture design, Distributed cloud, fog computing|34|||While cloud computing has brought paradigm shifts to computing services, researchers and developers have also found some problems inherent to its nature such as bandwidth bottleneck, communication overhead, and location blindness. The concept of fog/edge computing is therefore coined to extend the services from the core in cloud data centers to the edge of the network. In recent years, many systems are proposed to better serve ubiquitous smart devices closer to the user. This article provides a complete and up-to-date review of edge-oriented computing systems by encapsulating relevant proposals on their architecture features, management approaches, and design objectives.|10.1145/3154815|https://doi.org/10.1145/3154815|New York, NY, USA|Association for Computing Machinery||2018|Edge-Oriented Computing Paradigms: A Survey on Architecture Design and System Management|Li, Chao and Xue, Yushu and Wang, Jing and Zhang, Weigong and Li, Tao|article|10.1145/3154815|39|apr|ACM Comput. Surv.|03600300|2|51|March 2019||||23038|2,079|Q1|163|112|365|15859|6978|365|19,16|141,60|United States|Northern America|1969-2020|Computer Science (miscellaneous) (Q1); Theoretical Computer Science (Q1)|10,790|10.282|0.01438|1419013903|1517405264
||Internet-of-things, smart manufacturing, big data, data analytics, statistical analysis, vibration, soft sensor, process monitoring||562-567||The emergence of the industrial Internet of Things (IoT) and ever advancing computing and communication technologies have fueled a new industrial revolution which is happening worldwide to make current manufacturing systems smarter, safer, and more efficient. Although many general frameworks have been proposed for IoT enabled systems for industrial application, there is limited literature on demonstrations or testbeds of such systems. In addition, there is a lack of systematic study on the characteristics of IoT sensors and data analytics challenges associated with IoT sensor data. This study is an attempt to help fill this gap by exploring the characteristics of IoT vibration sensors and show how IoT sensors and big data analytics can be used to develop real time monitoring frameworks.|https://doi.org/10.1016/j.ifacol.2019.06.122|https://www.sciencedirect.com/science/article/pii/S2405896319302083||||2019|An Internet-of-things Enabled Smart Manufacturing Testbed|Devarshi Shah and Jin Wang and Q. Peter He|article|SHAH2019562|||IFAC-PapersOnLine|24058963|1|52||12th IFAC Symposium on Dynamics and Control of Process Systems, including Biosystems DYCOPS 2019|||21100456158|0,308|Q3|72|115|8407|1913|9863|8351|1,13|16,63|Austria|Western Europe|2002-2019|Control and Systems Engineering (Q3)||||1419025775|676980763
https://doi.org/10.1016/j.imu.2018.05.001|https://www.sciencedirect.com/science/article/pii/S2352914818300844||||2018|An optimal big data workflow for biomedical image analysis|Aurelle {Tchagna Kouanou} and Daniel Tchiotsop and Romanic Kengne and Djoufack Tansaa Zephirin and Ngo Mouelas {Adele Armele} and René Tchinda|article|TCHAGNAKOUANOU201868|||Informatics in Medicine Unlocked|23529148||11||||||||||||||||||||||||||||||1420175481|42
|||11|9–19||In late May, 2008, a group of database researchers, architects, users and pundits met at the Claremont Resort in Berkeley, California to discuss the state of the research field and its impacts on practice. This was the seventh meeting of this sort in twenty years, and was distinguished by a broad consensus that we are at a turning point in the history of the field, due both to an explosion of data and usage scenarios, and to major shifts in computing hardware and platforms. Given these forces, we are at a time of opportunity for research impact, with an unusually large potential for influential results across computing, the sciences and society. This report details that discussion, and highlights the group's consensus view of new focus areas, including new database engine architectures, declarative programming languages, the interplay of structured and unstructured data, cloud data services, and mobile and virtual worlds. We also report on discussions of the community's growth, including suggestions for changes in community processes to move the research agenda forward, and to enhance impact on a broader audience.|10.1145/1462571.1462573|https://doi.org/10.1145/1462571.1462573|New York, NY, USA|Association for Computing Machinery||2008|The Claremont Report on Database Research|Agrawal, Rakesh and Ailamaki, Anastasia and Bernstein, Philip A. and Brewer, Eric A. and Carey, Michael J. and Chaudhuri, Surajit and Doan, AnHai and Florescu, Daniela and Franklin, Michael J. and Garcia-Molina, Hector and Gehrke, Johannes and Gruenwald, Le and Haas, Laura M. and Halevy, Alon Y. and Hellerstein, Joseph M. and Ioannidis, Yannis E. and Korth, Hank F. and Kossmann, Donald and Madden, Samuel and Magoulas, Roger and Ooi, Beng Chin and O'Reilly, Tim and Ramakrishnan, Raghu and Sarawagi, Sunita and Stonebraker, Michael and Szalay, Alexander S. and Weikum, Gerhard|article|10.1145/1462571.1462573||sep|SIGMOD Rec.|01635808|3|37|September 2008||||13622|0,372|Q2|142|39|81|808|127|57|1,67|20,72|United States|Northern America|1969, 1973-1978, 1981-2020|Information Systems (Q2); Software (Q2)|1,819|0.775|7.5E-4|1428094305|962972343
IDEAS '21|Montreal, QC, Canada|Stream IoT Data, Metadata, Zone-based, Technical Architecture, Data Lake|9|94–102|Proceedings of the 25th International Database Engineering &amp; Applications Symposium|Data lakes are supposed to enable analysts to perform more efficient and efficacious data analysis by crossing multiple existing data sources, processes and analyses. However, it is impossible to achieve that when a data lake does not have a metadata governance system that progressively capitalizes on all the performed analysis experiments. The objective of this paper is to have an easily accessible, reusable data lake that capitalizes on all user experiences. To meet this need, we propose an analysis-oriented metadata model for data lakes. This model includes the descriptive information of datasets and their attributes, as well as all metadata related to the machine learning analyzes performed on these datasets. To illustrate our metadata solution, we implemented a web application of data lake metadata management. This application allows users to find and use existing data, processes and analyses by searching relevant metadata stored in a NoSQL data store within the data lake. To demonstrate how to easily discover metadata with the application, we present two use cases, with real data, including datasets similarity detection and machine learning guidance.|10.1145/3472163.3472185|https://doi.org/10.1145/3472163.3472185|New York, NY, USA|Association for Computing Machinery|9781450389914|2021|A Zone-Based Data Lake Architecture for IoT, Small and Big Data|Zhao, Yan and Megdiche, Imen and Ravat, Franck and Dang, Vincent-nam|inproceedings|10.1145/3472163.3472185|||||||||||||||||||||||||||||1430636454|42
DOLAP '13|San Francisco, California, USA|big data, olap, data warehousing, big multidimensional data|4|67–70|Proceedings of the Sixteenth International Workshop on Data Warehousing and OLAP|In this paper, we highlight open problems and actual research trends in the field of Data Warehousing and OLAP over Big Data, an emerging term in Data Warehousing and OLAP research. We also derive several novel research directions arising in this field, and put emphasis on possible contributions to be achieved by future research efforts.|10.1145/2513190.2517828|https://doi.org/10.1145/2513190.2517828|New York, NY, USA|Association for Computing Machinery|9781450324120|2013|Data Warehousing and OLAP over Big Data: Current Challenges and Future Research Directions|Cuzzocrea, Alfredo and Bellatreche, Ladjel and Song, Il-Yeol|inproceedings|10.1145/2513190.2517828|||||||||||||||||||||||||||||1432598757|42
||Big Data, Railways, Maintenance, Transportation||457-467||Two concepts currently at the leading edge of todays information technology revolution are Analytics and Big Data. The public transportation industry has been at the forefront in utilizing and implementing Analytics and Big Data, from ridership forecasting to transit operations Rail transit systems have been especially involved with these IT concepts, and tend to be especially amenable to the advantages of Analytics and Big Data because they are generally closed systems that involve sophisticated processing of large volumes of data. The more that public transportation professionals and decision makers understand the role of Analytics and Big Data in their industry in perspective, the more effectively they will be able to utilize its promise. This paper gives an overview of Big Data technologies in context of transportation with specific to Railways. This paper also gives an insight on how the existing data modules from the transport authority combines Big Data and how can be incorporated in providing maintenance decision making.|https://doi.org/10.1016/j.procs.2015.07.323|https://www.sciencedirect.com/science/article/pii/S1877050915018268||||2015|Railway Assets: A Potential Domain for Big Data Analytics|Adithya Thaduri and Diego Galar and Uday Kumar|article|THADURI2015457|||Procedia Computer Science|18770509||53||INNS Conference on Big Data 2015 Program San Francisco, CA, USA 8-10 August 2015|||19700182801|0,334|-|76|1907|6483|38511|13722|6359|2,09|20,19|Netherlands|Western Europe|2010-2020|Computer Science (miscellaneous)||||1434477099|2108686752
||Trajectory;Urban areas;Buildings;Noise measurement;Mobile communication;Navigation;Education||978-989|2016 IEEE 32nd International Conference on Data Engineering (ICDE)|With the ubiquity of location based services and applications, large volume of mobility data has been generated routinely, usually from heterogeneous data sources, such as different GPS-embedded devices, mobile apps or location based service providers. In this paper, we investigate efficient ways of identifying users across such heterogeneous data sources. We present a MapReduce-based framework called Automatic User Identification (AUI) which is easy to deploy and can scale to very large data set. Our framework is based on a novel similarity measure called the signal based similarity (SIG) which measures the similarity of users' trajectories gathered from different data sources, typically with very different sampling rates and noise patterns. We conduct extensive experimental evaluations, which show that our framework outperforms the existing methods significantly. Our study on one hand provides an effective approach for the mobility data integration problem on large scale data sets, i.e., combining the mobility data sets from different sources in order to enhance the data quality. On the other hand, our study provides an in-depth investigation for the widely studied human mobility uniqueness problem under heterogeneous data sources.|10.1109/ICDE.2016.7498306|||||2016|Automatic user identification method across heterogeneous mobility data sources|Cao, Wei and Wu, Zhengwei and Wang, Dong and Li, Jian and Wu, Haishan|inproceedings|7498306||May|||||||||||||||||||||||||||1434672796|42
CSAE 2021|Sanya, China|Fire Department component;, Data governance, Data standard system|5||The 5th International Conference on Computer Science and Application Engineering|This paper analyzes data governance elements, models and frameworks, provides a clear plan for data governance for fire department. Using the method of literature research, network investigation and conclude data system of fire departments, the china domestic and foreign research status of data governance is reviewed. We build the framework of data governance for fire department, including Data resource directory system, Data technology support system and Data standardization system. This paper preliminarily forms the framework of data governance for fire department. This framework was applied to the fire information planning work. The results indicate that based on the status and characteristics of fire industry, the implementation of this framework is effective and feasible, and it is also the basis of standard fire control data governance in future.|10.1145/3487075.3487110|https://doi.org/10.1145/3487075.3487110|New York, NY, USA|Association for Computing Machinery|9781450389853|2021|Research on Data Governance Framework for Fire Department|An, Zhenpeng and Zhang, Di and Liang, Yunjie|inproceedings|10.1145/3487075.3487110|35||||||||||||||||||||||||||||1435205834|42
CHI '21|Yokohama, Japan|Public health, COVID-19, Personal informatics, Self-tracking, Contact tracing, Crisis informatics|15||Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems|Various contact tracing approaches have been applied to help contain the spread of COVID-19, with technology-based tracing and human tracing among the most widely adopted. However, governments and communities worldwide vary in their adoption of digital contact tracing, with many instead choosing the human approach. We investigate how people perceive the respective benefits and risks of human and digital contact tracing through a mixed-methods survey with 291 respondents from the United States. Participants perceived digital contact tracing as more beneficial for protecting privacy, providing convenience, and ensuring data accuracy, and felt that human contact tracing could help provide security, emotional reassurance, advice, and accessibility. We explore the role of self-tracking technologies in public health crisis situations, highlighting how designs must adapt to promote societal benefit rather than just self-understanding. We discuss how future digital contact tracing can better balance the benefits of human tracers and technology amidst the complex contact tracing process and context.|10.1145/3411764.3445669|https://doi.org/10.1145/3411764.3445669|New York, NY, USA|Association for Computing Machinery|9781450380966|2021|Comparing Perspectives Around Human and Technology Support for Contact Tracing|Lu, Xi and L. Reynolds, Tera and Jo, Eunkyung and Hong, Hwajung and Page, Xinru and Chen, Yunan and A. Epstein, Daniel|inproceedings|10.1145/3411764.3445669|200||||||||||||||||||||||||||||1435470795|42
||Data-driven, Deep learning, Physical layer, Wireless communications||589-597||Advanced technologies are required in future mobile wireless networks to support services with highly diverse requirements in terms of high data rate and reliability, low latency, and massive access. Deep Learning (DL), one of the most exciting developments in machine learning and big data, has recently shown great potential in the study of wireless communications. In this article, we provide a literature review on the applications of DL in the physical layer. First, we analyze the limitations of existing signal processing techniques in terms of model accuracy, global optimality, and computational scalability. Next, we provide a brief review of classical DL frameworks. Subsequently, we discuss recent DL-based physical layer technologies, including both DL-based signal processing modules and end-to-end systems. Deep neural networks are used to replace a single or several conventional functional modules, whereas the objective of the latter is to replace the entire transceiver structure. Lastly, we discuss the open issues and research directions of the DL-based physical layer in terms of model complexity, data quality, data representation, and algorithm reliability.|https://doi.org/10.1016/j.dcan.2021.09.014|https://www.sciencedirect.com/science/article/pii/S2352864821000742||||2021|Toward intelligent wireless communications: Deep learning - based physical layer technologies|Siqi Liu and Tianyu Wang and Shaowei Wang|article|LIU2021589|||Digital Communications and Networks|23528648|4|7|||||21100823476|1,082|Q1|26|77|105|3226|881|96|8,81|41,90|China|Asiatic Region|2015-2020|Communication (Q1); Computer Networks and Communications (Q1); Hardware and Architecture (Q1)|823|6.797|0.00138|1435887279|493706778
||Artificial intelligence, Autonomy, Big data, Black box, Ethics, Informed consent, Liability, Privacy, Safety||395-410|Machine Learning in Cardiovascular Medicine|As the technology of artificial intelligence (AI) grows in cardiovascular medicine, so do the ethical and legal challenges that come with it. Currently, the medical community is ill-informed of what these challenges entail, and policy and ethical guidelines are lacking. Physicians and policy makers should be informed of these issues to minimize harm and promote patient care. Three overarching themes relating to the data, the algorithms, and the results comprise the foundation of these challenges and will be discussed in this chapter. The introduction of big data raises concern for patient privacy and security, with issues of data quality and inconsistent medical records. There is also risk for biases in the algorithms that could worsen health disparities or skew results for financial gain. Finally, the archetypal “black box” algorithm, questions of legal liability, and what happens when humans and machine disagree are discussed in depth. Ultimately, a code of ethics in the coming integration of AI is needed to ensure the preservation of human rights.|https://doi.org/10.1016/B978-0-12-820273-9.00017-8|https://www.sciencedirect.com/science/article/pii/B9780128202739000178||Academic Press|978-0-12-820273-9|2021|Chapter 17 - Ethical and legal challenges|Emily Tat and Mark Rabbat|incollection|TAT2021395||||||||||Subhi J. Al'Aref and Gurpreet Singh and Lohendran Baskaran and Dimitris Metaxas|||||||||||||||||||1435958481|42
|San Francisco, USA|||||It is our great pleasure to welcome you to <i>The Web Conference 2019</i>. The Web Conference is the premier venue focused on understanding the current state and the evolution of the Web through the lens of computer science, computational social science, economics, policy, and many other disciplines. The 2019 edition of the conference is a reflection point as we celebrate the 30th anniversary of the Web.|||New York, NY, USA|Association for Computing Machinery|9781450366755|2019|WWW '19: Companion Proceedings of The 2019 World Wide Web Conference||proceedings|10.1145/3308560|||||||||||||||||||||||||||||1437422377|42
MK Series on Business Intelligence||||141-156|Managing Data in Motion||https://doi.org/10.1016/B978-0-12-397167-8.00021-2|https://www.sciencedirect.com/science/article/pii/B9780123971678000212|Boston|Morgan Kaufmann|978-0-12-397167-8|2013|Chapter 21 - Big Data Integration|April Reeve|incollection|REEVE2013141||||||||||April Reeve|||||||||||||||||||1441054657|42
DTUC '20|Virtual Event, Tunisia|digital transformation, religious studies, research infrastructures, big religious data|5||Proceedings of the 2nd International Conference on Digital Tools &amp; Uses Congress|"\"Data in and for religion is arguably as old as humanity. Religious significance has been attached to an immense variety of artifacts and documents, often in written form, in nearly all spoken and written languages over the past millennia. The rise of the digital age gives to the scholar in religious studies the opportunity to build research over a much wider array of data than ever before; institutions which have data repositories (such as libraries, museums, universities, etc.) similarly have the chance to make their collections available to a larger community. On the other hand, however, there is a serious risk that a considerable amount of data gets lost during the \"\"Digital transition\"\". This paper presents the approach of the RESILIENCE Research Infrastructure in dealing with the issue of big data and data loss within the field of religious studies.\""|10.1145/3423603.3424007|https://doi.org/10.1145/3423603.3424007|New York, NY, USA|Association for Computing Machinery|9781405377539|2020|Towards Big Religious Data: RESILIENCE Research Infrastructure for Data on Religion in the Digital Age|"\"B\"\"{u}chler, Marco and Riegert, Sarah and Alpi, Federico and Cadeddu, Francesca\""|inproceedings|10.1145/3423603.3424007|9||||||||||||||||||||||||||||1441420071|42
||Data integrity;Machine learning;Data models;Training;Task analysis;Mathematical model;Gesture recognition;data selection;multimodal;data utility;data quality assessment||454-459|2019 IEEE SmartWorld, Ubiquitous Intelligence & Computing, Advanced & Trusted Computing, Scalable Computing & Communications, Cloud & Big Data Computing, Internet of People and Smart City Innovation (SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI)|Multimodal fusion is more and more widely used in the field of machine learning, but it faces a prominent problem in practical application: data utility is not stable. The data of different modalities may be missing and noisy randomly, which will interfere the machine learning model of multi-modal fusion. Most of the existing multi-modal fusion methods neglect data utility problems or only adopt simple data denoising methods to improve data utility. To solve the problem of unstable data utility, we propose a data selection method based on the evaluation of data utility. By training a special machine learning model, the optimal modal combination is predicted according to the quality evaluation of multi-modal data samples to accomplish the dynamic selection of data modalities. The experimental results show that the proposed method can effectively improve the accuracy of multi-modal recognition under low data utility.|10.1109/SmartWorld-UIC-ATC-SCALCOM-IOP-SCI.2019.00120|||||2019|A Collaborative Multi-modality Selection Method Based on Data Utility Assessment|Xiao, Yunlong and Gu, Yang and Wang, Jiwei and Wu, Tong|inproceedings|9060352||Aug|||||||||||||||||||||||||||1449659610|42
||affective states, data-driven decision-making, Affective computing, machine learning, pattern recognition|24|||The modern computing applications are presently adapting to the convenient availability of huge and diverse data for making their pattern recognition methods smarter. Identification of dominant emotion solely based on the text data generated by humans is essential for the modern human–computer interaction. This work presents a multimodal text-keystrokes dataset and associated learning methods for the identification of human emotions hidden in small text. For this, a text-keystrokes data of 69 participants is collected in multiple scenarios. Stimuli are induced through videos in a controlled environment. After the stimuli induction, participants write their reviews about the given scenario in an unguided manner. Afterward, keystroke and in-text features are extracted from the dataset. These are used with an assortment of learning methods to identify emotion hidden in the short text. An accuracy of 86.95% is achieved by fusing text and keystroke features. Whereas, 100% accuracy is obtained for pleasure-displeasure classes of emotions using the fusion of keystroke/text features, tree-based feature selection method, and support vector machine classifier. The present work is also compared with four state-of-the-art techniques for the same task, where the results suggest that the present proposal performs better in terms of accuracy.|10.1145/3480968|https://doi.org/10.1145/3480968|New York, NY, USA|Association for Computing Machinery||2022|Non-Acted Text and Keystrokes Database and Learning Methods to Recognize Emotions|Tahir, Madiha and Halim, Zahid and Rahman, Atta Ur and Waqas, Muhammad and Tu, Shanshan and Chen, Sheng and Han, Zhu|article|10.1145/3480968|61|feb|ACM Trans. Multimedia Comput. Commun. Appl.|15516857|2|18|May 2022||||||||||||||||||||||1450888543|343492898
||Smart city, Implementation gap, Success and failure factors, Utility, Usability, Context||101505||Planning support systems (PSS) enabled by smart city technologies (big data and information and communication technologies (ICTs)) are becoming more widespread in their availability, but have not yet been fully recognized as being useful in planning practice. Thus, a better understanding of the determinants of PSS usefulness in practice helps to improve the functional support of PSS for smart cities. This study is based on a recent international questionnaire (268 respondents) designed to evaluate the perceptions of scholars and practitioners in the smart city planning field. Based on the empirical evidence, this paper recommends that it is imperative for PSS developers and users to be more responsive to the fit for task-technology and user-technology (i.e., utility and usability, respectively) since they positively contribute to PSS usefulness in practice and to be more sensitive to the potential negative effects of contextual factors on PSS usefulness in smart cities. The empirical analyses further suggest that rather than merely striving for integrating smart city technologies into advancing PSS, the way that innovative PSS are integrated into the planning framework (i.e., how well PSS can satisfy the needs of planning tasks and users by considering context-specificities) is of great significance in promoting PSS's actual usefulness.|https://doi.org/10.1016/j.compenvurbsys.2020.101505|https://www.sciencedirect.com/science/article/pii/S0198971520302386||||2020|Ignorance is bliss? An empirical analysis of the determinants of PSS usefulness in practice|Huaxiong Jiang and Stan Geertman and Patrick Witte|article|JIANG2020101505|||Computers, Environment and Urban Systems|01989715||83|||||23269|1,549|Q1|92|85|327|4842|2135|324|6,11|56,96|United Kingdom|Western Europe|1980-1986, 1988-2020|Ecological Modeling (Q1); Environmental Science (miscellaneous) (Q1); Geography, Planning and Development (Q1); Urban Studies (Q1)||||1453572179|1571752529
ICIME 2018|Salford, United Kingdom|quality metric, quality dimension, Data quality, quality assessment, open government data|5|110–114|Proceedings of the 2018 10th International Conference on Information Management and Engineering|With the development in research of government open data, the issue of data quality becomes more prominent. It's important to accurately judge the data quality before using it. The microcosmic quality assessment not only provides criteria for users to pick up dataset, but also establishes standards for providers' data quality management. In this paper, it sums up 16 types of quality problems through the investigation of three Chinese local government datasets in Beijing, Guangzhou and Harbin, and then constructs 7 quality dimensions and metrics at different granular level to score three cities. The evaluation results reflect that overall score of completeness, accuracy and consistency is low, which will affect the availability of data and mislead users to make wrong decision. On the basis of this evaluation, government could take measures to overcome the weaknesses observed in the open data quality, addressing specific lower score quality aspects.|10.1145/3285957.3285962|https://doi.org/10.1145/3285957.3285962|New York, NY, USA|Association for Computing Machinery|9781450364898|2018|Quality Assessment for Open Government Data in China|Li, Xiao-Tong and Zhai, Jun and Zheng, Gui-Fu and Yuan, Chang-Feng|inproceedings|10.1145/3285957.3285962|||||||||||||||||||||||||||||1456490761|42
||data citation, data curation, data literacy, data management, data quality, data sharing, research data||71-84|Digital Information Strategies|The key topic of digital research data raises a multitude of issues: big data, data sharing, data quality, data management, data curation, data citation, data literacy. This chapter addresses questions related to the definition of these concepts, to the frameworks constructed for a better understanding and treatment of the different phenomena, as well as ethical considerations. The potential of libraries and information professionals in fulfilling data-related activities is outlined, together with the associated requirements of them.|https://doi.org/10.1016/B978-0-08-100251-3.00005-6|https://www.sciencedirect.com/science/article/pii/B9780081002513000056||Chandos Publishing|978-0-08-100251-3|2016|Chapter 5 - Digital Research Data: Where are we Now?|Tibor Koltay|incollection|KOLTAY201671||||||||||David Baker and Wendy Evans|||||||||||||||||||1459017413|42
||Big Data, Hadoop, Big Data distributions, Big Data analytics, NoSQL, Machine learning||431-448||Developing Big Data applications has become increasingly important in the last few years. In fact, several organizations from different sectors depend increasingly on knowledge extracted from huge volumes of data. However, in Big Data context, traditional data techniques and platforms are less efficient. They show a slow responsiveness and lack of scalability, performance and accuracy. To face the complex Big Data challenges, much work has been carried out. As a result, various types of distributions and technologies have been developed. This paper is a review that survey recent technologies developed for Big Data. It aims to help to select and adopt the right combination of different Big Data technologies according to their technological needs and specific applications’ requirements. It provides not only a global view of main Big Data technologies but also comparisons according to different system layers such as Data Storage Layer, Data Processing Layer, Data Querying Layer, Data Access Layer and Management Layer. It categorizes and discusses main technologies features, advantages, limits and usages.|https://doi.org/10.1016/j.jksuci.2017.06.001|https://www.sciencedirect.com/science/article/pii/S1319157817300034||||2018|Big Data technologies: A survey|Ahmed Oussous and Fatima-Zahra Benjelloun and Ayoub {Ait Lahcen} and Samir Belfkih|article|OUSSOUS2018431|||Journal of King Saud University - Computer and Information Sciences|13191578|4|30|||||||||||||||||||||||1461941465|1257083324
||democratization of artificial intelligence, predictive analytics, Automated machine learning, interactive data science|36|||As big data becomes ubiquitous across domains, and more and more stakeholders aspire to make the most of their data, demand for machine learning tools has spurred researchers to explore the possibilities of automated machine learning (AutoML). AutoML tools aim to make machine learning accessible for non-machine learning experts (domain experts), to improve the efficiency of machine learning, and to accelerate machine learning research. But although automation and efficiency are among AutoML’s main selling points, the process still requires human involvement at a number of vital steps, including understanding the attributes of domain-specific data, defining prediction problems, creating a suitable training dataset, and selecting a promising machine learning technique. These steps often require a prolonged back-and-forth that makes this process inefficient for domain experts and data scientists alike and keeps so-called AutoML systems from being truly automatic. In this review article, we introduce a new classification system for AutoML systems, using a seven-tiered schematic to distinguish these systems based on their level of autonomy. We begin by describing what an end-to-end machine learning pipeline actually looks like, and which subtasks of the machine learning pipeline have been automated so far. We highlight those subtasks that are still done manually—generally by a data scientist—and explain how this limits domain experts’ access to machine learning. Next, we introduce our novel level-based taxonomy for AutoML systems and define each level according to the scope of automation support provided. Finally, we lay out a roadmap for the future, pinpointing the research required to further automate the end-to-end machine learning pipeline and discussing important challenges that stand in the way of this ambitious goal.|10.1145/3470918|https://doi.org/10.1145/3470918|New York, NY, USA|Association for Computing Machinery||2021|AutoML to Date and Beyond: Challenges and Opportunities|Karmaker (“Santu”), Shubhra Kanti and Hassan, Md. Mahadi and Smith, Micah J. and Xu, Lei and Zhai, Chengxiang and Veeramachaneni, Kalyan|article|10.1145/3470918|175|oct|ACM Comput. Surv.|03600300|8|54|November 2022||||23038|2,079|Q1|163|112|365|15859|6978|365|19,16|141,60|United States|Northern America|1969-2020|Computer Science (miscellaneous) (Q1); Theoretical Computer Science (Q1)|10,790|10.282|0.01438|1463807303|1517405264
iiWAS2019|Munich, Germany|Big data management, Intelligent smart environments, Big data analytics|3|5–7|Proceedings of the 21st International Conference on Information Integration and Web-Based Applications &amp; Services|This paper focuses on big data management and analytics in intelligent smart environments, with particular regards to intelligent transportation and logistics systems, and provides relevant research directions that may represent a milestone for future years.|10.1145/3366030.3366044|https://doi.org/10.1145/3366030.3366044|New York, NY, USA|Association for Computing Machinery|9781450371797|2020|Big Data Management and Analytics in Intelligent Smart Environments: State-of-the-Art Analysis and Future Research Directions|Cuzzocrea, Alfredo|inproceedings|10.1145/3366030.3366044|||||||||||||||||||||||||||||1466325214|42
||Big data, Data quality and error, Data ethnics, Spatial information sciences||134-142||The recent explosive publications of big data studies have well documented the rise of big data and its ongoing prevalence. Different types of “big data” have emerged and have greatly enriched spatial information sciences and related fields in terms of breadth and granularity. Studies that were difficult to conduct in the past time due to data availability can now be carried out. However, big data brings lots of “big errors” in data quality and data usage, which cannot be used as a substitute for sound research design and solid theories. We indicated and summarized the problems faced by current big data studies with regard to data collection, processing and analysis: inauthentic data collection, information incompleteness and noise of big data, unrepresentativeness, consistency and reliability, and ethical issues. Cases of empirical studies are provided as evidences for each problem. We propose that big data research should closely follow good scientific practice to provide reliable and scientific “stories”, as well as explore and develop techniques and methods to mitigate or rectify those ‘big-errors’ brought by big data.|https://doi.org/10.1016/j.isprsjprs.2015.11.006|https://www.sciencedirect.com/science/article/pii/S0924271615002567||||2016|Rethinking big data: A review on the data quality and usage issues|Jianzheng Liu and Jie Li and Weifeng Li and Jiansheng Wu|article|LIU2016134|||ISPRS Journal of Photogrammetry and Remote Sensing|09242716||115||Theme issue 'State-of-the-art in photogrammetry, remote sensing and spatial information science'|||29161|2,960|Q1|138|264|677|16114|7306|668|10,56|61,04|Netherlands|Western Europe|1989-2020|Atomic and Molecular Physics, and Optics (Q1); Computer Science Applications (Q1); Computers in Earth Sciences (Q1); Engineering (miscellaneous) (Q1); Geography, Planning and Development (Q1)|18,026|8.979|0.02145|1468686007|660578442
||Microcomputers, Information storage, Physical sciences and engineering||92-97||There are unique challenges in managing data collection and management from instruments in the field in general. These issues become extreme when “in the field” means “in a plane over the Antarctic”. In this paper we present the design and function of the Forward Observer a computer cluster and data analysis system that flies in a plane in the Arctic and Antarctic to collect, analyze in real time, and store Synthetic Aperture Radar (SAR) data. SAR is used to analyze the thickness and structure of polar ice sheets. We also discuss the processing of data once it is returned to the continental US and made available via data grids. The needs for in-flight data analysis and storage in the Antarctic and Arctic are highly unusual, and we have developed a novel system to meet those needs. We describe the constraints and requirements that led to the creation of this system and the general functionality which it applies to any instrument. We discuss the main means for handling replication and creating checksum information to ensure that data collected in polar regions are returned safely to mainland US for analysis. So far, not a single byte of data collected in the field has failed to make it home to the US for analysis (although many particular data storage devices have failed or been damaged due to the challenges of the extreme environments in which this system is used). While the Forward Observer system is developed for the extreme situation of data management in the field in the Antarctic, the technology and solutions we have developed are applicable and potentially usable in many situations where researchers wish to do real time data management in the field in areas that are constrained in terms of electrical supply.|https://doi.org/10.1016/j.future.2017.05.031|https://www.sciencedirect.com/science/article/pii/S0167739X17310567||||2017|Forward Observer system for radar data workflows: Big data management in the field|Richard Knepper and Matthew Standish|article|KNEPPER201792|||Future Generation Computer Systems|0167739X||76|||||12264|1,262|Q1|119|798|1935|36054|16877|1868|9,11|45,18|Netherlands|Western Europe|1984-2021|Computer Networks and Communications (Q1); Hardware and Architecture (Q1); Software (Q1)||||1471686156|562237118
IDEAS '18|Villa San Giovanni, Italy|smart city, constraints, data graph, Query language, data quality|5|297–301|Proceedings of the 22nd International Database Engineering &amp; Applications Symposium|This paper presents a context-driven query system for urban computing where users are responsible for defining their own restrictions over which datalog-like queries are built. Instead of imposing constraints on databases, our goal is to filter consistent data during the query process. Our query language is able to express aggregates in recursive rules, allowing it to capture network properties typical of graph analysis. This paper presents our query system and analyzes its capabilities using use cases in Urban Computing.|10.1145/3216122.3216148|https://doi.org/10.1145/3216122.3216148|New York, NY, USA|Association for Computing Machinery|9781450365277|2018|A Context-Driven Querying System for Urban Graph Analysis|Chabin, Jacques and Gomes-Jr., Luiz and Halfeld-Ferrari, Mirian|inproceedings|10.1145/3216122.3216148|||||||||||||||||||||||||||||1473467023|42
||Spatial transcriptomics, Computational approaches, Data quality, Data interpretation, Multi-omics integration||||The development of spatial transcriptomics technologies has transformed genetic research from a single-cell data level to a two-dimensional spatial coordinate system and facilitated the study of the composition and function of various cell subsets in different environments and organs. The large-scale data generated by these spatial transcriptomics technologies, which contains spatial gene expression information, have elicited the need for spatially resolved approaches to meet the requirements of computational and biological data interpretation. These requirements include dealing with the explosive growth of data to determine the cell-level and gene-level expression, correcting the inner batch effect and loss of expression to improve the data quality, conducting efficient interpretation and in-depth knowledge mining both at the single-cell and tissue-wide levels, and conducting multi-omics integration analysis to provide an extensible framework towards the in-depth understanding of biological processes. However, algorithms designed specifically for spatial transcriptomics technologies to meet these requirements are still in their infancy. Here, we review computational approaches to these problems in light of corresponding issues and challenges, and present forward-looking insights into algorithm development.|https://doi.org/10.1016/j.gpb.2022.10.001|https://www.sciencedirect.com/science/article/pii/S1672022922001292||||2022|Computational Approaches and Challenges in Spatial Transcriptomics|Shuangsang Fang and Bichao Chen and Yong Zhang and Haixi Sun and Longqi Liu and Shiping Liu and Yuxiang Li and Xun Xu|article|FANG2022|||Genomics, Proteomics & Bioinformatics|16720229|||||||89440|3,114|Q1|49|36|158|1874|1152|135|6,41|52,06|China|Asiatic Region|2003-2020|Biochemistry (Q1); Computational Mathematics (Q1); Genetics (Q1); Medicine (miscellaneous) (Q1); Molecular Biology (Q1)||||1474560429|1963566185
||Distributed modeling, MapReduce framework, Parallel computing, Quality prediction, Big data analytics||1-13||With the ever increasing data collected from the process, the era of big data has arrived in the process industry. Therefore, the computational effort for data modeling and analytics in standalone modes has become increasingly demanding, particularly for large-scale processes. In this paper, a distributed parallel process modeling approach is presented based on a MapReduce framework for big data quality prediction. Firstly, the architecture for distributed parallel data modeling is formulated under the MapReduce framework. Secondly, a big data quality prediction scheme is developed based on the distributed parallel data modeling approach. As an example, the basic Semi-Supervised Probabilistic Principal Component Regression (SSPPCR) model is deployed to concurrently train a set of local models with split datasets. Meanwhile, Bayesian rule is utilized in a MapReduce way to integrate local models based on their predictive abilities. Two case studies demonstrate the effectiveness of the proposed method for big data quality prediction.|https://doi.org/10.1016/j.jprocont.2018.04.004|https://www.sciencedirect.com/science/article/pii/S0959152418300660||||2018|Big data quality prediction in the process industry: A distributed parallel modeling framework|Le Yao and Zhiqiang Ge|article|YAO20181|||Journal of Process Control|09591524||68|||||14414|1,102|Q1|114|136|413|5639|1820|408|4,39|41,46|United Kingdom|Western Europe|1991-2020|Computer Science Applications (Q1); Control and Systems Engineering (Q1); Industrial and Manufacturing Engineering (Q1); Modeling and Simulation (Q1)|7,446|3.666|0.00525|1475168380|26970220
||Electronic health record, Big data, Drug safety, Health care database, Cancer risk||599-604||As more and more health systems have converted to the use of electronic health records, the amount of searchable and analyzable data is exploding. This includes not just provider or laboratory created data but also data collected by instruments, personal devices, and patients themselves, among others. This has led to more attention being paid to the analysis of these data to answer previously unaddressed questions. This is especially important given the number of therapies previously found to be beneficial in clinical trials that are currently being re-scrutinized. Because there are orders of magnitude more information contained in these data sets, a fundamentally different approach needs to be taken to their processing and analysis and the generation of knowledge. Health care and medicine are drivers of this phenomenon and will ultimately be the main beneficiaries. Concurrently, many different types of questions can now be asked using these data sets. Research groups have become increasingly active in mining large data sets, including nationwide health care databases, to learn about associations of medication use and various unrelated diseases such as cancer. Given the recent increase in research activity in this area, its promise to radically change clinical research, and the relative lack of widespread knowledge about its potential and advances, we surveyed the available literature to understand the strengths and limitations of these new tools. We also outline new databases and techniques that are available to researchers worldwide, with special focus on work pertaining to the broad and rapid monitoring of drug safety and secondary effects.|https://doi.org/10.1016/j.jss.2019.09.053|https://www.sciencedirect.com/science/article/pii/S0022480419306985||||2020|What Can We Learn About Drug Safety and Other Effects in the Era of Electronic Health Records and Big Data That We Would Not Be Able to Learn From Classic Epidemiology?|Ali Zarrinpar and Ting-Yuan {David Cheng} and Zhiguang Huo|article|ZARRINPAR2020599|||Journal of Surgical Research|00224804||246|||||||||||||||||||||||1475233794|546217935
||Data-centric architecture, Data-intensive applications, Model-based development, Industry 4.0, Big data, Metamodel||263-284||The effective implementation of Industry 4.0 requires the reformulation of industrial processes in order to achieve the vertical and horizontal digitalization of the value chain. For this purpose, it is necessary to provide tools that enable their successful implementation. This paper therefore proposes a data-centric, distributed, dynamically scalable reference architecture that integrates cutting-edge technologies being aware of the existence of legacy technology typically present in these environments. In order to make its implementation easier, we have designed a metamodel that collects the description of all the elements involved in a digital platform (data, resources, applications and monitoring metrics) as well as the necessary information to configure, deploy and execute applications on it. Likewise, we provide a tool compliant to the metamodel that automates the generation of configuration, deployment and launch files and their corresponding transference and execution in the nodes of the platform. We show the flexibility, extensibility and validity of our software artefacts through their application in two case studies, one addressed to preprocess and store pollution data and the other one, more complex, which simulates the management of an electric power distribution of a smart city.|https://doi.org/10.1016/j.future.2021.06.020|https://www.sciencedirect.com/science/article/pii/S0167739X21002156||||2021|A big data-centric architecture metamodel for Industry 4.0|Patricia {López Martínez} and Ricardo Dintén and José María Drake and Marta Zorrilla|article|LOPEZMARTINEZ2021263|||Future Generation Computer Systems|0167739X||125|||||12264|1,262|Q1|119|798|1935|36054|16877|1868|9,11|45,18|Netherlands|Western Europe|1984-2021|Computer Networks and Communications (Q1); Hardware and Architecture (Q1); Software (Q1)||||1476654690|562237118
ICSE-SEIP '22|Pittsburgh, Pennsylvania|big data, SQL on hadoop, open source, empirical study|10|33–42|Proceedings of the 44th International Conference on Software Engineering: Software Engineering in Practice|Big data SQL analytics platform has evolved as the key infrastructure for business data analysis. Compared with traditional costly commercial RDBMS, scalable solutions with open-source projects, such as SQL-on-Hadoop, are more popular and attractive to enterprises. In eBay, we build Carmel, a company-wide interactive SQL analytics platform based on Apache Spark. Carmel has been serving thousands of customers from hundreds of teams globally for more than 3 years. Meanwhile, despite the popularity of open-source based big data SQL analytics platforms, few empirical studies on service quality issues (e.g., job failure) were carried out for them. However, a deep understanding of service quality issues and taking right mitigation are significant to the ease of manual maintenance efforts. To fill this gap, we conduct a comprehensive empirical study on 1,884 real-word service quality issues from Carmel. We summarize the common symptoms and identify the root causes with typical cases. Stakeholders including system developers, researchers, and platform maintainers can benefit from our findings and implications. Furthermore, we also present lessons learned from critical cases in our daily practice, as well as insights to motivate automatic tool support and future research directions.|10.1145/3510457.3513034|https://doi.org/10.1145/3510457.3513034|New York, NY, USA|Association for Computing Machinery|9781450392266|2022|An Empirical Study on Quality Issues of EBay's Big Data SQL Analytics Platform|Zhu, Feng and Xu, Lijie and Ma, Gang and Ji, Shuping and Wang, Jie and Wang, Gang and Zhang, Hongyi and Wan, Kun and Wang, Mingming and Zhang, Xingchao and Wang, Yuming and Li, Jingpin|inproceedings|10.1145/3510457.3513034|||||||||||||||||||||||||||||1477456101|42
||Urban vibrancy, Geographically weighted regression, mobile phone data, Social media, Points-of-interest, Big data||101428||Understanding urban vibrancy aids policy-making to foster urban space and therefore has long been a goal of urban studies. Recently, the emerging urban big data and urban analytic methods have enabled us to portray citywide vibrancy. From the social sensing perspective, this study presents a comprehensive and comparative framework to cross-validate urban vibrancy and uncover associated spatial effects. Spatial patterns of urban vibrancy indicated by multisource urban sensing data (points-of-interest, social media check-ins, and mobile phone records) were investigated. A comprehensive urban vibrancy metric was formed by adaptively weighting these metrics. The association between urban vibrancy and demographic, economic, and built environmental factors was revealed with global regression models and local regression models. An empirical experiment was conducted in Shenzhen. The results demonstrate that four urban vibrancy metrics are all higher in the special economic zone (SEZ) and lower in non-SEZs but with different degrees of spatial aggregation. The influences of employment and road density on all vibrancy metrics are significant and positive. However, the effects of metro stations, land use mix, building footprints, and distance to district center depend on the vibrancy indicator and location. These findings unravel the commonalities and differences in urban vibrancy metrics derived from multisource urban big data and the hidden spatial dynamics of the influences of associated factors. They further suggest that urban policies should be proposed to foster vibrancy in Shenzhen therefore benefit social wellbeing and urban development in the long term. They also provide valuable insights into the reliability of urban big data-driven urban studies.|https://doi.org/10.1016/j.compenvurbsys.2019.101428|https://www.sciencedirect.com/science/article/pii/S0198971519302674||||2020|Portraying the spatial dynamics of urban vibrancy using multisource urban big data|Wei Tu and Tingting Zhu and Jizhe Xia and Yulun Zhou and Yani Lai and Jincheng Jiang and Qingquan Li|article|TU2020101428|||Computers, Environment and Urban Systems|01989715||80|||||23269|1,549|Q1|92|85|327|4842|2135|324|6,11|56,96|United Kingdom|Western Europe|1980-1986, 1988-2020|Ecological Modeling (Q1); Environmental Science (miscellaneous) (Q1); Geography, Planning and Development (Q1); Urban Studies (Q1)||||1479309608|1571752529
||Cloud Computing, Software as a Service, Big Data, Elastic Extension Tables, Multi-tenant Database, Relational Tables, Virtual Relational Tables.||2168-2181||Multi-tenant database is a new database solution which is significant for Software as a service (SaaS) and Big Data applications in the context of cloud computing paradigm. This multi-tenant database has significant design challenges to develop a solution that ensures a high level of data quality, accessibility, and manageability for the tenants using this database. In this paper, we propose a multi-tenant data management service called Elastic Extension Tables Schema Handler Service (EETSHS), which is based on a multi-tenant database schema called Elastic Extension Tables (EET). This data management service satisfies tenants’ different business requirements, by creating, managing, organizing, and administratinglarge volumes of structured, semi-structured, and unstructured data. Furthermore, it combines traditional relational data with virtual relational data in a single database schema and allows tenants to manage this data by calling functions from this service. We present algorithms for frequently used functions of this service, and perform several experiments to measure the feasibility and effectiveness of managing multi-tenant data using these functions. We report experimental results of query execution timesfor managing tenants’ virtual and traditional relational data showing that EET schema is a good candidate for the management of multi-tenant data for SaaS and Big Data applications.|https://doi.org/10.1016/j.procs.2014.05.202|https://www.sciencedirect.com/science/article/pii/S1877050914003792||||2014|Multi-tenant Elastic Extension Tables Data Management|Haitham Yaish and Madhu Goyal and George Feuerlicht|article|YAISH20142168|||Procedia Computer Science|18770509||29||2014 International Conference on Computational Science|||19700182801|0,334|-|76|1907|6483|38511|13722|6359|2,09|20,19|Netherlands|Western Europe|2010-2020|Computer Science (miscellaneous)||||1479906335|2108686752
IOT '18|Santa Barbara, California, USA|IoT interoperability, resource slice, cloud computing|4||Proceedings of the 8th International Conference on the Internet of Things|Dealing with interoperability in the IoT domain is a complex matter that requires various techniques for tackling data, protocol and middleware interoperability. We cannot solve IoT interoperability problems by just developing (new) software components and (semantic) data models. In this tutorial, we will present interoperability techniques for complex IoT Cloud applications by leveraging dynamic solutions of provisioning and reconfiguring of IoT data processing pipelines, protocol bridges, IoT middleware and cloud services. First, the tutorial will examine cross-layered, cross-system inter-operability issues and present a DevOps IoT Interoperability approach for defining metadata, selecting resources and software artifacts, and provisioning and connecting resources to create various potential solutions for IoT Cloud interoperability using resource slice concepts. Second, the tutorial will present techniques for dynamically provisioning data pipelines, middleware services, protocol adapters and custom solutions to address cross-layered, cross-system interoperability for IoT Cloud applications. Such solutions also allow dynamic reconfiguration of resources to add/remove interoperability support. We will present the concepts and techniques with hands-on examples using our research tools rsiHub and IoTCloudSamples.|10.1145/3277593.3277642|https://doi.org/10.1145/3277593.3277642|New York, NY, USA|Association for Computing Machinery|9781450365642|2018|Dynamic IoT Data, Protocol, and Middleware Interoperability with Resource Slice Concepts and Tools: Tutorial|Truong, Hong-Linh|inproceedings|10.1145/3277593.3277642|48||||||||||||||||||||||||||||1482894197|42
||Costs;Correlation;Statistical analysis;Data integrity;Volume measurement;Project management;Big Data;Big Data;Project Management;Sensitive Rule;Quality||022-026|2020 11th International Conference on Information and Communication Systems (ICICS)|Big data is term of dataset with characteristic volume, value and veracity that lead to challenges unable proceed using traditional techniques to extract value, project management perspective is dynamic processing that utilizes the appropriate resources of organization in many phases by measuring in four-factor scope, time, cost and quality. In this research aim improve data quality from big data via project management scope depends on high trust which is getting high accuracy from confidence level in volume of data, confidence get with context and value of data which lead to determine accuracy deeply in it and finally choose from data depending on veracity of it, the experiment using three main factors time, cost and scope, strongest relation arranging between them start by project scope as strongest one then cost, product and finally time is weakest between them, in the final when select best quality use two sides generally from quality degree and be middle-quality interval and especially from relative distance with the strongest factor.|10.1109/ICICS49469.2020.239526|||||2020|An Approach to Improve Data Quality from Big Data Aspect by Sensitive Cost and Time|Mohammad, Banan and Alzyadat, Wael and Al-Fayoumi, Mohammad and EL Hawi, Ruba and Alhroob, Aysh|inproceedings|9079066||April||25733346|||||||||||||||||||||||||1485734651|109324705
||Chondrules, Matrix, Elemental composition, ChondritedDB, Database||1-14||Chondrules and matrix are the major components of chondritic meteorites and represent a significant evolutionary step in planet formation. The formation and evolution of chondrules and matrix and, in particular, the mechanics of chondrule formation remain the biggest unsolved challenge in meteoritics. A large number of studies of these major components not only helped to understand these in ever greater detail, but also produced a remarkably large body of data. Studying all available data has become known as ‹big data› analyses and promises deep insights – in this case – to chondrule and matrix formation and relationships. Looking at all data may also allow one to better understand the mechanism of chondrule formation or, equally important, what information we might be missing to identify this process. A database of all available chondrule and matrix data further provides an overview and quick visualisation, which will not only help to solve actual problems, but also enable students and future researchers to quickly access and understand all we know about these components. We collected all available data on elemental bulk chondrule and matrix compositions in a database that we call ChondriteDB. The database also contains petrographic and petrologic information on chondrules. Currently, ChondriteDB contains about 2388 chondrule and 1064 matrix data from 70 different publications and 161 different chondrites. Future iterations of ChondriteDB will include isotope data and information on other chondrite components. Data quality is of critical importance. However, as we discuss, quality is not an objective category, but a subjective judgement. Quantifiable data acquisition categories are required that allow selecting the appropriate data from a database in the context of a given research problem. We provide a comprehensive overview on the contents of ChondriteDB. The database is available as an Excel file upon request from the senior author of this paper, or can be accessed through MetBase.|https://doi.org/10.1016/j.chemer.2017.05.003|https://www.sciencedirect.com/science/article/pii/S0009281916302896||||2018|What we know about elemental bulk chondrule and matrix compositions: Presenting the ChondriteDB Database|Dominik C. Hezel and Markus Harak and Guy Libourel|article|HEZEL20181|||Geochemistry|00092819|1|78|||||25163|0,988|Q1|53|96|120|7626|371|118|2,49|79,44|Germany|Western Europe|1978-1990, 1993-2020|Geophysics (Q1); Geochemistry and Petrology (Q2)||||1491638870|501842240
WSDM '21|Virtual Event, Israel|federated learning, data quality evaluation, knowledge tracing, intelligent education, data isolation|9|662–670|Proceedings of the 14th ACM International Conference on Web Search and Data Mining|Knowledge tracing is a fundamental task in intelligent education for tracking the knowledge states of students on necessary concepts. In recent years, Deep Knowledge Tracing (DKT) utilizes recurrent neural networks to model student learning sequences. This approach has achieved significant success and has been widely used in many educational applications. However, in practical scenarios, it tends to suffer from the following critical problems due to data isolation: 1) Data scarcity. Educational data, which is usually distributed across different silos (e.g., schools), is difficult to gather. 2) Different data quality. Students in different silos have different learning schedules, which results in unbalanced learning records, meaning that it is necessary to evaluate the learning data quality independently for different silos. 3) Data incomparability. It is difficult to compare the knowledge states of students with different learning processes from different silos. Inspired by federated learning, in this paper, we propose a novel Federated Deep Knowledge Tracing (FDKT) framework to collectively train high-quality DKT models for multiple silos. In this framework, each client takes charge of training a distributed DKT model and evaluating data quality by leveraging its own local data, while a center server is responsible for aggregating models and updating the parameters for all the clients. In particular, in the client part, we evaluate data quality incorporating different education measurement theories, and we construct two quality-oriented implementations based on FDKT, i.e., FDKTCTT and FDKTIRT-where the means of data quality evaluation follow Classical Test Theory and Item Response Theory, respectively. Moreover, in the server part, we adopt hierarchical model interpolation to uptake local effects for model personalization. Extensive experiments on real-world datasets demonstrate the effectiveness and superiority of the FDKT framework.|10.1145/3437963.3441747|https://doi.org/10.1145/3437963.3441747|New York, NY, USA|Association for Computing Machinery|9781450382977|2021|Federated Deep Knowledge Tracing|Wu, Jinze and Huang, Zhenya and Liu, Qi and Lian, Defu and Wang, Hao and Chen, Enhong and Ma, Haiping and Wang, Shijin|inproceedings|10.1145/3437963.3441747|||||||||||||||||||||||||||||1492187065|42
Intelligent Data-Centric Systems||Industrial revolution, Healthcare 4.0, Blockchain technology, Internet of Things (IoT), Big data||3-21|Artificial Intelligence and Industry 4.0|With the advancement of technology, the world is changing and automating at a rapid pace. Digitization plays a major role in the automation of technology. In this context, Industry 4.0 shows how industrial production is developing along with the latest technology. In Industry 4.0, much manual work has been replaced by automated machines that can be controlled with developing technologies such as artificial intelligence (AI), big data, cloud computing, and so on. Various technologies have been introduced in the healthcare sector due to Industry 4.0. These include AI, three-dimensional printing, machine learning, cognitive systems, autonomous robots, autonomous vehicles, augmented reality, big data, Internet of Things (IoT), blockchain technology, and more. This chapter discusses the transformation of the healthcare industry in the context of Industry 4.0. It presents a detailed study on big data, IoT, and blockchain technology with different applications that can enhance what is known as Healthcare 4.0. The chapter includes three case studies that illustrate the use of innovation in Healthcare 4.0 to detect and diagnose disease using portable medical devices connected to the IoT.|https://doi.org/10.1016/B978-0-323-88468-6.00002-4|https://www.sciencedirect.com/science/article/pii/B9780323884686000024||Academic Press|978-0-323-88468-6|2022|Chapter 1 - Influence and implementation of Industry 4.0 in health care|Sumit Koul|incollection|KOUL20223||||||||||Aboul Ella Hassanien and Jyotir Moy Chatterjee and Vishal Jain|||||||||||||||||||1492406829|42
||Big data analytics, IT business value, Firm performance, Meta-analysis, Moderator analysis, Sociotechnical theory||128-149||Big data analytics (BDA) has recently gained importance as an emerging technology for handling big data. The use of advanced techniques with differing levels of intelligence, such as descriptive, predictive, prescriptive, and autonomous analytics, is expected to create value for firms. By viewing BDA as a sociotechnical system, we conduct a meta-analysis of 107 individual studies to integrate prior evidence on the role of the technical and social factors of BDA in creating BDA business value. The findings underline the predominant role of the social components in enhancing firm performance, such as the BDA system’s human factors and a nurturing organizational structure, in contrast to the minor role of the technological factors. However, both the technical and social factors are found to be strong determinants of BDA business value. Through the combined lens of sociotechnical theory and the IS business value framework, we contribute to research and practice by enhancing the understanding of the main technical and social determinants of BDA business value at the firm level.|https://doi.org/10.1016/j.jbusres.2022.08.028|https://www.sciencedirect.com/science/article/pii/S0148296322007111||||2022|The role of the social and technical factors in creating business value from big data analytics: A meta-analysis|Thuy Duong Oesterreich and Eduard Anton and Frank Teuteberg and Yogesh K Dwivedi|article|OESTERREICH2022128|||Journal of Business Research|01482963||153|||||20550|2,049|Q1|195|878|1342|73221|11507|1315|7,38|83,40|United States|Northern America|1973-2021|Marketing (Q1)|46,935|7.550|0.03523|1495066506|1502892296
||Marine vehicles;Artificial intelligence;Interpolation;Trajectory;Navigation;Rivers;Accidents;Ship domain;trajectory data;ship collision risk;automatic identification system;Monte Carlo method||128-134|2019 IEEE 4th International Conference on Big Data Analytics (ICBDA)|With the rapid development of maritime industries, the vessel traffic density has been gradually increased leading to increasing the potential risk of ship collision accidents in crowded inland waterways. It will bring negative effects on human life safety and enterprise economy. Therefore, it is of vital significance to study the risk of ship collision in practical applications. This paper proposes to quantitatively estimate the ship collision risk based on ship domain modeling and real-time vessel trajectory data. In particular, the trajectory data quality is improved using the cubic spline interpolation method. We assume that the ship collision risk is highly related to the cross areas of ship domains between different ships, which are computed via the Monte Carlo probabilistic algorithm. For the sake of better understanding, the kernel density estimation method is adopted to visually generate the ship collision risk in maps. Experimental results have illustrated the effectiveness of the proposed method in crowded inland waterways.|10.1109/ICBDA.2019.8712843|||||2019|Real-Time Vessel Trajectory Data-Based Collison Risk Assessment in Crowded Inland Waterways|Feng, Zikun and Yang, Haojie and Li, Xinyi and Li, Yan and Liu, Zhao and Liu, Ryan Wen|inproceedings|8712843||March|||||||||||||||||||||||||||1495299201|42
||Clinical Trials, Research, Multi-Teams, Non-Intrusive, Repeatable tests||99-111|Building Big Data Applications|One of the applications of big data applications and infrastructure is in the pharmaceutical industry. The complexity of the queries that are executed in these applications and the results they generate, make us feel the statement of torture the data and it will confess to anything. The relationships between the data in the different subject areas, the clinical trials and results, the communities in social media, the research labs and their outcomes, the clinical labs and patient results, and the financial outcomes of the pharmaceutical enterprise. Wow, think of all kinds of insights, add to this the markets, the competition, and the global industry, and we have phenomenal data to work with.|https://doi.org/10.1016/B978-0-12-815746-6.00005-3|https://www.sciencedirect.com/science/article/pii/B9780128157466000053||Academic Press|978-0-12-815746-6|2020|5 - Pharmacy industry applications and usage|Krish Krishnan|incollection|KRISHNAN202099||||||||||Krish Krishnan|||||||||||||||||||1495576368|42
||Big Data, CRM, Literature review, Critical Success Factors (CSFs), Word tree||818-846||This paper aims to figure out the potential impact of Big Data (BD) on Critical Success Factors (CSFs) of Customer Relationship Management (CRM). In fact, while some authors have posited a relationship between BD and CRM, literature lacks works that go into the heart of the matter. Through an extensive up-to-date in-depth literature review about CRM, twenty (20) CSFs were singled out from 104 selected papers, and organized within an ad-hoc classification framework. The consistency of the classification was checked by means of a content analysis. Evidences were discussed and linked to the BD literature, and five propositions about how BD could affect CRM CSFs were formalized. Our results suggest that BD-enabled CRM initiatives could require several changes in the pertinent CSFs. In order to get rid of the hype effect surrounding BD, we suggest to adopt an explorative approach towards them by defining a mandatory business direction through sound business cases and pilot tests. From a general standpoint, BD could be framed as an enabling factor of well-known projects, like CRM initiatives, in order to reap the benefits from the new technologies by addressing the efforts through already acknowledged management paths.|https://doi.org/10.1016/j.ipm.2017.10.005|https://www.sciencedirect.com/science/article/pii/S0306457317300067||||2018|Big Data-enabled Customer Relationship Management: A holistic approach|Pierluigi Zerbino and Davide Aloini and Riccardo Dulmin and Valeria Mininno|article|ZERBINO2018818|||Information Processing & Management|03064573|5|54||In (Big) Data we trust: Value creation in knowledge organizations|||||||||||||||||||||1498940051|1769516999
||||2461-2464||With thousands of publications per year, the volume of data published on perovskite solar cells since the spark of the “perovskite fever” in 2013 is enormous and far exceeds the amount that any individual researcher could digest. To tackle this issue, Jacobsson et al.1 have created The Perovskite Database, which is part of a larger trend to harness the power of big data and artificial intelligence to accelerate the commercialization of perovskite solar cells.|https://doi.org/10.1016/j.matt.2022.06.001|https://www.sciencedirect.com/science/article/pii/S259023852200296X||||2022|Finding the FAIRness in perovskite photovoltaics research|Kameron R. Hansen and Luisa Whittaker-Brooks|article|HANSEN20222461|||Matter|25902385|8|5|||||||||||||||||||||||1502294091|1197118539
||Data engineering;Data analysis;Data integration;Big Data;Distributed databases;Data models;Uncertainty;army data engineering;data management;data integration;data analysis;representation of data analysis results;data quality||149-152|2017 IEEE 2nd International Conference on Big Data Analysis (ICBDA)|This paper analyzed the challenges of data management in army data engineering, such as big data volume, data heterogeneous, high rate of data generation and update, high time requirement of data processing, and widely separated data sources. We discussed the disadvantages of traditional data management technologies to deal with these problems. We also highlighted the key problems of data management in army data engineering including data integration, data analysis, representation of data analysis results, and evaluation of data quality.|10.1109/ICBDA.2017.8078796|||||2017|Some key problems of data management in army data engineering based on big data|HongJu, Xiao and Fei, Wang and FenMei, Wang and XiuZhen, Wang|inproceedings|8078796||March|||||||||||||||||||||||||||1502312383|42
SIGIR '21|Virtual Event, Canada|friend network, group formation, personalized search|10|92–101|Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval|The key to personalized search is to build the user profile based on historical behaviour. To deal with the users who lack historical data, group based personalized models were proposed to incorporate the profiles of similar users when re-ranking the results. However, similar users are mostly found based on simple lexical or topical similarity in search behaviours. In this paper, we propose a neural network enhanced method to highlight similar users in semantic space. Furthermore, we argue that the behaviour-based similar users are still insufficient to understand a new query when user's historical activities are limited. To tackle this issue, we introduce the friend network into personalized search to determine the closeness between users in another way. Since the friendship is often formed based on similar background or interest, there are plenty of personalized signals hidden in the friend network naturally. Specifically, we propose a friend network enhanced personalized search model, which groups the user into multiple friend circles based on search behaviours and friend relations respectively. These two types of friend circles are complementary to construct a more comprehensive group profile for refining the personalization. Experimental results show the significant improvement of our model over existing personalized search models.|10.1145/3404835.3462918|https://doi.org/10.1145/3404835.3462918|New York, NY, USA|Association for Computing Machinery|9781450380379|2021|Group Based Personalized Search by Integrating Search Behaviour and Friend Network|Zhou, Yujia and Dou, Zhicheng and Wei, Bingzheng and Xie, Ruobing and Wen, Ji-Rong|inproceedings|10.1145/3404835.3462918|||||||||||||||||||||||||||||1502906229|42
|||||Data Cleaning|Data quality is one of the most important problems in data management, since dirty data often leads to inaccurate data analytics results and incorrect business decisions. Poor data across businesses and the U.S. government are reported to cost trillions of dollars a year. Multiple surveys show that dirty data is the most common barrier faced by data scientists. Not surprisingly, developing effective and efficient data cleaning solutions is challenging and is rife with deep theoretical and engineering problems.This book is about data cleaning, which is used to refer to all kinds of tasks and activities to detect and repair errors in the data. Rather than focus on a particular data cleaning task, we give an overview of the endto- end data cleaning process, describing various error detection and repair methods, and attempt to anchor these proposals with multiple taxonomies and views. Specifically, we cover four of the most common and important data cleaning tasks, namely, outlier detection, data transformation, error repair (including imputing missing values), and data deduplication. Furthermore, due to the increasing popularity and applicability of machine learning techniques, we include a chapter that specifically explores how machine learning techniques are used for data cleaning, and how data cleaning is used to improve machine learning models.This book is intended to serve as a useful reference for researchers and practitioners who are interested in the area of data quality and data cleaning. It can also be used as a textbook for a graduate course. Although we aim at covering state-of-the-art algorithms and techniques, we recognize that data cleaning is still an active field of research and therefore provide future directions of research whenever appropriate.||https://doi.org/10.1145/3310205.3310210|New York, NY, USA|Association for Computing Machinery|9781450371520|2019|Data Transformation||inbook|10.1145/3310205.3310210|||||||||||||||||||||||||||||1503714496|42
||Crowdsourcing;Computational modeling;Scalability;Big Data;Quality assessment;Time factors;Task analysis;Crowdsourcing;quality assessment;answer integration;MapReduce||335-341|2020 International Conference on Networking and Network Applications (NaNA)|Crowdsourcing is an emerging distributed computing model that is widely used. Aiming at the uneven quality of crowdsourcing answers due to different workers' capabilities and attitudes, it is necessary to effectively study the hotspot issue of crowdsourcing answer integration. A crowdsourced answer integration algorithm based on “filter-evaluate-vote” is proposed. This algorithm is implemented using MapReduce parallel programming model in the Hadoop platform, and experiments are performed on multiple data sets. The results show that the proposed algorithm can be effective. It improves the accuracy of crowdsourced answers, and has high computing performance and horizontal scalability, which is suitable for answer integration in a big data environment.|10.1109/NaNA51271.2020.00064|||||2020|Crowdsourcing Answer Integration Algorithm For Big Data Environment|Xia, Hong and Zhang, YongKang and Wang, Han and Chen, YanPing and Wang, ZhongMin|inproceedings|9353794||Dec|||||||||||||||||||||||||||1504341069|42
||||100244|||https://doi.org/10.1016/j.bdr.2021.100244|https://www.sciencedirect.com/science/article/pii/S2214579621000617||||2021|Editorial to the Special Issue on Big Data in Industrial and Commercial Applications|Lars Lundberg and Håkan Grahn and Valeria Cardellini and Andreas Polze and Sogand Shirinbab|article|LUNDBERG2021100244|||Big Data Research|22145796||26|||||21100356018|0,565|Q2|25|11|76|547|324|69|4,06|49,73|United States|Northern America|2014-2020|Computer Science Applications (Q2); Information Systems (Q2); Information Systems and Management (Q2); Management Information Systems (Q2)|586|3.578|0.00126|1506104682|1627174784
||Apriori algorithm;relay protection;defect data;integrity check||506-508|2019 International Conference on Electronic Engineering and Informatics (EEI)|Relay protection big data creates good conditions for the improvement of professional applications, and data integrity is an important aspect that reflects data quality. The association of relay protection big data is intense. This paper applies Apriori algorithm to mine data relevance and generate association rules. Based on this, the integrity of relay protection data is checked, and the incomplete data is predicted. Taking the relay protection defect data as an example, the paper explores the correlation among 251 items of the six dimensions of protection relay defect data such as type of protection, the severity of the defect, whether the protection is out of operation, the defect location, the cause of the defect, and the equipment manufacturer, completing the processing of incomplete data with great application results.|10.1109/EEI48997.2019.00115|||||2019|Relay Protection Data Integrity Check Method Based on Big Data Association Algorithm|Guo, Peng and Yang, Guosheng and Wang, Wenhuan and Yan, Zhoutian and Zhang, Lie and Zhang, Hanfang|inproceedings|8991080||Nov|||||||||||||||||||||||||||1510072229|42
||Big data, Big data framework, Government public open data, Open data, Public data for health||33-45|Big Data Analytics for Healthcare|Today, data is one of the most valuable assets on the planet. As a valuable resource, data may be used to develop a wide range of data applications, all of which are driven by creativity and innovation. In order to obtain information and provide services, data is also a critical component. In the recent years, big data has become a popular topic in global discussion. Big data is a new technology and knowledge generation phenomenon, that record, capture, and execute a significant amount of data for the usage in a variety of domains such as research, education, business, investing, health, and so on. The proliferation of data inspired by new methods of data gatherings such as via social media, wireless sensors, and data from government agencies which makes big data management an ultimate challenge. This study includes a thorough evaluation of existing theories and practical approach to address the public sector open data issues for determining the determinants of government public open data (GPOD) development of big data. To investigate the revolution of GPOD for health, the framework was dominantly used over architecture, infrastructures, followed by theoretical and conceptual framework, according to the review. This study revealed that most of the existing frameworks still lack consideration of the requirement for public open data in health. There is less number of existing research works that have sophisticated big data frameworks in GPOD for health. There also is still a lack of investment and adoption of big data in the public sector. The findings of this chapter will help academicians to empirically study the revealed requirement and provide decision-makers a better knowledge of how to leverage GPOD adoption in health by taking appropriate actions.|https://doi.org/10.1016/B978-0-323-91907-4.00024-8|https://www.sciencedirect.com/science/article/pii/B9780323919074000248||Academic Press|978-0-323-91907-4|2022|Chapter 4 - Towards big data framework in government public open data (GPOD) for health|Najhan {Muhamad Ibrahim} and Nur Hidayah Ilham {Ahmad Azri} and Norbik Bashah Idris|incollection|MUHAMADIBRAHIM202233||||||||||Pantea Keikhosrokiani|||||||||||||||||||1514215528|42
LAK '18|Sydney, New South Wales, Australia|strategy, ROMA model, policy, learning analytics, higher education|10|320–329|Proceedings of the 8th International Conference on Learning Analytics and Knowledge|This paper introduces a learning analytics policy development framework developed by a cross-European research project team - SHEILA (Supporting Higher Education to Integrate Learning Analytics), based on interviews with 78 senior managers from 51 European higher education institutions across 16 countries. The framework was developed using the RAPID Outcome Mapping Approach (ROMA), which is designed to develop effective strategies and evidence-based policy in complex environments. This paper presents three case studies to illustrate the development process of the SHEILA policy framework, which can be used to inform strategic planning and policy processes in real world environments, particularly for large-scale implementation in higher education contexts.|10.1145/3170358.3170367|https://doi.org/10.1145/3170358.3170367|New York, NY, USA|Association for Computing Machinery|9781450364003|2018|SHEILA Policy Framework: Informing Institutional Strategies and Policy Processes of Learning Analytics|Tsai, Yi-Shan and Moreno-Marcos, Pedro Manuel and Tammets, Kairit and Kollom, Kaire and Ga\v{s}evi\'{c}, Dragan|inproceedings|10.1145/3170358.3170367|||||||||||||||||||||||||||||1514440582|42
||Big data analytics, Barriers, Manufacturing supply chains, Best worst method (BWM)||5515-5519||Due to its potential utility, Big Data (BD) recently attracted researchers and practitioners in decision-making. Big Data analytics (BDA) becomes more common among manufacturing companies because it lets them gain insight and make decisions based on BD. Given the importance of both BD and BDA, this study aims to identify and analyse essential BDA adoption barriers in supply chains. This study explores the current knowledge base using a BWM (Best Worst Method) to discuss these barriers. Data were obtained from five Indian manufacturing companies. Research findings show that data-related barriers are most significant. The findings will help managers understand the exact nature of the challenges and possible advantages of the BDA and implement BDA policies for the growth and output of supply chain operations.|https://doi.org/10.1016/j.matpr.2021.03.374|https://www.sciencedirect.com/science/article/pii/S2214785321024263||||2021|A framework based on BWM for big data analytics (BDA) barriers in manufacturing supply chains|Vikrant Sharma and Atul Kumar and Mukesh Kumar|article|SHARMA20215515|||Materials Today: Proceedings|22147853||47||3rd International e-Conference on Frontiers in Mechanical Engineering and nanoTechnology|||21100370037|0,341|-|47|3996|10585|88521|14096|10409|1,24|22,15|United Kingdom|Western Europe|2005, 2014-2020|Materials Science (miscellaneous)||||1516483385|400517803
||||24-32||The massive rise of Big Data generated from smartphones, social media, Internet of Things (IoT), and multimedia, has produced an overwhelming flow of data in either structured or unstructured format. Big Data technologies are being developed and implemented in the food supply chain that gather and analyse these data. Such technologies demand new approaches in data collection, storage, processing and knowledge extraction. In this article, an overview of the recent developments in Big Data applications in food safety are presented. This review shows that the use of Big Data in food safety remains in its infancy but it is influencing the entire food supply chain. Big Data analysis is used to provide predictive insights in several steps in the food supply chain, support supply chain actors in taking real time decisions, and design the monitoring and sampling strategies. Lastly, the main research challenges that require research efforts are introduced.|https://doi.org/10.1016/j.cofs.2020.11.006|https://www.sciencedirect.com/science/article/pii/S2214799320301260||||2020|Big Data in food safety- A review|Cangyu Jin and Yamine Bouzembrak and Jiehong Zhou and Qiao Liang and Leonieke M. {van den Bulk} and Anand Gavai and Ningjing Liu and Lukas J. {van den Heuvel} and Wouter Hoenderdaal and Hans J.P. Marvin|article|JIN202024|||Current Opinion in Food Science|22147993||36||Food Safety|||21100370190|1,297|Q1|38|89|318|5095|1714|286|5,16|57,25|Netherlands|Western Europe|2015-2021|Applied Microbiology and Biotechnology (Q1); Food Science (Q1)|3,298|6.031|0.00491|1516579854|1392752036
||Data integration, RDF, big data, data discovery, semantic web|40|||A large number of published datasets (or sources) that follow Linked Data principles is currently available and this number grows rapidly. However, the major target of Linked Data, i.e., linking and integration, is not easy to achieve. In general, information integration is difficult, because (a) datasets are produced, kept, or managed by different organizations using different models, schemas, or formats, (b) the same real-world entities or relationships are referred with different URIs or names and in different natural languages,<!--?brk?-->(c) datasets usually contain complementary information, (d) datasets can contain data that are erroneous, out-of-date, or conflicting, (e) datasets even about the same domain may follow different conceptualizations of the domain, (f) everything can change (e.g., schemas, data) as time passes. This article surveys the work that has been done in the area of Linked Data integration, it identifies the main actors and use cases, it analyzes and factorizes the integration process according to various dimensions, and it discusses the methods that are used in each step. Emphasis is given on methods that can be used for integrating several datasets. Based on this analysis, the article concludes with directions that are worth further research.|10.1145/3345551|https://doi.org/10.1145/3345551|New York, NY, USA|Association for Computing Machinery||2019|Large-Scale Semantic Integration of Linked Data: A Survey|Mountantonakis, Michalis and Tzitzikas, Yannis|article|10.1145/3345551|103|sep|ACM Comput. Surv.|03600300|5|52|September 2020||||23038|2,079|Q1|163|112|365|15859|6978|365|19,16|141,60|United States|Northern America|1969-2020|Computer Science (miscellaneous) (Q1); Theoretical Computer Science (Q1)|10,790|10.282|0.01438|1517465910|1517405264
ICBDE '22|Shanghai, China|big data, cooperative education of university-industry institutions, postgraduate education|5|194–198|Proceedings of the 5th International Conference on Big Data and Education|University-industry cooperative education is an important way to cultivate graduate students' innovative ability and practical ability. However, there are some problems in the traditional joint training model of graduate students, such as low efficiency, conflict of objectives of cooperative subjects, a mismatch between supply and demand of cooperative entities, and so on. The big data technology has brought new opportunities and challenges to the joint training of graduate students by university-industry cooperation institutions. Based on analyzing the connotation and characteristics of the big data era, the paper points out that the arrival of the big data era can improve the information integration efficiency of university-industry cooperation institutions, optimize the traditional joint training model of graduate students, and provide an effective evaluation mechanism of educational quality for university-industry cooperation institutions. At the same time, the paper discusses the difficulties of data collection and disclosure of data privacy faced by university-industry cooperative education in the big data era. The paper also discusses how to deal with the challenges from the perspective of the government, colleges and universities, scientific research institutions and enterprises.|10.1145/3524383.3524431|https://doi.org/10.1145/3524383.3524431|New York, NY, USA|Association for Computing Machinery|9781450395793|2022|Opportunities and Challenges of Joint Training of Postgraduate Students by the University-Industry Collaboration Institutions in Big Data Era|Wu, Min and Hao, Xinxin and Wan, Xuehong and Ma, Chenwei and Wu, Yu|inproceedings|10.1145/3524383.3524431|||||||||||||||||||||||||||||1519139045|42
KDD '21|Virtual Event, Singapore|information extraction, knowledge graphs, data cleaning, taxonomy|2|4090–4091|Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining|Knowledge graphs have been pivotal in supporting downstream applications like search, recommendation, and question answering, among others. Therefore, knowledge graphs have naturally become key enabling technologies in e-Commerce platforms. Developing a high coverage product knowledge graph is more challenging than generic knowledge graphs. The highly specific and complex domain, the sparsity of training data, along with the dynamic taxonomies and product types, can constrain the resulting knowledge graphs. In this tutorial we present best practices and ML innovations in industry towards building a scalable product knowledge graph. Contributions in this domain benefit from the general literature in areas including information extraction and data mining, tailored to address the specific characteristics of e-Commerce platforms.|10.1145/3447548.3470825|https://doi.org/10.1145/3447548.3470825|New York, NY, USA|Association for Computing Machinery|9781450383325|2021|All You Need to Know to Build a Product Knowledge Graph|Zalmout, Nasser and Zhang, Chenwei and Li, Xian and Liang, Yan and Dong, Xin Luna|inproceedings|10.1145/3447548.3470825|||||||||||||||||||||||||||||1519557756|42
||Big data, Healthcare, Framework, Infrastructure, Analytics, Patterns, Tools||415-425||The domain of healthcare acquired its influence by the impact of big data since the data sources involved in the healthcare organizations are well-known for their volume, heterogeneous complexity and high dynamism. Though the role of big data analytical techniques, platforms, tools are realized among various domains, their impact on healthcare organization for implementing and delivering novel use-cases for potential healthcare applications shows promising research directions. In the context of big data, the success of healthcare applications solely depends on the underlying architecture and utilization of appropriate tools as evidenced in pioneering research attempts. Novel research works have been carried out for deriving application specific healthcare frameworks that offer diversified data analytical capabilities for handling sources of data ranging from electronic health records to medical images. In this paper, we have presented various analytical avenues that exist in the patient-centric healthcare system from the perspective of various stakeholders. We have also reviewed various big data frameworks with respect to underlying data sources, analytical capability and application areas. In addition, the implication of big data tools in developing healthcare eco system is also presented.|https://doi.org/10.1016/j.jksuci.2017.12.007|https://www.sciencedirect.com/science/article/pii/S1319157817302938||||2019|Implications of big data analytics in developing healthcare frameworks – A review|Venketesh Palanisamy and Ramkumar Thirunavukarasu|article|PALANISAMY2019415|||Journal of King Saud University - Computer and Information Sciences|13191578|4|31|||||||||||||||||||||||1520323594|1257083324
WWW '15 Companion|Florence, Italy|controlled experiments, objective bayes, optional stopping, prior, multiple testing, empirical bayes, a/b testing, bayesian statistics|6|923–928|Proceedings of the 24th International Conference on World Wide Web|As A/B testing gains wider adoption in the industry, more people begin to realize the limitations of the traditional frequentist null hypothesis statistical testing (NHST). The large number of search results for the query ``Bayesian A/B testing'' shows just how much the interest in the Bayesian perspective is growing. In recent years there are also voices arguing that Bayesian A/B testing should replace frequentist NHST and is strictly superior in all aspects. Our goal here is to clarify the myth by looking at both advantages and issues of Bayesian methods. In particular, we propose an objective Bayesian A/B testing framework for which we hope to bring the best from Bayesian and frequentist methods together. Unlike traditional methods, this method requires the existence of historical A/B test data to objectively learn a prior. We have successfully applied this method to Bing, using thousands of experiments to establish the priors.|10.1145/2740908.2742563|https://doi.org/10.1145/2740908.2742563|New York, NY, USA|Association for Computing Machinery|9781450334730|2015|Objective Bayesian Two Sample Hypothesis Testing for Online Controlled Experiments|Deng, Alex|inproceedings|10.1145/2740908.2742563|||||||||||||||||||||||||||||1520634999|42
||Big Data;Quality of service;Task analysis;Cloud computing;Data models;Analytic hierarchy process;Mathematical model;Big Data Task profile;Cloud service selection;QoS||149-156|2017 IEEE 7th International Symposium on Cloud and Service Computing (SC2)|Big data has emerged as promising technology to handle huge and special data. Processing Big data involves selecting the appropriate services and resources thanks to the variety of services offered by different Cloud providers. Such selection is difficult, especially if a set of Big data requirements should be met. In this paper, we propose a dynamic cloud service selection scheme that assess Big data requirements, dynamically map these to the most available cloud services, and then recommend the best match services that fulfill different Big data processing requests. Our selection is conducted in two stages: 1) relies on a Big data task profile that efficiently capture Big data task's requirements and map them to QoS parameters, and then classify cloud providers that best satisfy these requirements, 2) uses the list of selected providers from stage 1 to further select the appropriate Cloud services to fulfill the overall Big Data task requirements. We extend the Analytic Hierarchy Process (AHP) based ranking mechanism to cope with the problem of multi-criteria selection. We conduct a set of experiments using simulated cloud setup to evaluate our selection scheme as well as the extended AHP against other selection techniques. The results show that our selection approach outperforms the others and select efficiently the appropriate cloud services that guarantee Big data task's QoS requirements.|10.1109/SC2.2017.30|||||2017|Quality Profile-Based Cloud Service Selection for Fulfilling Big Data Processing Requirements|Serhani, Mohamed Adel and El Kassabi, Hadeel T. and Taleb, Ikbal|inproceedings|8315370||Nov|||||||||||||||||||||||||||1523135018|42
||Data models;Big Data;Predictive models;Inference algorithms;Prediction algorithms;Semisupervised learning;Computational modeling;Big data;Gaussian mixture model (GMM);multimode process modeling;quality prediction;semisupervised modeling;stochastic variational inference (SVI)||3681-3692||In this paper, a novel variational inference semisupervised Gaussian mixture model (VI-S2GMM) model is first proposed for semisupervised predictive modeling in multimode processes. Parameters of Gaussian components are identified more accurately with extra unlabeled samples, which improve the prediction performance of the regression model. Since all labeled and unlabeled data samples are involved in each iteration of parameter updating, intractable computing problems occur when facing high-dimension datasets. To tackle this problem, a scalable stochastic VI-S2GMM (SVI-S2GMM) is further proposed. Through taking advantage of a stochastic gradient optimization algorithm to maximize the evidence of lower bound, the VI-based algorithm becomes scalable. In the SVI-S2GMM, only one or a minibatch of samples is randomly selected to update parameters in each iteration, which is more efficient than the VI-S2GMM. Since the whole dataset is divided and transferred to iterations batch by batch, the scalable SVI-S2GMM algorithm can easily handle the big data modeling issue. In this way, a large number of unlabeled data can be useful in the modeling, which will further benefit the prediction performance. The SVI-S2GMM is then exploited for the prediction of a quality-related key performance index. Two examples demonstrate the feasibility and effectiveness of the proposed algorithms.|10.1109/TIE.2018.2856200|||||2019|Scalable Semisupervised GMM for Big Data Quality Prediction in Multimode Processes|Yao, Le and Ge, Zhiqiang|article|8415743||May|IEEE Transactions on Industrial Electronics|15579948|5|66|||||||||||||||||||||||1523963816|1063852316
CHI EA '19|Glasgow, Scotland Uk|wearable sensors, neuroscience, big data, smartphone application, electrophysiology, medical studies, user-centered design|6|1–6|Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems|We present a smartphone application for at-home participation in large-scale neuroscientific studies. Our goal is to establish user-experience design as a paradigm in basic neuroscientific research to overcome the limits of current studies, especially in rare neurological disorders.The presented application guides users through the fitting procedure of the EEG headset and automatically encrypts and uploads recorded data to a remote server. User-feedback and neurophysiological data from a pilot study with eighteen subjects indicate that the application can be used outside of a laboratory, without the need for external guidance. We hope to inspire future work on the intersection between basic neuroscience and human-computer interaction as a promising paradigm to accelerate research on rare neurological diseases and assistive neurotechnology.|10.1145/3290607.3313002|https://doi.org/10.1145/3290607.3313002|New York, NY, USA|Association for Computing Machinery|9781450359719|2019|MYND: A Platform for Large-Scale Neuroscientific Studies|"\"Hohmann, Matthias R. and Hackl, Michelle and Wirth, Brian and Zaman, Talha and Enficiaud, Raffi and Grosse-Wentrup, Moritz and Sch\"\"{o}lkopf, Bernhard\""|inproceedings|10.1145/3290607.3313002|||||||||||||||||||||||||||||1524421655|42
||Internet of things, Industry 4.0, Big data, Multi-channel communication, Wireless sensor network||100236||Industry 4.0 (I4.0) defines a new paradigm to produce high-quality products at the low cost by reacting quickly and effectively to changing demands in the highly volatile global markets. In Industry 4.0, the adoption of Internet of Things (IoT)-enabled Wireless Sensors (WSs) in the manufacturing processes, such as equipment, machining, assembly, material handling, inspection, etc., generates a huge volume of data known as Industrial Big Data (IBD). However, the reliable and efficient gathering and transmission of this big data from the source sensors to the floor inspection system for the real-time monitoring of unexpected changes in the production and quality control processes is the biggest challenge for Industrial Wireless Sensor Networks (IWSNs). This is because of the harsh nature of the indoor industrial environment that causes high noise, signal fading, multipath effects, heat and electromagnetic interference, which reduces the transmission quality and trigger errors in the IWSNs. Therefore, this paper proposes a novel cross-layer data gathering approach called CBI4.0 for active monitoring and control of manufacturing processes in the Industry 4.0. The key aim of the proposed CBI4.0 scheme is to exploit the multi-channel and multi-radio architecture of the sensor network to guarantee quality of service (QoS) requirements, such as higher data rates, throughput, and low packet loss, corrupted packets, and latency by dynamically switching between different frequency bands in the Multichannel Wireless Sensor Networks (MWSNs). By performing several simulation experiments through EstiNet 9.0 simulator, the performance of the proposed CBI4.0 scheme is compared against existing studies in the automobile Industry 4.0. The experimental outcomes show that the proposed scheme outperforms existing schemes and is suitable for effective control and monitoring of various events in the automobile Industry 4.0.|https://doi.org/10.1016/j.jii.2021.100236|https://www.sciencedirect.com/science/article/pii/S2452414X21000364||||2021|CBI4.0: A cross-layer approach for big data gathering for active monitoring and maintenance in the manufacturing industry 4.0|Muhammad Faheem and Rizwan Aslam Butt and Rashid Ali and Basit Raza and Md. Asri Ngadi and Vehbi Cagri Gungor|article|FAHEEM2021100236|||Journal of Industrial Information Integration|2452414X||24|||||21100787106|2,042|Q1|24|28|86|2152|1361|83|12,26|76,86|Netherlands|Western Europe|2016-2020|Industrial and Manufacturing Engineering (Q1); Information Systems and Management (Q1)|1,149|10.063|0.00148|1527430894|121356201
SIGMOD '21|Virtual Event, China|models, machine learning, database, AI|8|2859–2866|Proceedings of the 2021 International Conference on Management of Data|Database and Artificial Intelligence (AI) can benefit from each other. On one hand, AI can make database more intelligent (AI4DB). For example, traditional empirical database optimization techniques (e.g., cost estimation, join order selection, knob tuning, index and view advisor) cannot meet the high-performance requirement for large-scale database instances, various applications and diversified users, especially on the cloud. Fortunately, learning-based techniques can alleviate this problem. On the other hand, database techniques can optimize AI models (DB4AI). For example, AI is hard to deploy, because it requires developers to write complex codes and train complicated models. Database techniques can be used to reduce the complexity of using AI models, accelerate AI algorithms and provide AI capability inside databases. DB4AI and AI4DB have been extensively studied recently. In this tutorial, we review existing studies on AI4DB and DB4AI. For AI4DB, we review the techniques on learning-based database configuration, optimization, design, monitoring, and security. For DB4AI, we review AI-oriented declarative language, data governance, training acceleration, and inference acceleration. Finally, we provide research challenges and future directions in AI4DB and DB4AI.|10.1145/3448016.3457542|https://doi.org/10.1145/3448016.3457542|New York, NY, USA|Association for Computing Machinery|9781450383431|2021|AI Meets Database: AI4DB and DB4AI|Li, Guoliang and Zhou, Xuanhe and Cao, Lei|inproceedings|10.1145/3448016.3457542|||||||||||||||||||||||||||||1534180676|42
||Systematic Literature Review, Solar energy forecasting, Machine learning, Data mining||54-63||Solar power is expected to play a substantial role globally, due to it being one of the leading renewable electricity sources for future use. Even though the use of solar irradiation to generate electricity is currently at a fast deployment pace and technological evolution, its natural variability still presents an important barrier to overcome. Machine learning and data mining techniques arise as alternatives to aid solar electricity generation forecast reducing the impacts of its natural inconstant power supply. This paper presents a literature review on big data models for solar photovoltaic electricity generation forecasts, aiming to evaluate the most applicable and accurate state-of-art techniques to the problem, including the motivation behind each project proposal, the characteristics and quality of data used to address the problem, among other issues. A Systematic Literature Review (SLR) method was used, in which research questions were defined and translated into search strings. The search returned 38 papers for final evaluation, affirming that the use of these models to predict solar electricity generation is currently an ongoing academic research question. Machine learning is widely used, and neural networks is considered the most accurate algorithm. Extreme learning machine learning has reduced time and raised precision.|https://doi.org/10.1016/j.seta.2018.11.008|https://www.sciencedirect.com/science/article/pii/S2213138818301036||||2019|A Systematic Literature Review on big data for solar photovoltaic electricity generation forecasting|Gabriel {de Freitas Viscondi} and Solange N. Alves-Souza|article|DEFREITASVISCONDI201954|||Sustainable Energy Technologies and Assessments|22131388||31|||||21100239262|1,040|Q1|39|270|333|13311|1833|331|5,73|49,30|United Kingdom|Western Europe|2013-2020|Energy Engineering and Power Technology (Q1); Renewable Energy, Sustainability and the Environment (Q2)|3,234|5.353|0.00348|1535143594|1822851290
||service-oriented computing, empirical studies, systematic review, end-user service composition, mapshups, web services, qualitative studies, review framework, design guideline, User studies|46|||Context: End-user service composition (EUSC) is a service-oriented paradigm that aims to empower end users and allow them to compose their own web applications from reusable service components. User studies have been used to evaluate EUSC tools and processes. Such an approach should benefit software development, because incorporating end users’ feedback into software development should make software more useful and usable. Problem: There is a gap in our understanding of what constitutes a user study and how a good user study should be designed, conducted, and reported. Goal: This article aims to address this gap. Method: The article presents a systematic review of 47 selected user studies for EUSC. Guided by a review framework, the article systematically and consistently assesses the focus, methodology and cohesion of each of these studies. Results: The article concludes that the focus of these studies is clear, but their methodology is incomplete and inadequate, their overall cohesion is poor. The findings lead to the development of a design framework and a set of questions for the design, reporting, and review of good user studies for EUSC. The detailed analysis and the insights obtained from the analysis should be applicable to the design of user studies for service-oriented systems as well and indeed for any user studies related to software artifacts.|10.1145/3340294|https://doi.org/10.1145/3340294|New York, NY, USA|Association for Computing Machinery||2019|User Studies on End-User Service Composition: A Literature Review and a Design Framework|Zhao, Liping and Loucopoulos, Pericles and Kavakli, Evangelia and Letsholo, Keletso J.|article|10.1145/3340294|15|jul|ACM Trans. Web|15591131|3|13|August 2019||||5800207369|0,438|Q2|44|18|72|1309|250|71|4,46|72,72|United States|Northern America|2007-2020|Computer Networks and Communications (Q2)|782|2.043|7.7E-4|1538046457|1451030700
ICBDC '20|Chengdu, China|Trajectory recovery, Representation learning, Multi-task learning|5|79–83|Proceedings of the 5th International Conference on Big Data and Computing|Trajectory recovery can benefit many applications such as migration pattern studies of animal and finding hot routes in the urban city. It is necessary to recover trajectory with limited trajectory points to utilize collected trajectory data in a reasonable and efficient way and to provide the better location based service for users. However, the trajectory data involves complex and nonlinear spatial-temporal impacts which cannot be captured by traditional trajectory recovery methods. Moreover, the existing methods consider little about the correlations between trajectory and traffic pattern in the urban city. The superiority of deep neural network makes it possible to recover trajectory with low data quality. We propose a Multi-Task Representation Learning Network (MRL-Net) framework which models the complex nonlinear spatial-temporal correlations in trajectory data with representation learning technique and capture the dependencies of trajectory points with recurrent neural networks. To the best of our knowledge, it is the first paper to address the trajectory recovery problem with representation learning and multi-task learning. Experiments on real-world trajectory data show that our model is superior to state-of-the-art methods.|10.1145/3404687.3404703|https://doi.org/10.1145/3404687.3404703|New York, NY, USA|Association for Computing Machinery|9781450375474|2020|Multi-Task Representation Learning Network for Trajectory Recovery|Wu, Mingming|inproceedings|10.1145/3404687.3404703|||||||||||||||||||||||||||||1542931174|42
||ERP, Big Data, Research Agenda||242-249||The world is witnessing an unprecedented interest in big data. Big data is data that is big in size (volume), big in variety (structured; semi-structured; unstructured), and big in speed of change (velocity). It was reported that almost 90% of the data worldwide was just created in the past 2 years. Therefore, this paper is an attempt to align ERP systems with big data. The objective is to suggest a future research agenda to bring together big data and ERP. While almost everyone is talking about big data at the product or tool level, relationship with social media, relationship with Internet of things, etc. no one has tried to integrate big data and ERP. A research agenda is discussed and introduced in this paper.|https://doi.org/10.1016/j.protcy.2014.10.089|https://www.sciencedirect.com/science/article/pii/S2212017314003168||||2014|ERP and Big Data: The Inept Couple|Ahmed Elragal|article|ELRAGAL2014242|||Procedia Technology|22120173||16||CENTERIS 2014 - Conference on ENTERprise Information Systems / ProjMAN 2014 - International Conference on Project MANagement / HCIST 2014 - International Conference on Health and Social Care Information Systems and Technologies|||||||||||||||||||||1543728973|1649368223
AcademicMindTrek '13|Tampere, Finland|Data and knowledge visualization, Datamap visualization, Interactive data exploration and discovery, Visualization techniques and methodologies|7|52–58|Proceedings of International Conference on Making Sense of Converging Media|This article describes the novel datamap visualization technique which enables visualizing large datasets interactively and fairly, inspired by geographic maps and microscopes. The main contributions include introducing the datamap metaphor and datamap visualization architecture, specifying efficient methods of approximate rendering, and illustrating the basic concepts in terms of example applications.|10.1145/2523429.2523458|https://doi.org/10.1145/2523429.2523458|New York, NY, USA|Association for Computing Machinery|9781450319928|2013|Datamap Visualization Technique for Interactively Visualizing Large Datasets|"\"Nyk\"\"{a}nen, Ossi\""|inproceedings|10.1145/2523429.2523458|||||||||||||||||||||||||||||1545012952|42
||Artificial intelligence, Machine learning, Propensity model, Deep learning, B2B marketing, Technology acceptance model||300-314||With the growing popularity of artificial intelligence (AI) transforming business-to-business (B2B) marketing, there is a growing demand to comprehensively understand the adoption and application of AI to advance B2B marketing. This study examines AI methods and their applications in B2B marketing across the four customer life cycle stages of reach, acquisition, conversion, and retention. The paper also analyzes and synthesizes the findings of five B2B industry surveys conducted to do the following: 1) examine B2B marketers' knowledge and attitudes toward using AI in their businesses, 2) determine the various ways in which AI is used in B2B marketing, and 3) investigate the perceived merits and challenges of using AI in B2B marketing. The findings reconcile various machine learning (ML) techniques suitable for use by B2B marketers. Employing the technology acceptance model (TAM), the paper identifies how B2B marketers perceive the benefits of AI adoption. Furthermore, this study discusses the perceived barriers to AI adoption, including data privacy challenges and the replacement of human workforces. To further highlight the benefits of AI, the study showcases three examples of successful AI adoption in B2B marketing. The paper concludes by summarizing the theoretical and managerial implications of AI adoption in B2B marketing and directions for future studies.|https://doi.org/10.1016/j.indmarman.2022.10.016|https://www.sciencedirect.com/science/article/pii/S0019850122002553||||2022|Applications of artificial intelligence in B2B marketing: Challenges and future directions|Masoud Moradi and Mayukh Dass|article|MORADI2022300|||Industrial Marketing Management|00198501||107|||||22792|2,022|Q1|136|314|465|27989|3787|450|6,86|89,14|United States|Northern America|1971-2020|Marketing (Q1)|16,291|6.960|0.00901|1545095923|223518748
CHI '21|Yokohama, Japan|geographic diversity, HCI research, generalizability, sample bias, WEIRD|14||Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems|Computer technology is often designed in technology hubs in Western countries, invariably making it “WEIRD”, because it is based on the intuition, knowledge, and values of people who are Western, Educated, Industrialized, Rich, and Democratic. Developing technology that is universally useful and engaging requires knowledge about members of WEIRD and non-WEIRD societies alike. In other words, it requires us, the CHI community, to generate this knowledge by studying representative participant samples. To find out to what extent CHI participant samples are from Western societies, we analyzed papers published in the CHI proceedings between 2016-2020. Our findings show that 73% of CHI study findings are based on Western participant samples, representing less than 12% of the world’s population. Furthermore, we show that most participant samples at CHI tend to come from industrialized, rich, and democratic countries with generally highly educated populations. Encouragingly, recent years have seen a slight increase in non-Western samples and those that include several countries. We discuss suggestions for further broadening the international representation of CHI participant samples.|10.1145/3411764.3445488|https://doi.org/10.1145/3411764.3445488|New York, NY, USA|Association for Computing Machinery|9781450380966|2021|How WEIRD is CHI?|"\"Linxen, Sebastian and Sturm, Christian and Br\"\"{u}hlmann, Florian and Cassau, Vincent and Opwis, Klaus and Reinecke, Katharina\""|inproceedings|10.1145/3411764.3445488|143||||||||||||||||||||||||||||1547366678|42
||Anomaly Detection, Predictive Maintenance, Discrete Manufacturing, Big Data Analytics, Adaptive Self-learning Systems||1334-1338||In discrete manufacturing the variation in process parameters and duration is often large. Common data storage and analytics systems primarily store data in univariate time series, and when analysing machine components of strongly varying lifetime and behaviour this causes a challenge. This paper presents a data structure and an analysis method for outlier detection which intends to deal with this challenge, as an alternative to predictive maintenance which often requires more data with higher quality than what is available. A case study in aluminium extrusion billet manufacturing is used to demonstrate the approach, predominantly detecting anomalies at the end of a critical component’s lifetime.|https://doi.org/10.1016/j.procir.2021.11.224|https://www.sciencedirect.com/science/article/pii/S2212827121011227||||2021|An approach to data structuring and predictive analysis in discrete manufacturing|Christian Dalheim Øien and Sebastian Dransfeld|article|OIEN20211334|||Procedia CIRP|22128271||104||54th CIRP CMS 2021 - Towards Digitalized Manufacturing 4.0|||21100243809|0,683|-|65|1456|3191|30451|8225|3145|2,40|20,91|Netherlands|Western Europe|2012-2020|Control and Systems Engineering; Industrial and Manufacturing Engineering||||1548173988|2127027836
||Internet of Things, Cloud Computing, Context-Awareness, Big Data, Service Composition, Smart City||151-158||The smart city vision was born by the integration of ICT in the day to day city management operations and citizens lives, owing to the need for novel and smart ways to manage the cities resources; making them more efficient, sustainable and transparent. However, the understanding of the crucial elements to this integration and how they can benefit from each other proves difficult and unclear. In this article, we investigate the intricate synergies between different technologies and paradigms involved in the smart city vision, to help design a robust framework, capable of handling the challenges impeding its successful implementation. To this end, we propose a context-aware centered approach to present a holistic view of a smart city as viewed from the different angles (Cloud, IoT, Big Data). We also propose a framework encompassing elements from the different enablers, leveraging their strengths to build and develop smart-x applications and services.|https://doi.org/10.1016/j.procs.2017.06.072|https://www.sciencedirect.com/science/article/pii/S1877050917312486||||2017|C2IoT: A framework for Cloud-based Context-aware Internet of Things services for smart cities|Soufiane Faieq and Rajaa Saidi and Hamid Elghazi and Moulay Driss Rahmani|article|FAIEQ2017151|||Procedia Computer Science|18770509||110||14th International Conference on Mobile Systems and Pervasive Computing (MobiSPC 2017) / 12th International Conference on Future Networks and Communications (FNC 2017) / Affiliated Workshops|||19700182801|0,334|-|76|1907|6483|38511|13722|6359|2,09|20,19|Netherlands|Western Europe|2010-2020|Computer Science (miscellaneous)||||1549736062|2108686752
||Big Data Analytics, Sustainable auditing systems, Barriers, RSPO, Interpretive Structural Modelling||1015-1026||In the current scenario, sustainable auditing, for example roundtable of sustainable palm oil (RSPO), requires a huge amount of data to be manually collected and entered into paper forms by farmers. Such systems are inherently inefficient, time-consuming, and, prone to errors. Researchers have proposed Big Data Analytics (BDA) based framework for next-generation smart sustainable auditing systems. Though theoretically feasible, real-life implementation of such frameworks is extremely difficult. Thus, this paper aims to identify the critical barriers that hinder the application of BDA based smart sustainable auditing system. It also aims to explore the dynamic interrelations among the barriers. We applied Interpretive Structural Modelling (ISM) approach to develop the model that extrapolates BDA adoption barriers and their relationships. The proposed model illustrates how barriers are spread over various levels and how specific barriers impact other barriers through direct and/or transitive links. This study provides practitioners with a roadmap to prioritise the interventions to facilitate the adoption of BDA in the sustainable auditing systems. Insights of this study could be used by academics to enhance understanding of the barriers to BDA applications.|https://doi.org/10.1016/j.cie.2018.04.055|https://www.sciencedirect.com/science/article/pii/S0360835218301992||||2019|Next generation smart sustainable auditing systems using Big Data Analytics: Understanding the interaction of critical barriers|Manish Shukla and Lana Mattar|article|SHUKLA20191015|||Computers & Industrial Engineering|03608352||128|||||18164|1,315|Q1|128|641|1567|31833|9597|1557|5,97|49,66|United Kingdom|Western Europe|1976-2020|Computer Science (miscellaneous) (Q1); Engineering (miscellaneous) (Q1)||||1554484214|1798521593
||Context;Business;Metadata;Big data;Electronic mail;Reliability;Indexes;Enterprise data;Data catalogue;Metadata of data;Data context;Data quality;Data governance;Data as a service||134-139|2016 IEEE 2nd International Conference on Big Data Security on Cloud (BigDataSecurity), IEEE International Conference on High Performance and Smart Computing (HPSC), and IEEE International Conference on Intelligent Data and Security (IDS)|As the adoption of enterprise Big Data platforms mature, the necessity to maintain systematic catalogue of data being processed and managed by these platforms becomes imperative. Enterprise data catalogues serve as centralized repositories of storing such metadata about data being handled by such platforms, enabling better data governance, security and control. Standardized approaches, methodologies and tools for data catalogues are being discussed and evolved. This paper introduces some key variables, attributes and indexes that need to be handled in such cataloguing solutions -- such as data contexts, data-system relationships, data quality, reliability, sensitivity and accessibility. The paper also discusses specific approaches on how each of these aspects can be adopted and applied to different enterprise contexts effectively.|10.1109/BigDataSecurity-HPSC-IDS.2016.52|||||2016|Aspects of Data Cataloguing for Enterprise Data Platforms|Shanmugam, Srinivasan and Seshadri, Gokul|inproceedings|7502278||April|||||||||||||||||||||||||||1554552114|42
||Text analysis;Databases;Technological innovation;Frequency-domain analysis;Text mining;Quality assessment;Systematic Literature Review;Descriptive Analysis;Text Analysis;Master Data Management||1-6|2017 International Conference on Research and Innovation in Information Systems (ICRIIS)|Systematic Literature Review (SLR) is a structured way of conducting a review of existing research works produced by the earlier researchers. The application of right data analysis technique during the SLR evaluation stage would give an insight to the researcher in achieving the SLR objective. This paper presents how descriptive analysis and text analysis can be applied to achieve one of the common SLR objectives which is to study the progress of specific research domain. These techniques have been demonstrated to synthesis the progress of Master Data Management research domain. Using descriptive analysis technique, this study has identified a trend of related literary works distribution by years, sources, and publication types. Meanwhile, text analysis shows the common terms and interest topics in the Master Data Management research which are 1) master data, 2) data quality, 3) business intelligence, 4) business process, 5) data integration, 6) big data, 7) data governance, 8) information governance, 9) data management and 10) product data. It is hoped that other researchers would be able to replicate these analysis techniques in performing SLR for other research domains.|10.1109/ICRIIS.2017.8002473|||||2017|Descriptive analysis and text analysis in Systematic Literature Review: A review of Master Data Management|Haneem, Faizura and Ali, Rosmah and Kama, Nazri and Basri, Sufyan|inproceedings|8002473||July||23248157|||||||||||||||||||||||||1555020695|1546333701
SIGSPATIAL '19|Chicago, IL, USA|map matching, deep learning, data driven system|4|452–455|Proceedings of the 27th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems|Map matching is important in many trajectory based applications like route optimization and traffic schedule, etc. As the widely used methods, Hidden Markov Model and its variants are well studied to provide accurate and efficient map matching service. However, HMM based methods fail to utilize the value of enormous trajectory big data, which are useful for the map matching task. Furthermore, with many following-up works, they are still easily influenced by the noisy records, which are very common in the real system. To solve these problems, we revisit the map matching task from the data perspective, and propose to utilize the great power of data to help solve these problems. We build a deep learning based model to utilize all the trajectory data for joint training and knowledge sharing. With the help of embedding techniques and sequence learning model with attention enhancement, our system does the map matching in the latent space, which is tolerant to the noise in the physical space. Extensive experiments demonstrate that our model outperforms the widely used HMM based methods more than 10% (absolute accuracy) and works robustly in the noisy settings in the meantime.|10.1145/3347146.3359090|https://doi.org/10.1145/3347146.3359090|New York, NY, USA|Association for Computing Machinery|9781450369091|2019|DeepMM: Deep Learning Based Map Matching with Data Augmentation|Zhao, Kai and Feng, Jie and Xu, Zhao and Xia, Tong and Chen, Lin and Sun, Funing and Guo, Diansheng and Jin, Depeng and Li, Yong|inproceedings|10.1145/3347146.3359090|||||||||||||||||||||||||||||1556924312|42
||Satellites;Organizations;Research and development;Databases;Monitoring;Buildings;Data integrity;Satellite Data Management;Big Data;Data Quality;Central Data Repository||4364-4367|IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium|High volumes of satellite data management within an organization is still challenging and daunting in the era of big data. The increasing information technology costs and limited budgets, growing satellite data needs, data availability across multiple teams and projects, strategic goals of organization, and expected project outcomes require better satellite data management mechanism and system to facilitate research and development activities. An organization level centralized satellite data repository is a practical solution to satisfy these requirements. This paper describes the best practices and experiences from building such a central satellite data repository within our organization, including data management strategy and policy, scalable and extensible system infrastructure, comprehensive data management system, and technical support and user assistance. These practices can be borrowed and applied in other organizations with similar requirements.|10.1109/IGARSS.2019.8900190|||||2019|Practices and Experiences in High Volumes of Satellite Data Management|Han, Weiguo and Jochum, Matthew|inproceedings|8900190||July||21537003|||||||||||||||||||||||||1556965186|1296020740
|||||Data Cleaning|Data quality is one of the most important problems in data management, since dirty data often leads to inaccurate data analytics results and incorrect business decisions. Poor data across businesses and the U.S. government are reported to cost trillions of dollars a year. Multiple surveys show that dirty data is the most common barrier faced by data scientists. Not surprisingly, developing effective and efficient data cleaning solutions is challenging and is rife with deep theoretical and engineering problems.This book is about data cleaning, which is used to refer to all kinds of tasks and activities to detect and repair errors in the data. Rather than focus on a particular data cleaning task, we give an overview of the endto- end data cleaning process, describing various error detection and repair methods, and attempt to anchor these proposals with multiple taxonomies and views. Specifically, we cover four of the most common and important data cleaning tasks, namely, outlier detection, data transformation, error repair (including imputing missing values), and data deduplication. Furthermore, due to the increasing popularity and applicability of machine learning techniques, we include a chapter that specifically explores how machine learning techniques are used for data cleaning, and how data cleaning is used to improve machine learning models.This book is intended to serve as a useful reference for researchers and practitioners who are interested in the area of data quality and data cleaning. It can also be used as a textbook for a graduate course. Although we aim at covering state-of-the-art algorithms and techniques, we recognize that data cleaning is still an active field of research and therefore provide future directions of research whenever appropriate.||https://doi.org/10.1145/3310205.3310215|New York, NY, USA|Association for Computing Machinery|9781450371520|2019|References||inbook|10.1145/3310205.3310215|||||||||||||||||||||||||||||1558345794|42
||Big Data, Data-intensive computing, e-Science, Parallel and distributed computing, Cloud computing||314-347||It is already true that Big Data has drawn huge attention from researchers in information sciences, policy and decision makers in governments and enterprises. As the speed of information growth exceeds Moore’s Law at the beginning of this new century, excessive data is making great troubles to human beings. However, there are so much potential and highly useful values hidden in the huge volume of data. A new scientific paradigm is born as data-intensive scientific discovery (DISD), also known as Big Data problems. A large number of fields and sectors, ranging from economic and business activities to public administration, from national security to scientific researches in many areas, involve with Big Data problems. On the one hand, Big Data is extremely valuable to produce productivity in businesses and evolutionary breakthroughs in scientific disciplines, which give us a lot of opportunities to make great progresses in many fields. There is no doubt that the future competitions in business productivity and technologies will surely converge into the Big Data explorations. On the other hand, Big Data also arises with many challenges, such as difficulties in data capture, data storage, data analysis and data visualization. This paper is aimed to demonstrate a close-up view about Big Data, including Big Data applications, Big Data opportunities and challenges, as well as the state-of-the-art techniques and technologies we currently adopt to deal with the Big Data problems. We also discuss several underlying methodologies to handle the data deluge, for example, granular computing, cloud computing, bio-inspired computing, and quantum computing.|https://doi.org/10.1016/j.ins.2014.01.015|https://www.sciencedirect.com/science/article/pii/S0020025514000346||||2014|Data-intensive applications, challenges, techniques and technologies: A survey on Big Data|C.L. {Philip Chen} and Chun-Yang Zhang|article|PHILIPCHEN2014314|||Information Sciences|00200255||275|||||15134|1,524|Q1|184|928|2184|40007|17554|2168|7,89|43,11|United States|Northern America|1968-2021|Artificial Intelligence (Q1); Computer Science Applications (Q1); Control and Systems Engineering (Q1); Information Systems and Management (Q1); Software (Q1); Theoretical Computer Science (Q1)|44,038|6.795|0.04908|1559387016|1633962588
||future city, Big data, transport operation management, healthcare informationsystems, integrated systems, shared resources||1107-1114||The growth of cities in the 21st century has put more pressure on resources and conditions of urban life. There are several reasons why the health-care industry is the focus of this investigation. For instance, in the UK various studies point to the lack of failure of basic quality control procedures and misalignment between customer needs and provider services and duplication of logistics practices. The development of smart cities and big data present unprecedented challenges and opportunities for operations managers; they need to develop new tools and techniques for network planning and control. Our paper aims to make a contribution to big data and city operations theory by exploring how big data can lead to improvements in transport capacity sharing. We explore using Markov models the integration of big data with future city (health-care) transport sharing. A mathematical model was designed to illustrate how sharing transport load (and capacity) in a smart city can improve efficiencies in meeting demand for city services. The results from our analysis of 13 different sharing/demand scenarios are presented. A key finding is that the probability for system failure and performance variance tends to be highest in a scenario of high demand/zero sharing.|https://doi.org/10.1016/j.procs.2015.08.566|https://www.sciencedirect.com/science/article/pii/S1877050915027015||||2015|Big Data Logistics: A health-care Transport Capacity Sharing Model|Rashid Mehmood and Gary Graham|article|MEHMOOD20151107|||Procedia Computer Science|18770509||64||Conference on ENTERprise Information Systems/International Conference on Project MANagement/Conference on Health and Social Care Information Systems and Technologies, CENTERIS/ProjMAN / HCist 2015 October 7-9, 2015|||19700182801|0,334|-|76|1907|6483|38511|13722|6359|2,09|20,19|Netherlands|Western Europe|2010-2020|Computer Science (miscellaneous)||||1560040761|2108686752
WebSci '17|Troy, New York, USA|measurement, business intelligence, online social media, quality management, user generated content|4|219–222|Proceedings of the 2017 ACM on Web Science Conference|We present early findings on building a proof of concept for an automated system to identify emerging trends regarding vehicle defects. The proposed system functions by continuously collecting and monitoring publicly available data from several heterogeneous channels ranging from online social media to vehicle enthusiast forums and consumer reporting sites. By mining the collected data, the system would provide real-time detection of ongoing consumer issues with vehicles. In addition, our system has special emphasis on detecting early signals prior to the widespread knowledge of the general public. One of the system components involves estimating a baseline statistical distribution governing the frequency of observing specific types of vehicle defective complaints from our data sources and subsequently identifying irregular deviations from this distribution. A web interface is made available to visualize descriptive statistics derived from various channels, with the intent to provide timely insights for human analysts.|10.1145/3091478.3091521|https://doi.org/10.1145/3091478.3091521|New York, NY, USA|Association for Computing Machinery|9781450348966|2017|EDSV: Emerging Defect Surveillance for Vehicles|Xu, Jiejun and Xie, Daniel and Lu, Tsai-Ching and Cafeo, John|inproceedings|10.1145/3091478.3091521|||||||||||||||||||||||||||||1563164286|42
||Companies;Business;Process control;Graphics;Soft sensors;Social networking (online);Reliability||9-28|Monetizing Data: How to Uplift Your Business|This chapter deals with aspects of data that are relevant to the practitioner wishing to apply data analytics to monetise data. It reviews the types of data that are available and how they are accessed. The chapter considers the fast‐growing big data from internet exchanges and the attendant quality and storage issues, and which employees are best placed to maximise the value added from the data. It also then considers the slower build‐up of transactional data from small traders and experiments on consumer behaviour. The chapter defines scales of measurement and terms commonly used to distinguish different types of data, the meaning and necessity of data quality, amounts of data and its storage, the skills needed for different data functions, and data readiness and how to assess where a company is on the cycle of data improvement.|10.1002/9781119125167.ch2|https://ieeexplore.ieee.org/document/9821411||Wiley|9781119125143|2018|About Data and Data Science|Ahlemeyer-Stubbe, Andrea and Coleman, Shirley|inbook|9821411|||||||||||||||||||||||||||||1563833285|42
||Big data;Quality assessment;Metadata;Instruments;Ecosystems;Companies;big data quality assessment;quality measurement;velocity;volume;variety;SQA4BD;QUAMOCO;smart ecosystems;SPARK;HADOOP||115-124|2016 Joint Conference of the International Workshop on Software Measurement and the International Conference on Software Process and Product Measurement (IWSM-MENSURA)|High-quality data is a prerequisite for most types of analysis provided by software systems. However, since data quality does not come for free, it has to be assessed and managed continuously. The increasing quantity, diversity, and velocity that characterize big data today make these tasks even more challenging. We identified challenges that are specific for big data quality assessments with particular emphasis on their usage in smart ecosystems and make a proposal for a scalable cross-organizational approach that addresses these challenges. We developed an initial prototype to investigate scalability in a multi-node test environment using big data technologies. Based on the observed horizontal scalability behavior, there is an indication that the proposed approach also allows dealing with increasing volumes of heterogeneous data.|10.1109/IWSM-Mensura.2016.026|||||2016|Quality Evaluation for Big Data: A Scalable Assessment Approach and First Evaluation Results|Kläs, Michael and Putz, Wolfgang and Lutz, Tobias|inproceedings|7809598||Oct|||||||||||||||||||||||||||1569630043|42
WiPSCE '18|Potsdam, Germany|competency model, data literacy, data science, data, data management, CS education|10||Proceedings of the 13th Workshop in Primary and Secondary Computing Education|Today, data is everywhere: Our digitalized world depends on enormous amounts of data that are captured by and about everyone and considered a valuable resource. Not only in everyday life, but also in science, the relevance of data has clearly increased in recent years: Nowadays, data-driven research is often considered a new research paradigm. Thus, there is general agreement that basic competencies regarding gathering, storing, processing and visualizing data, often summarized under the term data literacy, are necessary for every scientist today. Moreover, data literacy is generally important for everyone, as it is essential for understanding how the modern world works. Yet, at the moment data literacy is hardly considered in CS teaching at schools. To allow deeper insight into this field and to structure related competencies, in this work we develop a competency model of data literacy by theoretically deriving central content and process areas of data literacy from existing empirical work, keeping a school education perspective in mind. The resulting competency model is contrasted to other approaches describing data literacy competencies from different perspectives. The practical value of this work is emphasized by giving insight into an exemplary lesson sequence fostering data literacy competencies.|10.1145/3265757.3265766|https://doi.org/10.1145/3265757.3265766|New York, NY, USA|Association for Computing Machinery|9781450365888|2018|Developing a Theoretically Founded Data Literacy Competency Model|Grillenberger, Andreas and Romeike, Ralf|inproceedings|10.1145/3265757.3265766|9||||||||||||||||||||||||||||1573357609|42
||Big Data, User-generated content, e-Social science, Computing, Data gathering||79-87||Online user-generated content is playing a progressively important role as information source for social scientists seeking for digging out value. Advances procedures and technologies to enable the capture, storage, management, and analysis of the data make possible to exploit increasing amounts of data generated directly by users. In that regard, Big Data is gaining meaning into social science from quantitative datasets side, which differs from traditional social science where collecting data has always been hard, time consuming, and resource intensive. Hence, the emergent field of computational social science is broadening researchers' perspectives. However, it also requires a multidisciplinary approach involving several and different knowledge areas. This paper outlines an architectural framework and methodology to collect Big Data from an electronic Word-of-Mouth (eWOM) website containing user-generated content. Although the paper is written from the social science perspective, it must be also considered together with other complementary disciplines such as data accessing and computing.|https://doi.org/10.1016/j.csi.2016.02.003|https://www.sciencedirect.com/science/article/pii/S0920548916300034||||2016|Harvesting Big Data in social science: A methodological approach for collecting online user-generated content|M. Olmedilla and M.R. Martínez-Torres and S.L. Toral|article|OLMEDILLA201679|||Computer Standards & Interfaces|09205489||46|||||24303|0,556|Q1|63|36|246|2302|1013|243|3,71|63,94|Netherlands|Western Europe|1985-2020|Law (Q1); Hardware and Architecture (Q2); Software (Q2)||||1574099940|827980402
WebSci '22|Barcelona, Spain|data collection, data quality, guidelines, dataset documentation, web data|2|478–479|14th ACM Web Science Conference 2022|With this half-day, in-person workshop, we attempt to collaboratively discover best practices as well as frequent pitfalls encountered when working with Web data. Participants will first be presented with different perspectives on the significance of data quality in this specific context and familiarized with existing, structured approaches for the critical reflection on and documentation of data collection processes, before being invited to share their own experiences with the collection, use and documentation of Web data. We hope to thereby inspire participants to further integrate data documentation practices into their research processes, and for us to learn from the participants’ experiences in order to improve upon existing documentation frameworks for Web data. More details of the workshop, including the planned activities can be found at&nbsp;https://frohleon.github.io/DocuWeb22/.|10.1145/3501247.3539504|https://doi.org/10.1145/3501247.3539504|New York, NY, USA|Association for Computing Machinery|9781450391917|2022|Documenting Web Data for Social Research (#DocuWeb22): A Participatory Workshop for Developing Structured and Reusable Practices|"\"Sen, Indira and Fr\"\"{o}hling, Leon and Weller, Katrin\""|inproceedings|10.1145/3501247.3539504|||||||||||||||||||||||||||||1574120680|42
||Agent-based modeling, Big data, Online review systems, Clicking position, Posting behavior||161-174||In online review systems, a participant's level of knowledge impacts his/her posting behaviors, and an increase in knowledge occurs when the participant reads the reviews posted on the systems. To capture the collective dynamics of posting reviews, we used real-world big data collected over 153 months to drive an agent-based model for replicating the operation process of online review systems. The model explains the effects of clicking position (e.g., on a review webpage's serial list) and the number of items per webpage on posting contributions. Reading reviews from the last webpage only, or from the first webpage and last webpage simultaneously, can promote a greater review volume than reading reviews in other positions. This illustrates that representing primacy (first items) and recency (recent items) within one page simultaneously, or displaying recent items in reverse chronological order, are relatively better strategies for the webpage display of online reviews. The number of items plays a nonlinear moderating role in bridging the clicking position and posting behavior, and we determine the optimal number of items. To effectively establish strategies for webpage design in online review systems, business managers must switch from reliance on experience to reliance on an agent-based model as a decision support system for the formalized webpage design of online review systems.|https://doi.org/10.1016/j.ins.2019.09.053|https://www.sciencedirect.com/science/article/pii/S0020025519309089||||2020|Clicking position and user posting behavior in online review systems: A data-driven agent-based modeling approach|Guoyin Jiang and Xiaodong Feng and Wenping Liu and Xingjun Liu|article|JIANG2020161|||Information Sciences|00200255||512|||||15134|1,524|Q1|184|928|2184|40007|17554|2168|7,89|43,11|United States|Northern America|1968-2021|Artificial Intelligence (Q1); Computer Science Applications (Q1); Control and Systems Engineering (Q1); Information Systems and Management (Q1); Software (Q1); Theoretical Computer Science (Q1)|44,038|6.795|0.04908|1574943551|1633962588
||Data quality-aware queries, Big data computing, Empirical evaluation||114858||Combining query processing techniques with data quality management approaches enables enforcement of quality constraints, such as timeliness, accuracy and completeness, as part of ad-hoc query specification and execution, improving the quality of query results. Despite the emergence of novel data quality processing tools, there is a dearth of studies assessing performance and scalability in the execution of data quality assessment tasks during query processing. This paper reports on an empirical study aiming to investigate the extent to which a big data computing framework (Spark) can offer significant gains in performance and scalability when executing data quality querying tasks over a range of computational platforms including a single commodity multi-core machine and a cluster-based platform for a wide range of workloads. Our results show that substantial performance and scalability gains can be obtained by using optimized data science libraries combined with the parallel and distributed capabilities of big data computing. We also provide guidelines on choosing the appropriate computational infrastructure for executing DQ-aware queries.|https://doi.org/10.1016/j.eswa.2021.114858|https://www.sciencedirect.com/science/article/pii/S0957417421002992||||2021|Experimenting with big data computing for scaling data quality-aware query processing|Sonia Cisneros-Cabrera and Anna-Valentini Michailidou and Sandra Sampaio and Pedro Sampaio and Anastasios Gounaris|article|CISNEROSCABRERA2021114858|||Expert Systems with Applications|09574174||178|||||24201|1,368|Q1|207|770|1945|42314|17345|1943|8,67|54,95|United Kingdom|Western Europe|1990-2021|Artificial Intelligence (Q1); Computer Science Applications (Q1); Engineering (miscellaneous) (Q1)|55,444|6.954|0.04053|1575285975|1377770283
||deep learning, software security, Machine learning|27|||Security patches in open source software, providing security fixes to identified vulnerabilities, are crucial in protecting against cyber attacks. Security advisories and announcements are often publicly released to inform the users about potential security vulnerability. Despite the National Vulnerability Database (NVD) publishes identified vulnerabilities, a vast majority of vulnerabilities and their corresponding security patches remain beyond public exposure, e.g., in the open source libraries that are heavily relied on by developers. As many of these patches exist in open sourced projects, the problem of curating and gathering security patches can be difficult due to their hidden nature. An extensive and complete security patches dataset could help end-users such as security companies, e.g., building a security knowledge base, or researcher, e.g., aiding in vulnerability research.To efficiently curate security patches including undisclosed patches at large scale and low cost, we propose a deep neural-network-based approach built upon commits of open source repositories. First, we design and build security patch datasets that include 38,291 security-related commits and 1,045 Common Vulnerabilities and Exposures (CVE) patches from four large-scale C programming language libraries. We manually verify each commit, among the 38,291 security-related commits, to determine if they are security related.We devise and implement a deep learning-based security patch identification system that consists of two composite neural networks: one commit-message neural network that utilizes pretrained word representations learned from our commits dataset and one code-revision neural network that takes code before revision and after revision and learns the distinction on the statement level. Our system leverages the power of the two networks for Security Patch Identification. Evaluation results show that our system significantly outperforms SVM and K-fold stacking algorithms. The result on the combined dataset achieves as high as 87.93% F1-score and precision of 86.24%.We deployed our pipeline and learned model in an industrial production environment to evaluate the generalization ability of our approach. The industrial dataset consists of 298,917 commits from 410 new libraries that range from a wide functionalities. Our experiment results and observation on the industrial dataset proved that our approach can identify security patches effectively among open sourced projects.|10.1145/3468854|https://doi.org/10.1145/3468854|New York, NY, USA|Association for Computing Machinery||2021|SPI: Automated Identification of Security Patches via Commits|Zhou, Yaqin and Siow, Jing Kai and Wang, Chenyu and Liu, Shangqing and Liu, Yang|article|10.1145/3468854|13|sep|ACM Trans. Softw. Eng. Methodol.|1049331X|1|31|January 2022||||||||||||||||||||||1575388065|1429302734
DSIT 2021|Shanghai, China|Big data, Decision tree, Data mining, Publishing topics|5|448–452|2021 4th International Conference on Data Science and Information Technology|As one of the important traditional industries in China, the printing and publishing industry is facing the current situation of the Internet era with the explosion of information and people's demands tend to be personalized and diversified.How to achieve accurate topic selection is the key.In this context, this paper combines the most popular big data technology with the traditional printing industry, improves the quality of the original data of the publishing house through data preprocessing technology, classifies different types of data by decision tree classifier, and finally completes the data mining.It provides a new thought and method for the topic planning of publishing industry.|10.1145/3478905.3478999|https://doi.org/10.1145/3478905.3478999|New York, NY, USA|Association for Computing Machinery|9781450390248|2021|Research on Publisher Topic Selection Based on Data Mining|Huang, Yongliang and Yang, Shulin and Li, Xiang and Peng, Jiao and Zhou, Meiqi|inproceedings|10.1145/3478905.3478999|||||||||||||||||||||||||||||1577930309|42
WIMS '14|Thessaloniki, Greece|Navigation, Semantic networks, User issues, Architectures, Hypertext/Hypermedia|8||Proceedings of the 4th International Conference on Web Intelligence, Mining and Semantics (WIMS14)|"\"The new world of big data, of the LOD cloud, of the app economy, and of social media means that organisations no longer own, much less control, all the data they need to make the best informed business decisions. In this paper, we describe how we built a system using Linked Data principles to bring in data from Web 2.0 sites (LinkedIn, Salesforce), and other external business sites such as OpenCorporates, linking these together with pertinent internal British Telecommunications enterprise data into that enterprise data space. We describe the challenges faced during the implementation, which include sourcing the datasets, finding the appropriate \"\"join points\"\" from the individual datasets, as well as developing the client application used for data publication. We describe our solutions to these challenges and discuss the design decisions made. We conclude by drawing some general principles from this work.\""|10.1145/2611040.2611086|https://doi.org/10.1145/2611040.2611086|New York, NY, USA|Association for Computing Machinery|9781450325387|2014|Linking Social, Open, and Enterprise Data|Omitola, Tope and Davies, John and Duke, Alistair and Glaser, Hugh and Shadbolt, Nigel|inproceedings|10.1145/2611040.2611086|41||||||||||||||||||||||||||||1580560221|42
DH '16|Montr\'{e}al, Qu\'{e}bec, Canada|forensics, ehealth, crime, big data, wearables, law|5|23–27|Proceedings of the 6th International Conference on Digital Health Conference|Advances in mobile and computer technology are combining to create massive changes in the way data about human health and well-being are gathered and used. As the trend toward wearable and ubiquitous health tracking devices moves forward, the sheer quantity of new data from a wide variety of devices presents challenges for analysts. In the coming years, this data will inevitably be used in the criminal and civil justice systems. However, the tools to make full use of it are currently lacking. This paper discusses scenarios where data collected from health and fitness related devices may intersect with legal requirements such as investigations into insurance fraud or even murder. The conclusion is that there is much work to be done to enable reliable investigations. This should include at least the establishment of an organization to promote development of the field, development of cross-disciplinary education materials, and the creation of an open data bank for information sharing.|10.1145/2896338.2896341|https://doi.org/10.1145/2896338.2896341|New York, NY, USA|Association for Computing Machinery|9781450342247|2016|Medical Device Data Goes to Court|Vandervort, David|inproceedings|10.1145/2896338.2896341|||||||||||||||||||||||||||||1585185888|42
||Big Data, Methodology, Decisional systems, Agile, Data governance, Data quality||101862||Decision making is the lifeblood of the enterprise — from the mundane to the strategically critical. However, the increasing deluge of data makes it more important than ever to understand and use it effectively in every context. Being “data driven” is more aspiration than reality in most organizations due to the complexity, volume, variability and velocity of data streams from every customer and employee interaction. The purpose of this paper is to provide a flexible and adaptable methodology for governing, managing and applying data throughout the enterprise, called DECIDE.|https://doi.org/10.1016/j.datak.2020.101862|https://www.sciencedirect.com/science/article/pii/S0169023X19303830||||2020|DECIDE: An Agile event-and-data driven design methodology for decisional Big Data projects|Lilia Sfaxi and Mohamed Mehdi Ben Aissa|article|SFAXI2020101862|||Data & Knowledge Engineering|0169023X||130|||||24437|0,480|Q2|87|37|155|1512|440|152|2,55|40,86|Netherlands|Western Europe|1985, 1987-2020|Information Systems and Management (Q2)||||1588842100|1516868485
||Interpolation;Task analysis;Inspection;Cloud computing;Roads;Data integrity;Conferences;Dependability;Interpolation;Sequence Matching;Data Quality;Big Data;Internet-of-Vehicles||1-7|2018 IEEE International Conference on Pervasive Computing and Communications (PerCom)|Interconnection and intelligence have become the latest trends of the new generation of vehicle and transportation technologies. Applications built upon platforms of cloud-centered vehicle networking, i.e., Internet-of-Vehicles (IoVs), have been increasingly developed and deployed to provide data-centric services (e.g., driving assistance). Because these services are often safety critical, assuring service dependability has become an important requirement. In this paper, we propose DQI, a platform-level solution of Data-Quality Improvement designed to assure service dependability for Internet-of-Vehicle services. As an example, DQI is deployed in CarStream, an industrial system of big data processing designed for chauffeured car services. Via CarStream, over 30,000 vehicles are organized in a virtual vehicle network by sharing vehicle-status data in a near real-time manner. Such data often have low-quality issues and compromise the dependability of data-centric services. DQI includes techniques of data-quality improvement, including detecting outliers, extracting frequent patterns, and interpolating sequences. DQI enhances the dependability of data-centric services in IoVs by addressing the common data-quality requirements at the platform level. Upper-level services can benefit from DQI for data-quality improvement and reduce the complexity of service logic. We evaluate DQI by using a three-year dataset of vehicles and real applications deployed in CarStream. The result shows that compared with existing approaches, DQI can effectively restore missing data and correct anomalies with more than 30.0% improvement in precision. By studying multiple real applications, we also show that this data-quality improvement can indeed enhance the dependability of IoV services.|10.1109/PERCOM.2018.8444581|||||2018|A Platform Solution of Data-Quality Improvement for Internet-of-Vehicle Services|Zhang, Mingming and Wo, Tianyu and Xie, Tao|inproceedings|8444581||March||2474249X|||||||||||||||||||||||||1591924443|1978417608
||privacy by design, Privacy attributes, privacy factors, privacy icons, privacy labels|37|||Privacy visualizations help users understand the privacy implications of using an online service. Privacy by Design guidelines provide generally accepted privacy standards for developers of online services. To obtain a comprehensive understanding of online privacy, we review established approaches, distill a unified list of 15 privacy attributes and rank them based on perceived importance by users and privacy experts. We then discuss similarities, explain notable differences, and examine trends in terms of the attributes covered. Finally, we show how our results provide a foundation for user-centric privacy visualizations, inspire best practices for developers, and give structure to privacy policies.|10.1145/3502288|https://doi.org/10.1145/3502288|New York, NY, USA|Association for Computing Machinery||2022|Understanding Online Privacy—A Systematic Review of Privacy Visualizations and Privacy by Design Guidelines|Barth, Susanne and Ionita, Dan and Hartel, Pieter|article|10.1145/3502288|63|feb|ACM Comput. Surv.|03600300|3|55|April 2023||||23038|2,079|Q1|163|112|365|15859|6978|365|19,16|141,60|United States|Northern America|1969-2020|Computer Science (miscellaneous) (Q1); Theoretical Computer Science (Q1)|10,790|10.282|0.01438|1593376978|1517405264
CHI '18|Montreal QC, Canada|law enforcement, data-driven organizations, data practices, challenges, policing, metis|13|1–13|Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems|Proponents of data-driven policing strategies claim that it makes policing organizations more effective, efficient, and accountable and has the potential to address some policing social criticisms (e.g. racial bias, lack of accountability and training). What remains less understood are the challenges when adopting data-driven policing as a response to these criticisms. We present results from a qualitative field study about the adoption of data-driven policing strategies in a Midwestern police department in the United States. We identify three key challenges police face with data-driven adoption efforts: data-driven frictions, precarious and inactionable insights, and police metis concerns. We demonstrate the issues that data-driven initiatives create for policing and the open questions police agents face. These findings contribute an empirical account of how policing agents attend to the strengths and limits of big data's knowledge claims. Lastly, we present data and design implications for policing.|10.1145/3173574.3174043|https://doi.org/10.1145/3173574.3174043|New York, NY, USA|Association for Computing Machinery|9781450356206|2018|Confronting Social Criticisms: Challenges When Adopting Data-Driven Policing Strategies|Verma, Nitya and Dombrowski, Lynn|inproceedings|10.1145/3173574.3174043|||||||||||||||||||||||||||||1601428264|42
|||4|50–53|||10.1145/3349629|https://doi.org/10.1145/3349629|New York, NY, USA|Association for Computing Machinery||2019|Digital Transformation in the Indian Government|Verma, Neeta and Dawar, Savita|article|10.1145/3349629||oct|Commun. ACM|00010782|11|62|November 2019||||||||||||||||||||||1602322708|647144465
||Ontology, domain ontology, evaluation, metrics, applied ontology, ontology application, ontology development, task-ontology fit, assessment|44|||The number of applications being developed that require access to knowledge about the real world has increased rapidly over the past two decades. Domain ontologies, which formalize the terms being used in a discipline, have become essential for research in areas such as Machine Learning, the Internet of Things, Robotics, and Natural Language Processing, because they enable separate systems to exchange information. The quality of these domain ontologies, however, must be ensured for meaningful communication. Assessing the quality of domain ontologies for their suitability to potential applications remains difficult, even though a variety of frameworks and metrics have been developed for doing so. This article reviews domain ontology assessment efforts to highlight the work that has been carried out and to clarify the important issues that remain. These assessment efforts are classified into five distinct evaluation approaches and the state of the art of each described. Challenges associated with domain ontology assessment are outlined and recommendations are made for future research and applications.|10.1145/3329124|https://doi.org/10.1145/3329124|New York, NY, USA|Association for Computing Machinery||2019|Evaluating Domain Ontologies: Clarification, Classification, and Challenges|McDaniel, Melinda and Storey, Veda C.|article|10.1145/3329124|70|sep|ACM Comput. Surv.|03600300|4|52|July 2020||||23038|2,079|Q1|163|112|365|15859|6978|365|19,16|141,60|United States|Northern America|1969-2020|Computer Science (miscellaneous) (Q1); Theoretical Computer Science (Q1)|10,790|10.282|0.01438|1602395948|1517405264
|||12|1118–1129||Driven by the dominance of the relational model, the requirements of modern applications, and the veracity of data, we revisit the fundamental notion of a key in relational databases with NULLs. In SQL database systems primary key columns are NOT NULL by default. NULL columns may occur in unique constraints which only guarantee uniqueness for tuples which do not feature null markers in any of the columns involved, and therefore serve a different function than primary keys. We investigate the notions of possible and certain keys, which are keys that hold in some or all possible worlds that can originate from an SQL table, respectively. Possible keys coincide with the unique constraint of SQL, and thus provide a semantics for their syntactic definition in the SQL standard. Certain keys extend primary keys to include NULL columns, and thus form a sufficient and necessary condition to identify tuples uniquely, while primary keys are only sufficient for that purpose. In addition to basic characterization, axiomatization, and simple discovery approaches for possible and certain keys, we investigate the existence and construction of Armstrong tables, and describe an indexing scheme for enforcing certain keys. Our experiments show that certain keys with NULLs do occur in real-world databases, and that related computational problems can be solved efficiently. Certain keys are therefore semantically well-founded and able to maintain data quality in the form of Codd's entity integrity rule while handling the requirements of modern applications, that is, higher volumes of incomplete data from different formats.|10.14778/2809974.2809975|https://doi.org/10.14778/2809974.2809975||VLDB Endowment||2018|Possible and Certain SQL Keys|"\"K\"\"{o}hler, Henning and Link, Sebastian and Zhou, Xiaofang\""|article|10.14778/2809974.2809975||oct|Proc. VLDB Endow.|21508097|11|8|July 2015||||21100199855|0,946|Q1|134|246|598|12401|3759|566|5,50|50,41|United States|Northern America|2008-2020|Computer Science (miscellaneous) (Q1)|7,198|2.047|0.00891|1604142901|1216159931
||Big data, Health care, Privacy security risk, Privacy measures||103-110||In order to explore the development of healthcare in China and the privacy and security risk factors in medical data under the background of big data, the development status of China’s healthcare sector is analyzed. The questionnaire is used to analyze the privacy and security risk factors of healthcare big data and protection measures are put forward based on the data privacy and security risk factors in the context of cloud services in the literature. The results show that in recent years, the number of health institutions, the number of medical personnel, the assets of medical institutions, the per capita hospitalization cost, and the insured population all show a trend of increasing year by year; while in 2017, the crude mortality rate of malignant tumor patients was the highest in China, and the mortality rate of rural patients was higher than that of urban patients. The results of the questionnaire show that the probability of data analysis, medical treatment process, disease diagnosis process, lack of protective measures, and imperfect access system is all greater than 0.8 when medical care big data is oriented to cloud services. Based on this, two levels of privacy protection measures are proposed: technology and management. It indicates that medical institutions need to pay attention to data privacy protection and grasp the use of digital medical data to provide decision support for subsequent medical data analysis.|https://doi.org/10.1016/j.future.2020.03.039|https://www.sciencedirect.com/science/article/pii/S0167739X20304829||||2020|Analysis of healthcare big data|Zhihan Lv and Liang Qiao|article|LV2020103|||Future Generation Computer Systems|0167739X||109|||||12264|1,262|Q1|119|798|1935|36054|16877|1868|9,11|45,18|Netherlands|Western Europe|1984-2021|Computer Networks and Communications (Q1); Hardware and Architecture (Q1); Software (Q1)||||1604236354|562237118
||Logic gates;Data integrity;Task analysis;Maintenance engineering;Blockchain;Industrial Internet of Things;Internet of Things;Data Validation;Cluster-Based Data Validation;Big Data||78-86|2019 Third World Conference on Smart Trends in Systems Security and Sustainablity (WorldS4)|Ensuring data quality is central to the digital transformation in industry. Business processes such as predictive maintenance or condition monitoring can be implemented or improved based on the available data. In order to guarantee high data quality, a single data validation system are usually used to validate the production data for further use. However, using a single system allows an attacker only to perform one successful attack to corrupt the whole system. We present a new approach in which a data validation system using multiple different validators minimizes the probability of success for the attacker. The validators are arranged in clusters based on their properties. For a validation process, a challenge is given that specifies which validators should perform the current validation. Validation results from other validators are dropped. This ensures that even for more than half of the validators being corrupted anomalies can be detected during the validation process.|10.1109/WorldS4.2019.8904039|||||2019|Safeguarding Data Integrity by Cluster-Based Data Validation Network|Wallis, Kevin and Schillinger, Fabian and Reich, Christoph and Schindelhauer, Christian|inproceedings|8904039||July|||||||||||||||||||||||||||1604385880|42
|||10|72–81||Knowing where you are in space and time promises a deeper understanding of neighbors, ecosystems, and the environment.|10.1145/2756547|https://doi.org/10.1145/2756547|New York, NY, USA|Association for Computing Machinery||2015|Spatial Computing|Shekhar, Shashi and Feiner, Steven K. and Aref, Walid G.|article|10.1145/2756547||dec|Commun. ACM|00010782|1|59|January 2016||||||||||||||||||||||1606615832|647144465
|||||Data Cleaning|Data quality is one of the most important problems in data management, since dirty data often leads to inaccurate data analytics results and incorrect business decisions. Poor data across businesses and the U.S. government are reported to cost trillions of dollars a year. Multiple surveys show that dirty data is the most common barrier faced by data scientists. Not surprisingly, developing effective and efficient data cleaning solutions is challenging and is rife with deep theoretical and engineering problems.This book is about data cleaning, which is used to refer to all kinds of tasks and activities to detect and repair errors in the data. Rather than focus on a particular data cleaning task, we give an overview of the endto- end data cleaning process, describing various error detection and repair methods, and attempt to anchor these proposals with multiple taxonomies and views. Specifically, we cover four of the most common and important data cleaning tasks, namely, outlier detection, data transformation, error repair (including imputing missing values), and data deduplication. Furthermore, due to the increasing popularity and applicability of machine learning techniques, we include a chapter that specifically explores how machine learning techniques are used for data cleaning, and how data cleaning is used to improve machine learning models.This book is intended to serve as a useful reference for researchers and practitioners who are interested in the area of data quality and data cleaning. It can also be used as a textbook for a graduate course. Although we aim at covering state-of-the-art algorithms and techniques, we recognize that data cleaning is still an active field of research and therefore provide future directions of research whenever appropriate.||https://doi.org/10.1145/3310205.3310207|New York, NY, USA|Association for Computing Machinery|9781450371520|2019|Introduction||inbook|10.1145/3310205.3310207|||||||||||||||||||||||||||||1606703907|42
||Technology roadmapping, Technology management, Data science, Digital transformation, Data-driven organization, Big data||121264||Leveraging data science can enable businesses to exploit data for competitive advantage by generating valuable insights. However, many industries cannot effectively incorporate data science into their business processes, as there is no comprehensive approach that allows strategic planning for organization-wide data science efforts and data assets. Accordingly, this study explores the Data Science Roadmapping (DSR) to guide organizations in aligning their business strategies with data-related, technological, and organizational resources. The proposed approach is built on the widely adopted technology roadmapping framework and customizes its context, architecture, and process by synthesizing data science, big data, and data-driven organization literature. Based on industry collaborations, the framework provides a hybrid and agile methodology comprising the recommended steps. We applied DSR with a research group with sector experience to create a comprehensive data science roadmap to validate and refine the framework. The results indicate that the framework facilitates DSR initiatives by creating a comprehensive roadmap capturing strategy, data, technology, and organizational perspectives. The contemporary literature illustrates prebuilt roadmaps to help businesses become data-driven. However, becoming data-driven also necessitates significant social change toward openness and trust. The DSR initiative can facilitate this social change by opening communication channels, aligning perspectives, and generating consensus among stakeholders.|https://doi.org/10.1016/j.techfore.2021.121264|https://www.sciencedirect.com/science/article/pii/S0040162521006983||||2022|Data science roadmapping: An architectural framework for facilitating transformation towards a data-driven organization|Kerem Kayabay and Mert Onuralp Gökalp and Ebru Gökalp and P. {Erhan Eren} and Altan Koçyiğit|article|KAYABAY2022121264|||Technological Forecasting and Social Change|00401625||174|||||14704|2,226|Q1|117|448|1099|35581|10127|1061|9,01|79,42|United States|Northern America|1970-2020|Applied Psychology (Q1); Business and International Management (Q1); Management of Technology and Innovation (Q1)|21,116|8.593|0.02416|1606862442|1949868303
|||4|10–13|||10.1145/2334184.2334188|https://doi.org/10.1145/2334184.2334188|New York, NY, USA|Association for Computing Machinery||2012|From Data Divination to Data-Aware Design|Churchill, Elizabeth F.|article|10.1145/2334184.2334188||sep|Interactions|10725520|5|19|September + October 2012||||4000148705|0,247|Q3|46|125|292|742|503|292|1,82|5,94|United States|Northern America|1994-1995, 1997, 2006-2020|Human-Computer Interaction (Q3)||||1609046406|2035703392
dg.o '16|Shanghai, China|and penal law, legal design, open justice, effectivity, smart governance, efficiency, Law enforcement|10|293–302|Proceedings of the 17th International Digital Government Research Conference on Digital Government Research|While in business and private settings the disruptive impact of advanced information communication technology (ICT) have already been felt, the legal sector is now starting to face great disruptions due to such ICTs. Bits and pieces of innovations in the legal sector have been emerging for some time, affecting the performance of core functions and the legitimacy of public institutions.In this paper, we present our framework for enabling the smart government vision, particularly for the case of criminal justice systems, by unifying different isolated ICT-based solutions. Our framework, coined as Legal Logistics, supports the well-functioning of a legal system in order to streamline the innovations in these legal systems. The framework targets the exploitation of all relevant data generated by the ICT-based solutions. As will be illustrated for the Dutch criminal justice system, the framework may be used to integrate different ICT-based innovations and to gain insights about the well-functioning of the system. Furthermore, Legal Logistics can be regarded as a roadmap towards a smart and open justice.|10.1145/2912160.2912180|https://doi.org/10.1145/2912160.2912180|New York, NY, USA|Association for Computing Machinery|9781450343398|2016|On Enabling Smart Government: A Legal Logistics Framework for Future Criminal Justice Systems|Netten, Niels and Bargh, Mortaza S. and van den Braak, Susan and Choenni, Sunil and Leeuw, Frans|inproceedings|10.1145/2912160.2912180|||||||||||||||||||||||||||||1609213525|42
||‘Big data’, Analytics, Business value, Issues, Case study, Emergency services, Literature review||234-246||Big data has the potential to revolutionize the art of management. Despite the high operational and strategic impacts, there is a paucity of empirical research to assess the business value of big data. Drawing on a systematic review and case study findings, this paper presents an interpretive framework that analyzes the definitional perspectives and the applications of big data. The paper also provides a general taxonomy that helps broaden the understanding of big data and its role in capturing business value. The synthesis of the diverse concepts within the literature on big data provides deeper insights into achieving value through big data strategy and implementation.|https://doi.org/10.1016/j.ijpe.2014.12.031|https://www.sciencedirect.com/science/article/pii/S0925527314004253||||2015|How ‘big data’ can make big impact: Findings from a systematic review and a longitudinal case study|Samuel {Fosso Wamba} and Shahriar Akter and Andrew Edwards and Geoffrey Chopin and Denis Gnanzou|article|FOSSOWAMBA2015234|||International Journal of Production Economics|09255273||165|||||19165|2,406|Q1|185|327|891|21712|8124|881|8,31|66,40|Netherlands|Western Europe|1991-2021|Business, Management and Accounting (miscellaneous) (Q1); Economics and Econometrics (Q1); Industrial and Manufacturing Engineering (Q1); Management Science and Operations Research (Q1)|32,606|7.885|0.0228|1609678343|850534974
MEDES '19|Limassol, Cyprus|Conceptual model, Data Quality Characteristics, Data quality, Data Quality requirements, Citizen science|8|166–173|Proceedings of the 11th International Conference on Management of Digital EcoSystems|Data quality is an important aspect in many fields. In citizen science application databases, data quality is often found lacking, which is why there needs to be a method of integrating data quality into the design. This paper tackles the problem by dividing data quality into separate characteristics according to the ISO / IEC 25012 standard. These characteristics are integrated into a conceptual model of the system and data model for citizen science applications. Furthermore, the paper describes a way to measure data quality using the data quality characteristics. The models and measuring methods are theoretical and can be adapted into case specific designs.|10.1145/3297662.3365797|https://doi.org/10.1145/3297662.3365797|New York, NY, USA|Association for Computing Machinery|9781450362382|2020|Integrating Data Quality Requirements to Citizen Science Application Design|Musto, Jiri and Dahanayake, Ajantha|inproceedings|10.1145/3297662.3365797|||||||||||||||||||||||||||||1611144771|42
BDCA'17|Tetouan, Morocco|Cloud Storage, Cloud Computing, Classification, Data Security|5||Proceedings of the 2nd International Conference on Big Data, Cloud and Applications|Cloud computing is a wide architecture based on diverse models for providing different services of software and hardware. Cloud computing paradigm attracts different users because of its several benefits such as high resource elasticity, expense reduction, scalability and simplicity which provide significant preserving in terms of investment and work force. However, the new approaches introduced by the cloud, related to computation outsourcing, distributed resources, multi-tenancy concept, high dynamism of the model, data warehousing and the nontransparent style of cloud increase the security and privacy concerns and makes building and handling trust among cloud service providers and consumers a critical security challenge. This paper proposes a new approach to improve security of data in cloud computing. It suggests a classification model to categorize data before being introduced into a suitable encryption system according to the category. Since data in cloud has not the same sensitivity level, encrypting it with the same algorithms can lead to a lack of security or of resources. By this method we try to optimize the resources consumption and the computation cost while ensuring data confidentiality.|10.1145/3090354.3090404|https://doi.org/10.1145/3090354.3090404|New York, NY, USA|Association for Computing Machinery|9781450348522|2017|Securing Data in Cloud Computing by Classification|Ennajjar, Ibtissam and Tabii, Youness and Benkaddour, Abdelhamid|inproceedings|10.1145/3090354.3090404|49||||||||||||||||||||||||||||1611878844|42
Semantics2017|Amsterdam, Netherlands||4|181–184|Proceedings of the 13th International Conference on Semantic Systems|Many data scientists make use of Linked Open Data (LOD) as a huge interconnected knowledge base represented in RDF. However, the distributed nature of the information and the lack of a scalable approach to manage and consume such Big Semantic Data makes it difficult and expensive to conduct large-scale studies. As a consequence, most scientists restrict their analyses to one or two datasets (often DBpedia) that contain at most hundreds of millions of triples. LOD-a-lot is a dataset that integrates a large portion (over 28 billion triples) of the LOD Cloud into a single ready-to-consume file that can be easily downloaded, shared and queried with a small memory footprint. This paper shows there exists a wide collection of Data Science use cases that can be performed over such a LOD-a-lot file. For these use cases LOD-a-lot significantly reduces the cost and complexity of conducting Data Science.|10.1145/3132218.3132241|https://doi.org/10.1145/3132218.3132241|New York, NY, USA|Association for Computing Machinery|9781450352963|2017|LOD-a-Lot: A Single-File Enabler for Data Science|Beek, Wouter and Fern\'{a}ndez, Javier D. and Verborgh, Ruben|inproceedings|10.1145/3132218.3132241|||||||||||||||||||||||||||||1613356283|42
||privacy, Human mobility, data mining|27|||Human mobility data are an important proxy to understand human mobility dynamics, develop analytical services, and design mathematical models for simulation and what-if analysis. Unfortunately mobility data are very sensitive since they may enable the re-identification of individuals in a database. Existing frameworks for privacy risk assessment provide data providers with tools to control and mitigate privacy risks, but they suffer two main shortcomings: (i) they have a high computational complexity; (ii) the privacy risk must be recomputed every time new data records become available and for every selection of individuals, geographic areas, or time windows. In this article, we propose a fast and flexible approach to estimate privacy risk in human mobility data. The idea is to train classifiers to capture the relation between individual mobility patterns and the level of privacy risk of individuals. We show the effectiveness of our approach by an extensive experiment on real-world GPS data in two urban areas and investigate the relations between human mobility patterns and the privacy risk of individuals.|10.1145/3106774|https://doi.org/10.1145/3106774|New York, NY, USA|Association for Computing Machinery||2017|A Data Mining Approach to Assess Privacy Risk in Human Mobility Data|Pellungrini, Roberto and Pappalardo, Luca and Pratesi, Francesca and Monreale, Anna|article|10.1145/3106774|31|dec|ACM Trans. Intell. Syst. Technol.|21576904|3|9|May 2018||||||||||||||||||||||1613843387|273436860
||Arrays;Sensors;Kernel;Data analysis;Engines;Feature extraction;ArrayUDF;distributed acoustic sensing;local similarity||37-46|2018 IEEE/ACM Machine Learning in HPC Environments (MLHPC)|As new scientific instruments generate ever more data, we need to parallelize advanced data analysis algorithms such as machine learning to harness the available computing power. The success of commercial Big Data systems demonstrated that it is possible to automatically parallelize many algorithms. However, these Big Data tools have trouble handling the complex analysis operations from scientific applications. To overcome this difficulty, we have started to build an automated parallel data processing engine for science, known as ARRAYUDF. This paper provides an overview of this data processing engine, and a use case involving a feature extraction task from a large-scale seismic recording technology, called distributed acoustic sensing (DAS). The key challenge associated with DAS data sets is that they are vast in volume and noisy in data quality. The existing methods used by the DAS team for extracting useful signals like traveling seismic waves are complex and very time-consuming. Our parallel data processing engine reduces the job execution time from 10s of hours to 10s of seconds, and achieves 95% parallelization efficiency. ARRAYUDF could be used to implement more advanced data processing algorithms including machine learning, and could work with many more applications.|10.1109/MLHPC.2018.8638638|||||2018|Automated Parallel Data Processing Engine with Application to Large-Scale Feature Extraction|Xing, Xin and Dong, Bin and Ajo-Franklin, Jonathan and Wu, Kesheng|inproceedings|8638638||Nov|||||||||||||||||||||||||||1615063646|42
WSC '19|National Harbor, Maryland||12|548–559|Proceedings of the Winter Simulation Conference|Simulation of Supply Chains comprises huge amounts of data, resulting in numerous entities flowing in the model. These networks are highly dynamic systems, where entities' relationships and other elements evolve with time, paving the way for real-time Supply Chain decision-support tools capable of using real data. In light of this, a solution comprising of a Big Data Warehouse to store relevant data and a simulation model of an automotive plant, are being developed. The purpose of this paper is to address the modelling approach, which allowed the simulation model to automatically adapt to the data stored in a Big Data Warehouse and thus adapt to new scenarios without manual intervention. The main characteristics of the conceived solution were demonstrated, with emphasis to the real-time and the ability to allow the model to load the state of the system from the Big Data Warehouse.||||IEEE Press|9781728132839|2020|Real-Time Supply Chain Simulation: A Big Data-Driven Approach|Vieira, Ant\'{o}nio A. C. and Dias, Lu\'{\i}s M. S. and Santos, Maribel Y. and Pereira, Guilherme A. B. and Oliveira, Jos\'{e} A.|inproceedings|10.5555/3400397.3400442|||||||||||||||||||||||||||||1617641834|42
||Data mining;Medical diagnostic imaging;Currencies;Tools;Medical services;Measurement;data quality;decision quality;healthcare;clinical data;EHRs;data quality assurance;stratified sampling;bias||2612-2619|2017 IEEE International Conference on Big Data (Big Data)|This paper provides a practical guideline for the assurance and (re-)usage of clinical data. It proposes a process which aims to provide a systematic data quality assurance even without involving a medical domain expert. Especially when (re-)using clinical data, data quality is an important topic because clinical data are not purposely collected. Therefore, data driven conclusions might be false, because a given dataset is not representative. These false data driven conclusions could even harm the life of patients. Thus, all researchers should adhere to some basic principles that can prevent false conclusions. Twelve empirical experiments were conducted in order to prove that my process is able to assure data quality with respect to the descriptive and predictive analysis. Descriptive results obtained by applying stratified sampling are conflicting in four out of nine population inputs. Sampling is carried based on the top ranked feature drawn by the Contextual Data Quality Assurance (CDQA). Between datasets these features are confirmed by the Mutual Data Quality Assurance (MDQA). Stratified sampled inputs improve predictive results compared to raw data. Both Area Under the Curve (AUC) scores and accuracy scores increase by three percent.|10.1109/BigData.2017.8258221|||||2017|Is data quality enough for a clinical decision?: Apply machine learning and avoid bias|Hee, Kim|inproceedings|8258221||Dec|||||||||||||||||||||||||||1617956913|42
|||14|6–19||"\"The analytics platform at Twitter has experienced tremendous growth over the past few years in terms of size, complexity, number of users, and variety of use cases. In this paper, we discuss the evolution of our infrastructure and the development of capabilities for data mining on \"\"big data\"\". One important lesson is that successful big data mining in practice is about much more than what most academics would consider data mining: life \"\"in the trenches\"\" is occupied by much preparatory work that precedes the application of data mining algorithms and followed by substantial effort to turn preliminary models into robust solutions. In this context, we discuss two topics: First, schemas play an important role in helping data scientists understand petabyte-scale data stores, but they're insufficient to provide an overall \"\"big picture\"\" of the data available to generate insights. Second, we observe that a major challenge in building data analytics platforms stems from the heterogeneity of the various components that must be integrated together into production workflows---we refer to this as \"\"plumbing\"\". This paper has two goals: For practitioners, we hope to share our experiences to flatten bumps in the road for those who come after us. For academic researchers, we hope to provide a broader context for data mining in production environments, pointing out opportunities for future work.\""|10.1145/2481244.2481247|https://doi.org/10.1145/2481244.2481247|New York, NY, USA|Association for Computing Machinery||2013|Scaling Big Data Mining Infrastructure: The Twitter Experience|Lin, Jimmy and Ryaboy, Dmitriy|article|10.1145/2481244.2481247||apr|SIGKDD Explor. Newsl.|19310145|2|14|December 2012||||||||||||||||||||||1621171855|22833317
SIGMOD '18|Houston, TX, USA|parallel scalable, fixed-parameter tractability, gfd discovery|13|427–439|Proceedings of the 2018 International Conference on Management of Data|This paper studies discovery of GFDs, a class of functional dependencies defined on graphs. We investigate the fixed-parameter tractability of three fundamental problems related to GFD discovery. We show that the implication and satisfiability problems are fixed-parameter tractable, but the validation problem is co-W[1]-hard. We introduce notions of reduced GFDs and their topological support, and formalize the discovery problem for GFDs. We develop algorithms for discovering GFDs and computing their covers. Moreover, we show that GFD discovery is feasible over large-scale graphs, by providing parallel scalable algorithms for discovering GFDs that guarantee to reduce running time when more processors are used. Using real-life and synthetic data, we experimentally verify the effectiveness and scalability of the algorithms.|10.1145/3183713.3196916|https://doi.org/10.1145/3183713.3196916|New York, NY, USA|Association for Computing Machinery|9781450347037|2018|Discovering Graph Functional Dependencies|Fan, Wenfei and Hu, Chunming and Liu, Xueli and Lu, Ping|inproceedings|10.1145/3183713.3196916|||||||||||||||||||||||||||||1625071639|42
||validation, discovery, Functional dependencies, implication, graphs|42|||This article studies discovery of Graph Functional Dependencies (GFDs), a class of functional dependencies defined on graphs. We investigate the fixed-parameter tractability of three fundamental problems related to GFD discovery. We show that the implication and satisfiability problems are fixed-parameter tractable, but the validation problem is co-W[1]-hard in general. We introduce notions of reduced GFDs and their topological support, and formalize the discovery problem for GFDs. We develop algorithms for discovering GFDs and computing their covers. Moreover, we show that GFD discovery is feasible over large-scale graphs, by providing parallel scalable algorithms that guarantee to reduce running time when more processors are used. Using real-life and synthetic data, we experimentally verify the effectiveness and scalability of the algorithms.|10.1145/3397198|https://doi.org/10.1145/3397198|New York, NY, USA|Association for Computing Machinery||2020|Discovering Graph Functional Dependencies|Fan, Wenfei and Hu, Chunming and Liu, Xueli and Lu, Ping|article|10.1145/3397198|15|sep|ACM Trans. Database Syst.|03625915|3|45|September 2020||||||||||||||||||||||1625071639|1726010902
HiPCNA-PG '13|Denver, Colorado|phasor measurement unit, exploratory data analysis, divide and recombine, R, Hadoop, power systems|9||Proceedings of the 3rd International Workshop on High Performance Computing, Networking and Analytics for the Power Grid|In this paper, we present an approach to large-scale data analysis, Divide and Recombine (D&amp;R), and describe a hardware and software implementation that supports this approach. We then illustrate the use of D&amp;R on large-scale power systems sensor data to perform initial exploration, discover multiple data integrity issues, build and validate algorithms to filter bad data, and construct statistical event detection algorithms. This paper also reports on experiences using a non-traditional Hadoop distributed computing setup on top of a HPC computing cluster.|10.1145/2536780.2536783|https://doi.org/10.1145/2536780.2536783|New York, NY, USA|Association for Computing Machinery|9781450325103|2013|Large-Scale Exploratory Analysis, Cleaning, and Modeling for Event Detection in Real-World Power Systems Data|Hafen, Ryan and Gibson, Tara D. and van Dam, Kerstin Kleese and Critchlow, Terence|inproceedings|10.1145/2536780.2536783|4||||||||||||||||||||||||||||1625476962|42
https://doi.org/10.1016/j.jclinepi.2022.06.006|https://www.sciencedirect.com/science/article/pii/S0895435622001524||||2022|Record linkage and big data—enhancing information and improving design|Leslie L. Roos and Elizabeth Wall-Wieler and Charles Burchill and Naomi C. Hamm and Amani F. Hamad and Lisa M. Lix|article|ROOS202218|||Journal of Clinical Epidemiology|08954356||150||||||||||||||||||||||||||||||1626352432|42
CHI '22|New Orleans, LA, USA|Social Acceptability, Data Glasses, Personal Information, Disclosure, User Acceptance, Consent, Mixed Reality, Augmented Reality, Public Experiences, Comfort|14||Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems|Social Media (SM) has shown that we adapt our communication and disclosure behaviors to available technological opportunities. Head-mounted Augmented Reality (AR) will soon allow to effortlessly display the information we disclosed not isolated from our physical presence (e.g., on a smartphone) but visually attached to the human body. In this work, we explore how the medium (AR vs. Smartphone), our role (being augmented vs. augmenting), and characteristics of information types (e.g., level of intimacy, self-disclosed vs. non-self-disclosed) impact the users’ comfort when displaying personal information. Conducting an online survey (N=148), we found that AR technology and being augmented negatively impacted this comfort. Additionally, we report that AR mitigated the effects of information characteristics compared to those they had on smartphones. In light of our results, we discuss that information augmentation should be built on consent and openness, focusing more on the comfort of the augmented rather than the technological possibilities.|10.1145/3491102.3502140|https://doi.org/10.1145/3491102.3502140|New York, NY, USA|Association for Computing Machinery|9781450391573|2022|Consent in the Age of AR: Investigating The Comfort With Displaying Personal Information in Augmented Reality|Rixen, Jan Ole and Colley, Mark and Askari, Ali and Gugenheimer, Jan and Rukzio, Enrico|inproceedings|10.1145/3491102.3502140|295||||||||||||||||||||||||||||1627225504|42
||Google;Metadata;Logistics;Uniform resource locators;Transportation;Databases;Text mining;technology infrastructure;text mining;last mile logistics;Google Scholar||3290-3296|2017 IEEE International Conference on Big Data (Big Data)|Technology infrastructure (TechInfra) refers to metadata describing an academic field, such as journals & conferences, authors, publications and organizations. Understanding the TechInfra is often the first step in performing a literature review on a particular topic. In this paper, a study is conducted to retrieve TechInfra for a topic in supply chain management, namely, last mile logistics. Google Scholar is used as the primary tool for data collection. The first 1,000 results returned by Google Scholar are downloaded as HTML files. Subsequently, various application programming interfaces (APIs) - e.g., ScienceDirect, IEEE, CrossRef APIs - are used to enhance the data quality. Some plots are used to provide visualization of TechInfra of last mile logistics.|10.1109/BigData.2017.8258313|||||2017|Performing literature review using text mining, Part I: Retrieving technology infrastructure using Google Scholar and APIs|Yang, Dazhi and Zhang, Allan N. and Yan, Wenjing|inproceedings|8258313||Dec|||||||||||||||||||||||||||1627411344|42
||Power control data, Monitoring, Visual identification, Iterative screening, CARIMA||645-657||The operation control of power units is usually carried out by the control personnel with the help of distributed control system. Although it can ensure the safety of unit operation and meet the requirements of power generation loads, the economy of unit operation and the accuracy of control process still need to be further improved. Therefore, by designing multiple view mapping and association, it provides interactive visualization support for relevant experts in the key links of model establishment and evaluation. In the exploration stage of estimating model parameter, the user can get the delay range by line chart and focus + context technology, while in the model screening stage, the user can provide the combination of screening views, selecting the model by its accuracy on different data sets, and finding the model anomalies by the model structure view. Besides, in the model evaluation stage, the user can get the delay range by predicting line chart and model accuracy radar chart. In addition, the method in this paper keeps between 4.2–7.2 in most distributions, and the maximum value is 18. The time series trend of the data segment is consistent, and the absolute value of the weight coefficient is basically 0 after being superimposed, which has great advantages compared with other methods, proving the effective results of the research content in this paper.|https://doi.org/10.1016/j.egyr.2021.09.205|https://www.sciencedirect.com/science/article/pii/S235248472101009X||||2021|Visual recognition processing of power monitoring data based on big data computing|Jianguo Qian and Bingquan Zhu and Ying Li and Zhengchai Shi|article|QIAN2021645|||Energy Reports|23524847||7||2021 International Conference on Energy Engineering and Power Systems|||21100389511|1,199|Q1|33|983|239|28463|1788|239|7,37|28,96|United Kingdom|Western Europe|2015-2020|Energy (miscellaneous) (Q1)|2,964|6.870|0.00318|1631868450|1436431897
||Participatory sensing, Big data, Big data management, Big data analytics, Mobile cloud computing||942-955||Participatory sensing has become an emerging technology of this era owing to its low cost in big sensor data collection. Prior to participatory sensing, large-scale deployment complexities were found in wireless sensor networks when collecting data from widespread resources. Participatory sensing systems employ handheld devices as sensors to collect data from communities and transmit to the cloud, where data are further analyzed by expert systems. The processes involved in participatory sensing, such as data collection, transmission, analysis, and visualization, exhibit certain management issues. This study aims to identify big data management issues that must be addressed at the cloud side during data processing and storing and at the participant side during data collection and visualization. It then proposes a framework for big data management in participatory sensing to resolve the contemporary big data management issues on the basis of suggested principles. Moreover, this work presents case studies to elaborate the existence of the highlighted issues. Finally, the limitations, recommendations, and future research directions for academia and industry in the domain of participatory sensing are discussed.|https://doi.org/10.1016/j.future.2017.10.007|https://www.sciencedirect.com/science/article/pii/S0167739X17311627||||2020|Big data management in participatory sensing: Issues, trends and future directions|Ahmad Karim and Aisha Siddiqa and Zanab Safdar and Maham Razzaq and Syeda Anum Gillani and Huma Tahir and Sana Kiran and Ejaz Ahmed and Muhammad Imran|article|KARIM2020942|||Future Generation Computer Systems|0167739X||107|||||12264|1,262|Q1|119|798|1935|36054|16877|1868|9,11|45,18|Netherlands|Western Europe|1984-2021|Computer Networks and Communications (Q1); Hardware and Architecture (Q1); Software (Q1)||||1632473260|562237118
||Linguistics;Sensors;Crowdsensing;Task analysis;Decision making;Probabilistic logic;Cloud computing;Mobile crowdsensing, Probabilistic linguistic term set, Participants ranking, TODIM, VIKOR||712-717|2019 IEEE Intl Conf on Parallel & Distributed Processing with Applications, Big Data & Cloud Computing, Sustainable Computing & Communications, Social Computing & Networking (ISPA/BDCloud/SocialCom/SustainCom)|In the mobile crowdsensing systems, the participants of great variety and diversity voluntarily submit their sensing data. Evaluating the participants and ranking them is a critical problem that should be solved to ensure the data quality. In this paper, we introduce the concept of probabilistic linguistic term sets (PLTSs) to model the group preference information during the process of ranking candidate participants and then propose novel VIKOR methods based on TODIM for solving the process of ranking reliable participants and selecting the best one in the mobile crowdsensing system. This proposed methods combine the advantages from the VIKOR method and TODIM method. To show the implementation process of evaluating participants and selecting the best one under the PLTS information context, a practical case is given to verify the feasibility of the proposed methods. Compared with the existing decision making methods, the proposed methods show their effectiveness.|10.1109/ISPA-BDCloud-SustainCom-SocialCom48970.2019.00108|||||2019|Probabilistic Linguistic VIKOR Method Based on TODIM for Reliable Participant Selection Problem in Mobile Crowdsensing|Huang, Chao and Lin, Mingwei and Chen, Riqing|inproceedings|9047270||Dec|||||||||||||||||||||||||||1632673996|42
||Industrial statistics, Data quality, Comprehensive evaluation, Coal-related industry||106310||The authenticity and quality of industrial statistical data directly affects all types of systematic research based on it. Considering the limitations of extant data quality evaluation literature on research objects and evaluation methods, we constructed a new data quality comprehensive inspection and evaluation model based on Benford's Law (BL) and the technique for order of preference by similarity to ideal solution (TOPSIS), selected coal-related industries as the research object, and conducted an empirical test along the research path of “Industry→Province→Indicator”. The results showed that, at industry level, the quality of statistical data for China's coal-related industries from 2001 to 2016 was generally poor. Among the eight sample industries selected, the data quality for five industries (including coal, electricity, and steel) was assessed as poor or slightly poor. Furthermore, at the provincial level, there is significant spatial heterogeneity in the quality of statistical data for various industries affected by factors such as economic structure, marketization level, and industrial diversity. Compared with other types of statistical indicators, industry financial indicators are more prone to data quality problems at the indicator level, and the suspicious indicators of different industries show certain common characteristics and some industry differences. To improve the quality of industrial statistical data and reduce the possible adverse impacts of data quality problems, based on the research findings, we propose targeted countermeasures and suggestions on how to prevent data fraud and effectively identify and rationally use suspicious data.|https://doi.org/10.1016/j.eneco.2022.106310|https://www.sciencedirect.com/science/article/pii/S014098832200439X||||2022|Are the official national data credible? Empirical evidence from statistics quality evaluation of China's coal and its downstream industries|Delu Wang and Fan Chen and Jinqi Mao and Nannan Liu and Fangyu Rong|article|WANG2022106310|||Energy Economics|01409883||114|||||29374|2,500|Q1|152|378|1081|21992|8195|1072|7,10|58,18|Netherlands|Western Europe|1979-2019|Economics and Econometrics (Q1); Energy (miscellaneous) (Q1)|26,186|7.042|0.02456|1633533719|980002867
||Big data, Airblast, Rapeseed oil, PDA, Probability density function, Likelihood||117792||In this paper, the droplet size distributions of high-velocity airblast atomization were analyzed. The spray measurement was performed by a Phase-Doppler anemometer at several points and different diameters across the spray for diesel oil, light heating oil, crude rapeseed oil, and water. The atomizing gauge pressure and the liquid preheating temperature varied from 0.3 to 2.4 bar and 25 to 100 °C, respectively. Approximately 400 million individual droplets were recorded; therefore, a big data evaluation technique was applied. 18 of the most commonly used probability density functions (PDF) were fitted to the histogram of each measuring point and evaluated by their relative log-likelihood. Among the three-parameter PDFs, Generalized Extreme Value and Burr PDFs provided the most desirable result to describe a complete drop size distribution. With restriction to two-parameter PDFs, the Nakagami PDF unexpectedly outperformed all the others, including Weibull (Rosin-Rammler) PDF, which is commonly used in atomization. However, if the spray is characterized by a single value, such as the Sauter Mean Diameter, i.e. an expected value-like parameter is of primary importance over the distribution, Gamma PDF is the best option, used in several papers of the atomization literature.|https://doi.org/10.1016/j.fuel.2020.117792|https://www.sciencedirect.com/science/article/pii/S0016236120307870||||2020|Application of big data analysis technique on high-velocity airblast atomization: Searching for optimum probability density function|András Urbán and Axel Groniewsky and Milan Malý and Viktor Józsa and Jan Jedelský|article|URBAN2020117792|||Fuel|00162361||273|||||16313|1,560|Q1|213|2396|5152|119785|35895|5139|6,88|49,99|Netherlands|Western Europe|1922, 1970-2021|Chemical Engineering (miscellaneous) (Q1); Energy Engineering and Power Technology (Q1); Fuel Technology (Q1); Organic Chemistry (Q1)|98,202|6.609|0.08033|1634189301|241913045
||collecting, survey, scripts, analyzing, managing, Provenance|38|||Scripts are widely used to design and run scientific experiments. Scripting languages are easy to learn and use, and they allow complex tasks to be specified and executed in fewer steps than with traditional programming languages. However, they also have important limitations for reproducibility and data management. As experiments are iteratively refined, it is challenging to reason about each experiment run (or trial), to keep track of the association between trials and experiment instances as well as the differences across trials, and to connect results to specific input data and parameters. Approaches have been proposed that address these limitations by collecting, managing, and analyzing the provenance of scripts. In this article, we survey the state of the art in provenance for scripts. We have identified the approaches by following an exhaustive protocol of forward and backward literature snowballing. Based on a detailed study, we propose a taxonomy and classify the approaches using this taxonomy.|10.1145/3311955|https://doi.org/10.1145/3311955|New York, NY, USA|Association for Computing Machinery||2019|A Survey on Collecting, Managing, and Analyzing Provenance from Scripts|Pimentel, Jo\~{a}o Felipe and Freire, Juliana and Murta, Leonardo and Braganholo, Vanessa|article|10.1145/3311955|47|jun|ACM Comput. Surv.|03600300|3|52|May 2020||||23038|2,079|Q1|163|112|365|15859|6978|365|19,16|141,60|United States|Northern America|1969-2020|Computer Science (miscellaneous) (Q1); Theoretical Computer Science (Q1)|10,790|10.282|0.01438|1634903164|1517405264
||data profiling, Property Graph|27|||Property Graph databases are being increasingly used within the industry as a powerful and flexible way to model real-world scenarios. With this flexibility, a great challenge appears regarding profiling tasks due to the need of adapting them to these new models while taking advantage of the Property Graphs’ particularities. This article proposes a set of data profiling tasks by integrating existing methods and techniques and an taxonomy to classify them. In addition, an application pipeline is provided while a formal specification of some tasks is defined.|10.1145/3409473|https://doi.org/10.1145/3409473|New York, NY, USA|Association for Computing Machinery||2020|Data Profiling in Property Graph Databases|Maiolo, Sof\'{\i}a and Etcheverry, Lorena and Marotta, Adriana|article|10.1145/3409473|20|oct|J. Data and Information Quality|19361955|4|12|December 2020||||||||||||||||||||||1635482989|833754770
IPEC '22|Dalian, China||9|137–145|Proceedings of the 3rd Asia-Pacific Conference on Image Processing, Electronics and Computers|In order to improve the security of cloud computing data center in the virtualized environment, a security system design of cloud computing data center is proposed based on the virtualization environment. Firstly, the security architecture of cloud computing data center is constructed, and the security of data center is evaluated. By optimizing the system equipment structure and operation steps, the security performance of cloud computing data center can be improved. The experimental results show that the design method of cloud computing data center security architecture based on Virtualization environment has high precision, good practical effect and fully meets the research requirements.|10.1145/3544109.3544134|https://doi.org/10.1145/3544109.3544134|New York, NY, USA|Association for Computing Machinery|9781450395786|2022|Design of Cloud Computing Data Center Security System Based on Virtualization Environment|Zhang, Shaochen and Qu, Youyang and Wang, Peng|inproceedings|10.1145/3544109.3544134|||||||||||||||||||||||||||||1636850689|42
||Big Data, Fingerprinting, Web tracking, Security, Analytics||1321-1337||Web-based device fingerprinting is the process of collecting security information through the browser to perform stateless device identification. Fingerprints may then be used to identify and track computing devices in the web. There are various reasons why device-related information may be needed. Among the others, this technique could help to efficiently analyze security information for sustainability. In this paper we introduce a fingerprinting analytics tool that discovers the most appropriate device fingerprints and their corresponding optimal implementations. The fingerprints selected in the result of the performed analysis are used to enrich and improve an open-source fingerprinting analytics tool Fingerprintjs2, daily consumed by hundreds of websites. As a result, the paper provides a noticeable progress in analytics of dozens of values of device fingerprints, and enhances analysis of fingerprints security information.|https://doi.org/10.1016/j.future.2017.12.061|https://www.sciencedirect.com/science/article/pii/S0167739X17329965||||2018|Big Data fingerprinting information analytics for sustainability|Anna Kobusińska and Kamil Pawluczuk and Jerzy Brzeziński|article|KOBUSINSKA20181321|||Future Generation Computer Systems|0167739X||86|||||12264|1,262|Q1|119|798|1935|36054|16877|1868|9,11|45,18|Netherlands|Western Europe|1984-2021|Computer Networks and Communications (Q1); Hardware and Architecture (Q1); Software (Q1)||||1641664684|562237118
||Business intelligence, Big data, Big data analytics, Advanced techniques, Decision-making||321-335||Although big data, big data analytics (BDA) and business intelligence have attracted growing attention of both academics and practitioners, a lack of clarity persists about how BDA has been applied in business and management domains. In reflecting on Professor Ayre's contributions, we want to extend his ideas on technological change by incorporating the discourses around big data, BDA and business intelligence. With this in mind, we integrate the burgeoning but disjointed streams of research on big data, BDA and business intelligence to develop unified frameworks. Our review takes on both technical and managerial perspectives to explore the complex nature of big data, techniques in big data analytics and utilisation of big data in business and management community. The advanced analytics techniques appear pivotal in bridging big data and business intelligence. The study of advanced analytics techniques and their applications in big data analytics led to identification of promising avenues for future research.|https://doi.org/10.1016/j.techfore.2018.06.009|https://www.sciencedirect.com/science/article/pii/S0040162517311319||||2019|Technology in the 21st century: New challenges and opportunities|Jie Sheng and Joseph Amankwah-Amoah and Xiaojun Wang|article|SHENG2019321|||Technological Forecasting and Social Change|00401625||143|||||14704|2,226|Q1|117|448|1099|35581|10127|1061|9,01|79,42|United States|Northern America|1970-2020|Applied Psychology (Q1); Business and International Management (Q1); Management of Technology and Innovation (Q1)|21,116|8.593|0.02416|1645196769|1949868303
||Chemometrics, Data science, Big data, Chemical analytical data, Methodology||1-10||Efficient and reliable analysis of chemical analytical data is a great challenge due to the increase in data size, variety and velocity. New methodologies, approaches and methods are being proposed not only by chemometrics but also by other data scientific communities to extract relevant information from big datasets and provide their value to different applications. Besides common goal of big data analysis, different perspectives and terms on big data are being discussed in scientific literature and public media. The aim of this comprehensive review is to present common trends in the analysis of chemical analytical data across different data scientific fields together with their data type-specific and generic challenges. Firstly, common data science terms used in different data scientific fields are summarized and discussed. Secondly, systematic methodologies to plan and run big data analysis projects are presented together with their steps. Moreover, different analysis aspects like assessing data quality, selecting data pre-processing strategies, data visualization and model validation are considered in more detail. Finally, an overview of standard and new data analysis methods is provided and their suitability for big analytical chemical datasets shortly discussed.|https://doi.org/10.1016/j.aca.2018.05.038|https://www.sciencedirect.com/science/article/pii/S0003267018306421||||2018|Modern data science for analytical chemical data – A comprehensive review|Ewa Szymańska|article|SZYMANSKA20181|||Analytica Chimica Acta|00032670||1028|||||23911|1,403|Q1|203|897|2235|45034|14014|2228|6,27|50,21|Netherlands|Western Europe|1947-2020|Analytical Chemistry (Q1); Biochemistry (Q1); Environmental Chemistry (Q1); Spectroscopy (Q1)|58,170|6.558|0.03972|1646607257|2139678982
||Deep learning;Reflectivity;Meteorological radar;Semantics;Neural networks;Radar detection;Weather forecasting;Deep learning;convolutional neural networks;encoder-decoder network;weather radar;image inpainting;blockage correction||482-487|2021 IEEE Intl Conf on Dependable, Autonomic and Secure Computing, Intl Conf on Pervasive Intelligence and Computing, Intl Conf on Cloud and Big Data Computing, Intl Conf on Cyber Science and Technology Congress (DASC/PiCom/CBDCom/CyberSciTech)|Doppler weather radar is the most widely used convection detector with the highest resolution in the ground. Echo reflectance data from the weather radar is the key reference for the meteorological department to carry out severe convective weather forecast and early warning, quantitative precipitation estimation(QPE) and quantitative precipitation forecast(QPF). However, in the process of radar detection, it is inevitable to be affected by obstacles, ground object echo interference, radar echo attenuation and other phenomena, resulting in poor data quality of detection results. Therefore, it is very important to correct the missing or disturbed data. On the other hand, with the rapid development of artificial intelligence technology in recent years, more and more meteorological researchers begin to introduce deep learning and other machine learning methods into the research of meteorological field such as weather radar data processing. In this paper, a deep convolutional encoder-decoder network is proposed to correct the beam blocking of weather radar. In this study, the correction of radar beam blockage is regarded as an image inpainting problem. It's the first trying to use deep learning to realize the correction of radar beam blockage. Experiment shows that the method proposed in this paper is significantly better than the traditional method in accuracy, error rate, false alarm rate and other aspects. The method can directly identify and correct the blocking area, and the operation procedure is simple compared traditional methods.|10.1109/DASC-PICom-CBDCom-CyberSciTech52372.2021.00086|||||2021|A FCN Approach to Blockage Correction in Radars|Wu, Hao and Liu, Qi and Liu, Xiaodong and Zhang, Yonghong and Xu, Xiaolong and Bilal, Muhammad|inproceedings|9730364||Oct|||||||||||||||||||||||||||1650765602|42
SSDBM '15|La Jolla, California||11||Proceedings of the 27th International Conference on Scientific and Statistical Database Management|The problem of privacy-preserving data mining has been studied extensively in recent years because of its importance as a key enabler in the sharing of massive data sets. Most of the work in privacy has focussed on issues involving the quality of privacy preservation and utility, though there has been little focus on the issue of scalability in privacy preservation. The reason for this is that anonymization has generally been seen as a batch and one-time process in the context of data sharing. However, in recent years, the sizes of data sets have grown tremendously to a point where the effective application of the current algorithms is becoming increasingly difficult. Furthermore, the transient nature of recent data sets has resulted in an increased need for the repeated application of such methods on the newer data sets which have been collected. Repeated application demands even greater computational efficiency in order to be practical. For example, an algorithm with quadratic complexity is unlikely to be implementable in reasonable time over terabyte scale data sets. A bigger issue is that larger data sets are likely to be addressed by distributed frameworks such as MapReduce. In such frameworks, one has to address the additional issue of minimizing data transfer across different nodes, which is the bottleneck. In this paper, we discuss the first approach towards privacy-preserving data mining of very massive data sets using MapReduce. We study two most widely-used privacy models k-anonymity and l-diversity for anonymization, and present experimental results illustrating the effectiveness of the approach.|10.1145/2791347.2791380|https://doi.org/10.1145/2791347.2791380|New York, NY, USA|Association for Computing Machinery|9781450337090|2015|Privacy-Preserving Big Data Publishing|Zakerzadeh, Hessam and Aggarwal, Charu C. and Barker, Ken|inproceedings|10.1145/2791347.2791380|26||||||||||||||||||||||||||||1651304925|42
||cloud computing in IoT, IoT big data, IoT big data survey, big data 2.0, cloud IoT services, V’s challenges for IoT big data|59|||Driven by the core technologies, i.e., sensor-based autonomous data acquisition and the cloud-based big data analysis, IoT automates the actuation of data-driven intelligent actions on the connected objects. This automation enables numerous useful real-life use-cases, such as smart transport, smart living, smart cities, and so on. However, recent industry surveys reflect that data-related challenges are responsible for slower growth of IoT in recent years. For this reason, this article presents a systematic and comprehensive survey on IoT Big Data (IoTBD) with the aim to identify the uncharted challenges for IoTBD. This article analyzes the state-of-the-art academic works in IoT and big data management across various domains and proposes a taxonomy for IoTBD management. Then, the survey explores the IoT portfolio of major cloud vendors and provides a classification of vendor services for the integration of IoT and IoTBD on their cloud platforms. After that, the survey identifies the IoTBD challenges in terms of 13 V’s challenges and envisions IoTBD as “Big Data 2.0.” Then the survey provides comprehensive analysis of recent works that address IoTBD challenges by highlighting their strengths and weaknesses to assess the recent trends and future research directions. Finally, the survey concludes with discussion on open research issues for IoTBD.|10.1145/3419634|https://doi.org/10.1145/3419634|New York, NY, USA|Association for Computing Machinery||2020|A Survey on IoT Big Data: Current Status, 13 V’s Challenges, and Future Directions|Bansal, Maggi and Chana, Inderveer and Clarke, Siobh\'{a}n|article|10.1145/3419634|131|dec|ACM Comput. Surv.|03600300|6|53|November 2021||||23038|2,079|Q1|163|112|365|15859|6978|365|19,16|141,60|United States|Northern America|1969-2020|Computer Science (miscellaneous) (Q1); Theoretical Computer Science (Q1)|10,790|10.282|0.01438|1651704311|1517405264
||Machine learning, Artificial intelligence, Deep learning, Big data, Neural networks, Chatbot, Innovation capability, Resources and capabilities||157-170||Machine learning holds great promise for lowering product and service costs, speeding up business processes, and serving customers better. It is recognized as one of the most important application areas in this era of unprecedented technological development, and its adoption is gaining momentum across almost all industries. In view of this, we offer a brief discussion of categories of machine learning and then present three types of machine-learning usage at enterprises. We then discuss the trade-off between the accuracy and interpretability of machine-learning algorithms, a crucial consideration in selecting the right algorithm for the task at hand. We next outline three cases of machine-learning development in financial services. Finally, we discuss challenges all managers must confront in deploying machine-learning applications.|https://doi.org/10.1016/j.bushor.2019.10.005|https://www.sciencedirect.com/science/article/pii/S0007681319301521||||2020|Machine learning for enterprises: Applications, algorithm selection, and challenges|In Lee and Yong Jae Shin|article|LEE2020157|||Business Horizons|00076813|2|63||ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING|||20209|2,174|Q1|87|64|278|2475|2066|227|6,68|38,67|United Kingdom|Western Europe|1957-2020|Business and International Management (Q1); Marketing (Q1)|7,443|6.361|0.00571|1652684315|847373672
||Big data, Soil microorganisms, Biodiversity analysis, Angelica cultivation, Hadoop systems, Metagenome research||102674||Angelica sinensis is a kind of traditional Chinese medicine with very good blood nourishing effect, and it is cultivated in many regions of China. But with the increasingly severe climate, angelica cultivation has become a big problem. Therefore, this paper starts from the soil microorganisms of angelica planting, and studies the influence of soil biodiversity and angelica planting in the context of big data. This paper proposes a Hadoop system for big data analysis, combining the biocommunity characteristics and metagenomes of soil microorganisms. It then calculates the distance between samples and generates a dissimilarity matrix. Finally, this paper proposes a soil optimization method for angelica planting based on big data analysis of soil microorganisms. In order to optimize the Angelica planting soil designed in this paper, a soil microbial genome comparison experiment and a big data concurrent control test experiment were designed in this paper. It then analyzes the data obtained from the experiment, and the results of the analysis are used to optimize the soil for angelica planting. It finally compares the soil method of Angelica planting designed in this paper with the traditional Angelica planting method. The experimental results show that the survival rate of Angelica sinensis planted by the soil optimization method based on big data analysis of soil microorganisms has increased by 16.09% compared with the traditional Angelica sinensis planting site. The growth rate of Angelica sinensis planted by the soil optimization method based on big data analysis of soil microorganisms increased by 9.64% compared with the traditional Angelica sinensis planting area.|https://doi.org/10.1016/j.seta.2022.102674|https://www.sciencedirect.com/science/article/pii/S2213138822007238||||2022|The relationship between soil microbial diversity and angelica planting based on network big data|Yinan Peng and Ze Ye and Peng Xi and Hongshan Qi and Bin Ji and Zhiye Wang|article|PENG2022102674|||Sustainable Energy Technologies and Assessments|22131388||53|||||21100239262|1,040|Q1|39|270|333|13311|1833|331|5,73|49,30|United Kingdom|Western Europe|2013-2020|Energy Engineering and Power Technology (Q1); Renewable Energy, Sustainability and the Environment (Q2)|3,234|5.353|0.00348|1652801784|1822851290
SIGSPATIAL '17|Redondo Beach, CA, USA|Geospatial Data Mining, Air Quality Modeling, PM2.5 Concentration Prediction|10||Proceedings of the 25th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems|Air quality models are important for studying the impact of air pollutant on health conditions at a fine spatiotemporal scale. Existing work typically relies on area-specific, expert-selected attributes of pollution emissions (e,g., transportation) and dispersion (e.g., meteorology) for building the model for each combination of study areas, pollutant types, and spatiotemporal scales. In this paper, we present a data mining approach that utilizes publicly available OpenStreetMap (OSM) data to automatically generate an air quality model for the concentrations of fine particulate matter less than 2.5 μm in aerodynamic diameter at various temporal scales. Our experiment shows that our (domain-) expert-free model could generate accurate PM2.5 concentration predictions, which can be used to improve air quality models that traditionally rely on expert-selected input. Our approach also quantifies the impact on air quality from a variety of geographic features (i.e., how various types of geographic features such as parking lots and commercial buildings affect air quality and from what distance) representing mobile, stationary and area natural and anthropogenic air pollution sources. This approach is particularly important for enabling the construction of context-specific spatiotemporal models of air pollution, allowing investigations of the impact of air pollution exposures on sensitive populations such as children with asthma at scale.|10.1145/3139958.3140013|https://doi.org/10.1145/3139958.3140013|New York, NY, USA|Association for Computing Machinery|9781450354905|2017|Mining Public Datasets for Modeling Intra-City PM2.5 Concentrations at a Fine Spatial Resolution|Lin, Yijun and Chiang, Yao-Yi and Pan, Fan and Stripelis, Dimitrios and Ambite, Jose Luis and Eckel, Sandrah P. and Habre, Rima|inproceedings|10.1145/3139958.3140013|25||||||||||||||||||||||||||||1656251539|42
||Conceptual modeling Big Data, Ecosystem, Integrate & analyze & visualize||96-99||The term Big Data denotes huge-volume, complex, rapid growing datasets with numerous, autonomous and independent sources. In these new circumstances Big Data bring many attractive opportunities; however, good opportunities are always followed by challenges, such as modelling, new paradigms, novel architectures that require original approaches to address data complexities. The purpose of this special issue on Modeling and Management of Big Data is to discuss research and experience in modelling and to develop as well as deploy systems and techniques to deal with Big Data. A summary of the selected papers is presented, followed by a conceptual modelling proposal for Big Data. Big Data creates new requirements based on complexities in data capture, data storage, data analysis and data visualization. These concerns are discussed in detail in this study and proposals are recommended for specific areas of future research.|https://doi.org/10.1016/j.future.2015.07.019|https://www.sciencedirect.com/science/article/pii/S0167739X15002514||||2016|Modeling and Management of Big Data: Challenges and opportunities|David Gil and Il-Yeol Song|article|GIL201696|||Future Generation Computer Systems|0167739X||63||Modeling and Management for Big Data Analytics and Visualization|||12264|1,262|Q1|119|798|1935|36054|16877|1868|9,11|45,18|Netherlands|Western Europe|1984-2021|Computer Networks and Communications (Q1); Hardware and Architecture (Q1); Software (Q1)||||1656617772|562237118
ICSSP '20|Seoul, Republic of Korea|Data technologies, DataOps, Agile Methodology, Data Pipelines, DevOps, Continuous Monitoring|10|165–174|Proceedings of the International Conference on Software and System Processes|The collection of high-quality data provides a key competitive advantage to companies in their decision-making process. It helps to understand customer behavior and enables the usage and deployment of new technologies based on machine learning. However, the process from collecting the data, to clean and process it to be used by data scientists and applications is often manual, non-optimized and error-prone. This increases the time that the data takes to deliver value for the business. To reduce this time companies are looking into automation and validation of the data processes. Data processes are the operational side of data analytic workflow.DataOps, a recently coined term by data scientists, data analysts and data engineers refer to a general process aimed to shorten the end-to-end data analytic life-cycle time by introducing automation in the data collection, validation, and verification process. Despite its increasing popularity among practitioners, research on this topic has been limited and does not provide a clear definition for the term or how a data analytic process evolves from ad-hoc data collection to fully automated data analytics as envisioned by DataOps.This research provides three main contributions. First, utilizing multi-vocal literature we provide a definition and a scope for the general process referred to as DataOps. Second, based on a case study with a large mobile telecommunication organization, we analyze how multiple data analytic teams evolve their infrastructure and processes towards DataOps. Also, we provide a stairway showing the different stages of the evolution process. With this evolution model, companies can identify the stage which they belong to and also, can try to move to the next stage by overcoming the challenges they encounter in the current stage.|10.1145/3379177.3388909|https://doi.org/10.1145/3379177.3388909|New York, NY, USA|Association for Computing Machinery|9781450375122|2020|From Ad-Hoc Data Analytics to DataOps|"\"Munappy, Aiswarya Raj and Mattos, David Issa and Bosch, Jan and Olsson, Helena Holmstr\"\"{o}m and Dakkak, Anas\""|inproceedings|10.1145/3379177.3388909|||||||||||||||||||||||||||||1659268091|42
||Big data, IoT, health care, telemedicine, WHO, image processing||1-30|An Industrial IoT Approach for Pharmaceutical Industry Growth|Health care is thought to be one of the business fields with the largest big data potential. Based on the prevailing definition, big data has a large amount of data which can be processed easily and can be modified or updated easily. These data can be quickly stored, processed, and transformed into valuable information using older technologies. At present, many new trends regarding new data resources and innovative data analysis are followed in medicine and health care. In practice, electronic health-care records, free open-source data, and the “quantified self” provide new approaches for analyzing data. Some of these advancements have been made in information extraction from the text data based on analytics, which is useful in data unlocking for analytics purposes from clinical documentation. Choosing big data approaches in the medicine and health-care fields has been lagging. This has led to the rise specific problems regarding data complexity and organizational, legal, and ethical challenges. With the growth of the uptake of big data in general, and medicine and health care in specific, innovative ideas and solutions are expected. Telemedicine is a new opportunity for the Internet of Things (IoT). This enables the specialist to consult a patient despite them being in different places. Medical image segmentation is needed for the analysis, storage, and protection of medical images in telemedicine. Telemedicine is defined by the World Health Organization (WHO) as “the practice of medical care using interactive audiovisual and data communications. This includes the delivery of medical care services, diagnosis, consultation, treatment, as well as health education and the transfer of medical data.” IoT-based applications mainly include remote patient monitoring and clinical monitoring. In addition, preventive measures-based applications are also part of smart health care. These applications require image processing-based technologies which could be integrated into medical health-care systems. Various types of input taken from cameras and processing of CT and MRI images could be integrated into IoT-based medical applications.|https://doi.org/10.1016/B978-0-12-821326-1.00001-2|https://www.sciencedirect.com/science/article/pii/B9780128213261000012||Academic Press|978-0-12-821326-1|2020|Chapter 1 - Medical big data mining and processing in e-health care|A. Vidhyalakshmi and C. Priya|incollection|VIDHYALAKSHMI20201||||||||||Valentina Emilia Balas and Vijender Kumar Solanki and Raghvendra Kumar|||||||||||||||||||1659628770|42
||Temperature sensors;COVID-19;Temperature measurement;Temperature distribution;Data analysis;Correlation;Databases;Data Quality;Data Analytics;Information Quality;Big Data;Data Integration||332-337|2021 IEEE 5th International Conference on Information Technology, Information Systems and Electrical Engineering (ICITISEE)|Due to the COVID-19 pandemic, much computer science research has been dedicated to utilizing sensor readings for medical purposes. Throughout this period, the need for virus symptom tracking has become a promising area for remotely deployed sensor networks and platforms. Our research goal is to prove that the temperature readings from these sensor network platforms can be statistically linked to public record, medical case study data. The expected outcome of our project is to prove the correlation between sensor network tracking of remote human temperature data and medical records for COVID cases. The results of this study will prove that tracking human temperature can assist in tracking disease outbreaks in various populations. Our framework platform is comprised of four main modules: (1) Temperature Collection, (2) Internal Data Validation (3) Internal-External data merger, (4) Data Analytics. The temperature data are collected from internal databases, mobile sensing devices and medical health professionals. After collection, the internal data are validated by our software, TAU-FIVE, a multi-tier data quality validation system, then merged with external data sources into a data analytic based data warehouse. The data mart queries are designed to compare the location and date of temperature sensor data with known data sets from government officials. Once blended into a fully operational data warehouse, these data marts produce high quality data analysis linking remotely sensed human temperature readings to sources of disease outbreaks.|10.1109/ICITISEE53823.2021.9655853|||||2021|Measuring the Impact of an IoT Temperature Sensor Framework for Tracking Contagious Diseases|Lawson, Victor J. and Banerjee, Madhushri|inproceedings|9655853||Nov|||||||||||||||||||||||||||1660580379|42
||Big data, Cyber security, Adaptation, Scalability, Configuration parameter, Spark||103294||Big Data Cyber Security Analytics (BDCA) systems use big data technologies (e.g., Apache Spark) to collect, store, and analyse a large volume of security event data for detecting cyber-attacks. The volume of digital data in general and security event data in specific is increasing exponentially. The velocity with which security event data is generated and fed into a BDCA system is unpredictable. Therefore, a BDCA system should be highly scalable to deal with the unpredictable increase/decrease in the velocity of security event data. However, there has been little effort to investigate the scalability of BDCA systems to identify and exploit the sources of scalability improvement. In this paper, we first investigate the scalability of a Spark-based BDCA system with default Spark settings. We then identify Spark configuration parameters (e.g., execution memory) that can significantly impact the scalability of a BDCA system. Based on the identified parameters, we finally propose a parameter-driven adaptation approach, SCALER, for optimizing a system's scalability. We have conducted a set of experiments by implementing a Spark-based BDCA system on a large-scale OpenStack cluster. We ran our experiments with four security datasets. We have found that (i) a BDCA system with default settings of Spark configuration parameters deviates from ideal scalability by 59.5% (ii) 9 out of 11 studied Spark configuration parameters significantly impact scalability and (iii) SCALER improves the BDCA system's scalability by 20.8% compared to the scalability with default Spark parameter setting. The findings of our study highlight the importance of exploring the parameter space of the underlying big data framework (e.g., Apache Spark) for scalable cyber security analytics.|https://doi.org/10.1016/j.jnca.2021.103294|https://www.sciencedirect.com/science/article/pii/S1084804521002897||||2022|On the scalability of Big Data Cyber Security Analytics systems|Faheem Ullah and M. Ali Babar|article|ULLAH2022103294|||Journal of Network and Computer Applications|10848045||198|||||||||||||||||||||||1661349748|2040591560
CSAE '18|Hohhot, China|scientific data, information carrier, relevance criteria, Relevance|7||Proceedings of the 2nd International Conference on Computer Science and Application Engineering|In1 the big data era, scientific data plays a crucial role in scientific research. Data sharing, retrieval and usage has become an inevitable trend. We study how the users of scientific data select relevant data from the data sharing platform. The study was conducted in two stages. In the first stage, a total of 14 subjects were selected to obtain their relevance criteria and usage of scientific data through semi-structured interviews. In the second stage, 671 questionnaires were collected in order to classify criteria. Finally, we determined 9 relevance criteria for scientific data: topicality, availability, comprehensiveness, currency, authority, quality, convenience, standardization, and usability, and divided them to 5 groups. In order to truly make a better data search engine and improve its search efficiency, moving beyond the criteria often used by users, we need to determine those criteria that are not often used, but still very important. What's more, a more convenient data search platform needs to be considered.|10.1145/3207677.3278010|https://doi.org/10.1145/3207677.3278010|New York, NY, USA|Association for Computing Machinery|9781450365123|2018|Scientific Data Relevance Criteria Classification and Usage|Zhang, Guilan and Wang, Jian and Zhou, Guomin and Liu, Jianping and Wei, Caoyuan|inproceedings|10.1145/3207677.3278010|30||||||||||||||||||||||||||||1661659651|42
||Big Data;Resource description framework;Semantics;Joining processes;Erbium;Organizations;Data quality;big data fusion;inferences;entity resolution;query rewriting||436-450||Within an organisation, the quality in big data is a cornerstone to operational, transactional processes and to the reliability of business analytics for decision making. In fact, as organizations are harnessing multi-sources data to rise the benefits of their business, the quality of data becomes important and crucial. This paper presents a new approach to query big data sources using Resource Description Framework (RDF) representation to ensure data quality by harvesting more relevant and complete query results. Our approach handles two important types of heterogeneity over multiple data sources: semantic heterogeneity and URI-based entity identification. It proposes (1) a semantic entity resolution method based on inference mechanism using rules to manage the misunderstanding of data, in real world entities (2) Data Quality enhancement using MapReduce-based query rewriting approach includes the entity resolution results to infer and adds implicit data into query results (3) a parallel combination of MapReduce jobs of saturation and query rewriting inferences to handle transitive and cyclic rules for a richer rules' expression language (4) experiments to assess the efficiency of the proposed approach over real big RDF data originating from insurance and synthetic data sets.|10.1109/TBDATA.2017.2710346|||||2021|Semantic-Based and Entity-Resolution Fusion to Enhance Quality of Big RDF Data|Benbernou, Salima and Huang, Xin and Ouziri, Mourad|article|7937830||June|IEEE Transactions on Big Data|23327790|2|7|||||21101019393|0,959|Q1|6|71|9|998|37|8|4,11|14,06|United States|Northern America|2020|Information Systems (Q1); Information Systems and Management (Q1)|636|3.344|0.00134|1662205116|1510992500
||smart buildings, cyber-physical systems, deep learning, Privacy preservation, k-anonymity|22|||Cyber-physical systems have enabled the collection of massive amounts of data in an unprecedented level of spatial and temporal granularity. Publishing these data can prosper big data research, which, in turn, helps improve overall system efficiency and resiliency. The main challenge in data publishing is to ensure the usefulness of published data while providing necessary privacy protection. In our previous work&nbsp;(Jia et al. 2017a), we presented a privacy-preserving data publishing framework (referred to as PAD hereinafter), which can guarantee k-anonymity while achieving better data utility than traditional anonymization techniques. PAD learns the information of interest to data users or features from their interactions with the data publishing system and then customizes data publishing processes to the intended use of data. However, our previous work is only applicable to the case where the desired features are linear in the original data record. In this article, we extend PAD to nonlinear features. Our experiments demonstrate that for various data-driven applications, PAD can achieve enhanced utility while remaining highly resilient to privacy threats.|10.1145/3275520|https://doi.org/10.1145/3275520|New York, NY, USA|Association for Computing Machinery||2018|A Framework for Privacy-Preserving Data Publishing with Enhanced Utility for Cyber-Physical Systems|Sangogboye, Fisayo Caleb and Jia, Ruoxi and Hong, Tianzhen and Spanos, Costas and Kj\ae{}rgaard, Mikkel Baun|article|10.1145/3275520|30|nov|ACM Trans. Sen. Netw.|15504859|3–4|14|November 2018||||4700152843|0,598|Q2|67|44|118|2352|447|117|3,52|53,45|United States|Northern America|2005-2020|Computer Networks and Communications (Q2)|1,365|2.253|9.9E-4|1662393663|1384228366
EDBT '13|Genoa, Italy|similarity join, content filter, similarity search, parallel algorithms|8|341–348|Proceedings of the Joint EDBT/ICDT 2013 Workshops|The quantity of data in real-world applications is growing significantly while the data quality is still a big problem. Similarity search and similarity join are two important operations to address the poor data quality problem. Although many similarity search and join algorithms have been proposed, they did not utilize the abilities of modern hardware with multi-core processors. It calls for new parallel algorithms to enable multi-core processors to meet the high performance requirement of similarity search and join on big data. To this end, in this paper we propose parallel algorithms to support efficient similarity search and join with edit-distance constraints. We adopt the partition-based framework and extend it to support parallel similarity search and join on multi-core processors. We also develop two novel pruning techniques. We have implemented our algorithms and the experimental results on two real datasets show that our parallel algorithms achieve high performance and obtain good speedup.|10.1145/2457317.2457382|https://doi.org/10.1145/2457317.2457382|New York, NY, USA|Association for Computing Machinery|9781450315999|2013|Efficient Parallel Partition-Based Algorithms for Similarity Search and Join with Edit Distance Constraints|Jiang, Yu and Deng, Dong and Wang, Jiannan and Li, Guoliang and Feng, Jianhua|inproceedings|10.1145/2457317.2457382|||||||||||||||||||||||||||||1663067972|42
||Big data, Right to privacy, Data protection, Group privacy, Collective interests, Data protection authorities, Risk assessment||238-255||In the big data era, new technologies and powerful analytics make it possible to collect and analyse large amounts of data in order to identify patterns in the behaviour of groups, communities and even entire countries. Existing case law and regulations are inadequate to address the potential risks and issues related to this change of paradigm in social investigation. This is due to the fact that both the right to privacy and the more recent right to data protection are protected as individual rights. The social dimension of these rights has been taken into account by courts and policymakers in various countries. Nevertheless, the rights holder has always been the data subject and the rights related to informational privacy have mainly been exercised by individuals. This atomistic approach shows its limits in the existing context of mass predictive analysis, where the larger scale of data processing and the deeper analysis of information make it necessary to consider another layer, which is different from individual rights. This new layer is represented by the collective dimension of data protection, which protects groups of persons from the potential harms of discriminatory and invasive forms of data processing. On the basis of the distinction between individual, group and collective dimensions of privacy and data protection, the author outlines the main elements that characterise the collective dimension of these rights and the representation of the underlying interests.|https://doi.org/10.1016/j.clsr.2016.01.014|https://www.sciencedirect.com/science/article/pii/S0267364916300280||||2016|Personal data for decisional purposes in the age of analytics: From an individual to a collective dimension of data protection|Alessandro Mantelero|article|MANTELERO2016238|||Computer Law & Security Review|02673649|2|32|||||28888|0,815|Q1|38|66|242|1245|707|223|3,39|18,86|United Kingdom|Western Europe|1985-2020|Business, Management and Accounting (miscellaneous) (Q1); Computer Networks and Communications (Q1); Law (Q1)||||1668598414|1769124433
||Big data, Cohorts, Medical data acquisition, Sources of medical data, Types of medical data||19-65|Medical Data Sharing, Harmonization and Analytics|This chapter presents the origin of medical data that comprises the core of any federated cloud platform that deals with medical data sharing and analytics. The different types and sources of medical data are extensively described along with applications and standard data acquisition protocols. Emphasis is given on the definition and the impact of the cohorts in clinical research, as well as the importance of the big data in medicine. The impact of the big data in medicine is also discussed along with emerging opportunities and challenges. The need to develop data standardization protocols across heterogeneous and dispersed data sources is finally highlighted, to enable the analysis of different types of medical big data.|https://doi.org/10.1016/B978-0-12-816507-2.00002-5|https://www.sciencedirect.com/science/article/pii/B9780128165072000025||Academic Press|978-0-12-816507-2|2020|Chapter 2 - Types and sources of medical and other related data|Vasileios C. Pezoulas and Themis P. Exarchos and Dimitrios I. Fotiadis|incollection|PEZOULAS202019||||||||||Vasileios C. Pezoulas and Themis P. Exarchos and Dimitrios I. Fotiadis|||||||||||||||||||1676645700|42
SIGMOD '19|Amsterdam, Netherlands|semi-supervised learning, classification, data cleaning, machine learning, historical data, clustering, error detection, label propagation|18|865–882|Proceedings of the 2019 International Conference on Management of Data|Detecting erroneous values is a key step in data cleaning. Error detection algorithms usually require a user to provide input configurations in the form of rules or statistical parameters. However, providing a complete, yet correct, set of configurations for each new dataset is not trivial, as the user has to know about both the dataset and the error detection algorithms upfront. In this paper, we present Raha, a new configuration-free error detection system. By generating a limited number of configurations for error detection algorithms that cover various types of data errors, we can generate an expressive feature vector for each tuple value. Leveraging these feature vectors, we propose a novel sampling and classification scheme that effectively chooses the most representative values for training. Furthermore, our system can exploit historical data to filter out irrelevant error detection algorithms and configurations. In our experiments, Raha outperforms the state-of-the-art error detection techniques with no more than 20 labeled tuples on each dataset.|10.1145/3299869.3324956|https://doi.org/10.1145/3299869.3324956|New York, NY, USA|Association for Computing Machinery|9781450356435|2019|Raha: A Configuration-Free Error Detection System|Mahdavi, Mohammad and Abedjan, Ziawasch and Castro Fernandez, Raul and Madden, Samuel and Ouzzani, Mourad and Stonebraker, Michael and Tang, Nan|inproceedings|10.1145/3299869.3324956|||||||||||||||||||||||||||||1678092297|42
||Hazardous materials, Inherent risk, Carbon emissions, Multi-objective optimization, Green supply chain management, Big data analysis||1085-1097||This paper presents a multi-objective optimization model for a green supply chain management scheme that minimizes the inherent risk occurred by hazardous materials, associated carbon emission and economic cost. The model related parameters are capitalized on a big data analysis. Three scenarios are proposed to improve green supply chain management. The first scenario divides optimization into three options: the first involves minimizing risk and then dealing with carbon emissions (and thus economic cost); the second minimizes both risk and carbon emissions first, with the ultimate goal of minimizing overall cost; and the third option attempts to minimize risk, carbon emissions, and economic cost simultaneously. This paper provides a case study to verify the optimization model. Finally, the limitations of this research and approach are discussed to lay a foundation for further improvement.|https://doi.org/10.1016/j.jclepro.2016.03.006|https://www.sciencedirect.com/science/article/pii/S0959652616300579||||2017|An optimization model for green supply chain management by using a big data analytic approach|Rui Zhao and Yiyun Liu and Ning Zhang and Tao Huang|article|ZHAO20171085|||Journal of Cleaner Production|09596526||142||Special Volume on Improving natural resource management and human health to ensure sustainable societal development based upon insights gained from working within ‘Big Data Environments’|||19167|1,937|Q1|200|5126|10603|304498|103295|10577|9,56|59,40|United Kingdom|Western Europe|1993-2021|Environmental Science (miscellaneous) (Q1); Industrial and Manufacturing Engineering (Q1); Renewable Energy, Sustainability and the Environment (Q1); Strategy and Management (Q1)|170,352|9.297|0.18299|1679420575|1121054297
||Ontologies;Social network services;Artificial neural networks;Synchronization;Maintenance engineering;ontology;social network;data quality assessment||11-18|2017 IEEE 2nd International Conference on Cloud Computing and Big Data Analysis (ICCCBDA)|the advent of online social networks has been one of the most exciting events in this decade. Many popular online social networks such as Twitter, Wechat, Weibo, LinkedIn, and Facebook have become increasingly popular. The consequences of the poor quality of data in a social network are often experienced in everyday life. This paper gives a domain ontology model, SNSQ Ontology, for data quality in the area of social networks. It could be a knowledge base for the quality assessment of the rich and linkage data in the social network. High-quality data would be relevant in the data searching, analyzing and mining. Based on the SNSQ Ontology the strategy for data quality assessment and repair is given. And the co-influence among the four quality dimensions, completeness, consistency, currency, and accuracy, are discussed to guarantee an effective assessment process.|10.1109/ICCCBDA.2017.7951876|||||2017|SNSQ ontology: A domain ontology for SNSs data quality|Liwei Zheng|inproceedings|7951876||April|||||||||||||||||||||||||||1679988341|42
||On-demand services, Customer agility, Systemic use, Big data, Big data analytics, IT assimilation, Process-level strategic alignment, Digital infrastructures, 311 services, Government||336-347||A theoretical framework for big data analytics-enabled customer agility and responsiveness was developed from extant IS research. In on-demand service environments, customer agility involves dynamic capabilities in sensing and responding to citizens. Using this framework, a case study examined a large city government's 311 on-demand services which had leveraged big data analytics. While we found the localized big data analytics use by some of the 22 departments for enhanced customer agility and on-demand 311 services, city-wide systemic change in on-demand service delivery through big data analytics use was not evident. From the case study we identified key institutional mechanisms for linking customer agility to public value creation through 311 services. We posit how systemic use of big data analytics embedded into critical processes enables the government to co-create public values with citizens through 311 on-demand services, indicating the importance of creating a culture of analytics driven by strong political leadership.|https://doi.org/10.1016/j.giq.2017.11.002|https://www.sciencedirect.com/science/article/pii/S0740624X17300394||||2018|Customer agility and responsiveness through big data analytics for public value creation: A case study of Houston 311 on-demand services|Akemi Takeoka Chatfield and Christopher G. Reddick|article|CHATFIELD2018336|||Government Information Quarterly|0740624X|2|35||Agile Government and Adaptive Governance in the Public Sector|||14735|2,121|Q1|103|76|205|5894|1946|193|8,39|77,55|United Kingdom|Western Europe|1984-2020|E-learning (Q1); Law (Q1); Library and Information Sciences (Q1); Sociology and Political Science (Q1)|5,379|7.279|0.00502|1682702341|1582933551
SSDBM 2020|Vienna, Austria|schema evolution, application-database co-evolution, model management, software ecosystems|12||32nd International Conference on Scientific and Statistical Database Management|Database evolution is a notoriously difficult task, and it is exacerbated by the necessity to evolve database-dependent applications. As science becomes increasingly dependent on sophisticated data management, the need to evolve an array of database-driven systems will only intensify. In this paper, we present an architecture for data-centric ecosystems that allows the components to seamlessly co-evolve by centralizing the models and mappings at the data service and pushing model-adaptive interactions to the database clients. Boundary objects fill the gap where applications are unable to adapt and need a stable interface to interact with the components of the ecosystem. Finally, evolution of the ecosystem is enabled via integrated schema modification and model management operations. We present use cases from actual experiences that demonstrate the utility of our approach.|10.1145/3400903.3400908|https://doi.org/10.1145/3400903.3400908|New York, NY, USA|Association for Computing Machinery|9781450388146|2020|Towards Co-Evolution of Data-Centric Ecosystems|Schuler, Robert and Czajkowski, Karl and D'Arcy, Mike and Tangmunarunkit, Hongsuda and Kesselman, Carl|inproceedings|10.1145/3400903.3400908|4||||||||||||||||||||||||||||1685927864|42
Middleware '16|Trento, Italy|policy specification and enforcement, audit, regulation, Law|15||Proceedings of the 17th International Middleware Conference|Internet of Things (IoT) applications, systems and services are subject to law. We argue that for the IoT to develop lawfully, there must be technical mechanisms that allow the enforcement of specified policy, such that systems align with legal realities. The audit of policy enforcement must assist the apportionment of liability, demonstrate compliance with regulation, and indicate whether policy correctly captures legal responsibilities. As both systems and obligations evolve dynamically, this cycle must be continuously maintained.This poses a huge challenge given the global scale of the IoT vision. The IoT entails dynamically creating new services through managed and flexible data exchange. Data management is complex in this dynamic environment, given the need to both control and share information, often across federated domains of administration.We see middleware playing a key role in managing the IoT. Our vision is for a middleware-enforced, unified policy model that applies end-to-end, throughout the IoT. This is because policy cannot be bound to things, applications, or administrative domains, since functionality is the result of composition, with dynamically formed chains of data flows.We have investigated the use of Information Flow Control (IFC) to manage and audit data flows in cloud computing; a domain where trust can be well-founded, regulations are more mature and associated responsibilities clearer. We feel that IFC has great potential in the broader IoT context. However, the sheer scale and the dynamic, federated nature of the IoT pose a number of significant research challenges.|10.1145/2988336.2988349|https://doi.org/10.1145/2988336.2988349|New York, NY, USA|Association for Computing Machinery|9781450343008|2016|Big Ideas Paper: Policy-Driven Middleware for a Legally-Compliant Internet of Things|Singh, Jatinder and Pasquier, Thomas and Bacon, Jean and Powles, Julia and Diaconu, Raluca and Eyers, David|inproceedings|10.1145/2988336.2988349|13||||||||||||||||||||||||||||1686126619|42
SAC '16|Pisa, Italy|trends, systematic mapping study, retrospective, SAC, symposium on applied computing, requirements engineering, scoping study, relevance|6|1264–1269|Proceedings of the 31st Annual ACM Symposium on Applied Computing|Context: The activities related to Requirements engineering (RE) are some of the most important steps in software development, since the requirements describe what will be provided in a software system in order to fulfill the stakeholders' needs. In this context, the ACM Symposium on Applied Computing (SAC) has been a primary gathering forum for many RE activities. When studying a research area, it is important to identify the most active groups, topics, the research trends and so forth. Objective: This study aims to investigate how the SAC RE-Track is evolving, by analyzing the papers published in its 8 previous editions. Method: We adopted a research strategy that combines scoping study and systematic review good practices. Results: We investigated the most active countries, institutions and authors, the main topics discussed, the types of the contributions, the conferences and journals that have most referenced SAC RE-Track papers, the phases of the RE process supported by the contributions, the publications with the greatest impact, and the trends in RE. Conclusions: We found 79 papers over the 8 previous SAC RE-Track editions, which were analyzed and discussed.|10.1145/2851613.2851757|https://doi.org/10.1145/2851613.2851757|New York, NY, USA|Association for Computing Machinery|9781450337397|2016|Retrospective, Relevance, and Trends of SAC Requirements Engineering Track|Vilela, J\'{e}ssyka and Gon\c{c}alves, Enyo and Holanda, Ana and Figueiredo, Bruno and Castro, Jaelson|inproceedings|10.1145/2851613.2851757|||||||||||||||||||||||||||||1686166923|42
||Big Data;Government;Data privacy;Reliability;Social network services;Big data;Data governance;Data governance framework;Case analysis||384-391|2017 IEEE International Congress on Big Data (BigData Congress)|Big Data governance requires a data governance that can satisfy the needs for corporate governance, IT governance, and ITA/EA. While the existing data governance focuses on the processing of structured data, Big Data governance needs to be established in consideration of a broad sense of Big Data services including unstructured data. To achieve the goals of Big Data, strategies need to be established together with goals that are aligned with the vision and objective of an organization. In addition to the preparation of the IT infrastructure, a proper preparation of the components is required to effectively implement the strategy for Big Data services. We propose the Big Data Governance Framework in this paper. The Big Data governance framework presents criteria different from existing criteria at the data quality level. It focuses on timely, reliable, meaningful, and sufficient data services, focusing on what data attributes should be achieved based on the data attributes of Big Data services. In addition to the quality level of Big Data, the personal information protection strategy and the data disclosure/accountability strategy are also needed to achieve goals and to prevent problems. This paper performed case analysis based on the Big Data Governance Framework with the National Pension Service of South Korea. Big Data services in the public sector are an inevitable choice to improve the quality of people's life. Big Data governance and its framework are the essential components for the realization of Big Data service.|10.1109/BigDataCongress.2017.56|||||2017|Data Governance Framework for Big Data Implementation with a Case of Korea|Kim, Hee Young and Cho, June-Suh|inproceedings|8029349||June|||||||||||||||||||||||||||1687520448|42
||Conceptual modeling, Big Data, Machine learning, Artificial Intelligence||101911||Since the first version of the Entity–Relationship (ER) model proposed by Peter Chen over forty years ago, both the ER model and conceptual modeling activities have been key success factors for modeling computer-based systems. During the last decade, conceptual modeling has been recognized as an important research topic in academia, as well as a necessity for practitioners. However, there are many research challenges for conceptual modeling in contemporary applications such as Big Data, data-intensive applications, decision support systems, e-health applications, and ontologies. In addition, there remain challenges related to the traditional efforts associated with methodologies, tools, and theory development. Recently, novel research is uniting contributions from both the conceptual modeling area and the Artificial Intelligence discipline in two directions. The first one is efforts related to how conceptual modeling can aid in the design of Artificial Intelligence (AI) and Machine Learning (ML) algorithms. The second one is how Artificial Intelligence and Machine Learning can be applied in model-based solutions, such as model-based engineering, to infer and improve the generated models. For the first time in the history of Conceptual Modeling (ER) conferences, we encouraged the submission of papers based on AI and ML solutions in an attempt to highlight research from both communities. In this paper, we present some of important topics in current research in conceptual modeling. We introduce the selected best papers from the 37th International Conference on Conceptual Modeling (ER’18) held in Xi’an, China and summarize some of the valuable contributions made based on the discussions of these papers. We conclude with suggestions for continued research.|https://doi.org/10.1016/j.datak.2021.101911|https://www.sciencedirect.com/science/article/pii/S0169023X21000380||||2021|Conceptual modeling in the era of Big Data and Artificial Intelligence: Research topics and introduction to the special issue|Juan Trujillo and Karen C. Davis and Xiaoyong Du and Ernesto Damiani and Veda C. Storey|article|TRUJILLO2021101911|||Data & Knowledge Engineering|0169023X||135|||||24437|0,480|Q2|87|37|155|1512|440|152|2,55|40,86|Netherlands|Western Europe|1985, 1987-2020|Information Systems and Management (Q2)||||1688025929|1516868485
ICISCAE 2021|Dalian, China||4|2783–2786|2021 4th International Conference on Information Systems and Computer Aided Education|The main purpose of physical function training is to solve the problems of many injuries, incorrect movement patterns, movement compensation and so on. In the era of big data, the scientific research field of sports training is gradually beginning to adopt the big data model for development, which will better form a new insight after data generation, collection, analysis and transformation. Therefore, this paper analyzes the application of big data intelligent cleaning system based on cloud computing in physical function training management. Students can quickly query running results through WEB query system, and administrators can download and modify students' results through WEB system. By using big data analysis technology, students are grouped according to their physical fitness test scores and BMI index, and their sports plans are formulated accordingly. A high-reliability real-time wireless receiving and sending sports monitoring system for big data is adopted, which realizes the functions of networking, unmanned and intelligent sports management.|10.1145/3482632.3487514|https://doi.org/10.1145/3482632.3487514|New York, NY, USA|Association for Computing Machinery|9781450390255|2021|Research on the Application of Big Data Cloud Cleaning System in Physical Function Sports Training Management|Wang, Wenwen|inproceedings|10.1145/3482632.3487514|||||||||||||||||||||||||||||1690990627|42
WSC '17|Las Vegas, Nevada||20||Proceedings of the 2017 Winter Simulation Conference|In this paper we have not attempted to produce any kind of systematic review of simulation in healthcare to compete with the dozen (at least) excellent and comprehensive survey papers on this topic that already exist. We begin with a glance back at the early days of Wintersim, but then proceed, in line with the theme of this special track, to reflect on general developments in healthcare simulation over the years from our own personal perspectives. We include some memories and reflections by several pioneers in this area, both academics and healthcare practitioners, on both sides of the Atlantic. We also asked four current simulation modelers, who all specialize in healthcare applications but from very diverse perspectives, to reflect on their experiences. We endeavor to identify some common or recurring themes across the years, and end with a glimpse into the future.||||IEEE Press|9781538634271|2017|Five Decades of Healthcare Simulation|Brailsford, Sally C and Carter, Michael W and Jacobson, Sheldon H|inproceedings|10.5555/3242181.3242205|23||||||||||||||||||||||||||||1693083683|42
||Data-driven networks, blockchain-empowered data-driven networks, networking technologies, blockchain|38|||The paths leading to future networks are pointing towards a data-driven paradigm to better cater to the explosive growth of mobile services as well as the increasing heterogeneity of mobile devices, many of which generate and consume large volumes and variety of data. These paths are also hampered by significant challenges in terms of security, privacy, services provisioning, and network management. Blockchain, which is a technology for building distributed ledgers that provide an immutable log of transactions recorded in a distributed network, has become prominent recently as the underlying technology of cryptocurrencies and is revolutionizing data storage and processing in computer network systems. For future data-driven networks (DDNs), blockchain is considered as a promising solution to enable the secure storage, sharing, and analytics of data, privacy protection for users, robust, trustworthy network control, and decentralized routing and resource managements. However, many important challenges and open issues remain to be addressed before blockchain can be deployed widely to enable future DDNs. In this article, we present a survey on the existing research works on the application of blockchain technologies in computer networks and identify challenges and potential solutions in the applications of blockchains in future DDNs. We identify application scenarios in which future blockchain-empowered DDNs could improve the efficiency and security, and generally the effectiveness of network services.|10.1145/3446373|https://doi.org/10.1145/3446373|New York, NY, USA|Association for Computing Machinery||2021|Blockchain-Empowered Data-Driven Networks: A Survey and Outlook|Li, Xi and Wang, Zehua and Leung, Victor C. M. and Ji, Hong and Liu, Yiming and Zhang, Heli|article|10.1145/3446373|58|apr|ACM Comput. Surv.|03600300|3|54|April 2022||||23038|2,079|Q1|163|112|365|15859|6978|365|19,16|141,60|United States|Northern America|1969-2020|Computer Science (miscellaneous) (Q1); Theoretical Computer Science (Q1)|10,790|10.282|0.01438|1693948333|1517405264
BigMine '12|Beijing, China||5|7–11|Proceedings of the 1st International Workshop on Big Data, Streams and Heterogeneous Source Mining: Algorithms, Systems, Programming Models and Applications|"\"Business analytics, occupying the intersection of the worlds of management science, computer science and statistical science, is a potent force for innovation in both the private and public sectors. The successes of business analytics in strategy, process optimization and competitive advantage has led to data being increasingly recognized as a valuable asset in many organizations. In recent years, thanks to a dramatic increase in the volume, variety and velocity of data, the loosely defined concept of \"\"Big Data\"\" has emerged as a topic of discussion in its own right -- with different viewpoints in both the business and technical worlds. From our perspective, it is important for discussions of \"\"Big Data\"\" to start from a well-defined business goal, and remain moored to fundamental principles of both cost/benefit analysis as well as core statistical science. This note discusses some business case considerations for analytics projects involving \"\"Big Data\"\", and proposes key questions that businesses should ask. With practical lessons from Big Data deployments in business, we also pose a number of research challenges that may be addressed to enable the business analytics community bring best data analytic practices when confronted with massive data sets.\""|10.1145/2351316.2351318|https://doi.org/10.1145/2351316.2351318|New York, NY, USA|Association for Computing Machinery|9781450315470|2012|Big Data, Big Business: Bridging the Gap|Gopalkrishnan, Vivekanand and Steier, David and Lewis, Harvey and Guszcza, James|inproceedings|10.1145/2351316.2351318|||||||||||||||||||||||||||||1694194765|42
||Supply chain management, Optimal strategy, Big data application, Two-period model, Social welfare||107550||We study a firm's strategy in adopting big data technology to motivate consumer demand over two periods. In the first period, the firm designs a product to sell to the market and determines whether to apply big data to attract more consumers. In the second period, the firm designs a new product and determines whether to sell the old product and the new product simultaneously, where big data can also be applied in this period to stimulate more demands. We formulate this problem into four models considering whether the firm adopts big data in the first period and/or the second period, and whether the firm only sells the new product or sells both the old and new products in the second period. We find that the firm prefers to apply big data over both periods when the cost is low, only over the second period when the cost is median and will not apply big data when the cost is high. Interestingly, only applying big data over the first period also may bring the most profits with heterogeneous big data coefficients. Furthermore, applying big data in the second period is the better choice for the social welfare.|https://doi.org/10.1016/j.cie.2021.107550|https://www.sciencedirect.com/science/article/pii/S036083522100454X||||2021|Optimal timing of big data application in a two-period decision model with new product sales|Lei Yang and Anqian Jiang and Jiahua Zhang|article|YANG2021107550|||Computers & Industrial Engineering|03608352||160|||||18164|1,315|Q1|128|641|1567|31833|9597|1557|5,97|49,66|United Kingdom|Western Europe|1976-2020|Computer Science (miscellaneous) (Q1); Engineering (miscellaneous) (Q1)||||1695863882|1798521593
ICIT 2018|Hong Kong, Hong Kong|The path, Traditional industries, Transformation and upgrading, Big data|6|54–59|Proceedings of the 6th International Conference on Information Technology: IoT and Smart City|"\"With the emergence of a new generation of information technology, big data has become an important driving force for current social development. Digitalization has become the main direction of the transformation and upgrading of traditional industries. As the product of current informatization, big data includes data quantity, data quality and data analysis ability. It is used as two different ways to interpret the value creation of big data, making it clear that it can promote the transformation and upgrading of traditional industries through value creation. Then, it puts forward the traditional industrial transformation and upgrading path from the perspective of big data, namely the linear path of \"\"traditional industry + digital\"\" and the transitional non-linear \"\"digital + traditional industry\"\". Its path selection will be analyzed by combining external and internal factors.\""|10.1145/3301551.3301610|https://doi.org/10.1145/3301551.3301610|New York, NY, USA|Association for Computing Machinery|9781450366298|2018|Research on the Transformation and Upgrading Path and Selection of Traditional Industries from the Perspective of Big Data|Li, Yonghong and Zhang, Shuwen and Jia, Nan|inproceedings|10.1145/3301551.3301610|||||||||||||||||||||||||||||1698384267|42
||Mobile crowdsensing, Incentive mechanism, Quality-aware, Reputation system||197-209||Mobile crowdsensing has become an efficient paradigm for performing large-scale sensing tasks. Many quality-aware incentive mechanisms for mobile crowdsensing have been proposed. However, most of them measure the data quality by one single metric from a specific perspective. Moreover, they usually use the real-time quality, which cannot provide sufficient incentive for the workers with long-term high quality. In this paper, we refine the generalized data quality into the fine-grained ability requirement. We present a mobile crowdsensing system to achieve the fine-grained quality control, and formulate the problem of maximizing the social cost such that the fine-grained ability requirement of all sensing tasks can be satisfied. To stimulate the workers with long-term high quality, we design two ability reputation systems to assess workers’ fine-grained abilities online. The incentive mechanism based on the reverse auction and fine-grained ability reputation system is proposed. We design a greedy algorithm to select the winners and determine the payment based on the bids and fine-grained ability reputation of workers. Through both rigorous theoretical analysis and extensive simulations, we demonstrate that the proposed mechanisms achieve computational efficiency, individual rationality, truthfulness, whitewashing proof, and guaranteed approximation. Moreover, the designed mechanisms show prominent advantage in terms of social cost and average ability achievement ratio.|https://doi.org/10.1016/j.comcom.2021.09.026|https://www.sciencedirect.com/science/article/pii/S0140366421003637||||2021|Towards high quality mobile crowdsensing: Incentive mechanism design based on fine-grained ability reputation|Zhuangye Luo and Jia Xu and Pengcheng Zhao and Dejun Yang and Lijie Xu and Jian Luo|article|LUO2021197|||Computer Communications|01403664||180|||||13681|0,627|Q1|105|616|599|24961|2424|591|4,08|40,52|Netherlands|Western Europe|1978-2020|Computer Networks and Communications (Q1)|6,725|3.167|0.00513|1701805969|550488617
||Big data definition, Small data, Data filtering, Data reduction||1-13|Principles and Practice of Big Data (Second Edition)|Big Data is not synonymous with lots and lots of data. Useful Big Data resources adhere to a set of data management principles that are fundamentally different from the traditional practices followed for small data projects. The areas of difference include: data collection; data annotation (including metadata and identifiers); location and distribution of stored data; classification of data; data access rules; data curation; data immutability; data permanence; verification and validity methods for the contained data; analytic methods; costs; and incumbent legal, social, and ethical issues. Skilled professionals who are adept in the design and management of small data resources may be unprepared for the unique challenges posed by Big Data. This chapter is an introduction to topics that will be fully explained in later chapters.|https://doi.org/10.1016/B978-0-12-815609-4.00001-7|https://www.sciencedirect.com/science/article/pii/B9780128156094000017||Academic Press|978-0-12-815609-4|2018|1 - Introduction|Jules J. Berman|incollection|BERMAN20181|||||||||Second Edition|Jules J. Berman|||||||||||||||||||1702971591|42
EuroPLoP '20|Virtual Event, Germany||25||Proceedings of the European Conference on Pattern Languages of Programs 2020|Remote Application Programming Interfaces (APIs) are used in almost any distributed system today, for instance in microservices-based systems, and are thus enablers for many digitalization efforts. API design not only impacts whether software provided as a service is easy and efficient to develop applications with, but also affects the long term evolution of the software system. In general, APIs are responsible for providing remote and controlled access to the functionality provided as services; however, APIs often are also used to expose and share information. We focus on such data-related aspects of microservice APIs in this paper. Depending on the life cycle of the information published through the API, its mutability and the endpoint role, data-oriented APIs can be designed following patterns such as Operational Data Holder, Master Data Holder, Reference Data Holder, Data Transfer Holder, and Link Lookup Resource. Known uses and examples of the patterns are drawn from public Web APIs as well as application development and integration projects we have been involved in.|10.1145/3424771.3424821|https://doi.org/10.1145/3424771.3424821|New York, NY, USA|Association for Computing Machinery|9781450377690|2020|Data-Oriented Interface Responsibility Patterns: Types of Information Holder Resources|"\"Zimmermann, Olaf and Pautasso, Cesare and L\"\"{u}bke, Daniel and Zdun, Uwe and Stocker, Mirko\""|inproceedings|10.1145/3424771.3424821|11||||||||||||||||||||||||||||1703409998|42
||Temperature sensors;Temperature distribution;Temperature measurement;Temperature control;Predictive models;Data models;Cooling;Mass concrete temperature;big data processing technology;CART prediction model||32845-32854||Due to the influence of temperature changes or temperature gradients in the construction process of mass concrete, temperature cracks will occur in the concrete. In order to achieve a reasonable prediction of the temperature change of the mass concrete during the construction process and accurately obtain the temperature change trend, this paper attempts to construct a CART prediction model based on the big data processing technology based on the characteristics of the temperature change of the mass concrete. This paper introduces in detail how to use data processing methods such as outlier identification, missing value filling and random error elimination to improve data quality, as well as the method for constructing the CART prediction model, and combines engineering examples to demonstrate the feasibility of the model method. The results show that the model and method can better predict the temperature change of mass concrete. It has high prediction accuracy and can provide necessary guidance for practical engineering.|10.1109/ACCESS.2022.3161556|||||2022|Research on CART Model of Mass Concrete Temperature Prediction Based on Big Data Processing Technology|Baoyu, Li and Guoxing, Li and Guiyu, Wang and Guofeng, Zhang and Man, Yang|article|9739736|||IEEE Access|21693536||10|||||21100374601|0,587|Q1|127|18036|24267|771081|116691|24200|4,48|42,75|United States|Northern America|2013-2020|Computer Science (miscellaneous) (Q1); Engineering (miscellaneous) (Q1); Materials Science (miscellaneous) (Q2)|105,968|3.367|0.15396|1704581511|1905633267
SIGMOD '13|New York, New York, USA|data streams, data warehousing, real-time analytics|4|949–952|Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data||10.1145/2463676.2465337|https://doi.org/10.1145/2463676.2465337|New York, NY, USA|Association for Computing Machinery|9781450320375|2013|Data Stream Warehousing|Golab, Lukasz and Johnson, Theodore|inproceedings|10.1145/2463676.2465337|||||||||||||||||||||||||||||1708525827|42
DEBS '21|Virtual Event, Italy|stream mining, machine and deep learning, edge analytics, cloud analytics, data stream analysis|11|103–113|Proceedings of the 15th ACM International Conference on Distributed and Event-Based Systems|The explosive increase in volume, velocity, variety, and veracity of data generated by distributed and heterogeneous nodes such as IoT and other devices, continuously challenge the state of art in big data processing platforms and mining techniques. Consequently, it reveals an urgent need to address the ever-growing gap between this expected exascale data generation and the extraction of insights from these data. To address this need, this position paper proposes Stream to Cloud &amp; Edge (S2CE), a first of its kind, optimized, multi-cloud and edge orchestrator, easily configurable, scalable, and extensible. S2CE will enable machine and deep learning over voluminous and heterogeneous data streams running on hybrid cloud and edge settings, while offering the necessary functionalities for practical and scalable processing: data fusion and preprocessing, sampling and synthetic stream generation, cloud and edge smart resource management, and distributed processing.|10.1145/3465480.3466926|https://doi.org/10.1145/3465480.3466926|New York, NY, USA|Association for Computing Machinery|9781450385558|2021|S2CE: A Hybrid Cloud and Edge Orchestrator for Mining Exascale Distributed Streams|Kourtellis, Nicolas and Herodotou, Herodotos and Grzenda, Maciej and Wawrzyniak, Piotr and Bifet, Albert|inproceedings|10.1145/3465480.3466926|||||||||||||||||||||||||||||1708587061|42
PETRA '16|Corfu, Island, Greece|Huntington's disease, Big Data, Machine Learning, Missing values, Health care|4||Proceedings of the 9th ACM International Conference on PErvasive Technologies Related to Assistive Environments|In our ever more complex world, the field of analytics has dramatically increased its importance. Gut feeling is no longer sufficient in decision making, but intuition has to be combined with support from the huge amount of data available today. Even if the amount of data is enormous, the quality of the data is not always good. Problems arise in at least two situations: i) the data is imprecise by nature and ii) the data is incomplete (or there are missing parts in the data set). Both situations are problematic and need to be addressed appropriately. If these problems are solved, applications are to be found in various interesting fields. We aim at achieving significant methodology development as well as creative solutions in the domain of medicine, information systems and risk management. This paper sets focus especially on missing data problems in the field of medicine when presenting a new project in its very first phase.|10.1145/2910674.2935861|https://doi.org/10.1145/2910674.2935861|New York, NY, USA|Association for Computing Machinery|9781450343374|2016|A New Application of Machine Learning in Health Care|"\"Bj\"\"{o}rk, Kaj-Mikael and Eirola, Emil and Miche, Yoan and Lendasse, Amaury\""|inproceedings|10.1145/2910674.2935861|49||||||||||||||||||||||||||||1708614156|42
||3D imaging, Big data, Genomic expression, Healthcare technology, Medical sensors, Personal medical information, Radiology images||127-143|Terahertz Biomedical and Healthcare Technologies|This chapter is mainly focused on the development of big data analytics in terahertz healthcare technology. In today's world, “big data” is a very familiar term, but the way it is interpreted is modified day by day. Healthcare is one aspect in which big data can be utilized to improve the overall system of healthcare, as, in the context of healthcare, the three primary “V's” of big data definition, volume, variety, and velocity, are very well suited. According to big data analysts, error-free analysis and outcome are ensured from big data, but this is really difficult for medical data due to the issues regarding the data quality. The final V related to big data is value, i.e., how much leverage the data can provide. Thus we can conclude that healthcare is a much preferred area for expert big data analysis. But there are several challenges to be faced in every aspect, starting from data collection to storage, analysis, prediction, etc. The main challenge is the unstructured nature of the data and the organizations from where the data are collected, following no standardized rules, which forms a big gap in the processing of information. Also, a huge investment is required for resources such as high-level expertise, knowledge, technologies used for data analytics, common data warehouses (for obtaining homogeneous data), etc. Despite having so many obstructions, big data analytics has already started growing rapidly in the healthcare sector.|https://doi.org/10.1016/B978-0-12-818556-8.00007-0|https://www.sciencedirect.com/science/article/pii/B9780128185568000070||Elsevier|978-0-12-818556-8|2020|Chapter 7 - A Framework Development on Big Data Analytics for Terahertz Healthcare|Debashis Das and Chinmay Chakraborty and Sourav Banerjee|incollection|DAS2020127||||||||||Amit Banerjee and Basabi Chakraborty and Hiroshi Inokawa and Jitendra {Nath Roy}|||||||||||||||||||1708881201|42
||Big data analytics, Tourism destination, Destination management information system, Business intelligence, Data mining, Online Analytical Processing (OLAP)||198-209||This paper presents a knowledge infrastructure which has recently been implemented as a genuine novelty at the leading Swedish mountain tourism destination, Åre. By applying a Business Intelligence approach, the Destination Management Information System Åre (DMIS-Åre) drives knowledge creation and application as a precondition for organizational learning at tourism destinations. Schianetz, Kavanagh, and Lockington’s (2007) concept of the ‘Learning Tourism Destination’ and the ‘Knowledge Destination Framework’ introduced by Höpken, Fuchs, Keil, and Lexhagen (2011) build the theoretical fundament for the technical architecture of the presented Business Intelligence application. After having introduced the development process of indicators measuring destination performance as well as customer behaviour and experience, the paper highlights how DMIS-Åre can be used by tourism managers to gain new knowledge about customer-based destination processes focused on pre- and post-travel phases, like “Web-Navigation”, “Booking” and “Feedback”. After a concluding discussion about the various components building the prototypically implemented BI-based DMIS infrastructure with data from destination stakeholders, the agenda of future research is sketched. The agenda considers, for instance, the application of real-time Business Intelligence to gain real-time knowledge on tourists’ on-site behaviour at tourism destinations.|https://doi.org/10.1016/j.jdmm.2014.08.002|https://www.sciencedirect.com/science/article/pii/S2212571X14000353||||2014|Big data analytics for knowledge generation in tourism destinations – A case from Sweden|Matthias Fuchs and Wolfram Höpken and Maria Lexhagen|article|FUCHS2014198|||Journal of Destination Marketing & Management|2212571X|4|3|||||21100228536|1,703|Q1|39|90|237|7390|1757|237|7,14|82,11|United Kingdom|Western Europe|2012-2020|Business and International Management (Q1); Marketing (Q1); Strategy and Management (Q1); Tourism, Leisure and Hospitality Management (Q1)||||1710332666|1131381100
||Analytical models;Area measurement;Government;Energy measurement;Medical services;Big Data;Data models;Big Data;Quality;Quality Models;Data Quality;Big Data Application;The 7v's Of Big Data||811-815|2021 International Conference on Information Technology (ICIT)|In this time the big data became an important part of all areas, it can be used in multiple industrials such as banking, education, government, networking, energy, health care, etc. So, because of that the huge amount of data became have problems or unnecessary data, and so that comes from the difficulty of measure the quality of these data. In this research we show the quality characteristic that can be help to increase the efficiency of quality measurement process of BDA by comparing it with other quality model of BDA and applying it on the 7V's of big data.|10.1109/ICIT52682.2021.9491629|||||2021|Recent Quality Models in BigData Applications|Hattawi, Waleed and Shaban, Sameeh and Shawabkah, Aon Al and Alzu’bi, Shadi|inproceedings|9491629||July|||||||||||||||||||||||||||1713968620|42
SAC '20|Brno, Czech Republic|software framework, big data, character computing, psychology, data collection|8|1906–1913|Proceedings of the 35th Annual ACM Symposium on Applied Computing|Data, and its collection, is one core aspect of technology and research, nowadays. Various scientific disciplines are interested in collecting human data in practically any context (at home, at work, during leisure time). For example, experts from the field of Psychology design studies for reliable and valid data collection in the laboratory and in the wild. We propose a generic platform for data-collection software development to be used by scientists without a programming background. This is done by adapting a basic Unity project through a configuration file provided by the platform users through an easy to use user interface. The scientific user can adapt and rearrange pre-defined data collection modules targeting a desired research question, implement it as application within the data collection platform and use and manage the application for data collection and later data analysis. As a proof of concept, the platform was embedded with build-in application modules for wide-spread Psychology data collection experiments. The versatility of the platform was tested by creating three diverse prototypical applications. Finally, the usability of the proposed platform evaluated using the System Usability Scale obtained high usability results. The robust module-based nature of the platform architecture makes is possible to create a various range of of psychologically-proven applications with different features to be decided by the researcher. This holds true for both the development phase of the applications, as well as, for after deployment.|10.1145/3341105.3373989|https://doi.org/10.1145/3341105.3373989|New York, NY, USA|Association for Computing Machinery|9781450368667|2020|AppGen: A Framework for Automatic Generation of Data Collection Apps|Alaa, Mostafa and Bolock, Alia El and Abas, Mostafa and Abdennadher, Slim and Herbert, Cornelia|inproceedings|10.1145/3341105.3373989|||||||||||||||||||||||||||||1714563366|42
||Big Data, Laboratory medicine, Machine learning, Direct-to-consumer testing, DTC, Harmonization||51-59|||https://doi.org/10.1016/j.cll.2019.11.009|https://www.sciencedirect.com/science/article/pii/S0272271219300927||||2020|Big Data Everywhere: The Impact of Data Disjunction in the Direct-to-Consumer Testing Model|Emily L. Gill and Stephen R. Master|article|GILL202051|||Clinics in Laboratory Medicine|02722712|1|40||Direct-to-Consumer Testing: The Role of Laboratory Medicine|||||||||||||||||||||1716022398|130738896
||Open data, Data quality||150-154||Open data aims to unlock the innovation potential of businesses, governments, and entrepreneurs, yet it also harbours significant challenges for its effective use. While numerous innovation successes exist that are based on the open data paradigm, there is uncertainty over the data quality of such datasets. This data quality uncertainty is a threat to the value that can be generated from such data. Data quality has been studied extensively over many decades and many approaches to data quality management have been proposed. However, these approaches are typically based on datasets internal to organizations, with known metadata, and domain knowledge of the data semantics. Open data, on the other hand, are often unfamiliar to the user and may lack metadata. The aim of this research note is to outline the challenges in dealing with data quality of open datasets, and to set an agenda for future research to address this risk to deriving value from open data investments.|https://doi.org/10.1016/j.ijinfomgt.2017.01.003|https://www.sciencedirect.com/science/article/pii/S0268401216309021||||2017|Open data: Quality over quantity|Shazia Sadiq and Marta Indulska|article|SADIQ2017150|||International Journal of Information Management|02684012|3|37|||||15631|2,770|Q1|114|224|382|20318|5853|373|16,16|90,71|United Kingdom|Western Europe|1970, 1986-2021|Artificial Intelligence (Q1); Computer Networks and Communications (Q1); Information Systems (Q1); Information Systems and Management (Q1); Library and Information Sciences (Q1); Management Information Systems (Q1); Marketing (Q1)|12,245|14.098|0.01167|1716420446|747927863
||Web services;Collaboration;Big data;Quality of service;Recommender systems;Scalability;Web Service;Big Data;Recommender System;MapReduce;Hadoop||1-4|2015 International Conference on Circuits, Power and Computing Technologies [ICCPCT-2015]|Web Services play a vital role in e-commerce and e-business applications. A WS (Web Service) application is interoperable and can work on any platform i.e.; platform independent, large scale distributed systems can be established easily. A Recommender System is a precious tool for providing appropriate recommendations to all users in a Hotel Reservation Website. User based, Top k and profile based approaches are used in collaborative filtering algorithm which does not provide personalized results to the users and inefficiency and scalability problem also occurs due to the increase in the size of large datasets. To address the above mentioned challenges, a Valence-Arousal Similarity based Recommendation Services, called VAS based RS, is proposed. Our proposed mechanism aims to presents a personalized service recommendation list and recommending the most suitable service to the end users. Moreover, it classifies the positive and negative preferences of the users from their reviews to improve the prediction accuracy. For improve its efficiency and scalability in big data environment, VAS based RS is implemented using collaborative filtering algorithm on MapReduce parallel processing paradigm in Hadoop, a widely-adopted distributed computing platform.|10.1109/ICCPCT.2015.7159309|||||2015|Valence arousal similarity based recommendation services|Subhashini, R. and Akila, G|inproceedings|7159309||March|||||||||||||||||||||||||||1717616329|42
||privacy-preserving computing, machine learning, Federated learning, data security, decentralized AI, user privacy, responsible AI, blockchain|22|||With the rapid advances of Artificial Intelligence (AI) technologies and applications, an increasing concern is on the development and application of responsible AI technologies. Building AI technologies or machine-learning models often requires massive amounts of data, which may include sensitive, user private information to be collected from different sites or countries. Privacy, security, and data governance constraints rule out a brute force process in the acquisition and integration of these data. It is thus a serious challenge to protect user privacy while achieving high-performance models. This article reviews recent progress of federated learning in addressing this challenge in the context of privacy-preserving computing. Federated learning allows global AI models to be trained and used among multiple decentralized data sources with high security and privacy guarantees, as well as sound incentive mechanisms. This article presents the background, motivations, definitions, architectures, and applications of federated learning as a new paradigm for building privacy-preserving, responsible AI ecosystems.|10.1145/3485875|https://doi.org/10.1145/3485875|New York, NY, USA|Association for Computing Machinery||2021|Toward Responsible AI: An Overview of Federated Learning for User-Centered Privacy-Preserving Computing|Yang, Qiang|article|10.1145/3485875|32|oct|ACM Trans. Interact. Intell. Syst.|21606455|3–4|11|December 2021||||||||||||||||||||||1720936418|668229522
ICEGOV 2020|Athens, Greece|Governance, Artificial Intelligence, Policies, Miles and Snow, Digitalization, Public Sector, Strategic Types, Strategy|10|44–53|Proceedings of the 13th International Conference on Theory and Practice of Electronic Governance|"\"This article aims to provide an interpretive display of the strategic stance toward innovation enabled by Artificial Intelligence (AI) of different governments based in Europe. The analysis includes the European Union (EU), some of its members as well as a non-member country, which we argue presents interesting characteristics. Based on a comprehensive analysis of a corpus of documents, which includes national strategies, external reports as well as web resources, the different countries considered in this article are subsequently classified using as interpretive lens, among other frameworks, the strategic types identified by Miles and Snow (defender, prospector, analyzer, reactor). The results show a prevalence of a \"\"prospector\"\" stance, interested in differentiation and the search of novelty. However, the results also show that similar strategic types may be driven by different values as well as governance orientation among the considered countries, thus leading to different potential ways to implement the expected AI-enabled innovation.\""|10.1145/3428502.3428508|https://doi.org/10.1145/3428502.3428508|New York, NY, USA|Association for Computing Machinery|9781450376747|2020|Governments' Strategic Stance toward Artificial Intelligence: An Interpretive Display on Europe|Viscusi, Gianluigi and Collins, Aengus and Florin, Marie-Valentine|inproceedings|10.1145/3428502.3428508|||||||||||||||||||||||||||||1726841190|42
||deep learning, Generative adversarial networks, privacy and security|38|||Generative Adversarial Networks (GANs) have promoted a variety of applications in computer vision and natural language processing, among others, due to its generative model’s compelling ability to generate realistic examples plausibly drawn from an existing distribution of samples. GAN not only provides impressive performance on data generation-based tasks but also stimulates fertilization for privacy and security oriented research because of its game theoretic optimization strategy. Unfortunately, there are no comprehensive surveys on GAN in privacy and security, which motivates this survey to summarize systematically. The existing works are classified into proper categories based on privacy and security functions, and this survey conducts a comprehensive analysis of their advantages and drawbacks. Considering that GAN in privacy and security is still at a very initial stage and has imposed unique challenges that are yet to be well addressed, this article also sheds light on some potential privacy and security applications with GAN and elaborates on some future research directions.|10.1145/3459992|https://doi.org/10.1145/3459992|New York, NY, USA|Association for Computing Machinery||2021|Generative Adversarial Networks: A Survey Toward Private and Secure Applications|Cai, Zhipeng and Xiong, Zuobin and Xu, Honghui and Wang, Peng and Li, Wei and Pan, Yi|article|10.1145/3459992|132|jul|ACM Comput. Surv.|03600300|6|54|July 2022||||23038|2,079|Q1|163|112|365|15859|6978|365|19,16|141,60|United States|Northern America|1969-2020|Computer Science (miscellaneous) (Q1); Theoretical Computer Science (Q1)|10,790|10.282|0.01438|1728202927|1517405264
AIEE 2022|Bangkok, Thailand|On-line monitoring, Anomaly detection, Dissolved gases in oil, Time series, Hierarchical agglomerative cluster|5|65–69|2022 The 3rd International Conference on Artificial Intelligence in Electronics Engineering|Dissolved gases in oil analysis has been a significant conventional condition detection method for condition evaluation for power transformers. But false data and wrong data do exist in DGA on-line monitoring system, which often lead to misjudgment. To handle this problem, the monitoring system often uses a threshold method based on data distribution statistics to determine the authenticity of the data. However, it is difficult to grasp the rules of the data distribution in advance, resulting in the problem of generally low detection rate of abnormal data. In this paper, according to the time series characteristics of on-line monitoring data of DGA, an abnormal data detection method based on condensed hierarchical clustering is proposed. First, the sliding time window is used to preprocess a variety of oil gas monitoring data to obtain a time series set of monitoring data, and comprehensively apply statistical indicators to classify them and establish Typical time series map; on this basis, the agglomerated hierarchical clustering model is used to perform similarity clustering on the distance between different characteristic data points and the typical abnormal map to determine the abnormal type of the monitoring data. The verification of the application of actual monitoring data shows that this method can detect data anomalies in the online monitoring data stream and determine its type in real time.|10.1145/3512826.3512840|https://doi.org/10.1145/3512826.3512840|New York, NY, USA|Association for Computing Machinery|9781450395489|2022|Research on Anomaly Detection Method of Online Monitoring Data of Dissolved Gas in Transformer Oil|Yang, Xiaohui and Guo, Chenxi and Ren, Huan and Dong, Ming|inproceedings|10.1145/3512826.3512840|||||||||||||||||||||||||||||1728309736|42
||Big Data, Smart city, Intelligent transportation system, Connected vehicle, Road traffic safety, Vision Zero||35-44||Research in Big Data and analytics offers tremendous opportunities to utilize evidence in making decisions in many application domains. To what extent can the paradigms of Big Data and analytics be used in the domain of transport? This article reports on an outcome of a systematic review of published articles in the last five years that discuss Big Data concepts and applications in the transportation domain. The goal is to explore and understand the current research, opportunities, and challenges relating to the utilization of Big Data and analytics in transportation. The review shows the potential of Big Data and analytics to garner insights and improve transportation systems through the analysis of various forms of data obtained from traffic monitoring systems, connected vehicles, crowdsourcing, and social media. We discuss some platforms and software architecture for the transport domain, along with a wide array of storage, processing, and analytical techniques, and describe challenges associated with the implementation of Big Data and analytics. This review contributes broadly to the various ways in which cities can utilize Big Data in transportation to guide the creation of sustainable and safer traffic systems. Since research in Big Data and transportation is, by and large, at infancy, this article does not prescribe recommendations to the various challenges identified, which also constitutes the limitation of the article.|https://doi.org/10.1016/j.bdr.2019.03.001|https://www.sciencedirect.com/science/article/pii/S2214579617303866||||2019|Systematic Review of the Literature on Big Data in the Transportation Domain: Concepts and Applications|Alex Neilson and  Indratmo and Ben Daniel and Stevanus Tjandra|article|NEILSON201935|||Big Data Research|22145796||17|||||21100356018|0,565|Q2|25|11|76|547|324|69|4,06|49,73|United States|Northern America|2014-2020|Computer Science Applications (Q2); Information Systems (Q2); Information Systems and Management (Q2); Management Information Systems (Q2)|586|3.578|0.00126|1731159461|1627174784
CNIOT2020|Sanya, China|data preprocessing, PCA, Big data analytics, urban driving cycle construction, feature engineering|7|1–7|Proceedings of the 2020 International Conference on Computing, Networks and Internet of Things|In recent years, with the rapid growth of car ownership, Chinese road traffic conditions have changed a lot. Governments, enterprises, and the public are increasingly finding that the increasing deviation between the actual fuel consumption and the results of the regulatory certification based on NEDC (New European Driving Cycle). In addition, this deviation has seriously affected the credibility of the government, energy saving and emission reduction of automobiles and environmental pollution. Thus, need to improve urban driving cycle construction methods to adapt the Chinese traffic and automobiles driving cycles.This paper proposes an advanced data science model based on big data analysis for accurate urban driving cycle construction in Chinese cities. In addition, we conduct a lot of data analysis and statistics. Then we design a data preprocessing method for cleaning the noise data to use in driving cycle construction. Extensive experiments and analysis on real-world datasets demonstrate that the proposed methods can significantly reduce the impact of missing and abnormal data on microtrips segmentation, and thus the proposed methods can be used for driving cycle construction in China more accurately.|10.1145/3398329.3398330|https://doi.org/10.1145/3398329.3398330|New York, NY, USA|Association for Computing Machinery|9781450377713|2020|An Advanced Data Science Model Based on Big Data Analytics for Urban Driving Cycle Construction in China|Qin, Yana and Yang, Haolin and Guo, Mengjie and Guo, Meicheng|inproceedings|10.1145/3398329.3398330|||||||||||||||||||||||||||||1731620328|42
||Predictive maintenance, Internet of things, Manufacturing systems, Artificial intelligence, Machine learning, Big data||114598||In this study, a data driven predictive maintenance system was developed for production lines in manufacturing. By utilizing the data generated from IoT sensors in real-time, the system aims to detect signals for potential failures before they occur by using machine learning methods. Consequently, it helps address the issues by notifying operators early such that preventive actions can be taken prior to a production stop. In current study, the effectiveness of the system was also assessed using real-world manufacturing system IoT data. The evaluation results indicated that the predictive maintenance system was successful in identifying the indicators of potential failures and it can help prevent some production stops from happening. The findings of comparative evaluations of machine learning algorithms indicated that models of Random Forest, a bagging ensemble algorithm, and XGBoost, a boosting method, appeared to outperform the individual algorithms in the assessment. The best performing machine learning models in this study have been integrated into the production system in the factory.|https://doi.org/10.1016/j.eswa.2021.114598|https://www.sciencedirect.com/science/article/pii/S0957417421000397||||2021|Predictive maintenance system for production lines in manufacturing: A machine learning approach using IoT data in real-time|Serkan Ayvaz and Koray Alpay|article|AYVAZ2021114598|||Expert Systems with Applications|09574174||173|||||24201|1,368|Q1|207|770|1945|42314|17345|1943|8,67|54,95|United Kingdom|Western Europe|1990-2021|Artificial Intelligence (Q1); Computer Science Applications (Q1); Engineering (miscellaneous) (Q1)|55,444|6.954|0.04053|1737875987|1377770283
||Need for big data, organizational buy-in, team building, big data evangelist, application architect, data integration, platform architect, data scientist, proof of concept, big data pilot, technology evaluation, technology selection, data management, appliance, application development, YARN, MapReduce, SDLC, training, project scoping, platform scoping, problem data size, computational complexity, storage configuration, integration plan, maintenance, management, assessment||105-120|Big Data Analytics|This final chapter reviews best practices for incrementally adopting big data into the enterprise. The chapter revisits assessing the need and value of big data, organizational buy-in, building the big data team, scoping and piloting a proof of concept, technology evaluation and selection, application development, testing, and implementation, platform and project scoping, the big data integration plan, management and maintenance, assessment of success criteria, and overall summary and considerations.|https://doi.org/10.1016/B978-0-12-417319-4.00011-9|https://www.sciencedirect.com/science/article/pii/B9780124173194000119|Boston|Morgan Kaufmann|978-0-12-417319-4|2013|Chapter 11 - Developing the Big Data Roadmap|David Loshin|incollection|LOSHIN2013105||||||||||David Loshin|||||||||||||||||||1739279610|42
||Internet of Things, Big-Data, Architecture, Cognitive, Data-flow||534-543||Big data and the Internet of Things (IoT) are considered as the main paradigms when defining new information architecture projects. Accordingly, technologies that make up these solutions could have an important role to play in business information architecture. Solutions that have approached big data and the IoT as unique technology initiatives, struggle in finding value in such efforts and in the technology itself. A connection to the requirements (volume, velocity, and variety) is mandatory to reach the potential business goals. In this context, we propose a new architecture for Cognitive Internet of Things (CIoT) and big data. The proposed architecture benefits computing mechanisms by combining the data WareHouse (DWH) and Data Lake (DL), and defining a tool for heterogeneous data collection.|https://doi.org/10.1016/j.procs.2019.09.208|https://www.sciencedirect.com/science/article/pii/S1877050919313924||||2019|A New Architecture for Cognitive Internet of Things and Big Data|Mohamed Saifeddine {Hadj Sassi} and Faiza Ghozzi Jedidi and Lamia Chaari Fourati|article|HADJSASSI2019534|||Procedia Computer Science|18770509||159||Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 23rd International Conference KES2019|||19700182801|0,334|-|76|1907|6483|38511|13722|6359|2,09|20,19|Netherlands|Western Europe|2010-2020|Computer Science (miscellaneous)||||1740322044|2108686752
||Big data;Pollution;Databases;Generators;Prototypes;Gold;Standards;Data quality;duplicate detection;data pollution;Apache Spark||396-411||Because of the increasing volume of autonomously collected data objects, duplicate detection is an important challenge in today's data management. To evaluate the efficiency of duplicate detection algorithms with respect to big data, large test data sets are required. Existing test data generation tools, however, are either not able to produce large test data sets or are domain-dependent which limits their usefulness to a few cases. In this paper, we describe a new framework that can be used to pollute a clean, homogeneous and large data set from an arbitrary domain with duplicates, errors and inhomogeneities. To prove its concept, we implemented a prototype which is built upon the cluster computing framework Apache Spark and evaluate its performance in several experiments.|10.1109/TBDATA.2016.2637378|||||2020|Large-Scale Data Pollution with Apache Spark|Hildebrandt, Kai and Panse, Fabian and Wilcke, Niklas and Ritter, Norbert|article|7809119||June|IEEE Transactions on Big Data|23327790|2|6|||||21101019393|0,959|Q1|6|71|9|998|37|8|4,11|14,06|United States|Northern America|2020|Information Systems (Q1); Information Systems and Management (Q1)|636|3.344|0.00134|1743319490|1510992500
||Electric vehicles, Battery energy storage, Temperature-dependent model, Battery management system, Big data, Deep learning||1259-1273||As one of the bottleneck technologies of electric vehicles (EVs), the battery hosts complex and hardly observable internal chemical reactions. Therefore, a precise mathematical model is crucial for the battery management system (BMS) to ensure the secure and stable operation of the battery in a multi-variable environment. First, a Cloud-based BMS (C-BMS) is established based on a database containing complete battery status information. Next, a data cleaning method based on machine learning is applied to the big data of batteries. Meanwhile, to improve the model stability under dynamic conditions, an F-divergence-based data distribution quality assessment method and a sampling-based data preprocess method is designed. Then, a lithium-ion battery temperature-dependent model is built based on Stacked Denoising Autoencoders- Extreme Learning Machine (SDAE-ELM) algorithm, and a new training method combined with data preprocessing is also proposed to improve the model accuracy. Finally, to improve reliability, a conjunction working mode between the C-BMS and the BMS in vehicles (V-BMS) is also proposed, providing as an applied case of the model. Using the battery data extracted from electric buses, the effectiveness and accuracy of the model are validated. The error of the estimated battery terminal voltage is within 2%, and the error of the estimated State of Charge (SoC) is within 3%.|https://doi.org/10.1016/j.apenergy.2019.03.154|https://www.sciencedirect.com/science/article/pii/S0306261919305495||||2019|Big data driven lithium-ion battery modeling method based on SDAE-ELM algorithm and data pre-processing technology|Shuangqi Li and Hongwen He and Jianwei Li|article|LI20191259|||Applied Energy|03062619||242|||||28801|3,035|Q1|212|1729|5329|100144|56804|5304|10,59|57,92|United Kingdom|Western Europe|1975-2020|Building and Construction (Q1); Energy (miscellaneous) (Q1); Management, Monitoring, Policy and Law (Q1); Mechanical Engineering (Q1)|122,712|9.746|0.15329|1744795062|892883729
https://doi.org/10.1016/j.compbiomed.2021.104305|https://www.sciencedirect.com/science/article/pii/S0010482521000998||||2021|Clinical notes as prognostic markers of mortality associated with diabetes mellitus following critical care: A retrospective cohort analysis using machine learning and unstructured big data|Kushan {De Silva} and Noel Mathews and Helena Teede and Andrew Forbes and Daniel Jönsson and Ryan T. Demmer and Joanne Enticott|article|DESILVA2021104305|||Computers in Biology and Medicine|00104825||132||||||||||||||||||||||||||||||1746679698|42
ICEEG 2019|Lyon, France|Big Data, Open Government Data, Linked Open Data, LOD, CRPI, OGD, Central Repository for Public Information, Open Data, Linked Data, E-government|6|15–20|Proceedings of the 2019 3rd International Conference on E-Commerce, E-Business and E-Government|The article describes the issues of Open Government Data (OGD) and problems with the use of such data. Good quality and proper publishing of OGD enable (apart from the control function) their business use. This affects the economic benefits. The author has identified the main problems of data publication based on Central Repositories for Public Information (CRPI) in Poland, the USA, the UK and Germany. The article focuses on the maturity of data formats, automated processing with Application Programming Interface (API), using the concept of Linked Open Data (LOD). The aim of the article is to identify barriers to the implementation of OGD-based solutions and to indicate recommendations to overcome these barriers. The research shows that the methods of sharing OGD differ significantly between countries despite common guidelines. The main problem is the use of unstructured data, unsuitable for the use of LOD.|10.1145/3340017.3340022|https://doi.org/10.1145/3340017.3340022|New York, NY, USA|Association for Computing Machinery|9781450362375|2019|Barriers to Using Open Government Data|Wieczorkowski, Jundefineddrzej|inproceedings|10.1145/3340017.3340022|||||||||||||||||||||||||||||1747157656|42
||artificial intelligence, big data, digital core, digital rock physics, multiphase flow physics information, oil recovery, molecule advanced planning and scheduling, system (MAPS), crude oil selection, crude oil property prediction, process optimization, CCR unit, FCC unit, ethylene cracking unit, correlation analysis, the anomaly detection, parameters optimization analysis, prediction analysis, equipment preventive maintenance, distributed equipment health monitoring system, time series, early warning system for equipment fault monitoring, residual life prediction for equipment||181-210|Machine Learning and Data Science in the Oil and Gas Industry|This chapter introduces typical cases of artificial intelligence and big data application in oil and gas industry. In the upstream field, it introduces how to combine digital rock physics with big data and AI to optimize recovery efficiency. Digital core (also known as digital petrophysics—DRP) technology enables more reliable physical information about pore-scale multiphase flows to determine the cause of low recovery and provides new ways for different injection solutions to improve reservoir performance. Combined with digital rock technology and AI, we can integrate the characteristics of digital rock databases into logging and well data, and use a variety of advanced classification techniques to identify the remaining oil and gas potential. Through the multi-phase flow simulation, the multi-scale model can predict the best injection method for maximum recovery under different conditions and propose possible solutions to optimize crude oil production. In the downstream field, the application of AI and big data analysis in planning and scheduling systems, process unit optimization, preventive maintenance of equipment, and other aspects is introduced. Among them, the molecular-level advanced planning and scheduling system (MAPS) can realize the cost performance measurement under different production schemes for potential types of processable crude oil, which is conducive to more accurate selection of crude oil and prediction of crude oil properties. In addition, the whole process simulation can be used to understand the product quality changes under different crude oil blending schemes and different unit operating conditions, which is conducive to timely adjusting the product blending schemes according to economic benefits or ex-factory demands. The operation conditions of secondary units and even the properties of mixed crude oil can be deduced according to different product quality requirements. In the process of optimization, the Continuous Catalytic Reforming (CCR) unit in the refinery process, for example, introduces the application of large data analysis, including correlation analysis, single index detection, multidimensional data anomaly detection, and the parameters of the single objective optimization, a multi-objective parameter optimization analysis, unstructured data analysis, and forecast analysis based on material properties. Good practices in CCR units have also been extended to other oil refining and chemical units, such as Fluid Catalytic Cracking (FCC) and ethylene cracking. In terms of equipment preventive maintenance, it introduced how to integrated application of Internet of things, deep machine learning, knowledge map and other technology to build real-time and on-line distributed equipment health monitoring and early warning system, for early detection of equipment hidden danger, early warning, early treatment of providing effective means, guarantee equipment run healthy and stable for a long period of time, to reduce unplanned downtime losses. In particular, the establishment of equipment prediction model based on time series and AI can realize effective monitoring and early warning of equipment faults such as shaft displacement, shaft fracture, shell cracking, power overload, and prediction of equipment remaining life.|https://doi.org/10.1016/B978-0-12-820714-7.00009-1|https://www.sciencedirect.com/science/article/pii/B9780128207147000091||Gulf Professional Publishing|978-0-12-820714-7|2021|Chapter 9 - Global Practice of AI and Big Data in Oil and Gas Industry|Wu Qing|incollection|QING2021181||||||||||Patrick Bangert|||||||||||||||||||1747503153|42
||person re-identification, computer vision, machine learning, biometrics, Video surveillance, gait analysis|34|||The way people walk is a strong correlate of their identity. Several studies have shown that both humans and machines can recognize individuals just by their gait, given that proper measurements of the observed motion patterns are available. For surveillance applications, gait is also attractive, because it does not require active collaboration from users and is hard to fake. However, the acquisition of good-quality measures of a person’s motion patterns in unconstrained environments, (e.g., in person re-identification applications) has proved very challenging in practice. Existing technology (video cameras) suffer from changes in viewpoint, daylight, clothing, accessories, and other variations in the person’s appearance. Novel three-dimensional sensors are bringing new promises to the field, but still many research issues are open. This article presents a survey of the work done in gait analysis for re-identification in the past decade, looking at the main approaches, datasets, and evaluation methodologies. We identify several relevant dimensions of the problem and provide a taxonomic analysis of the current state of the art. Finally, we discuss the levels of performance achievable with the current technology and give a perspective of the most challenging and promising directions of research for the future.|10.1145/3243043|https://doi.org/10.1145/3243043|New York, NY, USA|Association for Computing Machinery||2019|Gait-Based Person Re-Identification: A Survey|Nambiar, Athira and Bernardino, Alexandre and Nascimento, Jacinto C.|article|10.1145/3243043|33|apr|ACM Comput. Surv.|03600300|2|52|March 2020||||23038|2,079|Q1|163|112|365|15859|6978|365|19,16|141,60|United States|Northern America|1969-2020|Computer Science (miscellaneous) (Q1); Theoretical Computer Science (Q1)|10,790|10.282|0.01438|1748239802|1517405264
||Security, Sensor networks, Big data stream, Key exchange, Security verification||22-42||Big data streaming has become an important paradigm for real-time processing of massive continuous data flows in large scale sensing networks. While dealing with big sensing data streams, a Data Stream Manager (DSM) must always verify the security (i.e. authenticity, integrity, and confidentiality) to ensure end-to-end security and maintain data quality. Existing technologies are not suitable, because real time introduces delay in data stream. In this paper, we propose a Dynamic Prime Number Based Security Verification (DPBSV) scheme for big data streams. Our scheme is based on a common shared key that updated dynamically by generating synchronized prime numbers. The common shared key updates at both ends, i.e., source sensing devices and DSM, without further communication after handshaking. Theoretical analyses and experimental results of our DPBSV scheme show that it can significantly improve the efficiency of verification process by reducing the time and utilizing a smaller buffer size in DSM.|https://doi.org/10.1016/j.jcss.2016.02.005|https://www.sciencedirect.com/science/article/pii/S0022000016000209||||2017|A dynamic prime number based efficient security mechanism for big sensing data streams|Deepak Puthal and Surya Nepal and Rajiv Ranjan and Jinjun Chen|article|PUTHAL201722|||Journal of Computer and System Sciences|00220000|1|83|||||||||||||||||||||||1749595008|1380600830
||Big data, Data mining techniques (DMTs), Production management, Smart manufacturing, Statistical analysis, Knowledge discovery||1-13||Driven by the innovative improvement of information and communication technologies (ICTs) and their applications into manufacturing industry, the big data era in manufacturing is correspondingly arising, and the developing data mining techniques (DMTs) pave the way for pursuing the aims of smart production with the real-time, dynamic, self-adaptive and precise control. However, lots of factors in the ever-changing environment of manufacturing industry, such as, various of complex production processes, larger scale and uncertainties, more complicated constrains, coupling of operational performance, and so on, make production management face with more and more big challenges. The dynamic inflow of a large number of raw data which is collected from the physical manufacturing sites or generated in various related information systems, caused the heavy information overload problems. Indeed, most of traditional DMTs are not yet sufficient to process such big data for smart production management. Therefore, this paper reviews the development of DMTs in the big data era, and makes discussion on the applications of DMTs in production management, by selecting and analyzing the relevant papers since 2010. In the meantime, we point out limitations and put forward some suggestions about the smartness and further applications of DMTs used in production management.|https://doi.org/10.1016/j.jii.2017.08.001|https://www.sciencedirect.com/science/article/pii/S2452414X17300584||||2018|Data and knowledge mining with big data towards smart production|Ying Cheng and Ken Chen and Hemeng Sun and Yongping Zhang and Fei Tao|article|CHENG20181|||Journal of Industrial Information Integration|2452414X||9|||||21100787106|2,042|Q1|24|28|86|2152|1361|83|12,26|76,86|Netherlands|Western Europe|2016-2020|Industrial and Manufacturing Engineering (Q1); Information Systems and Management (Q1)|1,149|10.063|0.00148|1753409577|121356201
SCC '19|Portland, OR, USA|Privacy, Digital Inclusion, government services, Automatic decision systems, Digital equity|6||Proceedings of the 2nd ACM/EIGSCC Symposium on Smart Cities and Communities|This paper will showcase the work that the City of Portland has done around developing Privacy and Information Protection Principles considering the current state of technology, the social digital age, and advance inference algorithms like machine learning or other Artificial Intelligence tools. By creating more responsible data stewardship in the public sector, municipalities are set to build trusted information networks involving communities and complex social issues. Particularly, the promotion of data privacy can lead to the emergence of anti-poverty and economic development strategies.The City of Portland has developed seven Privacy and Information Protection Principles: Transparency and accountability, full lifecycle stewardship, equitable data management, ethical and non-discriminatory use of data, data openness, automated decision systems, and data utility. These principles have implications in social equity and the future of technology management in smart cities projects. Principle implementation involves the collaboration of different agencies, particularly focused on ethics and human rights supporting sustainable development.This work is part of emergent strategies for a new generation of city services based on data and information, which aim to improve civic engagement, social benefits to communities in city neighborhoods and better collaboration with partners and other government agencies.|10.1145/3357492.3358628|https://doi.org/10.1145/3357492.3358628|New York, NY, USA|Association for Computing Machinery|9781450369787|2019|Privacy and Information Protection for a New Generation of City Services|Dominguez, Hector and Mowry, Judith and Perez, Elisabeth and Kendrick, Christine and Martin, Kevin|inproceedings|10.1145/3357492.3358628|5||||||||||||||||||||||||||||1758476732|42
iiWAS2021|Linz, Austria|indicator, Data Lake, ontology, query-driven discovery|4|183–186|The 23rd International Conference on Information Integration and Web Intelligence|Data Lake (DL) architectures have recently emerged as an effective solution to the problem of data analytics with big, highly heterogeneous, and quickly changing data sources. However, novel challenges arise too, including how to make sense of disparate raw data and how to identify the sources that satisfy a data need. In the paper, we introduce a semantic model for a Data Lake aimed to support data discovery and integration in data analytics scenarios. By formally modeling indicators of interest, their computation formulas, and dimensions of analysis in a knowledge graph, and by seamlessly mapping them to relevant source metadata, the framework is suited for identifying the sources and the required transformation steps according to the analytical request.|10.1145/3487664.3487783|https://doi.org/10.1145/3487664.3487783|New York, NY, USA|Association for Computing Machinery|9781450395564|2022|A Semantic Data Lake Model for Analytic Query-Driven Discovery|Diamantini, Claudia and Potena, Domenico and Storti, Emanuele|inproceedings|10.1145/3487664.3487783|||||||||||||||||||||||||||||1761568747|42
CSCW '17|Portland, Oregon, USA|crowdsourcing for research, citizen science, interviews|18|1544–1561|Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing|Numerous crowdsourcing platforms are now available to support research as well as commercial goals. However, crowdsourcing is not yet widely adopted by researchers for generating, processing or analyzing research data. This study develops a deeper understanding of the circumstances under which crowdsourcing is a useful, feasible or desirable tool for research, as well as the factors that may influence researchers' decisions around adopting crowdsourcing technology. We conducted semi-structured interviews with 18 researchers in diverse disciplines, spanning the humanities and sciences, to illuminate how research norms and practitioners' dispositions were related to uncertainties around research processes, data, knowledge, delegation and quality. The paper concludes with a discussion of the design implications for future crowdsourcing systems to support research.|10.1145/2998181.2998197|https://doi.org/10.1145/2998181.2998197|New York, NY, USA|Association for Computing Machinery|9781450343350|2017|Crowdsourcing as a Tool for Research: Implications of Uncertainty|Law, Edith and Gajos, Krzysztof Z. and Wiggins, Andrea and Gray, Mary L. and Williams, Alex|inproceedings|10.1145/2998181.2998197|||||||||||||||||||||||||||||1763871183|42
SIGMOD '15|Melbourne, Victoria, Australia|cleansing abstraction, distributed data repair, distributed data cleansing, schema constraints|16|1215–1230|Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data|Data cleansing approaches have usually focused on detecting and fixing errors with little attention to scaling to big datasets. This presents a serious impediment since data cleansing often involves costly computations such as enumerating pairs of tuples, handling inequality joins, and dealing with user-defined functions. In this paper, we present BigDansing, a Big Data Cleansing system to tackle efficiency, scalability, and ease-of-use issues in data cleansing. The system can run on top of most common general purpose data processing platforms, ranging from DBMSs to MapReduce-like frameworks. A user-friendly programming interface allows users to express data quality rules both declaratively and procedurally, with no requirement of being aware of the underlying distributed platform. BigDansing takes these rules into a series of transformations that enable distributed computations and several optimizations, such as shared scans and specialized joins operators. Experimental results on both synthetic and real datasets show that BigDansing outperforms existing baseline systems up to more than two orders of magnitude without sacrificing the quality provided by the repair algorithms.|10.1145/2723372.2747646|https://doi.org/10.1145/2723372.2747646|New York, NY, USA|Association for Computing Machinery|9781450327589|2015|BigDansing: A System for Big Data Cleansing|Khayyat, Zuhair and Ilyas, Ihab F. and Jindal, Alekh and Madden, Samuel and Ouzzani, Mourad and Papotti, Paolo and Quian\'{e}-Ruiz, Jorge-Arnulfo and Tang, Nan and Yin, Si|inproceedings|10.1145/2723372.2747646|||||||||||||||||||||||||||||1765234676|42
||Survey data, Administrative data, Big data, Policy evaluation, Impact evaluation, Process evaluation||197-225|Implementing Data-Driven Strategies in Smart Cities|Public policies should be designed and implemented, whenever possible, using evidence as rigorous as possible. Urban interventions then should be no exception. In recent times, we have witnessed increasing efforts to transform information into knowledge, and thus help policymakers make better decisions. In this chapter, we will explore how public policy evaluation helps municipal governments tackle social problems and how big data can improve the design and implementation of more effective, efficient, and transparent policies.|https://doi.org/10.1016/B978-0-12-821122-9.00002-6|https://www.sciencedirect.com/science/article/pii/B9780128211229000026||Elsevier|978-0-12-821122-9|2022|Chapter 7 - Data-driven policy evaluation|Marçal Farré and Federico Todeschini and Didier Grimaldi and Carlos Carrasco-Farré|incollection|FARRE2022197||||||||||Didier Grimaldi and Carlos Carrasco-Farré|||||||||||||||||||1769217250|42
||Costs;Data integrity;Inspection;Safety;Blockchains;Task analysis;Information technology;Drones;Artificial Intelligence;Real-Estate Accounting;ERP systems;Revenue Recognition||1-4|2021 22nd International Arab Conference on Information Technology (ACIT)|The accounting profession has gone through radical changes due to recent technological advancements in AI, blockchain technologies, big data, etc. More recently, the accounting literature discussed the possibility of using the drone innovative technology in conducting inventory observation as well as internal and external auditing. This study is a visionary paper which investigates the applicability of a remotely auditing process using drone technology in real-estate accounting. The drone will be used to conduct site inspection to assess and monitor the construction progress through applying the percentage of completion method to recognize revenues from long-term contracts. This innovative technology has the ability to collect better data quality, saving cost and saving time with improved site safety which will help improve the auditor tasks.|10.1109/ACIT53391.2021.9677226|||||2021|The Impact of Drone Technology on The Accounting Profession: The Case of Revenue Recognition in Long-Term Construction Contracts|Qasim, Amer and El Refae, Ghaleb A. and Issa, Hussein and Eletter, Shorouq|inproceedings|9677226||Dec|||||||||||||||||||||||||||1770654367|42
||Big Data, Optimisation problem, Constraint programming, Distributed data, Heterogeneous data format||101180||The application of the optimisation problems in the daily decisions of companies is able to be used for finding the best management according to the necessities of the organisations. However, optimisation problems imply a high computational complexity, increased by the current necessity to include a massive quantity of data (Big Data), for the creation of optimisation problems to customise products and services for their clients. The irruption of Big Data technologies can be a challenge but also an important mechanism to tackle the computational difficulties of optimisation problems, and the possibility to distribute the problem performance. In this paper, we propose a solution that lets the query of a data set supported by Big Data technologies that imply the resolution of Constraint Optimisation Problem (COP). This proposal enables to: (1) model COPs whose input data are obtained from distributed and heterogeneous data; (2) facilitate the integration of different data sources to create the COPs; and, (3) solve the optimisation problems in a distributed way, to improve the performance. It is done by means of a framework and supported by a tool capable of modelling, solving and querying the results of optimisation problems. The tool integrates the Big Data technologies and commercial solvers of constraint programming. The suitability of the proposal and the development have been evaluated with real data sets whose computational study and results are included and discussed.|https://doi.org/10.1016/j.jocs.2020.101180|https://www.sciencedirect.com/science/article/pii/S1877750320304816||||2020|Unleashing Constraint Optimisation Problem solving in Big Data environments|Álvaro Valencia-Parra and Ángel Jesús Varela-Vaca and Luisa Parody and María Teresa Gómez-López|article|VALENCIAPARRA2020101180|||Journal of Computational Science|18777503||45|||||19700174607|0,704|Q1|46|123|485|5550|2325|465|4,97|45,12|Netherlands|Western Europe|2010-2020|Computer Science (miscellaneous) (Q1); Modeling and Simulation (Q2); Theoretical Computer Science (Q2)|3,198|3.976|0.00489|1770663713|185640112
||Quality 4.0, Certification, Smart manufacturing, Artificial intelligence, Big data||748-759||Industrial Big Data (IBD) and Artificial Intelligence (AI) are propelling the new era of manufacturing - smart manufacturing. Manufacturing companies can competitively position themselves amongst the most advanced and influential companies by successfully implementing Quality 4.0 practices. Despite the global impact of COVID-19 and the low deployment success rate, industrialization of the AI mega-trend has dominated the business landscape in 2020. Although these technologies have the potential to advance quality standards, it is not a trivial task. A significant portion of quality leaders do not yet have a clear deployment strategy and universally cite difficulty in harnessing such technologies. The lack of people power is one of the biggest challenges. From a career development standpoint, the higher-educated employees (such as engineers) are the most exposed to, and thus affected by, these new technologies. 79% of young professionals have reported receiving training outside of formal schooling to acquire the necessary skills for Industry 4.0. Strategically investing in training is thus important for manufacturing companies to generate value from IBD and AI. Following the path traced by Six Sigma, this article presents a certification curricula for Green, Black, and Master Black Belts. The proposed curriculum combines six areas of knowledge: statistics, quality, manufacturing, programming, learning, and optimization. These areas, along with an ad hoc 7-step problem solving strategy, must be mastered to obtain a certification. Certified professionals will be well positioned to deploy Quality 4.0 technologies and strategies. They will have the capacity to identify engineering intractable problems that can be formulated as machine learning problems and successfully solve them. These certifications are an efficient and effective way for professionals to advance in their career and thrive in Industry 4.0.|https://doi.org/10.1016/j.promfg.2021.06.085|https://www.sciencedirect.com/science/article/pii/S2351978921001086||||2021|Quality 4.0 — Green, Black and Master Black Belt Curricula|Carlos A. Escobar and Debejyo Chakraborty and Megan McGovern and Daniela Macias and Ruben Morales-Menendez|article|ESCOBAR2021748|||Procedia Manufacturing|23519789||53||49th SME North American Manufacturing Research Conference (NAMRC 49, 2021)|||21100792109|0,504|Q2|43|1390|3628|27760|8346|3572|1,79|19,97|Netherlands|Western Europe|2015-2020|Artificial Intelligence (Q2); Industrial and Manufacturing Engineering (Q2)||||1771363019|896540749
ICMHI '17|Taichung City, Taiwan|log analysis, Type-1 diabetes, Cloud, auditing|10|18–27|Proceedings of the 1st International Conference on Medical and Health Informatics 2017|The Environmental Determinants of Islet Auto- immunity (ENDIA) project is Australia's largest study into the causes of Type-1 Diabetes (T1D). The ENDIA study is supported by a Cloud-based software platform including a clinical registry comprising extensive longitudinal information on families at risk of having a child that might go on to develop T1D. This registry includes both demographic and clinical information on families and children as well as the environmental factors that might influence the onset of T1D. A multitude of samples are obtained through the study and used to support a diverse portfolio of bioinformatics data analytics. The quality of the data in the registry is essential to the overall success of the project. This paper presents a Cloud-based log-analytics platform that supports the detailed analysis of patterns of usage of the registry by the clinical centres and collaborators involved. We explore the impact that the usage patterns have on the overall data quality. We also consider ways of improving data quality by mothers entering their own data through targeted mobile applications that have been developed for dietary data collection.|10.1145/3107514.3107518|https://doi.org/10.1145/3107514.3107518|New York, NY, USA|Association for Computing Machinery|9781450352246|2017|Usage Patterns and Data Quality: A Case Study of a National Type-1 Diabetes Study|"\"Wu, Jinrong and Sinnott, Richard O. and Effendy, Jemie and Gl\"\"{o}ckner, Stephan and Hu, William and Li, Jiajie\""|inproceedings|10.1145/3107514.3107518|||||||||||||||||||||||||||||1771783241|42
AIAM2021|Manchester, United Kingdom||6|450–455|2021 3rd International Conference on Artificial Intelligence and Advanced Manufacture|Campus data is a subset of education big data. It is a variety of data generated by teachers and students in the life, teaching, scientific research, management and service process, as well as various school affairs management status data. It has the characteristics of a wide variety of data. Contains great information value, and giving full play to its role is an indispensable part of achieving the school's strategic goals. Taking the opportunity of building a smart campus, using advanced technologies such as cloud computing, big data, Internet of Things, and artificial intelligence, through a big data management platform, the full collection of existing business data inside and outside the school is provided, and a normal data governance model is provided to eliminate data islands. Realize normal data sharing services. The article conducts research on the data governance platform, and builds a data governance platform system with functions such as a normalized data quality monitoring system, information resource catalog system and full-link data monitoring to realize university data integration, business collaboration, service upgrades and assistance Decision-making provides a solid foundation for the application and expansion of smart campuses in universities and provides a reference for smart campus builders in universities.|10.1145/3495018.3495097|https://doi.org/10.1145/3495018.3495097|New York, NY, USA|Association for Computing Machinery|9781450385046|2022|Research and Construction of University Data Governance Platform Based on Smart Campus Environment|Chen, Zhangbin and Liu, Yang|inproceedings|10.1145/3495018.3495097|||||||||||||||||||||||||||||1773990738|42
||Energy, Big data analytics, Internet of energy, Smart grid||91-100||Electric power systems are taking drastic advances in deployment of information and communication technologies; numerous new measurement devices are installed in forms of advanced metering infrastructure, distributed energy resources (DER) monitoring systems, high frequency synchronized wide-area awareness systems that with great speed are generating immense volume of energy data. However, it is still questioned that whether the today’s power system data, the structures and the tools being developed are indeed aligned with the pillars of the big data science. Further, several requirements and especial features of power systems and energy big data call for customized methods and platforms. This paper provides an assessment of the distinguished aspects in big data analytics developments in the domain of power systems. We perform several taxonomy of the existing and the missing elements in the structures and methods associated with big data analytics in power systems. We also provide a holistic outline, classifications, and concise discussions on the technical approaches, research opportunities, and application areas for energy big data analytics.|https://doi.org/10.1016/j.egyr.2017.11.002|https://www.sciencedirect.com/science/article/pii/S2352484717300616||||2018|Power systems big data analytics: An assessment of paradigm shift barriers and prospects|Hossein Akhavan-Hejazi and Hamed Mohsenian-Rad|article|AKHAVANHEJAZI201891|||Energy Reports|23524847||4|||||21100389511|1,199|Q1|33|983|239|28463|1788|239|7,37|28,96|United Kingdom|Western Europe|2015-2020|Energy (miscellaneous) (Q1)|2,964|6.870|0.00318|1774831366|1436431897
||Big data, Big data analytics, Sustainability, Air pollution, Resource orchestration||103231||Under rapid urbanization, cities are facing many societal challenges that impede sustainability. Big data analytics (BDA) gives cities unprecedented potential to address these issues. As BDA is still a new concept, there is limited knowledge on how to apply BDA in a sustainability context. Thus, this study investigates a case using BDA for sustainability, adopting the resource orchestration perspective. A process model is generated, which provides novel insights into three aspects: data resource orchestration, BDA capability development, and big data value creation. This study benefits both researchers and practitioners by contributing to theoretical developments as well as by providing practical insights.|https://doi.org/10.1016/j.im.2019.103231|https://www.sciencedirect.com/science/article/pii/S0378720619302010||||2022|Orchestrating big data analytics capability for sustainability: A study of air pollution management in China|Dan Zhang and Shan L. Pan and Jiaxin Yu and Wenyuan Liu|article|ZHANG2022103231|||Information & Management|03787206|5|59||Big Data Analytics for Sustainability|||12303|2,147|Q1|162|130|248|11385|2482|246|8,94|87,58|Netherlands|Western Europe|1977-2020|Information Systems (Q1); Information Systems and Management (Q1); Management Information Systems (Q1)||||1775104033|1945939487
||Internet of Things, Big data, Analytics, Distributed computing, Smart city||459-471||The explosive growth in the number of devices connected to the Internet of Things (IoT) and the exponential increase in data consumption only reflect how the growth of big data perfectly overlaps with that of IoT. The management of big data in a continuously expanding network gives rise to non-trivial concerns regarding data collection efficiency, data processing, analytics, and security. To address these concerns, researchers have examined the challenges associated with the successful deployment of IoT. Despite the large number of studies on big data, analytics, and IoT, the convergence of these areas creates several opportunities for flourishing big data and analytics for IoT systems. In this paper, we explore the recent advances in big data analytics for IoT systems as well as the key requirements for managing big data and for enabling analytics in an IoT environment. We taxonomized the literature based on important parameters. We identify the opportunities resulting from the convergence of big data, analytics, and IoT as well as discuss the role of big data analytics in IoT applications. Finally, several open challenges are presented as future research directions.|https://doi.org/10.1016/j.comnet.2017.06.013|https://www.sciencedirect.com/science/article/pii/S1389128617302591||||2017|The role of big data analytics in Internet of Things|Ejaz Ahmed and Ibrar Yaqoob and Ibrahim Abaker Targio Hashem and Imran Khan and Abdelmuttlib Ibrahim Abdalla Ahmed and Muhammad Imran and Athanasios V. Vasilakos|article|AHMED2017459|||Computer Networks|13891286||129||Special Issue on 5G Wireless Networks for IoT and Body Sensors|||26811|0,798|Q1|135|383|896|19762|4981|877|5,93|51,60|Netherlands|Western Europe|1977-1984, 1989-1990, 1996-2020|Computer Networks and Communications (Q1)|11,644|4.474|0.00957|1777594172|424954694
SSDBM '17|Chicago, IL, USA|Time series data management, Data feed management, Data management for the Internet of Things (IoT)|11||Proceedings of the 29th International Conference on Scientific and Statistical Database Management|We present insights on data management resulting from a field deployment of approximately 30 sensor-equipped electric bicycles (e-bikes) at the University of Waterloo. The trial has been in operation for the last two-and-a-half years, and we have collected and analyzed more than 150 gigabytes of data. We discuss best practices for the entire data management process, spanning data collection, extract-transform-load, data cleaning, and choosing a suitable data management ecosystem. We also comment on how our experiences will inform the design of a future large-scale field trial involving several thousand fully-instrumented e-bikes.|10.1145/3085504.3085505|https://doi.org/10.1145/3085504.3085505|New York, NY, USA|Association for Computing Machinery|9781450352826|2017|Managing Sensor Data Streams: Lessons Learned from the WeBike Project|Gorenflo, Christian and Golab, Lukasz and Keshav, Srinivasan|inproceedings|10.1145/3085504.3085505|1||||||||||||||||||||||||||||1778049397|42
https://doi.org/10.1080/01441647.2019.1649315|https://www.sciencedirect.com/science/article/pii/S0144164722001568||||2019|How big data enriches maritime research – a critical review of Automatic Identification System (AIS) data applications|Dong Yang and Lingxiao Wu and Shuaian Wang and Haiying Jia and Kevin X. Li|article|YANG2019755|||Transport Reviews|01441647|6|39||||||||||||||||||||||||||||||1778512556|42
||Forensic science, Big data, Personalized medicine, Predictive medicine, Machine learning, Dimensionality||1-6||In less than a decade, big data in medicine has become quite a phenomenon and many biomedical disciplines got their own tribune on the topic. Perspectives and debates are flourishing while there is a lack for a consensual definition for big data. The 3Vs paradigm is frequently evoked to define the big data principles and stands for Volume, Variety and Velocity. Even according to this paradigm, genuine big data studies are still scarce in medicine and may not meet all expectations. On one hand, techniques usually presented as specific to the big data such as machine learning techniques are supposed to support the ambition of personalized, predictive and preventive medicines. These techniques are mostly far from been new and are more than 50 years old for the most ancient. On the other hand, several issues closely related to the properties of big data and inherited from other scientific fields such as artificial intelligence are often underestimated if not ignored. Besides, a few papers temper the almost unanimous big data enthusiasm and are worth attention since they delineate what is at stakes. In this context, forensic science is still awaiting for its position papers as well as for a comprehensive outline of what kind of contribution big data could bring to the field. The present situation calls for definitions and actions to rationally guide research and practice in big data. It is an opportunity for grounding a true interdisciplinary approach in forensic science and medicine that is mainly based on evidence.|https://doi.org/10.1016/j.jflm.2017.08.001|https://www.sciencedirect.com/science/article/pii/S1752928X17301154||||2018|Big data in forensic science and medicine|Thomas Lefèvre|article|LEFEVRE20181|||Journal of Forensic and Legal Medicine|1752928X||57||Thematic section: Big dataGuest editor: Thomas LefèvreThematic section: Health issues in police custodyGuest editors: Patrick Chariot and Steffen Heide|||5100154502|0,569|Q1|47|134|428|3921|668|407|1,57|29,26|United Kingdom|Western Europe|2007-2020|Law (Q1); Medicine (miscellaneous) (Q2); Pathology and Forensic Medicine (Q2)|2,328|1.614|0.00305|1781559648|1636581161
||Big Data, Big Data Analytics, Challenges, Methods, Systematic literature review||263-286||Big Data (BD), with their potential to ascertain valued insights for enhanced decision-making process, have recently attracted substantial interest from both academics and practitioners. Big Data Analytics (BDA) is increasingly becoming a trending practice that many organizations are adopting with the purpose of constructing valuable information from BD. The analytics process, including the deployment and use of BDA tools, is seen by organizations as a tool to improve operational efficiency though it has strategic potential, drive new revenue streams and gain competitive advantages over business rivals. However, there are different types of analytic applications to consider. Therefore, prior to hasty use and buying costly BD tools, there is a need for organizations to first understand the BDA landscape. Given the significant nature of the BD and BDA, this paper presents a state-of-the-art review that presents a holistic view of the BD challenges and BDA methods theorized/proposed/employed by organizations to help others understand this landscape with the objective of making robust investment decisions. In doing so, systematically analysing and synthesizing the extant research published on BD and BDA area. More specifically, the authors seek to answer the following two principal questions: Q1 – What are the different types of BD challenges theorized/proposed/confronted by organizations? and Q2 – What are the different types of BDA methods theorized/proposed/employed to overcome BD challenges?. This systematic literature review (SLR) is carried out through observing and understanding the past trends and extant patterns/themes in the BDA research area, evaluating contributions, summarizing knowledge, thereby identifying limitations, implications and potential further research avenues to support the academic community in exploring research themes/patterns. Thus, to trace the implementation of BD strategies, a profiling method is employed to analyze articles (published in English-speaking peer-reviewed journals between 1996 and 2015) extracted from the Scopus database. The analysis presented in this paper has identified relevant BD research studies that have contributed both conceptually and empirically to the expansion and accrual of intellectual wealth to the BDA in technology and organizational resource management discipline.|https://doi.org/10.1016/j.jbusres.2016.08.001|https://www.sciencedirect.com/science/article/pii/S014829631630488X||||2017|Critical analysis of Big Data challenges and analytical methods|Uthayasankar Sivarajah and Muhammad Mustafa Kamal and Zahir Irani and Vishanth Weerakkody|article|SIVARAJAH2017263|||Journal of Business Research|01482963||70|||||20550|2,049|Q1|195|878|1342|73221|11507|1315|7,38|83,40|United States|Northern America|1973-2021|Marketing (Q1)|46,935|7.550|0.03523|1783139993|1502892296
||Artificial intelligence, Big data, Machine learning, Data analytics, Wearable devices, Robotics, Digital health||101429||Digital health or eHealth technologies, notably pervasive computing, robotics, big-data, wearable devices, machine learning, and artificial intelligence (AI), have opened unprecedented opportunities as to how the diseases are diagnosed and managed with active patient engagement. Patient-related data have provided insights (real world data) into understanding the disease processes. Advanced analytics have refined these insights further to draw dynamic algorithms aiding clinicians in making more accurate diagnosis with the help of machine learning. AI is another tool, which, although is still in the evolution stage, has the potential to help identify early signs even before the clinical features are apparent. The evolving digital developments pose challenges on allowing access to health-related data for further research but, at the same time, protecting each patient's privacy. This review focuses on the recent technological advances and their applications and highlights the immense potential to enable early diagnosis of rheumatological diseases.|https://doi.org/10.1016/j.berh.2019.101429|https://www.sciencedirect.com/science/article/pii/S1521694219300981||||2019|Emerging role of eHealth in the identification of very early inflammatory rheumatic diseases|Suchitra Kataria and Vinod Ravindran|article|KATARIA2019101429|||Best Practice & Research Clinical Rheumatology|15216942|4|33||How to Investigate: Very Early Inflammatory Rheumatic Diseases|||19160|1,449|Q1|100|70|203|5685|872|185|3,48|81,21|United Kingdom|Western Europe|1995, 1999-2020|Medicine (miscellaneous) (Q1); Rheumatology (Q1)||||1783512190|1575570378
|||73|245–317||Predictions obtained by, e.g., artificial neural networks have a high accuracy but humans often perceive the models as black boxes. Insights about the decision making are mostly opaque for humans. Particularly understanding the decision making in highly sensitive areas such as healthcare or finance, is of paramount importance. The decision-making behind the black boxes requires it to be more transparent, accountable, and understandable for humans. This survey paper provides essential definitions, an overview of the different principles and methodologies of explainable Supervised Machine Learning (SML). We conduct a state-of-the-art survey that reviews past and recent explainable SML approaches and classifies them according to the introduced definitions. Finally, we illustrate principles by means of an explanatory case study and discuss important future directions.|10.1613/jair.1.12228|https://doi.org/10.1613/jair.1.12228|El Segundo, CA, USA|AI Access Foundation||2021|A Survey on the Explainability of Supervised Machine Learning|Burkart, Nadia and Huber, Marco F.|article|10.1613/jair.1.12228||may|J. Artif. Int. Res.|10769757||70|May 2021||||24330|0,790|Q2|123|75|199|4723|1285|199|7,15|62,97|United States|Northern America|1993, 1996-2020|Artificial Intelligence (Q2)|5,173|2.776|0.0033|1789802655|324503572
||Data integrity;Smart grids;Phasor measurement units;Big Data;Quality assessment;Big data;data quality;power system;smart grid||1-6|2018 20th National Power Systems Conference (NPSC)|Enormous amount of data gets generated in the Smart Grids (SGs) due to the large number of measuring devices, higher measurement rates and various types of sensors. Smart grid data contains important and critical information about the grid. Data driven applications are being developed for better planning, monitoring and operation of SGs. The outcome of data analytics heavily depends on the quality of SG data. However, not much work has been reported on the quality assessment of SG data. This paper addresses the objective assessment of SG data quality. Various dimensions of SG data quality are identified in this paper. Mathematical formulations are proposed to quantify the SG data quality. Proposed data quality metrics have been applied on the SCADA and PMU measurements collected from the Southern Regional Grid of India to demonstrate their effectiveness.|10.1109/NPSC.2018.8771733|||||2018|Quality Assessment of Smart Grid Data|Radhakrishnan, Asha and Das, Sarasij|inproceedings|8771733||Dec|||||||||||||||||||||||||||1794222676|42
|||16|3782–3797||Big data is typically characterized with 4V's: Volume, Velocity, Variety and Veracity. When it comes to big graphs, these challenges become even more staggering. Each and every of the 4V's raises new questions, from theory to systems and practice. Is it possible to parallelize sequential graph algorithms and guarantee the correctness of the parallelized computations? Given a computational problem, does there exist a parallel algorithm for it that guarantees to reduce parallel runtime when more machines are used? Is there a systematic method for developing incremental algorithms with effectiveness guarantees in response to frequent updates? Is it possible to write queries across relational databases and semistructured graphs in SQL? Can we unify logic rules and machine learning, to improve the quality of graph-structured data, and deduce associations between entities? This paper aims to incite interest and curiosity in these topics. It raises as many questions as it answers.|10.14778/3554821.3554899|https://doi.org/10.14778/3554821.3554899||VLDB Endowment||2022|Big Graphs: Challenges and Opportunities|Fan, Wenfei|article|10.14778/3554821.3554899||sep|Proc. VLDB Endow.|21508097|12|15|August 2022||||21100199855|0,946|Q1|134|246|598|12401|3759|566|5,50|50,41|United States|Northern America|2008-2020|Computer Science (miscellaneous) (Q1)|7,198|2.047|0.00891|1796944013|1216159931
EnSEmble 2018|Lake Buena Vista, FL, USA|Data Quality, Big Data, Smart Data|6|19–24|Proceedings of the 1st ACM SIGSOFT International Workshop on Ensemble-Based Software Engineering|Big Data (BD) solutions are designed to better support decision-making processes in order to optimize organizational performance. These BD solutions use company’s core business data, using typically large datasets. However, data that doesn’t meet adequate quality levels will lead to BD solutions that will not produce useful results, and consequently may not be used to make adequate business decisions. For a long time, companies have collected and stored large amounts of data without being able to exploit the advantage of exploring it. Nowadays, and thanks to the Big Data explosion, organizations have begun to recognize the need for estimating the value of their data and, vice-versa, managing data accordingly to their value. This need of managing the Value of data has led to the concept of Smart Data. It not only involves the datasets, but also the set of technologies, tools, processes and methodologies that enable all the Values from the data to the End-users (Business, data scientist, BI…). Consequently, Smart data is data actionable. We discovered that data quality is one of the most important issues when it comes to “smartizing” data. In this paper, we introduce a methodology to make data smarter, taking as a reference point, the quality level of the data itself.|10.1145/3281022.3281026|https://doi.org/10.1145/3281022.3281026|New York, NY, USA|Association for Computing Machinery|9781450360548|2018|From Big Data to Smart Data: A Data Quality Perspective|Baldassarre, Maria Teresa and Caballero, Ismael and Caivano, Danilo and Rivas Garcia, Bibiano and Piattini, Mario|inproceedings|10.1145/3281022.3281026|||||||||||||||||||||||||||||1797530237|42
https://doi.org/10.1016/j.cie.2016.09.023|https://www.sciencedirect.com/science/article/pii/S0360835216303631||||2016|Big data applications in operations/supply-chain management: A literature review|Richard Addo-Tenkorang and Petri T. Helo|article|ADDOTENKORANG2016528|||Computers & Industrial Engineering|03608352||101||||||||||||||||||||||||||||||1798521593|42
ICEBT '22|Beijing, China|Classification, Association, Machine Learning, Data Management, Data Warehousing, Big Data, Information systems applications Data Mining, Prediction, Data Analytics|9|102–110|Proceedings of the 2022 6th International Conference on E-Education, E-Business and E-Technology|According to research conducted by Allied Market Research (Kale and Deshmukh, 2020), the Quick-service restaurant (QSR) industry has been enjoying significant growth over the last several years and the trend is expected to continue at least over the next five years. This is due to the agility of most QSR brands in adapting to consumer needs, quality food products and service innovation. Offering the convenience of brands reach for their consumers such as a restaurant within the reach of the customer, as well as providing online ordering and food delivery convenience.The convenience of the omnichannel digital services offered by typical QSR restaurant brands to their customer provides convenience for service delivery and at the same time provides an opportunity to collect, collate, check, consume and analyse the data to examine, explore and provide hindsight, insight and foresight for the QSR industry to be ahead of the competition.Not only are these digital customer channels a great way of providing a better customer experience, but they also help QSR operators in capturing massive data such as sales transactions, customer details, product performance, and daypart (breakfast, lunch and dinner) sales insights.With the ability to analyse and perform analytics on the data and from a data strategy perspective, it is paramount to have a robust big data platform, data warehouse and ability to consolidate data from all the IT systems. This is to enable the development of BI/dashboards to provide better business insights, such as real-time and daypart product sales. Data could also be used to develop new digital products for more positive customer engagements such as upselling and loyalty programmes or for implementing of “smart kitchen” that is able to connect various datasets for maximum kitchen efficiency and cost optimisation.Due to a lack of data science approach and machine learning capability, most QSR brands are missing this sales uplifting opportunity.To develop an effective A.I. and machine learning in the QSR business, there is a need to evaluate and select the best machine learning techniques to address the business challenges.Our objective is to explore and recommend the various machine learning techniques that can be applied to the QSR industry to gain insights and provide the necessary analysis and analytics to ensure quality service and also address changing business challenges in the competitive and ever-changing market by deploying various machine learning models for prediction and pattern analysis through classification and association approaches.|10.1145/3549843.3549859|https://doi.org/10.1145/3549843.3549859|New York, NY, USA|Association for Computing Machinery|9781450397216|2022|Artificial Intelligence Adoption in QSR Industry|Lee, Angela S.H and Bengeri, Atul and Kan, Chong Chin|inproceedings|10.1145/3549843.3549859|||||||||||||||||||||||||||||1806809844|42
||Big data analysis, Privacy-preserving, Outsourced big data, Oblivious RAM, Security, Practical-oriented, Secure query||97-113||With the significant increase in the volume, variety, velocity and veracity of data generated, collected and transmitted through computing and networking systems, it is of little surprise that big data analysis and processing is the subject of focus from enterprise, academia and government. Outsourcing is one popular solution considered in big data processing, although security and privacy are two key concerns often attributed to the underutilization of outsourcing and other promising big data analysis and processing technologies. In this paper, we survey the state-of-the-art literature on cryptographic solutions designed to ensure the security and/or privacy in big data outsourcing. For example, we provide concrete examples to explain how these cryptographic solutions can be deployed. We summarize the existing state-of-play before discussing research opportunities.|https://doi.org/10.1016/j.cose.2016.12.006|https://www.sciencedirect.com/science/article/pii/S0167404816301778||||2017|Practical-oriented protocols for privacy-preserving outsourced big data analysis: Challenges and future research directions|Zhe Liu and Kim-Kwang Raymond Choo and Minghao Zhao|article|LIU201797|||Computers & Security|01674048||69||Security Data Science and Cyber Threat Management|||28898|0,861|Q1|92|321|559|17204|3843|550|6,75|53,60|United Kingdom|Western Europe|1982-2020|Computer Science (miscellaneous) (Q1); Law (Q1)||||1808011199|1687137310
SIGIR'19|Paris, France|personalized web search, generative adversarial network|10|555–564|Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval|Personalized search aims to adapt document ranking to user's personal interests. Traditionally, this is done by extracting click and topical features from historical data in order to construct a user profile. In recent years, deep learning has been successfully used in personalized search due to its ability of automatic feature learning. However, the small amount of noisy personal data poses challenges to deep learning models to learn the personalized classification boundary between relevant and irrelevant results. In this paper, we propose PSGAN, a Generative Adversarial Network (GAN) framework for personalized search. By means of adversarial training, we enforce the model to pay more attention to training data that are difficult to distinguish. We use the discriminator to evaluate personalized relevance of documents and use the generator to learn the distribution of relevant documents. Two alternative ways to construct the generator in the framework are tested: based on the current query or based on a set of generated queries. Experiments on data from a commercial search engine show that our models can yield significant improvements over state-of-the-art models.|10.1145/3331184.3331218|https://doi.org/10.1145/3331184.3331218|New York, NY, USA|Association for Computing Machinery|9781450361729|2019|PSGAN: A Minimax Game for Personalized Search with Limited and Noisy Click Data|Lu, Shuqi and Dou, Zhicheng and Jun, Xu and Nie, Jian-Yun and Wen, Ji-Rong|inproceedings|10.1145/3331184.3331218|||||||||||||||||||||||||||||1809051026|42
ISCA '17|Toronto, ON, Canada|Approximate Computing, Networks-On-Chip, Data Compression|12|666–677|Proceedings of the 44th Annual International Symposium on Computer Architecture|The trend of unsustainable power consumption and large memory bandwidth demands in massively parallel multicore systems, with the advent of the big data era, has brought upon the onset of alternate computation paradigms utilizing heterogeneity, specialization, processor-in-memory and approximation. Approximate Computing is being touted as a viable solution for high performance computation by relaxing the accuracy constraints of applications. This trend has been accentuated by emerging data intensive applications in domains like image/video processing, machine learning and big data analytics that allow inaccurate outputs within an acceptable variance. Leveraging relaxed accuracy for high throughput in Networks-on-Chip (NoCs), which have rapidly become the accepted method for connecting a large number of on-chip components, has not yet been explored. We propose APPROX-NoC, a hardware data approximation framework with an online data error control mechanism for high performance NoCs. APPROX-NoC facilitates approximate matching of data patterns, within a controllable value range, to compress them thereby reducing the volume of data movement across the chip.Our evaluation shows that APPROX-NoC achieves on average up to 9% latency reduction and 60% throughput improvement compared with state-of-the-art NoC data compression mechanisms, while maintaining low application error. Additionally, with a data intensive graph processing application we achieve a 36.7% latency reduction compared to state-of-the-art compression mechanisms.|10.1145/3079856.3080241|https://doi.org/10.1145/3079856.3080241|New York, NY, USA|Association for Computing Machinery|9781450348928|2017|APPROX-NoC: A Data Approximation Framework for Network-On-Chip Architectures|Boyapati, Rahul and Huang, Jiayi and Majumder, Pritam and Yum, Ki Hwan and Kim, Eun Jung|inproceedings|10.1145/3079856.3080241|||||||||||||||||||||||||||||1812272659|42
|||2|235–236|Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice|||https://doi.org/10.1145/3447404.3447417|New York, NY, USA|Association for Computing Machinery|9781450390293|2021|Ethics and Smart Cities|McMenemy, David|inbook|10.1145/3447404.3447417|||||||||1||||||||||||||||||||1812935001|42
||machine learning, business value, data scientists, managers, people factor||757-764||Corporations are leveraging machine learning (ML) to create business value (BV). So, it becomes relevant to not only ponder the antecedents that influence the ML BV process but also, the main actors that influence the creation of such value within organizations: data scientists and managers. Grounded in the dynamic-capabilities theory, a model is proposed and tested with 319 responses to a survey. While for both groups, platform maturity and data quality are equally important factors for financial performance, information intensity is an equally important factor for organizational performance. On one hand, data scientists care more about the catalytic effect of data quality on the relationship between platform maturity and financial performance, and the compatibility factor for organizational performance. On the other hand, managers care more about the feasibility factor for financial performance. The findings presented here offer insights on how data scientists and managers perceive the ML BV creation process.|https://doi.org/10.1016/j.procs.2021.01.228|https://www.sciencedirect.com/science/article/pii/S1877050921002714||||2021|How do data scientists and managers influence machine learning value creation?|Humberto Ferreira and Pedro Ruivo and Carolina Reis|article|FERREIRA2021757|||Procedia Computer Science|18770509||181||CENTERIS 2020 - International Conference on ENTERprise Information Systems / ProjMAN 2020 - International Conference on Project MANagement / HCist 2020 - International Conference on Health and Social Care Information Systems and Technologies 2020, CENTERIS/ProjMAN/HCist 2020|||19700182801|0,334|-|76|1907|6483|38511|13722|6359|2,09|20,19|Netherlands|Western Europe|2010-2020|Computer Science (miscellaneous)||||1816898715|2108686752
||Temperature measurement;Temperature distribution;Wind speed;Sea measurements;Weather forecasting;Prediction algorithms;Mathematical models;Solar Power Forecasting;Preprocessing;Missing data;Multivariate Imputation By Chained Equations||21-24|2022 8th International Conference on Applied System Innovation (ICASI)|In the era of big data, large period of missing data is a common problem which affect the data quality and final forecasting results if not handled properly. Therefore, filling missing data in datasets is importance since the most of real-time datasets have a huge number of missing values. This paper first gives a comprehensive overview of various imputation methods for filling missing data. Then proposes a technique based on a popular Multivariate Imputation by Chained Equation (MICE) to fill numeric data in PV dataset. Finally analyses the impact of this technique and compares the performance with other imputation algorithms. For practice, this study uses historical measurement PV generation from the North PV site of Taiwan, and Numerical Weather Prediction (NWP) data consists of solar irradiance, temperature, sea level pressure, humidity, rainfall, wind speed. The NWP dataset is provided by Taiwan Central Weather Bureau (CWB) which is called Deterministic Weather Research and Forecasting (WRFD). Experimental results showed that the proposed imputation algorithm can improve short-term PV generation forecasting accuracy based on RMSE.|10.1109/ICASI55125.2022.9774453|||||2022|A Study on Missing Data Imputation Methods for Improving Hourly Solar Dataset|Phan, Quoc-Thang and Wu, Yuan-Kang and Phan, Quoc-Dung and Lo, Hsin-Yen|inproceedings|9774453||April||27684156|||||||||||||||||||||||||1821298746|1477072354
CAIN '22|Pittsburgh, Pennsylvania|natural language processing, empirical study, data requirements|12|145–156|Proceedings of the 1st International Conference on AI Engineering: Software Engineering for AI|Businesses continue to operate under increasingly complex demands such as ever-evolving regulatory landscape, personalization requirements from software apps, and stricter governance with respect to security and privacy. In response to these challenges, large enterprises have been emphasizing automation across a wide range, starting with business processes all the way to customer experience. As AI continues to be a core component of software systems being developed, data assumes a predominant role. AI-centric software systems of industrial scale need large amounts of training data, that in our experience, has introduced several challenges. In this paper, through an empirical study based on interviews with AI practitioners, we present current challenges that need to be addressed in 'data requirements' of Software Systems with NLP at the Core (SSNLPCore). We further discuss the impact of the challenges and techniques currently employed by practitioners for addressing them. Our findings reveal that a focus on details pertaining to data is required early into the project lifecycle, which include aspects such as how we may select, process, and annotate data. This can ensure that the AI component is effective in meeting business goals of software systems.|10.1145/3522664.3528604|https://doi.org/10.1145/3522664.3528604|New York, NY, USA|Association for Computing Machinery|9781450392754|2022|Data is about Detail: An Empirical Investigation for Software Systems with NLP at Core|Singhal, Anmol and Anish, Preethu Rose and Sonar, Pratik and Ghaisas, Smita S|inproceedings|10.1145/3522664.3528604|||||||||||||||||||||||||||||1823743636|42
||Internet of Things;Medical services;Edge computing;Sensors;Blockchain;Quality of service;Big Data;H-IoT;WBAN;machine learning;fog computing;edge computing;blockchain;software defined networks||1121-1167||The impact of the Internet of Things (IoT) on the advancement of the healthcare industry is immense. The ushering of the Medicine 4.0 has resulted in an increased effort to develop platforms, both at the hardware level as well as the underlying software level. This vision has led to the development of Healthcare IoT (H-IoT) systems. The basic enabling technologies include the communication systems between the sensing nodes and the processors; and the processing algorithms for generating an output from the data collected by the sensors. However, at present, these enabling technologies are also supported by several new technologies. The use of Artificial Intelligence (AI) has transformed the H-IoT systems at almost every level. The fog/edge paradigm is bringing the computing power close to the deployed network and hence mitigating many challenges in the process. While the big data allows handling an enormous amount of data. Additionally, the Software Defined Networks (SDNs) bring flexibility to the system while the blockchains are finding the most novel use cases in H-IoT systems. The Internet of Nano Things (IoNT) and Tactile Internet (TI) are driving the innovation in the H-IoT applications. This paper delves into the ways these technologies are transforming the H-IoT systems and also identifies the future course for improving the Quality of Service (QoS) using these new technologies.|10.1109/COMST.2020.2973314|||||2020|The Future of Healthcare Internet of Things: A Survey of Emerging Technologies|Qadri, Yazdan Ahmad and Nauman, Ali and Zikria, Yousaf Bin and Vasilakos, Athanasios V. and Kim, Sung Won|article|8993839||Secondquarter|IEEE Communications Surveys & Tutorials|1553877X|2|22|||||17900156715|6,605|Q1|197|91|368|18147|15073|360|37,55|199,42|United States|Northern America|2005-2020|Electrical and Electronic Engineering (Q1)|22,146|25.249|0.03684|1825242083|1689569609
||||xvii-xvii|2017 IEEE Third International Conference on Big Data Computing Service and Applications (BigDataService)|Provides a listing of current committee members and society officers.|10.1109/BigDataService.2017.57|||||2017|Big Data Quality Assurance Workshop Chairs and Committee||inproceedings|7944912||April|||||||||||||||||||||||||||1825574653|42
||Cybermanufacturing, Internet of Things, sensors, statistical process monitoring, fault detection, fault diagnosis, statistics pattern analysis||14946-14951||Initiated from services and consumer products industries, there is a growing interest in using Internet of Things (IoT) technologies in various industries. In particular, IoT-enabled cybermanufacturing starts to draw increasing attention. Because IoT devices such as IoT sensors are usually much cheaper and smaller than the traditional sensors, there is a potential for instrumenting manufacturing systems with massive number of sensors. The premise is that the big data subsequently collected from IoT sensors can be utilized to advance manufacturing. Therefore, data-driven statistical process monitoring (SPM) is expected to contribute significantly to the advancement of cybermanufacturing. In this work, the state-of-the-art in cybermanufacturing is reviewed; an IoT-enabled manufacturing technology testbed (MTT) was built to explore the potential of IoT sensors for manufacturing, as well as to understand the characteristics of data produced by the IoT sensors; finally, the potentials and challenges associated with big data analytics presented by cybermanufacturing systems is discussed; and we propose statistics pattern analysis (SPA) as a promising SPM tool for cybermanufacturing.|https://doi.org/10.1016/j.ifacol.2017.08.2546|https://www.sciencedirect.com/science/article/pii/S2405896317334717||||2017|Statistical Process Monitoring for IoT-Enabled Cybermanufacturing: Opportunities and Challenges|Q. Peter He and Jin Wang and Devarshi Shah and Nader Vahdat|article|HE201714946|||IFAC-PapersOnLine|24058963|1|50||20th IFAC World Congress|||21100456158|0,308|Q3|72|115|8407|1913|9863|8351|1,13|16,63|Austria|Western Europe|2002-2019|Control and Systems Engineering (Q3)||||1825700051|676980763
||federated learning, security and privacy, machine learning, Edge learning, edge computing|36|||Machine Learning (ML) has demonstrated great promise in various fields, e.g., self-driving, smart city, which are fundamentally altering the way individuals and organizations live, work, and interact. Traditional centralized learning frameworks require uploading all training data from different sources to a remote data server, which incurs significant communication overhead, service latency, and privacy issues.To further extend the frontiers of the learning paradigm, a new learning concept, namely, Edge Learning (EL) is emerging. It is complementary to the cloud-based methods for big data analytics by enabling distributed edge nodes to cooperatively training models and conduct inferences with their locally cached data. To explore the new characteristics and potential prospects of EL, we conduct a comprehensive survey of the recent research efforts on EL. Specifically, we first introduce the background and motivation. We then discuss the challenging issues in EL from the aspects of data, computation, and communication. Furthermore, we provide an overview of the enabling technologies for EL, including model training, inference, security guarantee, privacy protection, and incentive mechanism. Finally, we discuss future research opportunities on EL. We believe that this survey will provide a comprehensive overview of EL and stimulate fruitful future research in this field.|10.1145/3464419|https://doi.org/10.1145/3464419|New York, NY, USA|Association for Computing Machinery||2021|Edge Learning: The Enabling Technology for Distributed Big Data Analytics in the Edge|Zhang, Jie and Qu, Zhihao and Chen, Chenxi and Wang, Haozhao and Zhan, Yufeng and Ye, Baoliu and Guo, Song|article|10.1145/3464419|151|jul|ACM Comput. Surv.|03600300|7|54|September 2022||||23038|2,079|Q1|163|112|365|15859|6978|365|19,16|141,60|United States|Northern America|1969-2020|Computer Science (miscellaneous) (Q1); Theoretical Computer Science (Q1)|10,790|10.282|0.01438|1829699681|1517405264
||Big data analytics, IT-enabled transformation, Practice-based view, Business value of IT, Healthcare, Content analysis||64-79||A big data analytics-enabled transformation model based on practice-based view is developed, which reveals the causal relationships among big data analytics capabilities, IT-enabled transformation practices, benefit dimensions, and business values. This model was then tested in healthcare setting. By analyzing big data implementation cases, we sought to understand how big data analytics capabilities transform organizational practices, thereby generating potential benefits. In addition to conceptually defining four big data analytics capabilities, the model offers a strategic view of big data analytics. Three significant path-to-value chains were identified for healthcare organizations by applying the model, which provides practical insights for managers.|https://doi.org/10.1016/j.im.2017.04.001|https://www.sciencedirect.com/science/article/pii/S0378720617303129||||2018|An integrated big data analytics-enabled transformation model: Application to health care|Yichuan Wang and LeeAnn Kung and William Yu Chung Wang and Casey G. Cegielski|article|WANG201864|||Information & Management|03787206|1|55|||||12303|2,147|Q1|162|130|248|11385|2482|246|8,94|87,58|Netherlands|Western Europe|1977-2020|Information Systems (Q1); Information Systems and Management (Q1); Management Information Systems (Q1)||||1832642883|1945939487
ACAI 2018|Sanya, China|Dissolved Gas Analysis, Missing Values, Iterative KNN, Interpolation Priority|7||Proceedings of the 2018 International Conference on Algorithms, Computing and Artificial Intelligence|Power transformers are an important part of the power system. Accurate monitoring of its operating status is particularly important for the normal and stable operation of the entire power system and the timely diagnosis of potential faults. Dissolved Gas Analysis (DGA) can detect and judge the oil-immersed power transformer failure by comparing the dissolved gas content of the power transformer in the normal operating state and the oil in the fault state. However, in the operation process of the grid transformer, the detection data is often missing. This paper proposes an effective method based on iterative KNN and XGBoost method for missing values. Firstly, according to the XGBoost integration tree, there are missing values. Information such as the number of attribute divisions obtained by data set training calculates the importance scores of different attributes to determine the priority of the attributes, and then performs interpolation on the missing values ?in an iterative manner. The experimental results in the case of DGA dataset and different missing rate show that the proposed method is superior to the existing similar methods in accuracy, and the dataset after interpolation has a significant improvement on the classification effect of the classifier.|10.1145/3302425.3302447|https://doi.org/10.1145/3302425.3302447|New York, NY, USA|Association for Computing Machinery|9781450366250|2018|Imputation Method of Missing Values for Dissolved Gas Analysis Data Based on Iterative KNN and XGBoost|Qiao, Lin and Ran, Ran and Wu, He and Zhou, Qiaoni and Liu, Sai and Liu, Yunfei|inproceedings|10.1145/3302425.3302447|11||||||||||||||||||||||||||||1836310669|42
||Big data use, Big data use in municipalities, Smart cities, E-government, Municipal use of IT, Digital government||101600||It is estimated that by 2050, 70% of the population will be urban (Nations Unies, 2014). This massive urbanization has created unprecedented challenges for cities and city managers which has led many of them to look for technological solutions to address them, including the use of Big Data, which is among the most considered technological support to help improve the overall operational and service delivery of cities. It is estimated that around 7 billion connected objects will soon be implemented in cities worldwide which will produce an unprecedented and massive amount of real-time data that will have to be managed, used, and analyzed effectively. If this massive amount of data is effectively managed and used, it can provide important benefits and produce real positive impacts on the functioning of cities. Nonetheless, despite these benefits, only a few cities are able to use and exploit big data, and some studies have shown that less than 0.5% of all the available data has been explored. The objective of this study is to understand the factors that influence cities to use big data and the nature of such use. Based on a field survey involving 106 municipalities, this study investigates the antecedents of big data use by cities and shows how different sets of antecedents influence three different types of big data use by cities.|https://doi.org/10.1016/j.giq.2021.101600|https://www.sciencedirect.com/science/article/pii/S0740624X21000368||||2021|Is big data used by cities? Understanding the nature and antecedents of big data use by municipalities|Hamza Ali and Ryad Titah|article|ALI2021101600|||Government Information Quarterly|0740624X|4|38|||||14735|2,121|Q1|103|76|205|5894|1946|193|8,39|77,55|United Kingdom|Western Europe|1984-2020|E-learning (Q1); Law (Q1); Library and Information Sciences (Q1); Sociology and Political Science (Q1)|5,379|7.279|0.00502|1840765760|1582933551
WebSci '22|Barcelona, Spain|machine learning, low resource environments, automatic speech recognition, voice-based technologies, under-resourced/indigenous languages, neural networks|8|451–458|14th ACM Web Science Conference 2022|Development of fully featured Automatic Speech Recognition (ASR) systems for a complete language vocabulary generally requires large data repositories, massive computing power, and a stable digital network infrastructure. These conditions are not met in the case of many indigenous languages. Based on our research for over a decade in West Africa, we present a lightweight and downscaled approach to AI-based ASR and describe a set of associated experiments. The aim is to produce a variety of limited-vocabulary ASRs as a basis for the development of practically useful (mobile and radio) voice-based information services that fit needs, preferences and knowledge of local rural communities.|10.1145/3501247.3539017|https://doi.org/10.1145/3501247.3539017|New York, NY, USA|Association for Computing Machinery|9781450391917|2022|A Lightweight Downscaled Approach to Automatic Speech Recognition for Small Indigenous Languages|Stan, George Vlad and Baart, Andr\'{e} and Dittoh, Francis and Akkermans, Hans and Bon, Anna|inproceedings|10.1145/3501247.3539017|||||||||||||||||||||||||||||1844313716|42
|||4|32–35||In March 2017, PIs and co-PIs funded through the NSF BIGDATA program were brought together along with selected industry and government invitees to discuss current research, identify current challenges, discuss promising future directions, foster new collaborations, and share accomplishments, at BDPI-2017. Given that two recent NITRD [2] and NSF [1] meeting reports contained a set of recommendations, grand challenges, and high impact priorities for Big Data, the organizers of this meeting shifted the focus of the breakout sessions to discuss problems and available data sets that exist in five application domains - policy, health, education, economy &amp; finance, and environment &amp; energy. These domains were selected based on a survey of the PIs/co-PIs and should not be interpreted as being more important than others. Slides that were presented by the different breakout group leaders are available at https://www.bi.vt.edu/ nsf-big-data/. We hope this report will serve as a blueprint for promising big data research in five application domains.|10.1145/3316416.3316425|https://doi.org/10.1145/3316416.3316425|New York, NY, USA|Association for Computing Machinery||2019|NSF BIGDATA PI Meeting - Domain-Specific Research Directions and Data Sets|Singh, Lisa and Deshpande, Amol and Zhou, Wenchao and Banerjee, Arindam and Bowers, Alex and Friedler, Sorelle and Jagadish, H.V. and Karypis, George and Obradovic, Zoran and Vullikanti, Anil and Zuo, Wangda|article|10.1145/3316416.3316425||feb|SIGMOD Rec.|01635808|3|47|September 2018||||13622|0,372|Q2|142|39|81|808|127|57|1,67|20,72|United States|Northern America|1969, 1973-1978, 1981-2020|Information Systems (Q2); Software (Q2)|1,819|0.775|7.5E-4|1845888997|962972343
KDD '21|Virtual Event, Singapore|model fairness, distributed learning, collaborative fairness, federated learning, data leakage, data privacy|2|4100–4101|Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining|Federated learning has become increasingly popular as it facilitates collaborative training of machine learning models among multiple clients while preserving their data privacy. In practice, one major challenge for federated learning is to achieve fairness in collaboration among the participating clients, because different clients' contributions to a model are usually far from equal due to various reasons. Besides, as machine learning models are deployed in more and more important applications, how to achieve model fairness, that is, to ensure that a trained model has no discrimination against sensitive attributes, has become another critical desiderata for federated learning. In this tutorial, we discuss formulations and methods such that collaborative fairness, model fairness, and privacy can be fully respected in federated learning. We review the existing efforts and the latest progress, and discuss a series of potential directions.|10.1145/3447548.3470814|https://doi.org/10.1145/3447548.3470814|New York, NY, USA|Association for Computing Machinery|9781450383325|2021|Towards Fair Federated Learning|Zhou, Zirui and Chu, Lingyang and Liu, Changxin and Wang, Lanjun and Pei, Jian and Zhang, Yong|inproceedings|10.1145/3447548.3470814|||||||||||||||||||||||||||||1848091586|42
||healthcare, distributed ledger technology, survey, medical, blockchain, Distributed systems|27|||Blockchain technology has been gaining visibility owing to its ability to enhance the security, reliability, and robustness of distributed systems. Several areas have benefited from research based on this technology, such as finance, remote sensing, data analysis, and healthcare. Data immutability, privacy, transparency, decentralization, and distributed ledgers are the main features that make blockchain an attractive technology. However, healthcare records that contain confidential patient data make this system very complicated because there is a risk of a privacy breach. This study aims to address research into the applications of the blockchain healthcare area. It sets out by discussing the management of medical information, as well as the sharing of medical records, image sharing, and log management. We also discuss papers that intersect with other areas, such as the Internet of Things, the management of information, tracking of drugs along their supply chain, and aspects of security and privacy. As we are aware that there are other surveys of blockchain in healthcare, we analyze and compare both the positive and negative aspects of their papers. Finally, we seek to examine the concepts of blockchain in the medical area, by assessing their benefits and drawbacks and thus giving guidance to other researchers in the area. Additionally, we summarize the methods used in healthcare per application area and show their pros and cons.|10.1145/3376915|https://doi.org/10.1145/3376915|New York, NY, USA|Association for Computing Machinery||2020|A Survey of Blockchain-Based Strategies for Healthcare|De Aguiar, Erikson J\'{u}lio and Fai\c{c}al, Bruno S. and Krishnamachari, Bhaskar and Ueyama, J\'{o}|article|10.1145/3376915|27|mar|ACM Comput. Surv.|03600300|2|53|March 2021||||23038|2,079|Q1|163|112|365|15859|6978|365|19,16|141,60|United States|Northern America|1969-2020|Computer Science (miscellaneous) (Q1); Theoretical Computer Science (Q1)|10,790|10.282|0.01438|1848399612|1517405264
||Technology life cycle, Data quality, Non-stationary data, Hype cycle, Data biases, Data phase triangulation||113139||Where a technology is its life cycle can make a difference in the data generated by or about adopting, using or implementing that technology. As a result, it is arguable that adoption, usage or implementation data generated early in a technology's life cycle is directly comparable to data generated later in the life cycle. In particular, comparisons of early and late data can result in a number of disparate results and limit replicability, because of biases in the data and non-stationary data. This paper suggests that it can be important for information systems researchers to disclose an estimate of the location in the technology's life cycle, as part of their analysis. The data life cycle discussion is then extended to the notion of “data phase triangulation,” which is contrasted with “methodology triangulation” and “data (collection) triangulation.” In addition, we discuss the importance of being able to use the findings from life cycle-based research to “push” a technology from one phase to another phase.|https://doi.org/10.1016/j.dss.2019.113139|https://www.sciencedirect.com/science/article/pii/S016792361930168X||||2019|Technology life cycle and data quality: Action and triangulation|Daniel E. O'Leary|article|OLEARY2019113139|||Decision Support Systems|01679236||126||Perspectives on Numerical Data Quality in IS Research|||21933|1,564|Q1|151|115|342|7152|2672|338|7,04|62,19|Netherlands|Western Europe|1985-2020|Arts and Humanities (miscellaneous) (Q1); Developmental and Educational Psychology (Q1); Information Systems (Q1); Information Systems and Management (Q1); Management Information Systems (Q1)|13,580|5.795|0.0081|1849288403|1234879127
|||26|||Online experiments allow researchers to collect data from large, demographically diverse global populations. Unlike in-lab studies, however, online experiments often fail to inform participants about the research to which they contribute. This paper is the first to investigate barriers that prevent researchers from providing such science communication in online experiments. We found that the main obstacles preventing researchers from including such information are assumptions about participant disinterest, limited time, concerns about losing anonymity, and concerns about experimental bias. Researchers also noted the dearth of tools to help them close the information loop with their study participants. Based on these findings, we formulated design requirements and implemented Digestif, a new web-based tool that supports researchers in providing their participants with science communication pages. Our evaluation shows that Digestif's scaffolding, examples, and nudges to focus on participants make researchers more aware of their participants' curiosity about research and more likely to disclose pertinent research information.|10.1145/3274353|https://doi.org/10.1145/3274353|New York, NY, USA|Association for Computing Machinery||2018|Digestif: Promoting Science Communication in Online Experiments|Jun, Eunice and Jo, Blue A. and Oliveira, Nigini and Reinecke, Katharina|article|10.1145/3274353|84|nov|Proc. ACM Hum.-Comput. Interact.||CSCW|2|November 2018||||||||||||||||||||||1853118810|42
SIGIR '22|Madrid, Spain|graph neural networks, recommender systems, denoising|11|122–132|Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval|Recently, graph neural networks (GNN) have been successfully applied to recommender systems as an effective collaborative filtering (CF) approach. However, existing GNN-based CF models suffer from noisy user-item interaction data, which seriously affects the effectiveness and robustness in real-world applications. Although there have been several studies on data denoising in recommender systems, they either neglect direct intervention of noisy interaction in the message-propagation of GNN, or fail to preserve the diversity of recommendation when denoising. To tackle the above issues, this paper presents a novel GNN-based CF model, named Robust Graph Collaborative Filtering (RGCF), to denoise unreliable interactions for recommendation. Specifically, RGCF consists of a graph denoising module and a diversity preserving module. The graph denoising module is designed for reducing the impact of noisy interactions on the representation learning of GNN, by adopting both a hard denoising strategy (i.e., discarding interactions that are confidently estimated as noise) and a soft denoising strategy (i.e., assigning reliability weights for each remaining interaction). In the diversity preserving module, we build up a diversity augmented graph and propose an auxiliary self-supervised task based on mutual information maximization (MIM) for enhancing the denoised representation and preserving the diversity of recommendation. These two modules are integrated in a multi-task learning manner that jointly improves the recommendation performance. We conduct extensive experiments on three real-world datasets and three synthesized datasets. Experiment results show that RGCF is more robust against noisy interactions and achieves significant improvement compared with baseline models.|10.1145/3477495.3531889|https://doi.org/10.1145/3477495.3531889|New York, NY, USA|Association for Computing Machinery|9781450387323|2022|Learning to Denoise Unreliable Interactions for Graph Collaborative Filtering|Tian, Changxin and Xie, Yuexiang and Li, Yaliang and Yang, Nan and Zhao, Wayne Xin|inproceedings|10.1145/3477495.3531889|||||||||||||||||||||||||||||1854648834|42
||Cloud computing, Reputation mechanism, Trustworthiness, Data veracity||293-302||Cloud computing and the mobile Internet have been the two most influential information technology revolutions, which intersect in mobile cloud computing (MCC). The burgeoning MCC enables the large-scale collection and processing of big data, which demand trusted, authentic, and accurate data to ensure an important but often overlooked aspect of big data — data veracity. Troublesome internal attacks launched by internal malicious users is one key problem that reduces data veracity and remains difficult to handle. To enhance data veracity and thus improve the performance of big data computing in MCC, this paper proposes a Data Trustworthiness enhanced Reputation Mechanism (DTRM) which can be used to defend against internal attacks. In the DTRM, the sensitivity-level based data category, Metagraph theory based user group division, and reputation transferring methods are integrated into the reputation query and evaluation process. The extensive simulation results based on real datasets show that the DTRM outperforms existing classic reputation mechanisms under bad mouthing attacks and mobile attacks.|https://doi.org/10.1016/j.future.2018.01.026|https://www.sciencedirect.com/science/article/pii/S0167739X17317247||||2018|DTRM: A new reputation mechanism to enhance data trustworthiness for high-performance cloud computing|Hui Lin and Jia Hu and Chuanfeng Xu and Jianfeng Ma and Mengyang Yu|article|LIN2018293|||Future Generation Computer Systems|0167739X||83|||||12264|1,262|Q1|119|798|1935|36054|16877|1868|9,11|45,18|Netherlands|Western Europe|1984-2021|Computer Networks and Communications (Q1); Hardware and Architecture (Q1); Software (Q1)||||1855823051|562237118
|||||Data Cleaning|Data quality is one of the most important problems in data management, since dirty data often leads to inaccurate data analytics results and incorrect business decisions. Poor data across businesses and the U.S. government are reported to cost trillions of dollars a year. Multiple surveys show that dirty data is the most common barrier faced by data scientists. Not surprisingly, developing effective and efficient data cleaning solutions is challenging and is rife with deep theoretical and engineering problems.This book is about data cleaning, which is used to refer to all kinds of tasks and activities to detect and repair errors in the data. Rather than focus on a particular data cleaning task, we give an overview of the endto- end data cleaning process, describing various error detection and repair methods, and attempt to anchor these proposals with multiple taxonomies and views. Specifically, we cover four of the most common and important data cleaning tasks, namely, outlier detection, data transformation, error repair (including imputing missing values), and data deduplication. Furthermore, due to the increasing popularity and applicability of machine learning techniques, we include a chapter that specifically explores how machine learning techniques are used for data cleaning, and how data cleaning is used to improve machine learning models.This book is intended to serve as a useful reference for researchers and practitioners who are interested in the area of data quality and data cleaning. It can also be used as a textbook for a graduate course. Although we aim at covering state-of-the-art algorithms and techniques, we recognize that data cleaning is still an active field of research and therefore provide future directions of research whenever appropriate.||https://doi.org/10.1145/3310205.3310206|New York, NY, USA|Association for Computing Machinery|9781450371520|2019|Preface||inbook|10.1145/3310205.3310206|||||||||||||||||||||||||||||1857154949|42
https://doi.org/10.1016/j.idh.2018.10.002|https://www.sciencedirect.com/science/article/pii/S2468045118301445||||2019|Artificial Intelligence for infectious disease Big Data Analytics|Zoie S.Y. Wong and Jiaqi Zhou and Qingpeng Zhang|article|WONG201944|||Infection, Disease & Health|24680451|1|24||||||||||||||||||||||||||||||1857456408|42
||Height, Pollen, Aerobiology, Monitoring network, Big data||160-169||The effect of height on pollen concentration is not well documented and little is known about the near-ground vertical profile of airborne pollen. This is important as most measuring stations are on roofs, but patient exposure is at ground level. Our study used a big data approach to estimate the near-ground vertical profile of pollen concentrations based on a global study of paired stations located at different heights. We analyzed paired sampling stations located at different heights between 1.5 and 50 m above ground level (AGL). This provided pollen data from 59 Hirst-type volumetric traps from 25 different areas, mainly in Europe, but also covering North America and Australia, resulting in about 2,000,000 daily pollen concentrations analyzed. The daily ratio of the amounts of pollen from different heights per location was used, and the values of the lower station were divided by the higher station. The lower station of paired traps recorded more pollen than the higher trap. However, while the effect of height on pollen concentration was clear, it was also limited (average ratio 1.3, range 0.7–2.2). The standard deviation of the pollen ratio was highly variable when the lower station was located close to the ground level (below 10 m AGL). We show that pollen concentrations measured at >10 m are representative for background near-ground levels.|https://doi.org/10.1016/j.envres.2019.04.027|https://www.sciencedirect.com/science/article/pii/S0013935119302439||||2019|Near-ground effect of height on pollen exposure|Jesús Rojo and Jose Oteros and Rosa Pérez-Badia and Patricia Cervigón and Zuzana Ferencova and A. Monserrat Gutiérrez-Bustillo and Karl-Christian Bergmann and Gilles Oliver and Michel Thibaudon and Roberto Albertini and David {Rodríguez-De la Cruz} and Estefanía Sánchez-Reyes and José Sánchez-Sánchez and Anna-Mari Pessi and Jukka Reiniharju and Annika Saarto and M. Carmen Calderón and César Guerrero and Daniele Berra and Maira Bonini and Elena Chiodini and Delia Fernández-González and José García and M. Mar Trigo and Dorota Myszkowska and Santiago Fernández-Rodríguez and Rafael Tormo-Molina and Athanasios Damialis and Franziska Kolek and Claudia Traidl-Hoffmann and Elena Severova and Elsa Caeiro and Helena Ribeiro and Donát Magyar and László Makra and Orsolya Udvardy and Purificación Alcázar and Carmen Galán and Katarzyna Borycka and Idalia Kasprzyk and Ed Newbigin and Beverley Adams-Groom and Godfrey P. Apangu and Carl A. Frisk and Carsten A. Skjøth and Predrag Radišić and Branko Šikoparija and Sevcan Celenk and Carsten B. Schmidt-Weber and Jeroen Buters|article|ROJO2019160|||Environmental Research|00139351||174|||||||||||||||||||||||1861078009|2119809022
ICACS '17|Jeju Island, Republic of Korea|big data strategy, Big data analytics, big data challenges, big data success factors|5|88–92|Proceedings of the International Conference on Algorithms, Computing and Systems|This research investigates the large hype surrounding big data (BD) and Analytics (BDA) in both academia and the business world. Initial insights pointed to large and complex amalgamations of different fields, techniques and tools. Above all, BD as a research field and as a business tool found to be under developing and is fraught with many challenges. The intention here in this research is to develop an adoption model of BD that could detect key success predictors. The research finds a great interest and optimism about BD value that fueled this current buzz behind this novel phenomenon. Like any disruptive innovation, its assimilation in organizations oppressed with many challenges at various contextual levels. BD would provide different advantages to organizations that would seriously consider all its perspectives alongside its lifecycle in the pre-adoption or adoption or implementation phases. The research attempts to delineate the different facets of BD as a technology and as a management tool highlighting different contributions, implications and recommendations. This is of great interest to researchers, professional and policy makers.|10.1145/3127942.3127961|https://doi.org/10.1145/3127942.3127961|New York, NY, USA|Association for Computing Machinery|9781450352840|2017|Determinants of Big Data Adoption and Success|Al-Qirim, Nabeel and Tarhini, Ali and Rouibah, Kamel|inproceedings|10.1145/3127942.3127961|||||||||||||||||||||||||||||1863578019|42
ESEM '16|Ciudad Real, Spain|Mining Software Repositories, Missing Link Recovery, Non-Source Documents, Software Maintenance|10||Proceedings of the 10th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement|Background: Links between issue reports and their fixing commits play an important role in software maintenance. Such link data are often missing in practice and many approaches have been proposed in order to recover them automatically. Most of existing approaches focus on comparing log messages and source code files in commits with issues reports. Besides the two kinds of data in commits, non-source documents (NSDs) such as change logs usually record the fixing activities and sometimes share similar texts as those in issue reports. However, few discussions have been made on the role of NSDs in designing link recovery approaches.Aims: This paper aims at understanding whether and how NSDs affect the performance of link recovery approaches.Method: An empirical study is conducted to evaluate the role of NSDs in link recovery approaches in 18 open source projects with 6370 issues and 22761 commits.Results: With the inclusion of NSDs, link recovery approaches can get an average increase in F-Measure ranging from 2.76% - 25.63%. Further examinations show NSDs contribute to the performance improvement in 15 projects and have exceptions in 3 projects. The performance improvement in the 15 projects is mainly from the filtering of noisy links. On average, 23.59% - 76.30% false links can be excluded by exploiting NSDs in the link recovery approach. We also analyze the 3 projects in which NSDs cannot improve the performance. Our finding shows sophisticated data selection in NSDs is necessary.Conclusions: Our preliminary findings demonstrate that involving NSDs can improve the performance of link recovery approaches in most cases.|10.1145/2961111.2962605|https://doi.org/10.1145/2961111.2962605|New York, NY, USA|Association for Computing Machinery|9781450344272|2016|Understanding the Contribution of Non-Source Documents in Improving Missing Link Recovery: An Empirical Study|Sun, Yan and Wang, Qing and Li, Mingshu|inproceedings|10.1145/2961111.2962605|39||||||||||||||||||||||||||||1865119810|42
||Business Intelligence, Competitive Intelligence, Strategic Intelligence, Science information management, Mapping analysis||22-38||The critical factors in the big data era are collection, analysis, and dissemination of information to improve an organization’s competitive position and enhance its products and services. In this scenario, it is imperative that organizations use Intelligence, which is understood as a process of gathering, analyzing, interpreting, and disseminating high-value data and information at the right time for use in the decision-making process. Earlier, the concept of Intelligence was associated with the military and national security sector; however, in present times, and as organizations evolve, Intelligence has been defined in several ways for the purposes of different applications. Given that the purpose of Intelligence is to obtain real value from data, information, and the dynamism of the organizations, the study of this discipline provides an opportunity to analyze the core trends related to data collection and processing, information management, decision-making process, and organizational capabilities. Therefore, the present study makes a conceptual analysis of the existing definitions of intelligence in the literature by quantifying the main bibliometric performance indicators, identifying the main authors and research areas, and evaluating the development of the field using SciMAT as a bibliometric analysis software.|https://doi.org/10.1016/j.ijinfomgt.2019.01.013|https://www.sciencedirect.com/science/article/pii/S026840121730244X||||2019|30 years of intelligence models in management and business: A bibliometric review|J.R. López-Robles and J.R. Otegi-Olaso and I. {Porto Gómez} and M.J. Cobo|article|LOPEZROBLES201922|||International Journal of Information Management|02684012||48|||||15631|2,770|Q1|114|224|382|20318|5853|373|16,16|90,71|United Kingdom|Western Europe|1970, 1986-2021|Artificial Intelligence (Q1); Computer Networks and Communications (Q1); Information Systems (Q1); Information Systems and Management (Q1); Library and Information Sciences (Q1); Management Information Systems (Q1); Marketing (Q1)|12,245|14.098|0.01167|1866566454|747927863
Next Gen Tech Driven Personalized Med&Smart Healthcare||Artificial intelligence, big data, human voice, imbalanced classification, machine learning, medical screening, smart city, smart healthcare, support vector machine, voice disorders||197-208|Artificial Intelligence and Big Data Analytics for Smart Healthcare|Voice disorders are common diseases; most of the people have had experienced in their life. Voice disorder sufferers are usually not seeking medical consultation attributable to time-consuming and costly medical expenditure. Recently, researchers have proposed various machine learning algorithms for rapid detection of voice disorders based on the analysis of human voice. In this chapter, we have taken the pronunciation of vowel /a/ as the input of support vector machine algorithm. The research problem is formulated as binary classification which output will be either healthy or pathological status. Our work achieves an accuracy of 69.3% (sensitivity of 83.3% and specificity of 33.3%) which improves by 6.4%–19.3% compared with existing works. The implication of research work suggests tackling the imbalanced classification by adding penalty or generating new training data to class of smaller size. Everybody could contribute the voice signal of vowel /a/ and serving as big data pool.|https://doi.org/10.1016/B978-0-12-822060-3.00014-0|https://www.sciencedirect.com/science/article/pii/B9780128220603000140||Academic Press|978-0-12-822060-3|2021|Chapter 13 - A support vector machine–based voice disorders detection using human voice signal|Pak Ho Leung and Kwok Tai Chui and Kenneth Lo and Patricia Ordóñez {de Pablos}|incollection|LEUNG2021197||||||||||Miltiadis D. Lytras and Akila Sarirete and Anna Visvizi and Kwok Tai Chui|||||||||||||||||||1866897907|42
||Couplings;Big Data;Programming;Data integration;Task analysis;Data models;Databases;Big Data;Big Data Integration;blocking;entity matching;entity resolution;Hadoop;machine learning;MapReduce;Record Linkage||224-230|2017 13th International Computer Engineering Conference (ICENCO)|Record Linkage aims to find records in a dataset that represent the same real-world entity across many different data sources. It is a crucial task for data quality. With the evolution of Big Data, new difficulties appeared to deal mainly with the 5Vs of Big Data properties; i.e. Volume, Variety, Velocity, Value, and Veracity. Therefore Record Linkage in Big Data is more challenging. This paper investigates ways to apply Record Linkage algorithms that handle the Volume property of Big Data. Our investigation revealed four major issues. First, the techniques used to resolve the Volume property of Big Data mainly depend on partitioning the data into a number of blocks. The processing of those blocks is parallelly distributed among many executers. Second, MapReduce is the most famous programming model that is designed for parallel processing of Big Data. Third, a blocking key is usually used for partitioning the big dataset into smaller blocks; it is often created by the concatenation of the prefixes of chosen attributes. Partitioning using a blocking key may lead to unbalancing blocks, which is known as data skew, where data is not evenly distributed among blocks. An uneven distribution of data degrades the performance of the overall execution of the MapReduce model. Fourth, to the best of our knowledge, a small number of studies has been done so far to balance the load between data blocks in a MapReduce framework. Hence more work should be dedicated to balancing the load between the distributed blocks.|10.1109/ICENCO.2017.8289792|||||2017|Record linkage approaches in big data: A state of art study|El-Ghafar, Randa M. Abd and Gheith, Mervat H. and El-Bastawissy, Ali H. and Nasr, Eman S.|inproceedings|8289792||Dec||24752320|||||||||||||||||||||||||1867212389|1614468271
||Big data, Neuroimaging, Machine learning, Health informatics, fMRI||393-401||The field of functional neuroimaging has substantially advanced as a big data science in the past decade, thanks to international collaborative projects and community efforts. Here we conducted a literature review on functional neuroimaging, with focus on three general challenges in big data tasks: data collection and sharing, data infrastructure construction, and data analysis methods. The review covers a wide range of literature types including perspectives, database descriptions, methodology developments, and technical details. We show how each of the challenges was proposed and addressed, and how these solutions formed the three core foundations for the functional neuroimaging as a big data science and helped to build the current data-rich and data-driven community. Furthermore, based on our review of recent literature on the upcoming challenges and opportunities toward future scientific discoveries, we envisioned that the functional neuroimaging community needs to advance from the current foundations to better data integration infrastructure, methodology development toward improved learning capability, and multi-discipline translational research framework for this new era of big data.|https://doi.org/10.1016/j.gpb.2018.11.005|https://www.sciencedirect.com/science/article/pii/S1672022919301603||||2019|Functional Neuroimaging in the New Era of Big Data|Xiang Li and Ning Guo and Quanzheng Li|article|LI2019393|||Genomics, Proteomics & Bioinformatics|16720229|4|17||Big Data in Brain Science|||89440|3,114|Q1|49|36|158|1874|1152|135|6,41|52,06|China|Asiatic Region|2003-2020|Biochemistry (Q1); Computational Mathematics (Q1); Genetics (Q1); Medicine (miscellaneous) (Q1); Molecular Biology (Q1)||||1869218974|1963566185
||spark, connectivity, linked data, Data quality, big data, dataset discovery, mapreduce, dataset selection, lattice of measurements|49|||"\"Although the ultimate objective of Linked Data is linking and integration, it is not currently evident how connected the current Linked Open Data (LOD) cloud is. In this article, we focus on methods, supported by special indexes and algorithms, for performing measurements related to the connectivity of more than two datasets that are useful in various tasks including (a) Dataset Discovery and Selection; (b) Object Coreference, i.e., for obtaining complete information about a set of entities, including provenance information; (c) Data Quality Assessment and Improvement, i.e., for assessing the connectivity between any set of datasets and monitoring their evolution over time, as well as for estimating data veracity; (d) Dataset Visualizations; and various other tasks. Since it would be prohibitively expensive to perform all these measurements in a na\"\"{\\i}ve way, in this article, we introduce indexes (and their construction algorithms) that can speed up such tasks. In brief, we introduce (i) a namespace-based prefix index, (ii) a sameAs catalog for computing the symmetric and transitive closure of the owl:sameAs relationships encountered in the datasets, (iii) a semantics-aware element index (that exploits the aforementioned indexes), and, finally, (iv) two lattice-based incremental algorithms for speeding up the computation of the intersection of URIs of any set of datasets. For enhancing scalability, we propose parallel index construction algorithms and parallel lattice-based incremental algorithms, we evaluate the achieved speedup using either a single machine or a cluster of machines, and we provide insights regarding the factors that affect efficiency. Finally, we report measurements about the connectivity of the (billion triples-sized) LOD cloud that have never been carried out so far.\""|10.1145/3165713|https://doi.org/10.1145/3165713|New York, NY, USA|Association for Computing Machinery||2018|Scalable Methods for Measuring the Connectivity and Quality of Large Numbers of Linked Datasets|Mountantonakis, Michalis and Tzitzikas, Yannis|article|10.1145/3165713|15|jan|J. Data and Information Quality|19361955|3|9|September 2017||||||||||||||||||||||1870674507|833754770
||Artificial intelligence, Big Data Analytics, Machine learning, Security and privacy||107215||We live in an interconnected and pervasive world where huge amount of data are collected every second. Fully exploiting data through advanced analytics, machine learning and artificial intelligence, becomes crucial for businesses, from micro to large enterprises, resulting in a key advantage (or shortcoming) in the global market competition, as well as in a strong market driver for business analytics solutions. This scenario is deeply changing the security landscape, introducing new risks and threats that affect security and privacy of systems, on one side, and safety of users, on the other side. Many domains that can benefit from novel solutions based on data analytics have stringent security requirements to fulfill. The Energy domain’s Smart Grid is a major example of systems at the crossroads of security and data-driven intelligence. The Smart Grid plays a crucial role in modern energy infrastructure. However, it must face two major challenges related to security: managing front-end intelligent devices such as power assets and smart meters securely, and protecting the huge amount of data received from these devices. Starting from these considerations, setting up proper analytics is a complex problem because security controls could have the undesired side effect of decreasing the accuracy of the analytics themselves. This is even more critical when the configuration of security controls is let to the security expert, who often has only basic skills in data science. In this paper, we propose a solution based on the concept of Model-Based Big Data Analytics-as-a-Service (MBDAaaS) that bridges the gap between security experts and data scientists. Our solution acts as a middleware allowing a security expert and a data scientist to collaborate to the deployment of an analytics addressing their needs.|https://doi.org/10.1016/j.compeleceng.2021.107215|https://www.sciencedirect.com/science/article/pii/S0045790621002081||||2021|Big Data Analytics-as-a-Service: Bridging the gap between security experts and data scientists|Claudio A. Ardagna and Valerio Bellandi and Ernesto Damiani and Michele Bezzi and Cedric Hebert|article|ARDAGNA2021107215|||Computers & Electrical Engineering|00457906||93|||||18159|0,630|Q1|64|298|1040|7464|4626|979|4,79|25,05|United Kingdom|Western Europe|1973-1984, 1986-2020|Computer Science (miscellaneous) (Q1); Control and Systems Engineering (Q2); Electrical and Electronic Engineering (Q2)||||1878441890|1285201041
||Data quality, Big data, Secondary data, Numerical data, Quality threshold||113135||An IS researcher may obtain Big Data from primary or secondary data sources. Sometimes, acquiring primary Big Data is infeasible due to availability, accessibility, cost, time, and/or complexity considerations. In this paper, we focus on Big Data-based IS research and discuss ways in which one may, post hoc, establish quality thresholds for numerical Big Data obtained from secondary sources. We also present guidelines for developing journal policies aimed at ensuring the veracity and verifiability of such data when used for research purposes.|https://doi.org/10.1016/j.dss.2019.113135|https://www.sciencedirect.com/science/article/pii/S0167923619301642||||2019|Numerical, secondary Big Data quality issues, quality threshold establishment, & guidelines for journal policy development|Anita Lee-Post and Ram Pakath|article|LEEPOST2019113135|||Decision Support Systems|01679236||126||Perspectives on Numerical Data Quality in IS Research|||21933|1,564|Q1|151|115|342|7152|2672|338|7,04|62,19|Netherlands|Western Europe|1985-2020|Arts and Humanities (miscellaneous) (Q1); Developmental and Educational Psychology (Q1); Information Systems (Q1); Information Systems and Management (Q1); Management Information Systems (Q1)|13,580|5.795|0.0081|1878678007|1234879127
||Big Data, Consumer analytics, Consumer insights, Resource-based theory, Induction, Ignorance||897-904||Consumer analytics is at the epicenter of a Big Data revolution. Technology helps capture rich and plentiful data on consumer phenomena in real time. Thus, unprecedented volume, velocity, and variety of primary data, Big Data, are available from individual consumers. To better understand the impact of Big Data on various marketing activities, enabling firms to better exploit its benefits, a conceptual framework that builds on resource-based theory is proposed. Three resources—physical, human, and organizational capital—moderate the following: (1) the process of collecting and storing evidence of consumer activity as Big Data, (2) the process of extracting consumer insight from Big Data, and (3) the process of utilizing consumer insight to enhance dynamic/adaptive capabilities. Furthermore, unique resource requirements for firms to benefit from Big Data are discussed.|https://doi.org/10.1016/j.jbusres.2015.07.001|https://www.sciencedirect.com/science/article/pii/S0148296315002842||||2016|Big Data consumer analytics and the transformation of marketing|Sunil Erevelles and Nobuyuki Fukawa and Linda Swayne|article|EREVELLES2016897|||Journal of Business Research|01482963|2|69|||||20550|2,049|Q1|195|878|1342|73221|11507|1315|7,38|83,40|United States|Northern America|1973-2021|Marketing (Q1)|46,935|7.550|0.03523|1879033429|1502892296
ICCBD '20|Taichung, Taiwan|Digital transformation, Multi-stage assembly, Industrie 4.0, Big data|7|48–54|2020 the 3rd International Conference on Computing and Big Data|The aim of this paper is to analyze how Industrie 4.0 triggers changes in the business models of manufacturing SMEs (small and medium-sized enterprises) by big data platforms in selected casting manufacturer in Taiwan. A generalized model is presented to determine the optimal production run time, production rate, the advertising effort and demand with observation features that minimize the total cost per unit time. Advances in science and technology such as IoT technology, big data platform to investigate information asymmetry between manufacturer and customers. Numerical examples and sensitivity analysis are then provided by the collecting real data from Taiwan. Finally, concluding remarks are offered.|10.1145/3418688.3418697|https://doi.org/10.1145/3418688.3418697|New York, NY, USA|Association for Computing Machinery|9781450387866|2020|The Effect of Big Data Platforms on Multi-Stage Production System in Industrie 4.0|Liou, Teau-Jiuan and Weng, Ming-Wei and Lee, Liza|inproceedings|10.1145/3418688.3418697|||||||||||||||||||||||||||||1879718474|42
CCGRID '15|Shenzhen, China|blocking, MapReduce, redundancy elimination, entity resolution|4|1233–1236|Proceedings of the 15th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing|Entity resolution is the basic operation of data quality management, and the key step to find the value of data. The parallel data processing framework based on MapReduce can deal with the challenge brought by big data. However, there exist two important issues, avoiding redundant pairs led by the multi-pass blocking method and optimizing candidate pairs based on the transitive relations of similarity. In this paper, we propose a multi-signature based parallel entity resolution method, called multi-sig-er, which supports unstructured data and structured data. Two redundancy elimination strategies are adopted to prune the candidate pairs and reduce the number of similarity computation without affecting the resolution accuracy. Experimental results on real-world datasets show that our method tends to handle large datasets and it is more suitable for complex similarity computation than simple object matching.|10.1109/CCGrid.2015.24|https://doi.org/10.1109/CCGrid.2015.24||IEEE Press|9781479980062|2015|Eliminating the Redundancy in MapReduce-Based Entity Resolution|Yan, Cairong and Song, Yalong and Wang, Jian and Guo, Wenjing|inproceedings|10.1109/CCGrid.2015.24|||||||||||||||||||||||||||||1880530000|42
||Big data, Production safety management, Big-data-driven, Challenges, Opportunities||592-599||Big data has caused the scientific community to re-examine the scientific research methodologies and has triggered a revolution in scientific thinking. As a branch of scientific research, production safety management is also exploring methods to take advantage of big data. This research aims to provide a theoretical basis for promoting the application of big data in production safety management. First, four different types of production safety management paradigms were identified, namely small-data-based, static-oriented, interpretation-based and causal-oriented paradigm, and the challenges to these paradigms in the presence of big data were introduced. Second, the opportunities of employing big data in production safety management were identified from four aspects, including better predict the future production safety phenomena, promote production safety management highlight relevance, achieve the balance between deductive and inductive approaches and promote the interdisciplinary development of production safety management. Third, the paradigm shifting trend of production safety management was concluded, and the discipline foundation of the new paradigm was considered as the integration of data science, production management and safety science. Fourth, a new big-data-driven production safety management paradigm was developed, which consists of the logical line of production safety management, the macro-meso-micro data spectrum, the key big data analytics, and the four-dimensional morphology. At last, the strengths (e.g., supporting better-informed safety description, safety inquisition, safety prediction) and future research direction (e.g., theory research focuses on safety-related data mining/capturing/cleansing) of the new paradigm were discussed. The research results not only can provide theoretical and practical basis for big-data-driven production safety management, but also can offer advice to managerial consideration and scholarly investigation.|https://doi.org/10.1016/j.jclepro.2019.05.245|https://www.sciencedirect.com/science/article/pii/S0959652619317810||||2019|Challenges, opportunities and paradigm of applying big data to production safety management: From a theoretical perspective|Lang Huang and Chao Wu and Bing Wang|article|HUANG2019592|||Journal of Cleaner Production|09596526||231|||||19167|1,937|Q1|200|5126|10603|304498|103295|10577|9,56|59,40|United Kingdom|Western Europe|1993-2021|Environmental Science (miscellaneous) (Q1); Industrial and Manufacturing Engineering (Q1); Renewable Energy, Sustainability and the Environment (Q1); Strategy and Management (Q1)|170,352|9.297|0.18299|1886025317|1121054297
||Solution Development Life Cycle (SDLC), Agile, Scrum, sequential, waterfall, project objectives, project team, project roles, project timing, project approach||253-267|Executing Data Quality Projects (Second Edition)|It is essential that those using the Ten Steps do a good job of organizing their work. This chapter guides readers’ choices when setting up their projects and assembling a project team. Three general types of projects are detailed: 1) Focused data quality improvement project, 2) Data quality activities in another project, such as application development, data migration or integration of any kind, and 3) Ad hoc use of data quality steps, activities, or techniques from the Ten Steps. Additional information is given for incorporating data quality activities into another project using various SDLCs (solution/system/software life cycles). Relevant data quality activities from the Ten Steps can be incorporated into any SDLC that is the basis for the larger project (Agile, sequential, hybrid, etc.). To that end, several tables list data governance, stewardship, data quality and readiness activities and where they would take place in typical SDLC phases. A table with Agile Scrum activities are cross-referenced to the same SDLC phases. The chapter concludes with general tips for project timing, communication, and engagement.|https://doi.org/10.1016/B978-0-12-818015-0.00001-3|https://www.sciencedirect.com/science/article/pii/B9780128180150000013||Academic Press|978-0-12-818015-0|2021|Chapter 5 - Structuring Your Project|Danette McGilvray|incollection|MCGILVRAY2021253|||||||||Second Edition|Danette McGilvray|||||||||||||||||||1886917546|42
||Quality of service;Planning;Training;Computers;Measurement;Principal component analysis;3GPP;Machine Learning;Big Data;Quality of Service;Prediction;Network planning;Minimization of Drive Tests||477-483|2016 IEEE Symposium on Computers and Communication (ISCC)|Planning of current and future mobile networks is becoming increasingly complex due to the heterogeneity of deployments, which feature not only macrocells, but also an underlying layer of small cells whose deployment is not fully under the control of the operator. In this paper, we focus on selecting the most appropriate Quality of Service (QoS) prediction techniques for assisting network operators in planning future dense deployments. We propose to use machine learning as a tool to extract the relevant information from the huge amount of data generated in current 4G and future 5G networks during normal operation, which is then used to appropriately plan networks. In particular, we focus on radio measurements to develop correlative statistical models with the purpose of improving QoS-based network planning. In this direction, we combine multiple learners by building ensemble methods and use them to do regression in a reduced space rather than in the original one. We then compare the QoS prediction accuracy of various approaches that take as input the 3GPP Minimization of Drive Tests (MDT) measurements collected throughout a heterogeneous network and analyse their trade-offs. We also explain how the collected data is processed and used to predict QoS expressed in terms of Physical Resource Block (PRB)/ Megabit (MB) transmitted. This metric was selected because of the interest it may have for operators in planning, since it relates lower layer resources with their impact in terms of QoS up in the protocol stack, hence closer to the end-user.|10.1109/ISCC.2016.7543784|||||2016|On the potential of ensemble regression techniques for future mobile network planning|Moysen, Jessica and Giupponi, Lorenza and Mangues-Bafalluy, Josep|inproceedings|7543784||June|||||||||||||||||||||||||||1887420579|42
||anomaly detection, visual analytics, Federated learning|23|||Federated Learning (FL) provides a powerful solution to distributed machine learning on a large corpus of decentralized data. It ensures privacy and security by performing computation on devices (which we refer to as clients) based on local data to improve the shared global model. However, the inaccessibility of the data and the invisibility of the computation make it challenging to interpret and analyze the training process, especially to distinguish potential client anomalies. Identifying these anomalies can help experts diagnose and improve FL models. For this reason, we propose a visual analytics system, VADAF, to depict the training dynamics and facilitate analyzing potential client anomalies. Specifically, we design a visualization scheme that supports massive training dynamics in the FL environment. Moreover, we introduce an anomaly detection method to detect potential client anomalies, which are further analyzed based on both the client model’s visual and objective estimation. Three case studies have demonstrated the effectiveness of our system in understanding the FL training process and supporting abnormal client detection and analysis.|10.1145/3426866|https://doi.org/10.1145/3426866|New York, NY, USA|Association for Computing Machinery||2021|VADAF: Visualization for Abnormal Client Detection and Analysis in Federated Learning|Meng, Linhao and Wei, Yating and Pan, Rusheng and Zhou, Shuyue and Zhang, Jianwei and Chen, Wei|article|10.1145/3426866|26|sep|ACM Trans. Interact. Intell. Syst.|21606455|3–4|11|December 2021||||||||||||||||||||||1891581324|668229522
|||9|997–1005||Extracting structured information from templatic documents is an important problem with the potential to automate many real-world business workflows such as payment, procurement, and payroll. The core challenge is that such documents can be laid out in virtually infinitely different ways. A good solution to this problem is one that generalizes well not only to known templates such as invoices from a known vendor, but also to unseen ones.We developed a system called Glean to tackle this problem. Given a target schema for a document type and some labeled documents of that type, Glean uses machine learning to automatically extract structured information from other documents of that type. In this paper, we describe the overall architecture of Glean, and discuss three key data management challenges : 1) managing the quality of ground truth data, 2) generating training data for the machine learning model using labeled documents, and 3) building tools that help a developer rapidly build and improve a model for a given document type. Through empirical studies on a real-world dataset, we show that these data management techniques allow us to train a model that is over 5 F1 points better than the exact same model architecture without the techniques we describe. We argue that for such information-extraction problems, designing abstractions that carefully manage the training data is at least as important as choosing a good model architecture.|10.14778/3447689.3447703|https://doi.org/10.14778/3447689.3447703||VLDB Endowment||2021|Glean: Structured Extractions from Templatic Documents|Tata, Sandeep and Potti, Navneet and Wendt, James B. and Costa, Lauro Beltr\~{a}o and Najork, Marc and Gunel, Beliz|article|10.14778/3447689.3447703||apr|Proc. VLDB Endow.|21508097|6|14|February 2021||||21100199855|0,946|Q1|134|246|598|12401|3759|566|5,50|50,41|United States|Northern America|2008-2020|Computer Science (miscellaneous) (Q1)|7,198|2.047|0.00891|1893327332|1216159931
||Big data, Geospatial, Data handling, Analytics, Spatial modeling, Review||119-133||Big data has now become a strong focus of global interest that is increasingly attracting the attention of academia, industry, government and other organizations. Big data can be situated in the disciplinary area of traditional geospatial data handling theory and methods. The increasing volume and varying format of collected geospatial big data presents challenges in storing, managing, processing, analyzing, visualizing and verifying the quality of data. This has implications for the quality of decisions made with big data. Consequently, this position paper of the International Society for Photogrammetry and Remote Sensing (ISPRS) Technical Commission II (TC II) revisits the existing geospatial data handling methods and theories to determine if they are still capable of handling emerging geospatial big data. Further, the paper synthesises problems, major issues and challenges with current developments as well as recommending what needs to be developed further in the near future.|https://doi.org/10.1016/j.isprsjprs.2015.10.012|https://www.sciencedirect.com/science/article/pii/S0924271615002439||||2016|Geospatial big data handling theory and methods: A review and research challenges|Songnian Li and Suzana Dragicevic and Francesc Antón Castro and Monika Sester and Stephan Winter and Arzu Coltekin and Christopher Pettit and Bin Jiang and James Haworth and Alfred Stein and Tao Cheng|article|LI2016119|||ISPRS Journal of Photogrammetry and Remote Sensing|09242716||115||Theme issue 'State-of-the-art in photogrammetry, remote sensing and spatial information science'|||29161|2,960|Q1|138|264|677|16114|7306|668|10,56|61,04|Netherlands|Western Europe|1989-2020|Atomic and Molecular Physics, and Optics (Q1); Computer Science Applications (Q1); Computers in Earth Sciences (Q1); Engineering (miscellaneous) (Q1); Geography, Planning and Development (Q1)|18,026|8.979|0.02145|1894675349|660578442
||Big data, Data ecosystem, Normalization, Normalization process theory, Big data user, Big data user experience||837-854||This study seeks to understand big data ecology, how it is perceived by different stakeholders, the potential value and challenges, and the implications for the private sector and public organizations, as well as for policy makers. With Normalization Process Theory in place, this study conducts socio-technical evaluation on the big data phenomenon to understand the developmental processes through which new practices of thinking and enacting are implemented, embedded, and integrated in South Korea. It also undertakes empirical analyses of user modeling to explore the factors influencing users׳ adoption of big data by integrating cognitive motivations as well as user values as the primary determining factors. Based on the qualitative and quantitative findings, this study concludes that big data should be developed with user-centered ideas and that users should be the focus of big data design.|https://doi.org/10.1016/j.telpol.2015.03.007|https://www.sciencedirect.com/science/article/pii/S0308596115000567||||2016|Demystifying big data: Anatomy of big data developmental process|Dong-Hee Shin|article|SHIN2016837|||Telecommunications Policy|03085961|9|40|||||20870|0,840|Q1|69|87|224|5352|831|212|3,51|61,52|United Kingdom|Western Europe|1976-2020|Electrical and Electronic Engineering (Q1); Human Factors and Ergonomics (Q1); Information Systems (Q1)|2,745|3.036|0.00269|1896156268|321150663
MuC '21|Ingolstadt, Germany|awareness, survey, privacy, data sharing|13|281–293|Proceedings of Mensch Und Computer 2021|As a result of the ongoing digitalization of our everyday lives, the amount of data produced by everyone is steadily increasing. This happens through personal decisions and items, such as the use of social media or smartphones, but also through more and more data acquisition in public spaces, such as e.g., Closed Circuit Television. Are people aware of the data they are sharing? What kind of data do people want to share with whom? Are people aware if they have Wi-Fi, GPS, or Bluetooth activated as potential data sharing functionalities on their phone? To answer these questions, we conducted a representative online survey as well as face-to-face interviews with users in Germany. We found that most users wanted to share private data on premise with most entities, indicating that willingness to share data depends on who has access to the data. Almost half of the participants would be more willing to share data with specific entities (state bodies &amp; rescue forces) in the event that an acquaintance is endangered. For Wi-Fi and GPS the frequencies of self-reported and actual activation on the smartphone are almost equal, but 17% of participants were unaware of the Bluetooth status on their smartphone. Our research is therefore in line with other studies suggesting relatively low privacy awareness of users.|10.1145/3473856.3473879|https://doi.org/10.1145/3473856.3473879|New York, NY, USA|Association for Computing Machinery|9781450386456|2021|Who Should Get My Private Data in Which Case? Evidence in the Wild|Herbert, Franziska and Schmidbauer-Wolf, Gina Maria and Reuter, Christian|inproceedings|10.1145/3473856.3473879|||||||||||||||||||||||||||||1896382614|42
||Asset management, Component replacement, Data quality costs, Electric power distribution, Optimization, Trade-off||116057||Data play an essential role in asset management decisions. The amount of data is increasing through accumulating historical data records, new measuring devices, and communication technology, notably with the evolution toward smart grids. Consequently, the management of data quantity and quality is becoming even more relevant for asset managers to meet efficiency and reliability requirements for power grids. In this work, we propose an innovative data quality management framework enabling asset managers (i) to quantify the impact of poor data quality, and (ii) to determine the conditions under which an investment in data quality improvement is required. To this end, an algorithm is used to determine the optimal year for component replacement based on three scenarios, a Reference scenario, an Imperfect information scenario, and an Investment in higher data quality scenario. Our results indicate that (i) the impact on the optimal year of replacement is the highest for middle-aged components; (ii) the profitability of investments in data quality improvement depends on various factors, including data quality, and the cost of investment in data quality improvement. Finally, we discuss the implementation of the proposed models to control data quality in practice, while taking into account real-world technological and economic limitations.|https://doi.org/10.1016/j.apenergy.2020.116057|https://www.sciencedirect.com/science/article/pii/S0306261920314896||||2021|Investments in data quality: Evaluating impacts of faulty data on asset management in power systems|Sylvie Koziel and Patrik Hilber and Per Westerlund and Ebrahim Shayesteh|article|KOZIEL2021116057|||Applied Energy|03062619||281|||||28801|3,035|Q1|212|1729|5329|100144|56804|5304|10,59|57,92|United Kingdom|Western Europe|1975-2020|Building and Construction (Q1); Energy (miscellaneous) (Q1); Management, Monitoring, Policy and Law (Q1); Mechanical Engineering (Q1)|122,712|9.746|0.15329|1897416609|892883729
EITCE 2021|Xiamen, China|Data fusion technology, Multi-source big data, Tourism prediction|7|1030–1036|Proceedings of the 2021 5th International Conference on Electronic Information Technology and Computer Engineering|In the practical application of existing big data tourism prediction, there are some practical problems, such as complicated data sources and difficult fusion, low prediction accuracy and poor guiding practice effect. In view of this situation, this paper intends to build a tourism big data index prediction model suitable for the characteristics of tourism development through core data extraction, multi-source data fusion, complex data modeling and other key technologies. With the help of the improved tourism prediction model based on multi-source big data fusion technology, the tourist flow and consumption characteristics of Shandong province are more accurately identified and predicted. It can provide help for optimizing public service of tourism, strengthening early warning of tourist flow and improving marketing strategy of tourist destination. This study innovatively supplements the effective integration theory of multi-source tourism big data and the organic integration theory of big data and traditional sampling survey data. At the same time, the relevant methods of tourism big data forecasting model are extended.|10.1145/3501409.3501593|https://doi.org/10.1145/3501409.3501593|New York, NY, USA|Association for Computing Machinery|9781450384322|2022|Tourism Prediction Based on Multi-Source Big Data Fusion Technology|Diao, Yanhua|inproceedings|10.1145/3501409.3501593|||||||||||||||||||||||||||||1899190119|42
||Predictive maintenance, Artificial intelligence, Machine learning, Smart manufacturing, Industry 4.0||317-348|Design and Operation of Production Networks for Mass Personalization in the Era of Cloud Technology|With the advent of new methods usually identified under the banners of artificial intelligence (AI) and machine learning (ML), statistical analysis methods of complex and uncertain manufacturing systems have been undergoing significant changes. Therefore, various definitions of AI, a brief history, and its differences with traditional statistics are presented. Moreover, ML is introduced to identify its place in data science and differences to topics such as big data analytics and manufacturing problems that use AI and ML are then characterized. Next, a lifecycle-based approach is adopted and the use of various methods in each phase is analyzed, identifying the most useful techniques and the unifying attributes of AI in manufacturing. Finally, the chapter maps out future developments of AI and the emerging trends and identifies a vision based on combining machine and human intelligence in a productive and empowering manner as well. This vision presents humans and increasingly more intelligent machines, not as competitors, but as partners allowing creative and innovative paradigms to emerge.|https://doi.org/10.1016/B978-0-12-823657-4.00002-6|https://www.sciencedirect.com/science/article/pii/B9780128236574000026||Elsevier|978-0-12-823657-4|2022|Chapter 11 - Review of machine learning technologies and artificial intelligence in modern manufacturing systems|Aydin Nassehi and Ray Y. Zhong and Xingyu Li and Bogdan I. Epureanu|incollection|NASSEHI2022317||||||||||Dimitris Mourtzis|||||||||||||||||||1899322353|42
||||1-5|International ETG Congress 2017|With the increasing share of renewable generation in low voltage distribution, the edge of the power grid is slowly replacing classical, large-scale power stations, taking up roles and responsibilities that were unimaginable only a few years ago. However, while feed-in on the 400V level is now commonplace, grid state and power quality monitoring still lag far behind because of obvious cost and complexity reasons. In fact, only very few parts of Germanys approx. 1,1 million km long 400V distribution grid are actively monitored today and even basic grid quality parameters such as the voltage level at the end of the line are typically unknown. Reducing the cost per measurement point and the complexity of data analysis is thus of paramount importance for enabling wide-area monitoring of the LV power grid. The authors explore the feasibility of this goal by examining the performance of a nonconventional measurement system. It consists of a network of Internet-of-Things (IOT)-based power quality sensors, connected to a cloud-based big data analysis platform. Specifically, measurement nodes comprise of voltage sensors attached to consumer-grade smartphones and WIFI access points. Sensor data is automatically uploaded to the cloud system with the MQTT protocol. First results from a field trial in a rural area in Germany indicate good data quality and show excellent promise for detailed assessments of the edge of the power grid.||||||2017|VEREDELE-FACDS Field Trial: Wide Area Power Quality Assessment With IOT Sensors and Cloud-Based Analytics|Ruester, Christian and Haussel, Fabian and Huehn, Thomas and El Sayed, Nadim|inproceedings|8278765||Nov|||||||||||||||||||||||||||1900323980|42
||Industrial AI, Industry 4.0, Big data, Smart manufacturing, Cyber physical systems||20-23||The recent White House report on Artificial Intelligence (AI) (Lee, 2016) highlights the significance of AI and the necessity of a clear roadmap and strategic investment in this area. As AI emerges from science fiction to become the frontier of world-changing technologies, there is an urgent need for systematic development and implementation of AI to see its real impact in the next generation of industrial systems, namely Industry 4.0. Within the 5C architecture previously proposed in Lee et al. (2015), this paper provides an insight into the current state of AI technologies and the eco-system required to harness the power of AI in industrial applications.|https://doi.org/10.1016/j.mfglet.2018.09.002|https://www.sciencedirect.com/science/article/pii/S2213846318301081||||2018|Industrial Artificial Intelligence for industry 4.0-based manufacturing systems|Jay Lee and Hossein Davari and Jaskaran Singh and Vibhor Pandhare|article|LEE201820|||Manufacturing Letters|22138463||18|||||21100283796|1,072|Q1|26|84|123|1707|646|122|5,53|20,32|United States|Northern America|2013-2020|Industrial and Manufacturing Engineering (Q1); Mechanics of Materials (Q1)||||1900734930|1786129110
||routing, vanet, data analysis, survey, topology, Vehicular networks, data mining, mobility|38|||Intelligent vehicular networks emerge as a promising technology to provide efficient data communication in transportation systems and smart cities. At the same time, the popularization of devices with attached sensors has allowed the obtaining of a large volume of data with spatiotemporal information from different entities. In this sense, we are faced with a large volume of vehicular mobility traces being recorded. Those traces provide unprecedented opportunities to understand the dynamics of vehicular mobility and provide data-driven solutions. In this article, we give an overview of the main publicly available vehicular mobility traces; then, we present the main issues for preprocessing these traces. Also, we present the methods used to characterize and model mobility data. Finally, we review existing proposals that apply the hidden knowledge extracted from the mobility trace for vehicular networks. This article provides a survey on studies that use vehicular mobility traces and provides a guideline for the proposition of data-driven solutions in the domain of vehicular networks. Moreover, we discuss open research problems and give some directions to undertake them.|10.1145/3446679|https://doi.org/10.1145/3446679|New York, NY, USA|Association for Computing Machinery||2021|Mobility Trace Analysis for Intelligent Vehicular Networks: Methods, Models, and Applications|Celes, Clayson and Boukerche, Azzedine and Loureiro, Antonio A. F.|article|10.1145/3446679|49|apr|ACM Comput. Surv.|03600300|3|54|April 2022||||23038|2,079|Q1|163|112|365|15859|6978|365|19,16|141,60|United States|Northern America|1969-2020|Computer Science (miscellaneous) (Q1); Theoretical Computer Science (Q1)|10,790|10.282|0.01438|1901145110|1517405264
||design-reality gap, healthcare, technology adoption, user-centred design, Diffusion of innovation|19|||Clinical decision-support tools (DSTs) represent a valuable resource in healthcare. However, lack of Human Factors considerations and early design research has often limited their successful adoption. To complement previous technically focused work, we studied adoption opportunities of a future DST built on a predictive model of Alzheimer’s Disease (AD) progression. Our aim is two-fold: exploring adoption opportunities for DSTs in AD clinical care, and testing a novel combination of methods to support this process. We focused on understanding current clinical needs and practices, and the potential for such a tool to be integrated into the setting, prior to its development. Our user-centred approach was based on field observations and semi-structured interviews, analysed through workflow analysis, user profiles, and a design-reality gap model. The first two are common practice, whilst the latter provided added value in highlighting specific adoption needs. We identified the likely early adopters of the tool as being both psychiatrists and neurologists based in research-oriented clinical settings. We defined ten key requirements for the translation and adoption of DSTs for AD around IT, user, and contextual factors. Future works can use and build on these requirements to stand a greater chance to get adopted in the clinical setting.|10.1145/3462764|https://doi.org/10.1145/3462764|New York, NY, USA|Association for Computing Machinery||2021|Opportunities and Barriers for Adoption of a Decision-Support Tool for Alzheimer’s Disease|Bellio, Maura and Furniss, Dominic and Oxtoby, Neil P. and Garbarino, Sara and Firth, Nicholas C. and Ribbens, Annemie and Alexander, Daniel C. and Blandford, Ann|article|10.1145/3462764|32|sep|ACM Trans. Comput. Healthcare|26911957|4|2|October 2021||||||||||||||||||||||1904177727|1983512862
||||S198|||https://doi.org/10.1016/j.neurom.2022.08.222|https://www.sciencedirect.com/science/article/pii/S1094715922009977||||2022|PO048 / #665 THE BIG CHANGE IS COMMING: BIG DATA: E-POSTER VIEWING|Mark Plazier and Vincent Raymaekers and Wim Duyvendak and Sacha Meeuws and Maarten Wissels and Steven Vanvolsem and Gert Roosen and Sven Bamps and Salah-Edine Achabar and Stefan Schu and Anna Keil and Björn Carsten Schultheis and Philipp Slotty and Dirk {De Ridder} and Jan Vesper|article|PLAZIER2022S198|||Neuromodulation: Technology at the Neural Interface|10947159|7, Supplement|25||INS 15th World Congress|||||||||||||||||||||1904241024|522435327
||Bandwidth;Quality of service;Big Data applications;Throughput;Writing;Reliability;Distributed file system;I/O bandwidth enforcement;HDFS||124-131|2018 IEEE 20th International Conference on High Performance Computing and Communications; IEEE 16th International Conference on Smart City; IEEE 4th International Conference on Data Science and Systems (HPCC/SmartCity/DSS)|A distributed file system (DFS) is a core component to implement big data applications. On the one hand, a DFS is capable of managing a large volume of data with desirable properties that strike the balance between high availability, reliability, and so on. On the other hand, a DFS relies on underlying storage systems (e.g., hard drives, solid state drives, etc.) and suffer from slow read/write operations. In big data era, large-scale data processing applications start to leverage the in-memory processing to improve the performance by reducing the inhibitive cost of I/O operations. However, it is still inevitable to read input data from or write outputs to the storage system. Slow I/O operations are often the main bottleneck of emerging big data applications. In particular, while these applications often use DFSs to store their results for the high availability and reliability, the unmanaged I/O bandwidth contention results in the QoS violation of high priority applications when multiple applications share the same DFS. To enable I/O management and allocation on big-data platforms, we propose a Cluster-Level I/O Bandwidth Enforcement (CLIBE) approach that consists of a cluster-level I/O bandwidth quota manager, multiple node-level I/O bandwidth controllers, and a feedback-based quota reallocator. The quota manager splits and distributes the I/O bandwidth quota of an application to the active nodes that are serving this application. The bandwidth controller on a node ensures that the I/O bandwidth used by an application would not exceed its bandwidth quota on the node. For an application affected by slow or overloaded nodes, the quota reallocator reallocates the idle I/O bandwidth on underloaded nodes to this application to guarantee its throughput. Our experiment on a real-system cluster shows that CLIBE is able to precisely control the I/O bandwidth used by an application at the cluster level, with the deviation smaller than 2.51%.|10.1109/HPCC/SmartCity/DSS.2018.00048|||||2018|CLIBE: Precise Cluster-Level I/O Bandwidth Enforcement in Distributed File System|Zheng, Ningxin and Chen, Quan and Chen, Chen and Guo, Minyi|inproceedings|8622786||June|||||||||||||||||||||||||||1908310312|42
CIKM '14|Shanghai, China|semantic annotation, query suggest, graph search|2|2094–2095|Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management|"\"There is an increasing amount of structure on the Web as a result of modern Web languages, user tagging and annotation, emerging robust NLP tools, and an ever growing volume of linked data. These meaningful, semantic, annotations hold the promise to significantly enhance information access, by enhancing the depth of analysis of today's systems. The goal of the ESAIR'14 workshop remains to advance the general research agenda on this core problem, with an explicit focus on one of the most challenging aspects to address in the coming years. The main remaining challenge is on the user's side - the potential of rich document annotations can only be realized if matched by more articulate queries exploiting these powerful retrieval cues - and a more dynamic approach is emerging by exploiting new forms of query autosuggest. How can the query suggestion paradigm be used to encourage searcher to articulate longer queries, with concepts and relations linking their statement of request to existing semantic models? How do entity results and social network data in \"\"graph search\"\" change the classic division between searchers and information and lead to extreme personalization - are you the query? How to leverage transaction logs and recommendation, and how adaptive should we make the system? What are the privacy ramifications and the UX aspects - how to not creep out users?\""|10.1145/2661829.2663539|https://doi.org/10.1145/2661829.2663539|New York, NY, USA|Association for Computing Machinery|9781450325981|2014|Seventh Workshop on Exploiting Semantic Annotations in Information Retrieval (ESAIR'14): CIKM 2014 Workshop|Alonso, Omar and Kamps, Jaap and Karlgren, Jussi|inproceedings|10.1145/2661829.2663539|||||||||||||||||||||||||||||1910233802|42
||Data-driven design, Engineering design, Product development, Big data analytics, Bibliographic analysis, Network analysis||101774||In the last two decades, data regarding engineering design and product development has increased rapidly. Big data exploration and mining offer numerous opportunities for engineering design; however, owing to the multitude of data sources and formats coupled with the high complexity of the design process, these techniques are yet to be utilised to the best of their full potential. In this study, a comprehensive assessment of the state-of-the-art data-driven engineering design (DDED) in the last 20 years was conducted. A scientometric approach was employed wherein first, a systematic article acquisition procedure was performed, where a dataset of 3339 articles related to engineering design and big data analytics applications were extracted from Web of Science (WoS) and Scopus. Thereafter, this dataset was reduced to a dataset of 366 articles based on concise data screening. The resulting articles were used to analyse the dynamics of research in DDED throughout the last 20 years and to detect the primary research topics related to DDED, the most influential authors, and the papers with the highest impact in the DDED domain. Furthermore, the co-occurrence network of keywords/keyphrases and co-authorship networks were constructed and analysed to reveal the interconnection of the research topics and the collaboration between the most prolific authors. Finally, an insight how big data analytics is being applied through product development activities to support decision-making in engineering design was presented.|https://doi.org/10.1016/j.aei.2022.101774|https://www.sciencedirect.com/science/article/pii/S1474034622002324||||2022|Data-driven engineering design: A systematic review using scientometric approach|Daria Vlah and Andrej Kastrin and Janez Povh and Nikola Vukašinović|article|VLAH2022101774|||Advanced Engineering Informatics|14740346||54|||||23640|1,107|Q1|81|146|295|8184|1973|289|6,41|56,05|United Kingdom|Western Europe|2002-2020|Artificial Intelligence (Q1); Information Systems (Q1)|4,432|5.603|0.00428|1913904341|876312848
||ExpoQual, BEES-C, Exposure, Human, Quality, Fit-for-purpose, Instrument, Biomonitoring, Model uncertainty||302-312||Recent rapid technological advances are producing exposure data sets for which there are no available data quality assessment tools. At the same time, regulatory agencies are moving in the direction of data quality assessment for environmental risk assessment and decision-making. A transparent and systematic approach to evaluating exposure data will aid in those efforts. Any approach to assessing data quality must consider the level of quality needed for the ultimate use of the data. While various fields have developed approaches to assess data quality, there is as yet no general, user-friendly approach to assess both measured and modeled data in the context of a fit-for-purpose risk assessment. Here we describe ExpoQual, an instrument developed for this purpose which applies recognized parameters and exposure data quality elements from existing approaches for assessing exposure data quality. Broad data streams such as quantitative measured and modeled human exposure data as well as newer and developing approaches can be evaluated. The key strength of ExpoQual is that it facilitates a structured, reproducible and transparent approach to exposure data quality evaluation and provides for an explicit fit-for-purpose determination. ExpoQual was designed to minimize subjectivity and to include transparency in aspects based on professional judgment. ExpoQual is freely available on-line for testing and user feedback (exposurequality.com).|https://doi.org/10.1016/j.envres.2019.01.039|https://www.sciencedirect.com/science/article/pii/S0013935119300465||||2019|ExpoQual: Evaluating measured and modeled human exposure data|Judy S. LaKind and Cian O’Mahony and Thomas Armstrong and Rosalie Tibaldi and Benjamin C. Blount and Daniel Q. Naiman|article|LAKIND2019302|||Environmental Research|00139351||171|||||||||||||||||||||||1914381080|2119809022
ICEGOV '17|New Delhi AA, India|open data, data supply, value creation, data value chain, data value network, exploitation, impacts, data demand, innovation|10|475–484|Proceedings of the 10th International Conference on Theory and Practice of Electronic Governance|Open data is increasingly permeating into all dimensions of our society and has become an indispensable commodity that serves as a basis for many products and services. Governments are generating a huge amount of data spanning different dimensions. This dataification shows the paramount need to identify the means and methods in which the value of data and knowledge can be exploited. While not restricted to the government domain, this dataification is certainly relevant in a government context, particularly due to the large volume of data generated by public institutions. In this paper we identify the various activities and roles within a data value chain, and hence proceed to provide our own definition of a Data Value Network. We specifically cater for non-tangible data products and characterise three dimensions that play a vital role within the Data Value Network. We also propose a Demand and Supply Distribution Model with the aim of providing insight on how an entity can participate in the global data market by producing a data product, as well as a concrete implementation through the Demand and Supply as a Service. Through our contributions we therefore project our vision of enhancing the process of open (government) data exploitation and innovation, with the aim of achieving the highest possible impact.|10.1145/3047273.3047299|https://doi.org/10.1145/3047273.3047299|New York, NY, USA|Association for Computing Machinery|9781450348256|2017|Exploiting the Value of Data through Data Value Networks|"\"Attard, Judie and Orlandi, Fabrizio and Auer, S\"\"{o}ren\""|inproceedings|10.1145/3047273.3047299|||||||||||||||||||||||||||||1915439950|42
||Deep learning, Machine learning, Computer vision, Product Lifecycle Management, Digital mock-up, Shape retrieval||227-243||With the increasing amount of available data, computing power and network speed for a decreasing cost, the manufacturing industry is facing an unprecedented amount of data to process, understand and exploit. Phenomena such as Big Data, the Internet-of-Things, Closed-Loop Product Lifecycle Management, and the advances of Smart Factories tend to produce humanly unmanageable quantities of data. The paper approaches the aforesaid context by assuming that any data processing automation is not only desirable but rather necessary in order to prevent prohibitive data analytics costs. This study focuses on highlighting the major specificities of engineering data and the data-processing difficulties which are inherent to data coming from the manufacturing industry. The artificial intelligence field of research is able to provide methods and tools to address some of the identified issues. A special attention was paid to provide a literature review of the most recent (in 2017) applications, that could present a high potential for the manufacturing industry, in the fields of machine learning and deep learning. In order to illustrate the proposed work, a case study was conducted on the challenging research question of object recognition in heterogeneous formats (3D models, photos and videos) with deep learning techniques. The DICE project – DMU Imagery Comparison Engine – is presented and has been completely open-sourced in order to encourage reuse and improvements of the proposed case-study. This project also leads to the development of an open-source research dataset of 2000 CAD Models, called DMU-Net available at: https://www.dmu-net.org.|https://doi.org/10.1016/j.compind.2018.04.005|https://www.sciencedirect.com/science/article/pii/S0166361517305560||||2018|Deep learning for big data applications in CAD and PLM – Research review, opportunities and case study|Jonathan Dekhtiar and Alexandre Durupt and Matthieu Bricogne and Benoit Eynard and Harvey Rowson and Dimitris Kiritsis|article|DEKHTIAR2018227|||Computers in Industry|01663615||100|||||19080|1,432|Q1|100|115|323|6926|3165|319|9,96|60,23|Netherlands|Western Europe|1979-2020|Computer Science (miscellaneous) (Q1); Engineering (miscellaneous) (Q1)|6,018|7.635|0.00584|1919986211|605146181
||process mining, six sigma, DMAIC, big data analytics, data science, project management||107083||Six Sigma is one of the most successful quality management philosophies of the past 20 years. However, the current challenges facing companies, such as rising process and supply chain complexity, as well as high volumes of unstructured data, cannot easily be answered by relying on traditional Six Sigma tools. Instead, the Process Mining (PM) technology using big data analytics promises valuable support for 6S and its data analysis capabilities. The article presents a design science research project in which a method for the integration of PM in Six Sigma’s DMAIC project structure was developed. This method could be extended, refined and tested during three evaluation cycles: an expert evaluation with Six Sigma professionals, a technical experiment and finally a multi case study in a company. The method therefore was eventually endorsed by 6S experts and successfully applied in a first pilot setting. This article presents the first developed method for the integration of PM and Six Sigma. It follows the recommendations of many researchers to test Six Sigma as an application field of PM as well as using the potential of big data analytics. The method can be used by researchers and practitioners alike to implement, test and verify its design in organisations.|https://doi.org/10.1016/j.cie.2020.107083|https://www.sciencedirect.com/science/article/pii/S0360835220307531||||2021|Process Mining for Six Sigma: Utilising Digital Traces|I. Kregel and D. Stemann and J. Koch and A. Coners|article|KREGEL2021107083|||Computers & Industrial Engineering|03608352||153|||||18164|1,315|Q1|128|641|1567|31833|9597|1557|5,97|49,66|United Kingdom|Western Europe|1976-2020|Computer Science (miscellaneous) (Q1); Engineering (miscellaneous) (Q1)||||1920546691|1798521593
||||209-210|||https://doi.org/10.1016/j.diagmicrobio.2018.12.006|https://www.sciencedirect.com/science/article/pii/S0732889318306710||||2019|Pitfalls in big data analysis: next-generation technologies, last-generation data|Susanna K.P. Lau and Patrick C.Y. Woo|article|LAU2019209|||Diagnostic Microbiology and Infectious Disease|07328893|2|94|||||||||||||||||||||||1923252459|1887539492
||Cloud computing, Processing, Healthcare, Big data, Review||271-289||Recently, patient safety and healthcare have gained high attention in professional and health policy-makers. This rapid growth causes generating a high amount of data, which is known as big data. Therefore, handling and processing of this data are attracted great attention. Cloud computing is one of the main choices for handling and processing of this type of data. But, as far as we know, the detailed review and deep discussion in this filed are very rare. Therefore, this paper reviews and discusses the recently introduced mechanisms in this field as well as providing a deep analysis of their applied mechanisms. Moreover, the drawbacks and benefits of the reviewed mechanisms have been discussed and the main challenges of these mechanisms are highlighted for developing more efficient healthcare big data processing techniques over cloud computing in the future.|https://doi.org/10.1016/j.ijinfomgt.2019.05.017|https://www.sciencedirect.com/science/article/pii/S0268401217304917||||2019|Healthcare big data processing mechanisms: The role of cloud computing|Lila Rajabion and Abdusalam Abdulla Shaltooki and Masoud Taghikhah and Amirhossein Ghasemi and Arshad Badfar|article|RAJABION2019271|||International Journal of Information Management|02684012||49|||||15631|2,770|Q1|114|224|382|20318|5853|373|16,16|90,71|United Kingdom|Western Europe|1970, 1986-2021|Artificial Intelligence (Q1); Computer Networks and Communications (Q1); Information Systems (Q1); Information Systems and Management (Q1); Library and Information Sciences (Q1); Management Information Systems (Q1); Marketing (Q1)|12,245|14.098|0.01167|1926602052|747927863
||Big data utilization, Data quality, Decision quality, Data diagnosticity||38-49||Anecdotal evidence suggests that, despite the large variety of data, the huge volume of generated data, and the fast velocity of obtaining data (i.e., big data), quality of big data is far from perfect. Therefore, many firms defer collecting and integrating big data as they have concerns regarding the impact of utilizing big data on data diagnosticity (i.e., retrieval of valuable information from data) and firm decision making quality. In this study, we use the Organizational Learning Theory and Wang and Strong's data quality framework to explore the impact of processing big data on firm decision quality and the mediating role of data quality (DQ) and data diagnosticity on this relationship. We validate the proposed research model using survey data from 130 firms, obtained from data analysts and IT managers. Results confirm the critical role of DQ in increasing data diagnosticity and improving firm decision quality when processing big data; suggesting important implications for practice and theory. Findings also reveal that while big data utilization positively impacts contextual DQ, accessibility DQ, and representational DQ, interestingly, it negatively impacts intrinsic DQ. Furthermore, findings show that while intrinsic DQ, contextual DQ, and representational DQ significantly increase data diagnosticity, accessibility DQ does not influence it. Most importantly, the findings show that big data utilization does not significantly impact the quality of firm decisions and it is fully mediated through DQ and data diagnosticity. The results of this study contribute to practice by providing important guidelines for managers to improve firm decision quality through the use of big data.|https://doi.org/10.1016/j.dss.2019.03.008|https://www.sciencedirect.com/science/article/pii/S0167923619300466||||2019|Can big data improve firm decision quality? The role of data quality and data diagnosticity|Maryam Ghasemaghaei and Goran Calic|article|GHASEMAGHAEI201938|||Decision Support Systems|01679236||120|||||21933|1,564|Q1|151|115|342|7152|2672|338|7,04|62,19|Netherlands|Western Europe|1985-2020|Arts and Humanities (miscellaneous) (Q1); Developmental and Educational Psychology (Q1); Information Systems (Q1); Information Systems and Management (Q1); Management Information Systems (Q1)|13,580|5.795|0.0081|1930409215|1234879127
ICMECG 2021|Jeju, Republic of Korea|Big Data, Information systems, Quality measures, Smart data|6|112–117|2021 8th International Conference on Management of E-Commerce and e-Government|The term “smartness” in the data framework indicates relevancy based on the intended purpose of data. The Internet of Things (IoT) and advancements in technology have resulted in an ever-increasing pool of data available to all institutions to derive meaning and make sound decisions from them. The research presented in this paper explored the role smart data play in information systems quality through a qualitative study of how using the large pool of data (big data) and fusing it to smart data organizations can make sound and intelligent decisions using the available techniques. We use an existing architecture for a public institution to analyze how data ingestion can be achieved with minimum challenges. The findings suggest that even though there is a large pool of data for most organizations, it is becoming more challenging to use this data to make organizational sense due to the challenges posed by such data. The realization of smart data and its benefits in information systems helps improve the quality of information systems, reducing cost and promoting the smartness agenda of today's organization.|10.1145/3483816.3483836|https://doi.org/10.1145/3483816.3483836|New York, NY, USA|Association for Computing Machinery|9781450390545|2022|Fusion from Big Data to Smart Data to Enhance Quality of Information Systems|Febiri, Frank and Yihum Amare, Meseret and Hub, Miloslav|inproceedings|10.1145/3483816.3483836|||||||||||||||||||||||||||||1931947628|42
||Public transport, Accessibility, Smart card data, In-vehicle crowding, Travel time reliability||102671||Accessibility metrics are gaining momentum in public transportation planning and policy-making. However, critical user experience issues such as crowding discomfort and travel time unreliability are still not considered in those accessibility indicators. This paper aims to apply a methodology to build spatiotemporal crowding data and estimate travel time variability in a congested public transport network to improve accessibility calculations. It relies on using multiple big data sources available in most transit systems such as smart card and automatic vehicle location (AVL) data. São Paulo, Brazil, is used as a case study to show the impact of crowding and travel time variability on accessibility to jobs. Our results evidence a population-weighted average reduction of 56.8% in accessibility to jobs in a regular workday morning peak due to crowding discomfort, as well as reductions of 6.2% due to travel time unreliability and 59.2% when both are combined. The findings of this study can be of invaluable help to public transport planners and policymakers, as they show the importance of including both aspects in accessibility indicators for better decision making. Despite some limitations due to data quality and consistency throughout the study period, the proposed approach offers a new way to leverage big data in public transport to enhance policy decisions.|https://doi.org/10.1016/j.jtrangeo.2020.102671|https://www.sciencedirect.com/science/article/pii/S0966692319300092||||2020|Estimating the influence of crowding and travel time variability on accessibility to jobs in a large public transport network using smart card big data|Renato Arbex and Claudio B. Cunha|article|ARBEX2020102671|||Journal of Transport Geography|09666923||85|||||29295|1,809|Q1|108|264|515|15444|2931|514|5,21|58,50|United Kingdom|Western Europe|1993-2020|Environmental Science (miscellaneous) (Q1); Geography, Planning and Development (Q1); Transportation (Q1)|11,719|4.986|0.01053|1933482464|745814393
||Information management;Data handling;Data storage systems;Databases;Maintenance engineering;Quality management;Cleaning||1294-1297|2014 IEEE 30th International Conference on Data Engineering|In our Big Data era, data is being generated, collected and analyzed at an unprecedented scale, and data-driven decision making is sweeping through all aspects of society. Recent studies have shown that poor quality data is prevalent in large databases and on the Web. Since poor quality data can have serious consequences on the results of data analyses, the importance of veracity, the fourth `V' of big data is increasingly being recognized. In this tutorial, we highlight the substantial challenges that the first three `V's, volume, velocity and variety, bring to dealing with veracity in big data. Due to the sheer volume and velocity of data, one needs to understand and (possibly) repair erroneous data in a scalable and timely manner. With the variety of data, often from a diversity of sources, data quality rules cannot be specified a priori; one needs to let the “data to speak for itself” in order to discover the semantics of the data. This tutorial presents recent results that are relevant to big data quality management, focusing on the two major dimensions of (i) discovering quality issues from the data itself, and (ii) trading-off accuracy vs efficiency, and identifies a range of open problems for the community.|10.1109/ICDE.2014.6816764|||||2014|Data quality: The other face of Big Data|Saha, Barna and Srivastava, Divesh|inproceedings|6816764||March||2375026X|||||||||||||||||||||||||1941250103|986587582
||Big data, Contractual governance, Relational governance, Big data analytics capability, Culture, Decision-making performance, Emerging markets||120315||This study examines the role of big data contractual and relational governance in big data decision-making performance of firms based in China. It investigates the mediation of big data analytics (BDA) capability in the association of contractual and relational governance with decision-making performance. Furthermore, moderating role of data-driven culture in the relationship of BDA capability and decision-making performance is examined. Data are collected from 108 Chinese firms engaged in big data-related activities. Structural equation modeling is employed to test the hypotheses. This study contributes towards the literature on big data management and governance mechanisms, by establishing the relationship of decision-making performance with big data contractual and relational governance directly and through the mediation of BDA capabilities. It also contributes towards knowledge based dynamic capabilities (KBDCs) view of firms, arguing that dynamic capabilities such as BDA capabilities can be influenced through knowledge sources and activities. We add to the discussions on whether contractual and relational governance are alternatives or they complement each other, by establishing the moderating role of big data relational governance in the relationship of contractual governance and decision-making performance. Finally, we argue that social capital can enhance KBDCs through contractual and relational governance in big data context.|https://doi.org/10.1016/j.techfore.2020.120315|https://www.sciencedirect.com/science/article/pii/S0040162520311410||||2020|Big data analytics capability and decision making performance in emerging market firms: The role of contractual and relational governance mechanisms|Saqib Shamim and Jing Zeng and Zaheer Khan and Najam Ul Zia|article|SHAMIM2020120315|||Technological Forecasting and Social Change|00401625||161|||||14704|2,226|Q1|117|448|1099|35581|10127|1061|9,01|79,42|United States|Northern America|1970-2020|Applied Psychology (Q1); Business and International Management (Q1); Management of Technology and Innovation (Q1)|21,116|8.593|0.02416|1942748204|1949868303
||Big Data;Data integrity;Detection algorithms;Databases;Time complexity;Hazards;Business;Inconsistency detection;big data;one-pass algorithm;data quality;denial constraint||22377-22394||Data in the real world is often dirty. Inconsistency is an important kind of dirty data; before repairing inconsistency, we need to detect them first. The time complexities of the current inconsistency detection algorithms are super-linear to the size of data and not suitable for the big data. For the inconsistency detection of big data, we develop an algorithm that detects inconsistency within the one-pass scan of the data according to both the functional dependency (FD) and the conditional functional dependency (CFD) in our previous work. In this paper, we propose inconsistency detection algorithms in terms of FD, CFD, and Denial Constraint (DC). DCs are more expressive than FDs and CFDs. Developing the algorithm to detect the violation of DCs increases the applicability of our inconsistency detection algorithms. We compare the performance of our algorithm with the performance of implementing SQL queries in MySQL and BigQuery. The experimental results indicate the high efficiency of our algorithms.|10.1109/ACCESS.2019.2898707|||||2019|One-Pass Inconsistency Detection Algorithms for Big Data|Zhang, Meifan and Wang, Hongzhi and Li, Jianzhong and Gao, Hong|article|8641478|||IEEE Access|21693536||7|||||21100374601|0,587|Q1|127|18036|24267|771081|116691|24200|4,48|42,75|United States|Northern America|2013-2020|Computer Science (miscellaneous) (Q1); Engineering (miscellaneous) (Q1); Materials Science (miscellaneous) (Q2)|105,968|3.367|0.15396|1943665566|1905633267
||Big Data;Quality of experience;Training;Tensile stress;Machine learning;Data models;Quality of service||1-6|2018 IEEE International Conference on Communications (ICC)|In the age of big data, the services in pervasive edge environment are expected to offer end-users better Quality of Experience (QoE) than that in a normal edge environment. Nevertheless, various types of edge devices with storage, delivery, and sensing are coming into our environment and produce the high-dimensional big data accompanied by a volume of pervasive big data increasingly with a lot of redundancy. Therefore, the satisfaction of QoE becomes the primary challenge in high dimensional big data on the basis of pervasive edge environment. In this paper, we first propose a QoE model to evaluate the quality of service in pervasive edge environment. The value of QoE does not only include the accurate data, but also the transmission rate. Then, on the basis of the accuracy, we propose a Tensor-Fast Convolutional Neural Network (TF-CNN) algorithm based on Deep Learning, which is suitable for pervasive edge environment with high-dimensional big data analysis. Simulation results reveal that our proposals could achieve high QoE performance.|10.1109/ICC.2018.8422106|||||2018|QoE-Based Big Data Analysis with Deep Learning in Pervasive Edge Environment|Meng, Qianyu and Wang, Kun and Liu, Bo and Miyazaki, Toshiaki and He, Xiaoming|inproceedings|8422106||May||19381883|||||||||||||||||||||||||1945094069|276554849
https://doi.org/10.1016/j.envint.2020.106143|https://www.sciencedirect.com/science/article/pii/S0160412020320985||||2020|Ensemble-based deep learning for estimating PM2.5 over California with multisource big data including wildfire smoke|Lianfa Li and Mariam Girguis and Frederick Lurmann and Nathan Pavlovic and Crystal McClure and Meredith Franklin and Jun Wu and Luke D. Oman and Carrie Breton and Frank Gilliland and Rima Habre|article|LI2020106143|||Environment International|01604120||145||||||||||||||||||||||||||||||1945854082|42
||Information management;Data handling;Data storage systems;Educational institutions;Physics;Security;Cameras||4-4|2013 10th International Conference on Service Systems and Service Management|Summary form only given. At present, it is projected that about 4 zettabytes (or 10**21 bytes) of electronic data are being generated per year by everything from underground physics experiments to retail transactions to security cameras to global positioning systems. In the U. S., major research programs are being funded to deal with big data in all five economic sectors (i.e., services, manufacturing, construction, agriculture and mining) of the economy. Big Data is a term applied to data sets whose size is beyond the ability of available tools to undertake their acquisition, access, analytics and/or application in a reasonable amount of time. Whereas Tien (2003) forewarned about the data rich, information poor (DRIP) problems that have been pervasive since the advent of large-scale data collections or warehouses, the DRIP conundrum has been somewhat mitigated by the Big Data approach which has unleashed information in a manner that can support informed - yet, not necessarily defensible or knowledgeable - decisions or choices. Thus, by somewhat overcoming data quality issues with data quantity, data access restrictions with on-demand cloud computing, causative analysis with correlative data analytics, and model-driven with evidence-driven applications, appropriate actions can be undertaken with the obtained information. New acquisition, access, analytics and application technologies are being developed to further Big Data as it is being employed to help resolve the 14 grand challenges (identified by the National Academy of Engineering in 2008), underpin the 10 breakthrough technologies (compiled by the Massachusetts Institute of Technology in 2013) and support the Third Industrial Revolution of mass customization.|10.1109/ICSSSM.2013.6602615|||||2013|Big Data: Unleashing information|Tien, James M.|inproceedings|6602615||July||21611904|||||||||||||||||||||||||1947539508|1477259195
||sensor selection, predictive maintenance, machine learning, fleet management, Controller Area Network, predictive analytics, Internet of Things, J1939|36|||In recent years, big data produced by the Internet of Things has enabled new kinds of useful applications. One such application is monitoring a fleet of vehicles in real time to predict their remaining useful life. The consensus self-organized models (COSMO) approach is an example of a predictive maintenance system. The present work proposes a novel Internet of Things based architecture for predictive maintenance that consists of three primary nodes: the vehicle node, the server leader node, and the root node, which enable on-board vehicle data processing, heavy-duty data processing, and fleet administration, respectively. A minimally viable prototype of the proposed architecture was implemented and deployed to a local bus garage in Gatineau, Canada.The present work proposes improved consensus self-organized models (ICOSMO), a fleet-wide unsupervised dynamic sensor selection algorithm. To analyze the performance of ICOSMO, a fleet simulation was implemented. The J1939 data gathered from a hybrid bus was used to generate synthetic data in the simulations. Simulation results that compared the performance of the COSMO and ICOSMO approaches revealed that in general ICOSMO improves the average area under the curve of COSMO by approximately 1.5% when using the Cosine distance and 0.6% when using Hellinger distance.|10.1145/3530991|https://doi.org/10.1145/3530991|New York, NY, USA|Association for Computing Machinery||2022|Unsupervised Dynamic Sensor Selection for IoT-Based Predictive Maintenance of a Fleet of Public Transport Buses|Killeen, Patrick and Kiringa, Iluju and Yeap, Tet|article|10.1145/3530991|21|jul|ACM Trans. Internet Things|26911914|3|3|August 2022||||||||||||||||||||||1947759116|2142544135
||Data integrity;Petri nets;Computer architecture;Aerospace electronics;Data processing;Data models;Cleaning;data space;data quality;business flow;petri net;BPNN||505-509|2021 3rd International Conference on Artificial Intelligence and Advanced Manufacture (AIAM)|With the continuous improvement of the information technology and communications of Smart Grid, the electric power big data environment has been formed. The data shows diversity and multi-source characteristics. How to ensure the quality of power data in the computer organization under the condition of heterogeneity is the premise of making relevant decisions. This paper firstly gives the definition of Data Space of power enterprises, analyzes the factors affecting the quality of data in the computer environment, and gives the relevant architecture of processing power data in the data space. Secondly, based on business flow and Petri net in the computer environment, this paper constructs the data flow and quality control model of the front and back platforms. The former represents the data flow in the power business and abstracts it to form Petri net computer information flow, so that the data can achieve the effect of cleaning while flowing in the business process. Finally, an evaluation index system is built and back-propagation neural network (BPNN) is used to determine the weight, a case study is given to verify the effectiveness of the proposed method.|10.1109/AIAM54119.2021.00106|||||2021|Power Data Quality Optimization and Evaluation Based on BPNN|Feng, Xinyi|inproceedings|9724934||Oct|||||||||||||||||||||||||||1948264333|42
https://doi.org/10.1016/j.techfore.2020.120420|https://www.sciencedirect.com/science/article/pii/S0040162520312464||||2021|Role of institutional pressures and resources in the adoption of big data analytics powered artificial intelligence, sustainable manufacturing practices and circular economy capabilities|Surajit Bag and Jan Ham Christiaan Pretorius and Shivam Gupta and Yogesh K. Dwivedi|article|BAG2021120420|||Technological Forecasting and Social Change|00401625||163||||||||||||||||||||||||||||||1949868303|42
||Smart city, Big data, Digital sustainability, Resource orchestration, Socio-technical issues||101626||Smart cities are expected to improve the efficiency and effectiveness of urban management, including public services, public security, and environmental protection, and to ultimately achieve Sustainable Development Goal (SDG) 11 for making cities inclusive, safe, resilient, and sustainable. Big data have been identified as a key enabler in the development of smart cities. However, our understanding of how different data sources should be managed and integrated remains limited. By analyzing data applications in the development of a sustainable smart city, this case study identified three phases of development, each requiring a different approach to orchestrating diverse data sources. A framework identifying the phases, data-related issues, data orchestration and its interaction with other resources, focal capabilities, and development approaches is developed. This study benefits both researchers and practitioners by making theoretical contributions and by offering practical insights in the fields of smart cities and big data.|https://doi.org/10.1016/j.giq.2021.101626|https://www.sciencedirect.com/science/article/pii/S0740624X21000629||||2022|Big data analytics, resource orchestration, and digital sustainability: A case study of smart city development|Dan Zhang and L.G. Pee and Shan L. Pan and Lili Cui|article|ZHANG2022101626|||Government Information Quarterly|0740624X|1|39|||||14735|2,121|Q1|103|76|205|5894|1946|193|8,39|77,55|United Kingdom|Western Europe|1984-2020|E-learning (Q1); Law (Q1); Library and Information Sciences (Q1); Sociology and Political Science (Q1)|5,379|7.279|0.00502|1950732857|1582933551
||Semantics;Trajectory;Location awareness;Hidden Markov models;Deep learning;Noise measurement;Indoor environment;Deep learning;indoor localization;the Internet of Things;rule-based refinement;semantic trajectories||73152-73168||Indoor location-based services have been widely investigated to take advantage of semantic trajectories for providing user oriented services in indoor environments. Although indoor semantic trajectories can provide seamless understanding to users regarding the provided location-based services, studies on the application of deep learning approaches for robust and valid semantic indoor localization are lacking. In this study, we combined a stacked denoising autoencoder and long short term memory technique with a rule-based refinement method applying a rule-based hidden Markov model (HMM) to perform robust and valid semantic trajectory extraction. In particular, our rule-based HMM approach incorporates a direct set of rules into HMM to resolve invalid movements of the extracted semantic trajectories and is extensible to various deep learning techniques. We compared the performance of our proposed approach with that of other cutting-edge deep learning approaches on two different real-world data sets. The experimental results demonstrate the feasibility of our proposed approach to produce more robust and valid semantic trajectories.|10.1109/ACCESS.2021.3080288|||||2021|A Stacked Denoising Autoencoder and Long Short-Term Memory Approach With Rule-Based Refinement to Extract Valid Semantic Trajectories|Yustiawan, Yoga and Ramadhan, Hani and Kwon, Joonho|article|9431193|||IEEE Access|21693536||9|||||21100374601|0,587|Q1|127|18036|24267|771081|116691|24200|4,48|42,75|United States|Northern America|2013-2020|Computer Science (miscellaneous) (Q1); Engineering (miscellaneous) (Q1); Materials Science (miscellaneous) (Q2)|105,968|3.367|0.15396|1951175392|1905633267
https://doi.org/10.1016/j.heliyon.2022.e10312|https://www.sciencedirect.com/science/article/pii/S2405844022016000||||2022|Impact of big data resources on clinicians’ activation of prior medical knowledge|Sufen Wang and Junyi Yuan and Changqing Pan|article|WANG2022e10312|||Heliyon|24058440|9|8||||||||||||||||||||||||||||||1953619154|42
Semantics2017|Amsterdam, Netherlands|Outlier Detection, Semantic Technologies, Wireless Sensor Network, Knowledge Discovery in Databases|8|152–159|Proceedings of the 13th International Conference on Semantic Systems|Outlier detection in the preprocessing phase of Knowledge Discovery in Databases (KDD) processes has been a widely researched topic for many years. However, identifying the potential outlier cause still remains an unsolved challenge even though it could be very helpful for determining what actions to take after detecting it. Furthermore, conventional outlier detection methods might still overlook outliers in certain complex contexts. In this article, Semantic Technologies are used to contribute overcoming these problems by proposing the SemOD (Semantic Outlier Detection) Framework. This framework guides the data-scientist towards the detection of certain types of outliers in WSNs (Wireless Sensor Network). Feasibility of the approach has been tested in outdoor temperature sensors and results show that the proposed approach is generic enough to apply it to different sensors, even improving the accuracy, specificity and sensitivity of outlier detection as well as spotting their potential cause.|10.1145/3132218.3132226|https://doi.org/10.1145/3132218.3132226|New York, NY, USA|Association for Computing Machinery|9781450352963|2017|Towards a Semantic Outlier Detection Framework in Wireless Sensor Networks|Esnaola-Gonzalez, Iker and Berm\'{u}dez, Jes\'{u}s and Fern\'{a}ndez, Izaskun and Fern\'{a}ndez, Santiago and Arnaiz, Aitor|inproceedings|10.1145/3132218.3132226|||||||||||||||||||||||||||||1954371187|42
||Data quality, Data quality management, IoT, ISO 8000, Process-centric, Process reference model, Maturity, Process maturity, process attribute||100256||Data quality management (DQM) is one of the most critical aspects to ensure successful applications of the Internet of Things (IoT). So far, most of the approaches for assuring data quality are typically data-centric, i.e., mainly focus on fixing data issues for specific values. However, organizations can also benefit from improving their capabilities of their DQM processes by developing organizational best DQM practices. In this regard, our investigation addresses how well organizations perform their DQM processes in the IoT domain. The main contribution of this study is to establish a framework for IoT DQM maturity. This framework is compliant with ISO 8000-61 (DQM: process reference model) and ISO 8000-62 (DQM: organizational process maturity assessment) and can be used to assess and improve the capabilities of the DQM processes for IoT data. The framework is composed of two elements. First, a process reference model (PRM) for IoT DQM is proposed by extending the PRM for DQM defined in ISO 8000-61, tailoring some existing processes and adding new ones. Second, a maturity model suitable for IoT data is proposed based on the PRM for IoT DQM. The maturity model, named IoT DQM3, is proposed by extending the maturity model defined in ISO 8000-62. However, in order to increase the usability of IoT DQM3, we consider adequate the proposition of a simplification of the IoT DQM3, by introducing a lightweight version to reduce assessment indicators and facilitate its industrial adoption. A simplified method to measure the capability of a process is also suggested considering the relationship of process attributes with the measurement stack defined in ISO 8000-63. The empirical validation of the maturity model is twofold. First, the appropriateness of the two models is surveyed with data quality experts who are currently working in various organizations around the world. Second, in order to demonstrate the feasibility of the proposal, the light-weight version is applied to a manufacturing company as a case study.|https://doi.org/10.1016/j.jii.2021.100256|https://www.sciencedirect.com/science/article/pii/S2452414X21000480||||2022|Organizational process maturity model for IoT data quality management|Sunho Kim and Ricardo Pérez-Castillo and Ismael Caballero and Downgwoo Lee|article|KIM2022100256|||Journal of Industrial Information Integration|2452414X||26|||||21100787106|2,042|Q1|24|28|86|2152|1361|83|12,26|76,86|Netherlands|Western Europe|2016-2020|Industrial and Manufacturing Engineering (Q1); Information Systems and Management (Q1)|1,149|10.063|0.00148|1955290851|121356201
||Big data;Industries;Insurance;Companies;Finance;big data;data quality;big data quality;antecedents;finance||116-121|2016 IEEE International Conference on Big Data (Big Data)|Big data has been acknowledged for its enormous potential. In contrast to the potential, in a recent survey more than half of financial service organizations reported that big data has not delivered the expected value. One of the main reasons for this is related to data quality. The objective of this research is to identify the antecedents of big data quality in financial institutions. This will help to understand how data quality from big data analysis can be improved. For this, a literature review was performed and data was collected using three case studies, followed by content analysis. The overall findings indicate that there are no fundamentally new data quality issues in big data projects. Nevertheless, the complexity of the issues is higher, which makes it harder to assess and attain data quality in big data projects compared to the traditional projects. Ten antecedents of big data quality were identified encompassing data, technology, people, process and procedure, organization, and external aspects.|10.1109/BigData.2016.7840595|||||2016|Antecedents of big data quality: An empirical examination in financial service organizations|Haryadi, Adiska Fardani and Hulstijn, Joris and Wahyudi, Agung and van der Voort, Haiko and Janssen, Marijn|inproceedings|7840595||Dec|||||||||||||||||||||||||||1957220653|42
||||326-332||The application of big data to the quality assurance of radiation therapy is multifaceted. Big data can be used to detect anomalies and suboptimal quality metrics through both statistical means and more advanced machine learning and artificial intelligence. The application of these methods to clinical practice is discussed through examples of guideline adherence, contour integrity, treatment delivery mechanics, and treatment plan quality. The ultimate goal is to apply big data methods to direct measures of patient outcomes for care quality. The era of big data and machine learning is maturing and the implementation for quality assurance promises to improve the quality of care for patients.|https://doi.org/10.1016/j.semradonc.2019.05.006|https://www.sciencedirect.com/science/article/pii/S1053429619300384||||2019|Use of Big Data for Quality Assurance in Radiation Therapy|Todd R. McNutt and Kevin L. Moore and Binbin Wu and Jean L. Wright|article|MCNUTT2019326|||Seminars in Radiation Oncology|10534296|4|29||Big Data in Radiation Oncology|||13121|1,761|Q1|93|40|129|2575|654|118|5,29|64,38|United Kingdom|Western Europe|1991-2020|Oncology (Q1); Radiology, Nuclear Medicine and Imaging (Q1); Cancer Research (Q2)|2,837|5.934|0.00271|1957237047|665117643
||Facsimile;social network service;data quality;currency;trustworthiness||1-5|2017 IEEE 2nd International Conference on Cloud Computing and Big Data Analysis (ICCCBDA)|Along with the explosive growth of the information in Social Network Service, the research of the quality of data has become a new hot point in related research field. High quality social data can more effectively support data mining, knowledge discovery, and can provide reliable and efficient data for users. Based on the measure problems of data quality, this paper discussed the measurement of two important dimensions of data quality: currency and trustworthiness. Computing models for currency measurement of data with or without time stamp are given. And based on the currency values, a trustworthiness measurement method is also given.|10.1109/ICCCBDA.2017.7951874|||||2017|Measurement for social network data currency and trustworthiness|Fan Xiaojiang and Zheng Liwei and Liu Jianbin|inproceedings|7951874||April|||||||||||||||||||||||||||1957279495|42
||Data quality, Data design, System implementation, Information governance, Leadership, Data supply chain||209-237|Roadmap to Successful Digital Health Ecosystems|Data is the core for digital health systems; that data needs to be accurate, consistent, and available. The health workforce needs to understand how to define and govern data quality throughout the data supply chain, from origin through sharing, aggregated reporting, and eventually to enable the discovery of new knowledge based upon that data. Data quality applies to the data supply chain as a whole. Data needs to be collectable and useful at origin and able to represent and explain things that may not be known at the time of collection. Consistency of concept representation throughout the data supply chain needs to be clearly specified and transparent. Professional bodies need to provide leadership into the specification and governance of data, information, and computable knowledge, augmenting their traditional role of knowledge acquisition and publication based upon evidence. Throughout all levels of healthcare, the necessity of data quality needs to be better understood and managed.|https://doi.org/10.1016/B978-0-12-823413-6.00013-6|https://www.sciencedirect.com/science/article/pii/B9780128234136000136||Academic Press|978-0-12-823413-6|2022|Chapter 9 - Quality data, design, implementation, and governance|Evelyn Hovenga and Heather Grain|incollection|HOVENGA2022209||||||||||Evelyn Hovenga and Heather Grain|||||||||||||||||||1957680069|42
||Big data, Management research, Literature review||97-112||In recent years, big data has emerged as one of the prominent buzzwords in business and management. In spite of the mounting body of research on big data across the social science disciplines, scholars have offered little synthesis on the current state of knowledge. To take stock of academic research that contributes to the big data revolution, this paper tracks scholarly work's perspectives on big data in the management domain over the past decade. We identify key themes emerging in management studies and develop an integrated framework to link the multiple streams of research in fields of organisation, operations, marketing, information management and other relevant areas. Our analysis uncovers a growing awareness of big data's business values and managerial changes led by data-driven approach. Stemming from the review is the suggestion for research that both structured and unstructured big data should be harnessed to advance understanding of big data value in informing organisational decisions and enhancing firm competitiveness. To discover the full value, firms need to formulate and implement a data-driven strategy. In light of these, the study identifies and outlines the implications and directions for future research.|https://doi.org/10.1016/j.ijpe.2017.06.006|https://www.sciencedirect.com/science/article/pii/S092552731730169X||||2017|A multidisciplinary perspective of big data in management research|Jie Sheng and Joseph Amankwah-Amoah and Xiaojun Wang|article|SHENG201797|||International Journal of Production Economics|09255273||191|||||19165|2,406|Q1|185|327|891|21712|8124|881|8,31|66,40|Netherlands|Western Europe|1991-2021|Business, Management and Accounting (miscellaneous) (Q1); Economics and Econometrics (Q1); Industrial and Manufacturing Engineering (Q1); Management Science and Operations Research (Q1)|32,606|7.885|0.0228|1958672871|850534974
||missing data, big data paradox, under-representation, bias, crowdsourced data||100587||Our “digified” lives have provided researchers with an unprecedented opportunity to study society at a much higher frequency and granularity. Such data can have a large sample size but can be sparse, biased, and exclusively contributed by the users of the technologies. We look at the increasing importance of missing data and under-representation and propose a new perspective that considers missing data as useful data to understand the underlying reasons for missingness and that provides a realistic view of the sample size of large but under-represented data.|https://doi.org/10.1016/j.patter.2022.100587|https://www.sciencedirect.com/science/article/pii/S2666389922002057||||2022|Missing data as data|Anahid Basiri and Chris Brunsdon|article|BASIRI2022100587|||Patterns|26663899|9|3|||||||||||||||||||||||1958892543|1200898061
||TBM, Surrounding rock class, Perception model, Unsupervised learning, Supervised learning||104285||The perception of surrounding rock geological conditions ahead the tunnel face is essential for TBM safe and efficient tunnelling. This paper developed a perception approach of surrounding rock class based on TBM operational big data and combined unsupervised-supervised learning. In data preprocessing, four data mining techniques (i.e., Z-score, K-NN, Kalman filtering, and wavelet packet decomposition) were used to detect outliers, substitute outliers, suppress noise, and extract features, respectively. Then, GMM was used to revise the original surrounding rock class through clustering TBM load parameters and performance parameters in view of the shortcomings of the HC method in the TBM-excavated tunnel. After that, five various ensemble learning classification models were constructed to identify the surrounding rock class, in which model hyper-parameters were automatically tuned by Bayes optimization. In order to evaluate model performance, balanced accuracy, Kappa, F1-score, and training time were taken into account, and a novel multi-metric comprehensive ranking system was designed. Engineering application results indicated that LightGBM achieved the most superior performance with the highest comprehensive score of 6.9066, followed by GBDT (5.9228), XGBoost (5.4964), RF (3.7581), and AdaBoost (0.9946). Through the weighted purity reduction algorithm, the contributions of input features on the five models were quantitatively analyzed. Finally, the impact of class imbalance on model performance was discussed using the ADASYN algorithm, showing that eliminating class imbalance can further improve the model's perception ability.|https://doi.org/10.1016/j.tust.2021.104285|https://www.sciencedirect.com/science/article/pii/S0886779821004764||||2022|Perception model of surrounding rock geological conditions based on TBM operational big data and combined unsupervised-supervised learning|Xin Yin and Quansheng Liu and Xing Huang and Yucong Pan|article|YIN2022104285|||Tunnelling and Underground Space Technology|08867798||120|||||14642|2,172|Q1|98|375|946|16500|6440|942|6,53|44,00|United Kingdom|Western Europe|1986-2020|Building and Construction (Q1); Geotechnical Engineering and Engineering Geology (Q1)|16,336|5.915|0.0149|1960164652|701092564
CIKM '18|Torino, Italy|robust regression, contextual outlier detection|10|287–296|Proceedings of the 27th ACM International Conference on Information and Knowledge Management|Advances in sensor technology have enabled the collection of large-scale datasets. Such datasets can be extremely noisy and often contain a significant amount of outliers that result from sensor malfunction or human operation faults. In order to utilize such data for real-world applications, it is critical to detect outliers so that models built from these datasets will not be skewed by outliers. In this paper, we propose a new outlier detection method that utilizes the correlations in the data (e.g., taxi trip distance vs. trip time). Different from existing outlier detection methods, we build a robust regression model that explicitly models the outliers and detects outliers simultaneously with the model fitting. We validate our approach on real-world datasets against methods specifically designed for each dataset as well as the state of the art outlier detectors. Our outlier detection method achieves better performances, demonstrating the robustness and generality of our method. Last, we report interesting case studies on some outliers that result from atypical events.|10.1145/3269206.3271798|https://doi.org/10.1145/3269206.3271798|New York, NY, USA|Association for Computing Machinery|9781450360142|2018|Detecting Outliers in Data with Correlated Measures|Kuo, Yu-Hsuan and Li, Zhenhui and Kifer, Daniel|inproceedings|10.1145/3269206.3271798|||||||||||||||||||||||||||||1963659550|42
MET '17|Buenos Aires, Argentina|metamorphic testing, support vector machine, software validation, neural network, deep learning|7|28–34|Proceedings of the 2nd International Workshop on Metamorphic Testing|Deep learning has become an important tool for image classification and natural language processing. However, the effectiveness of deep learning is highly dependent on the quality of the training data as well as the net model for the learning. The training data set for deep learning normally is fairly large, and the net model is pretty complex. It is necessary to validate the deep learning framework including the net model, executing environment, and training data set before it is used for any applications. In this paper, we propose an approach for validating the classification accuracy of a deep learning framework that includes a convolutional neural network, a deep learning executing environment, and a massive image data set. The framework is first validated with a classifier built on support vector machine, and then it is tested using a metamorphic validation approach. The effectiveness of the approach is demonstrated by validating a deep learning classifier for automated classification of biology cell images. The proposed approach can be used for validating other deep learning framework for different applications.||||IEEE Press|9781538604243|2017|Validating a Deep Learning Framework by Metamorphic Testing|Ding, Junhua and Kang, Xiaojun and Hu, Xin-Hua|inproceedings|10.5555/3103620.3103631|||||||||||||||||||||||||||||1963739139|42
||poor-quality streaming data, real-time imputation, Missing value|38|||Missing value (MV) imputation is a critical preprocessing means for data mining. Nevertheless, existing MV imputation methods are mostly designed for batch processing, and thus are not applicable to streaming data, especially those with poor quality. In this article, we propose a framework, called Real-time and Error-tolerant Missing vAlue ImputatioN (REMAIN), to impute MVs in poor-quality streaming data. Instead of imputing MVs based on all the observed data, REMAIN first initializes the MV imputation model based on a-RANSAC which is capable of detecting and rejecting anomalies in an efficient manner, and then incrementally updates the model parameters upon the arrival of new data to support real-time MV imputation. As the correlations among attributes of the data may change over time in unforseenable ways, we devise a deterioration detection mechanism to capture the deterioration of the imputation model to further improve the imputation accuracy. Finally, we conduct an extensive evaluation on the proposed algorithms using real-world and synthetic datasets. Experimental results demonstrate that REMAIN achieves significantly higher imputation accuracy over existing solutions. Meanwhile, REMAIN improves up to one order of magnitude in time cost compared with existing approaches.|10.1145/3412364|https://doi.org/10.1145/3412364|New York, NY, USA|Association for Computing Machinery||2020|REMIAN: Real-Time and Error-Tolerant Missing Value Imputation|Ma, Qian and Gu, Yu and Lee, Wang-Chien and Yu, Ge and Liu, Hongbo and Wu, Xindong|article|10.1145/3412364|77|sep|ACM Trans. Knowl. Discov. Data|15564681|6|14|December 2020||||5800173377|0,728|Q1|59|71|169|3729|816|168|4,54|52,52|United States|Northern America|2007-2020|Computer Science (miscellaneous) (Q1)|1,740|2.713|0.00224|1965773966|1302859451
iiWAS '20|Chiang Mai, Thailand|Machine Learning as a Service, MLaaS, Machine Learning Services, Machine Learning Platform, Machine Learning|11|396–406|Proceedings of the 22nd International Conference on Information Integration and Web-Based Applications &amp; Services|This study aims to evaluate the current state of research with regards to Machine Learning as a Service (MLaaS) and to identify challenges and research fields of this novel topic. First, a literature review on a basket of eight leading journals was performed. We motivate this study by identifying a lack of studies in the field of MLaaS. The structured literature review was further extended to established scientific databases relevant in this field. We found 30 contributions on MLaaS. As a result of the analysis we grouped them into four key concepts: Platform, Applications; Performance Enhancements and Challenges. Three of the derived concepts are discussed in detail to identify future research areas and to reveal challenges in research as well as in applications.|10.1145/3428757.3429152|https://doi.org/10.1145/3428757.3429152|New York, NY, USA|Association for Computing Machinery|9781450389228|2021|Machine Learning as a Service: Challenges in Research and Applications|"\"Philipp, Robert and Mladenow, Andreas and Strauss, Christine and V\"\"{o}lz, Alexander\""|inproceedings|10.1145/3428757.3429152|||||||||||||||||||||||||||||1966710764|42
||Big data analytics, Machine learning, Internet of things, Smart cities, Wireless technologies||||Wireless networking has made enormous improvements. These developments have brought about new paradigms of wireless networking and communications Environmental protection has been in recent years a more intelligent and linked system for all facets of a global city. With the rise in data gathering, machine learning (ML) approaches can be used to boost the knowledge and the skill of an application. As the numbers increase and technology develops, the number of available data increases. Smart collection and interpretation of these Big Data is the underground to the rising of smart Internet of Things IoT apps. This study discovers the diverse methods of machine learning that resolve data difficulties in smart cities. The discussion takes place on applications such as air quality, water pollution, radiation pollution, smart buildings, smart transport, etc., which pose genuine environmental challenges. Adequate monitoring is needed to ensure sustainable growth in the world by safeguarding a healthy society. The potential and challenges in particular the role of machine learning technology for the Internet of Things and Big Data Analytics.|https://doi.org/10.1016/j.matpr.2021.06.046|https://www.sciencedirect.com/science/article/pii/S2214785321043911||||2021|A study on artificial intelligence for monitoring smart environments|Karthika D.|article|D2021|||Materials Today: Proceedings|22147853|||||||21100370037|0,341|-|47|3996|10585|88521|14096|10409|1,24|22,15|United Kingdom|Western Europe|2005, 2014-2020|Materials Science (miscellaneous)||||1966828341|400517803
||Smart Farming, Energy Harvesting Capabilities, IoT, Big Data, Agriculture 4.0, Water Management, Sustainability||102093||The use of Internet of Things (IoT) networks offers great advantages over wired networks, especially due to their simple installation, low maintenance costs, and automatic configuration. IoT facilitates the integration of sensing and communication for various industries, including smart farming and precision agriculture. For several years, many researchers have strived to find new sources of energy that are always “cleaner” and more environmentally friendly. Energy harvesting technology is one of the most promising environment-friendly solutions that extend the lifetime of these IoT devices. In this paper, the state-of-art of IoT energy harvesting capabilities and communication technologies in smart agriculture is presented. In addition, this work proposes a comprehensive architecture that includes big data technologies, IoT components, and knowledge-based systems for innovative farm architecture. The solution answers some of the biggest challenges the agriculture industry faces, especially when handling small files in a big data environment without impacting the computation performance. The solution is built on top of a pre-defined big data architecture that includes an abstraction layer of the data lake that handles data quality following a data migration strategy to ensure the data's insights. Furthermore, in this paper, we compared several machine learning algorithms to find the most suitable smart farming analytics tools in terms of forecasting and predictions.|https://doi.org/10.1016/j.seta.2022.102093|https://www.sciencedirect.com/science/article/pii/S221313882200145X||||2022|AI-based modeling and data-driven evaluation for smart farming-oriented big data architecture using IoT with energy harvesting capabilities|El Mehdi Ouafiq and Rachid Saadane and Abdellah Chehri and Seunggil Jeon|article|OUAFIQ2022102093|||Sustainable Energy Technologies and Assessments|22131388||52|||||21100239262|1,040|Q1|39|270|333|13311|1833|331|5,73|49,30|United Kingdom|Western Europe|2013-2020|Energy Engineering and Power Technology (Q1); Renewable Energy, Sustainability and the Environment (Q2)|3,234|5.353|0.00348|1966944188|1822851290
IDEAS '17|Bristol, United Kingdom|Data Quality, Data Analysis, Entity Matching, Smart Cities|5|304–308|Proceedings of the 21st International Database Engineering &amp; Applications Symposium|As cities are becoming green and smart, public information systems are being revamped to adopt digital technologies. There are several sources (official or not) that can provide information related to a city. The availability of multiple sources enables the design of advanced analyses for offering valuable services to both citizens and municipalities. However, such analyses would fail if the considered data were affected by errors and uncertainties: Data Quality is one of the main requirements for the successful exploitation of the available information. This paper highlights the importance of the Data Quality evaluation in the context of geographical data sources. Moreover, we describe how the Entity Matching task can provide additional information to refine the quality assessment and, consequently, obtain a better evaluation of the reliability data sources. Data gathered from the public transportation and urban areas of Curitiba, Brazil, are used to show the strengths and effectiveness of the presented approach.|10.1145/3105831.3105834|https://doi.org/10.1145/3105831.3105834|New York, NY, USA|Association for Computing Machinery|9781450352208|2017|Towards Reliable Data Analyses for Smart Cities|Ara\'{u}jo, Tiago Brasileiro and Cappiello, Cinzia and Kozievitch, Nadia Puchalski and Mestre, Demetrio Gomes and Pires, Carlos Eduardo Santos and Vitali, Monica|inproceedings|10.1145/3105831.3105834|||||||||||||||||||||||||||||1973324359|42
SIGMOD '12|Scottsdale, Arizona, USA|data integration, visualization|10|807–816|Proceedings of the 2012 ACM SIGMOD International Conference on Management of Data|Tableau is a commercial business intelligence (BI) software tool that supports interactive, visual analysis of data. Armed with a visual interface to data and a focus on usability, Tableau enables a wide audience of end-users to gain insight into their datasets. The user experience is a fluid process of interaction in which exploring and visualizing data takes just a few simple drag-and-drop operations (no programming or DB experience necessary). In this context of exploratory, ad-hoc visual analysis, we describe a novel approach to integrating large, heterogeneous data sources. We present a new feature in Tableau called data blending, which gives users the ability to create data visualization mashups from structured, heterogeneous data sources dynamically without any upfront integration effort. Users can author visualizations that automatically integrate data from a variety of sources, including data warehouses, data marts, text files, spreadsheets, and data cubes. Because our data blending system is workload driven, we are able to bypass many of the pain-points and uncertainty in creating mediated schemas and schema-mappings in current pay-as-you-go integration systems.|10.1145/2213836.2213961|https://doi.org/10.1145/2213836.2213961|New York, NY, USA|Association for Computing Machinery|9781450312479|2012|Dynamic Workload Driven Data Integration in Tableau|Morton, Kristi and Bunker, Ross and Mackinlay, Jock and Morton, Robert and Stolte, Chris|inproceedings|10.1145/2213836.2213961|||||||||||||||||||||||||||||1975703262|42
||Solid modeling;Electronics industry;Predictive models;Reliability engineering;Manufacturing;Safety;Problem-solving;Automotive semiconductor industry;root-cause problem-solving;failure prevention;data analytics;machine learning;big data||1-10|2021 IEEE International Symposium on the Physical and Failure Analysis of Integrated Circuits (IPFA)|Quality requirements in the semiconductor industry for automotive products are increasing rapidly with the movement to autonomous vehicles and higher levels of safety. It is no longer possible to express maximum failure requirements in parts per million (ppm). Individual failing parts observed in the field and reported by customers can trigger a significant quality response. ‘Zero-defect’ (ZD) is no longer considered a utopian ideal, but a required potentially reachable goal for semiconductor manufacturers. Projects and studies that include artificial intelligence and big data, are seen as key drivers to reach a ZD level of quality. Competing objectives targeted in any industrial project, such as quality improvement and gross margin, must also be considered. Initial projects in machine learning (ML), focusing on yield-loss issues, are being deployed within the manufacturing sites. These projects interconnect typical internal data collected from the manufacturing and assembly lines with engineering, qualification and reliability data. For a specific case study of unexpected abnormally high variability on some parameters, this paper presents a problem-solving approach in a big-data environment. Models implemented and results obtained towards root-cause problem solving for this issue, are discussed. This overall approach may be replicated in other ML projects.|10.1109/IPFA53173.2021.9617238|||||2021|Data analytics and machine learning: root-cause problem-solving approach to prevent yield loss and quality issues in semiconductor industry for automotive applications|Bergès, Corinne and Bird, Jim and Shroff, Mehul D. and Rongen, René and Smith, Chris|inproceedings|9617238||Sep.||19461550|||||||||||||||||||||||||1981058120|1151384890
ESEM '16|Ciudad Real, Spain|Software Developers, Empirical Research, Ethics, Sampling|6||Proceedings of the 10th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement|Background: Reaching out to professional software developers is a crucial part of empirical software engineering research. One important method to investigate the state of practice is survey research. As drawing a random sample of professional software developers for a survey is rarely possible, researchers rely on various sampling strategies. Objective: In this paper, we report on our experience with different sampling strategies we employed, highlight ethical issues, and motivate the need to maintain a collection of key demographics about software developers to ease the assessment of the external validity of studies. Method: Our report is based on data from two studies we conducted in the past. Results: Contacting developers over public media proved to be the most effective and efficient sampling strategy. However, we not only describe the perspective of researchers who are interested in reaching goals like a large number of participants or a high response rate, but we also shed light onto ethical implications of different sampling strategies. We present one specific ethical guideline and point to debates in other research communities to start a discussion in the software engineering research community about which sampling strategies should be considered ethical.|10.1145/2961111.2962628|https://doi.org/10.1145/2961111.2962628|New York, NY, USA|Association for Computing Machinery|9781450344272|2016|Worse Than Spam: Issues In Sampling Software Developers|Baltes, Sebastian and Diehl, Stephan|inproceedings|10.1145/2961111.2962628|52||||||||||||||||||||||||||||1981461236|42
||machine learning, data mining, computational health informatics, clinical decision support, Big data analytics, survey, 4V challenges|36|||The explosive growth and widespread accessibility of digital health data have led to a surge of research activity in the healthcare and data sciences fields. The conventional approaches for health data management have achieved limited success as they are incapable of handling the huge amount of complex data with high volume, high velocity, and high variety. This article presents a comprehensive overview of the existing challenges, techniques, and future directions for computational health informatics in the big data age, with a structured analysis of the historical and state-of-the-art methods. We have summarized the challenges into four Vs (i.e., volume, velocity, variety, and veracity) and proposed a systematic data-processing pipeline for generic big data in health informatics, covering data capturing, storing, sharing, analyzing, searching, and decision support. Specifically, numerous techniques and algorithms in machine learning are categorized and compared. On the basis of this material, we identify and discuss the essential prospects lying ahead for computational health informatics in this big data age.|10.1145/2932707|https://doi.org/10.1145/2932707|New York, NY, USA|Association for Computing Machinery||2016|Computational Health Informatics in the Big Data Age: A Survey|Fang, Ruogu and Pouyanfar, Samira and Yang, Yimin and Chen, Shu-Ching and Iyengar, S. S.|article|10.1145/2932707|12|jun|ACM Comput. Surv.|03600300|1|49|March 2017||||23038|2,079|Q1|163|112|365|15859|6978|365|19,16|141,60|United States|Northern America|1969-2020|Computer Science (miscellaneous) (Q1); Theoretical Computer Science (Q1)|10,790|10.282|0.01438|1981508830|1517405264
||Atlas, Big data, Collaborative, CranialCloud, DBS, Normalization||137-145|Neuromodulation (Second Edition)|Surgeons, neurologists, researchers, and patients have lacked the technology-based tools to facilitate sharing the tremendously valuable data about patients’ treatment and research in regard to what is working and what is not. Today, only 9% of patients who could benefit from complex therapies to address neurologic conditions actually receive them, and the medical information for each patient who does is hidden away in disconnected databases. To optimize and accelerate our understanding of the brain, we need to gather intelligence around every case, every research subject, every study while connecting that information through a unified, Health Insurance Portability and Accountability Act of 1996 (HIPAA)-compliant network that leverages technology and harnesses the Internet to drive advancements and better connect patients to their care teams. In this chapter, we highlight the key aspects needed to fulfill the requirements of a robust, HIPAA-compliant archive for brain data and highlight the impact of normalization on the accuracy of statistical analyses.|https://doi.org/10.1016/B978-0-12-805353-9.00013-9|https://www.sciencedirect.com/science/article/pii/B9780128053539000139||Academic Press|978-0-12-805353-9|2018|Chapter 13 - Big Data and Deep Brain Stimulation|Pierre-Francois D’Haese and Peter E. Konrad and Benoit M. Dawant|incollection|DHAESE2018137|||||||||Second Edition|Elliot S. Krames and P. Hunter Peckham and Ali R. Rezai|||||||||||||||||||1981949307|42
SIGMOD '15|Melbourne, Victoria, Australia|rule management, big data, classification|12|265–276|Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data|Big Data industrial systems that address problems such as classification, information extraction, and entity matching very commonly use hand-crafted rules. Today, however, little is understood about the usage of such rules. In this paper we explore this issue. We discuss how these systems differ from those considered in academia. We describe default solutions, their limitations, and reasons for using rules. We show examples of extensive rule usage in industry. Contrary to popular perceptions, we show that there is a rich set of research challenges in rule generation, evaluation, execution, optimization, and maintenance. We discuss ongoing work at WalmartLabs and UW-Madison that illustrate these challenges. Our main conclusions are (1) using rules (together with techniques such as learning and crowdsourcing) is fundamental to building semantics-intensive Big Data systems, and (2) it is increasingly critical to address rule management, given the tens of thousands of rules industrial systems often manage today in an ad-hoc fashion.|10.1145/2723372.2742784|https://doi.org/10.1145/2723372.2742784|New York, NY, USA|Association for Computing Machinery|9781450327589|2015|Why Big Data Industrial Systems Need Rules and What We Can Do About It|G.C., Paul Suganthan and Sun, Chong and K., Krishna Gayatri and Zhang, Haojun and Yang, Frank and Rampalli, Narasimhan and Prasad, Shishir and Arcaute, Esteban and Krishnan, Ganesh and Deep, Rohit and Raghavendra, Vijay and Doan, AnHai|inproceedings|10.1145/2723372.2742784|||||||||||||||||||||||||||||1983392889|42
||Big data, Multi-agent systems, Wireless sensor network, Fuzzy logic, Smart data||465-478||The era of big data has brought new challenges in data processing ad management. Existing analytical tools are now close to facing ongoing challenges thus providing satisfactory results at a reasonable cost. However, the velocity at which new data are flooded and the noise generated from such a large volume leads to various new challenges. The present research combines two artificial intelligence fields the represented by multi-agent technologies and fuzzy logic inference systems in order to extract the needed smart data from big noisy ones. A multi-fuzzy agent-based large-scale wireless sensor network has been used to demonstrate the effectiveness of the proposed approach. It handles sensors as autonomous fuzzy agents to measure the relevance of the collected data and eliminate the irrelevant ones. The results of the simulation exhibit a high quality of the data with a decrease in the sensors energy consumption, leading to a longer lifetime of the network.|https://doi.org/10.1016/j.jksuci.2019.05.009|https://www.sciencedirect.com/science/article/pii/S1319157819302010||||2020|A fuzzy agent approach for smart data extraction in big data environments|Zakarya Elaggoune and Ramdane Maamri and Imane Boussebough|article|ELAGGOUNE2020465|||Journal of King Saud University - Computer and Information Sciences|13191578|4|32||Emerging Software Systems|||||||||||||||||||||1985691277|1257083324
||Data integrity;Big Data;Tools;Data models;Standards;Machine learning;Couplings;Data accuracy;contextual accuracy;data quality;word embeddings;record linkage;k-nearest neighbors;logistic regression;decision trees||193-197|2019 IEEE 4th International Conference on Big Data Analytics (ICBDA)|Data analysis is the most important aspect of any business as it is critical to decision-making. Data quality assessment is a necessary function to be performed before data analysis, as the quality of data has high impact on the outcome of the analysis. Data quality is a multi-dimensional factor that affects the analysis in numerous ways. Among all the dimensions, accuracy is the most important and hardest dimension to assess. With the advent of big data, this problem becomes more complicated. There are only few studies that focus on data accuracy with minimal domain expert dependency. In this paper, we propose an extensive data accuracy assessment tool that uses machine learning to determine the accuracy of data. In addition, our model also addresses the intrinsic and contextual categories of data accuracy. Our model was developed on Apache Spark which serves as the big data environment for handling large datasets.|10.1109/ICBDA.2019.8713218|||||2019|An Automated Big Data Accuracy Assessment Tool|Mylavarapu, Goutam and Thomas, Johnson P and Viswanathan, K Ashwin|inproceedings|8713218||March|||||||||||||||||||||||||||1985957500|42
||5G technology, Sustainability, Smart building, Facilities management, Build environment||100116||Sustainable and smart building is a recent concept that is gaining momentum in public opinion, and thus, it is making its way into the agendas of researchers and city authorities all over the world. To move towards sustainable development goals, 5G technology would make significant impacts are building construction, operation, and management by facilitating high-class services, providing efficient functionalities. It's well known that the Singapore is one of top smart cities in this world and from the first counties that adopted of 5G technology in various sectors including smart buildings. Based on these facts, this paper discusses the international trends in 5G applications for smart buildings, and R&D and test bedding works conducted in 5G labs. As well as, the manuscript widely reviewed and discussed the 5G technology development, use cases, applications and future projects which supported by Singapore government. Finally, the 5G use cases for smart buildings and build environment improvement application were discussed. This study can serve as a benchmark for researchers and industries for the future progress and development of smart cities in the context of big data.|https://doi.org/10.1016/j.egyai.2021.100116|https://www.sciencedirect.com/science/article/pii/S2666546821000653||||2022|A review on 5G technology for smart energy management and smart buildings in Singapore|Ghasan Fahim Huseien and Kwok Wei Shah|article|HUSEIEN2022100116|||Energy and AI|26665468||7|||||||||||||||||||||||1986883843|492960112
||Big data;Cloud computing;Organizations;Face;Conferences;Abstracts;Instruments||91-91|2014 IEEE 18th International Enterprise Distributed Object Computing Conference|Summary form only given. Instrumentation of processes and an organization's environment provides vast amounts of data that can be used to drivedecisions. Next to setting up data collection, supervising data quality, and applying proper methods of analysis, organizationsface the challenge to set up an infrastructure and architecture to do so efficiently and cost-effectively. Virtualized platformssuch as private or public clouds are the method of choice for deployment, in particular for data analyses not occurringconstantly. A cloud provider, either a commercial Cloud company or an IT organization within an enterprise, will like to set upa cloud platform such that clients can run big data workloads effectively on. Cloud customers would like to set up big dataapplications in a cost-effective and performant way on their platform.This keynote will walk through a few real life big data analysis scenarios from different industries and discuss thechallenges Cloud providers face making trade-offs. Understanding those challenges and solutions help cloud users choose theright match between their algorithm, big data system and cloud platform.|10.1109/EDOC.2014.21|||||2014|Managing Big Data Effectively - A Cloud Provider and a Cloud Consumer Perspective|Ludwig, Heiko|inproceedings|6972054||Sep.||15417719|||||||21100384313|0,232|-|16|18|95|393|87|86|0,77|21,83|United States|Northern America|1998, 2014, 2019|Computer Networks and Communications; Computer Science Applications; Hardware and Architecture; Software; Theoretical Computer Science||||1989072651|1072795039
ICEMIS '18|Istanbul, Turkey|Cloud Computing, Data Warehouse, Big Data, Business Intelligence|9||Proceedings of the Fourth International Conference on Engineering &amp; MIS 2018|Business intelligence suppose retrieving value from data floating in the organization environment. It provides methods and tools for collecting, storing, formatting and analyzing data for the purpose of helping managers in decision-making. At the start, only data from enterprise internal activities were examined. Now and in this turbulent business environment, organizations should incorporate analysis of the huge amount of external data gathered from multifarious sources. It is argued that BI systems accuracy depends on quantity of data at their disposal, yet some storage and analysis methods are phased out and should be reviewed by academics and practitioners.This paper presents an overview of BI challenges in the context of Big Data (BD) and some available solutions provided, either by using Cloud Computing (CC) or improving Data Warehouse (DW) efficiency.|10.1145/3234698.3234723|https://doi.org/10.1145/3234698.3234723|New York, NY, USA|Association for Computing Machinery|9781450363921|2018|Investigating Business Intelligence in the Era of Big Data: Concepts, Benefits and Challenges|El Bousty, Hicham and krit, Salah-ddine and Elasikri, Mohamed and Dani, Hassan and Karimi, Khaoula and Bendaoud, Kaoutar and Kabrane, Mustapha|inproceedings|10.1145/3234698.3234723|25||||||||||||||||||||||||||||1991182374|42
||Data stream, Linked Data, Business Intelligence, Stream reasoning||100-107||The Semantic Web technologies are being increasingly used for exploiting relations between data. In addition, new tendencies of real-time systems, such as social networks, sensors, cameras or weather information, are continuously generating data. This implies that data and links between them are becoming extremely vast. Such huge quantity of data needs to be analyzed, processed, as well as stored if necessary. In this position paper, we will introduce recent work on Real-Time Business Intelligence combined with semantic data stream management. We will present underlying approaches such as continuous queries, data summarization and matching, and stream reasoning.|https://doi.org/10.1016/j.future.2015.11.015|https://www.sciencedirect.com/science/article/pii/S0167739X15003635||||2016|From Business Intelligence to semantic data stream management|Marie-Aude Aufaure and Raja Chiky and Olivier Curé and Houda Khrouf and Gabriel Kepeklian|article|AUFAURE2016100|||Future Generation Computer Systems|0167739X||63||Modeling and Management for Big Data Analytics and Visualization|||12264|1,262|Q1|119|798|1935|36054|16877|1868|9,11|45,18|Netherlands|Western Europe|1984-2021|Computer Networks and Communications (Q1); Hardware and Architecture (Q1); Software (Q1)||||1994685952|562237118
DIS '19|San Diego, CA, USA|qualitative dataanalysis, public inpu, community engagement, digital civics|11|1171–1181|Proceedings of the 2019 on Designing Interactive Systems Conference|Advancements in digital civics have enabled leaders to engage and gather input from a broader spectrum of the public. However, less is known about the analysis process around community input and the challenges faced by civic leaders as engagement practices scale up. To understand these challenges, we conducted 21 interviews with leaders on civic-oriented projects. We found that at a small-scale, civic leaders manage to facilitate sensemaking through collaborative or individual approaches. However, as civic leaders scale engagement practices to account for more diverse perspectives, making sense of the large quantity of qualitative data becomes a challenge. Civic leaders could benefit from training in qualitative data analysis and simple, scalable collaborative analysis tools that would help the community form a shared understanding. Drawing from these insights, we discuss opportunities for designing tools that could improve civic leaders' ability to utilize and reflect public input in decisions.|10.1145/3322276.3322354|https://doi.org/10.1145/3322276.3322354|New York, NY, USA|Association for Computing Machinery|9781450358507|2019|The Civic Data Deluge: Understanding the Challenges of Analyzing Large-Scale Community Input|Mahyar, Narges and Nguyen, Diana V. and Chan, Maggie and Zheng, Jiayi and Dow, Steven P.|inproceedings|10.1145/3322276.3322354|||||||||||||||||||||||||||||1995988493|42
||Artificial intelligence, Dermatology, Skin image, Chinese Skin Image Database||56-60||After more than 60 years of development, artificial intelligence (AI) has been widely used in various fields. Especially in recent years, with the development of deep learning, AI has made many remarkable achievements in the medical field. Dermatology, as a clinical discipline with morphology as its main feature, is particularly suitable for the development of AI. The rapid development of skin imaging technology has helped dermatologists to assist in the diagnosis of diseases and has greatly improved the accuracy of diagnosis. Skin imaging data have natural big data attributes, which is important for AI research. The establishment of the Chinese Skin Image Database (CSID) has solved many problems such as isolated data islands and inconsistent data quality. Based on the CSID, many pioneering achievements have been made in the research and development of AI-assisted decision-making software, the establishment of expert organizations, personnel training, scientific research, and so on. At present, there are still many problems with AI in the field of dermatology, such as clinical validation, medical device licensing, interdisciplinary, and standard formulation, which urgently need to be solved by joint efforts of all parties.|https://doi.org/10.1016/j.imed.2021.04.003|https://www.sciencedirect.com/science/article/pii/S266710262100005X||||2021|Construction of an artificial intelligence system in dermatology: effectiveness and consideration of Chinese Skin Image Database (CSID)|Chengxu Li and Wenmin Fei and Yang Han and Xiaoli Ning and Ziyi Wang and Keke Li and Ke Xue and Jingkai Xu and Ruixing Yu and Rusong Meng and Feng Xu and Weimin Ma and Yong Cui|article|LI202156|||Intelligent Medicine|26671026|2|1|||||||||||||||||||||||1997454361|714385722
||||838-839|||https://doi.org/10.1016/j.jtos.2019.07.010|https://www.sciencedirect.com/science/article/pii/S1542012419301740||||2019|The perils and pitfalls of big data analysis in medicine|C.J. Puranik and Sreenivasa Rao and S. Chennamaneni|article|PURANIK2019838|||The Ocular Surface|15420124|4|17|||||130050|3,505|Q1|65|120|234|5895|1932|204|3,81|49,13|United States|Northern America|2003-2020|Ophthalmology (Q1)|4,145|5.033|0.00623|1998911369|715290072
AICS 2019|Wuhan, Hubei, China|agro-meteorological disasters, early warning, big data, framework, quality control|5|74–78|Proceedings of the 2019 International Conference on Artificial Intelligence and Computer Science|Agricultural meteorological disasters, including floods, droughts, dry hot winds, low temperature chills, typhoons, hail and continuous rain, can lead to significant reduction in agricultural output. Big data platform for early warning of agricultural meteorological disaster is the basis of business operation system for early warning of agricultural meteorological disasters, and the data quality is an important guarantee for success of the early warning. Quality control of big data for early warning of agricultural meteorological disaster involves names of data sets, metadata, data documents and content of data sets. The quality control for contents of data sets is divided into quality control of attribute data and that of spatial data, and quality control of spatial data is divided into quality control of vector data and that of raster data. Methods for data quality control are divided into fully automatic, semi-automatic and full manual control methods.|10.1145/3349341.3349371|https://doi.org/10.1145/3349341.3349371|New York, NY, USA|Association for Computing Machinery|9781450371506|2019|Quality Control Framework of Big Data for Early Warning of Agricultural Meteorological Disasters|Li, Jiale and Liao, Shunbao|inproceedings|10.1145/3349341.3349371|||||||||||||||||||||||||||||2000326470|42
||Connected vehicles, V2I communication, Automotive big data, Big data architecture, Groupe PSA||374-387||Nowadays, using their onboard built-in sensors and communication devices, connected vehicles (CVs) can perform numerous measurements (speed, temperature, fuel consumption, etc.) and transmit them, in a real-time fashion, to dedicated infrastructure, usually via 4G/5G wireless communications. This raises many opportunities to develop new innovative telematics services, including, among others, driver safety, customer experience, location-based services and infotainment. Indeed, it is expected that there will be roughly 2 billion connected cars by the end of 2025 on the world’s roadways, where each of which can produce up to 30 terabytes of data per day. Managing this big automotive data, in real and batch modes, imposes tight constraints on the underlying data management platform. To contribute to this research area, in this paper, we report on a real, in-production automotive big data platform; specifically, the one deployed by Groupe PSA (a French car manufacturer known also as Peugeot-Citroën). In particular, we present the technologies and open-source products used within the different components of this CV platform to gather, store, process, and leverage big automotive data. The proposed architecture is then assessed through realistic experiments, and the obtained results are reported and analyzed. Finally, we also provide examples of deployed automotive applications and reveal the implementation details of one of them (an eco-driving service).|https://doi.org/10.1016/j.future.2022.04.020|https://www.sciencedirect.com/science/article/pii/S0167739X22001492||||2022|Big data architecture for connected vehicles: Feedback and application examples from an automotive group|Ahmed Mostefaoui and Mohammed Amine Merzoug and Amir Haroun and Anthony Nassar and François Dessables|article|MOSTEFAOUI2022374|||Future Generation Computer Systems|0167739X||134|||||12264|1,262|Q1|119|798|1935|36054|16877|1868|9,11|45,18|Netherlands|Western Europe|1984-2021|Computer Networks and Communications (Q1); Hardware and Architecture (Q1); Software (Q1)||||2000370246|562237118
||Data integrity;Knowledge based systems;Big Data;Cleaning;Classification algorithms;Noise measurement;knowledge graph;graph embedding;big data;data cleaning;data quality||539-542|2022 5th International Conference on Artificial Intelligence and Big Data (ICAIBD)|Data quality of knowledge graph is a one of the most important guareentees for many knowledge-based applications. We investigate the konwledge graph cleaning problem. We propose a knowledge graph error detection framework and design an optimized embedding based clean algorithm. The framework maps the knowledge graph into an numerical space and keeps the relationship between different nodes. With this framework, both miss data error and errous relationship can be cleaned. Extensive experimental study over different data sets validate the effectiveness of the method.|10.1109/ICAIBD55127.2022.9820258|||||2022|An optimized Graph Embedding based Knowledge Graph Cleaning Algorithm|He, Min|inproceedings|9820258||May|||||||||||||||||||||||||||2002122281|42
LAK '16|Edinburgh, United Kingdom|ethics, educational data mining, learning analytics, implementation, privacy, legal aspects, trust, data management|10|89–98|Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge|The widespread adoption of Learning Analytics (LA) and Educational Data Mining (EDM) has somewhat stagnated recently, and in some prominent cases even been reversed following concerns by governments, stakeholders and civil rights groups about privacy and ethics applied to the handling of personal data. In this ongoing discussion, fears and realities are often indistinguishably mixed up, leading to an atmosphere of uncertainty among potential beneficiaries of Learning Analytics, as well as hesitations among institutional managers who aim to innovate their institution's learning support by implementing data and analytics with a view on improving student success. In this paper, we try to get to the heart of the matter, by analysing the most common views and the propositions made by the LA community to solve them. We conclude the paper with an eight-point checklist named DELICATE that can be applied by researchers, policy makers and institutional managers to facilitate a trusted implementation of Learning Analytics.|10.1145/2883851.2883893|https://doi.org/10.1145/2883851.2883893|New York, NY, USA|Association for Computing Machinery|9781450341905|2016|Privacy and Analytics: It's a DELICATE Issue a Checklist for Trusted Learning Analytics|Drachsler, Hendrik and Greller, Wolfgang|inproceedings|10.1145/2883851.2883893|||||||||||||||||||||||||||||2002301431|42
||||18-20||After its people, data is arguably an organisation's most valuable asset. According to recent figures by the Networked Systems and Services department at SINTEF, some 90% of all the data in the world has been created in the past two years. That shouldn't be too much of a surprise to us given the data-driven world we now live in; but what is promising is that companies are increasingly switching on to its strategic and commercial value. The challenge is how do we extract value from this data in a way that empowers people and organisations alike?|https://doi.org/10.1016/S1353-4858(15)30051-9|https://www.sciencedirect.com/science/article/pii/S1353485815300519||||2015|The data quality paradox|Boris Huard|article|HUARD201518|||Network Security|13534858|6|2015|||||27334|0,177|Q4|24|50|155|295|145|155|0,90|5,90|Netherlands|Western Europe|1994-2020|Computer Networks and Communications (Q4); Information Systems and Management (Q4); Safety, Risk, Reliability and Quality (Q4)||||2008662607|1035914955
||Virtual and real fusion learning, Big data learning and analysis models, Digital twin, Industrial internet, Smart manufacturing||16-32||Digital twin takes Industrial Internet as a carrier deeply coordinating and integrating virtual spaces with physical spaces, which effectively promotes smart factory development. Digital twin-based big data learning and analysis (BDLA) deepens virtual and real fusion, interaction and closed-loop iterative optimization in smart factories. This paper proposes a digital twin-based big data virtual and real fusion (DT-BDVRL) reference framework supported by Industrial Internet towards smart manufacturing. The reference framework is synthetically designed from three perspectives. The first one is an overall framework of DT-BDVRL supported by Industrial Internet. The second one is the establishment method and flow of BDLA models based on digital twin. The final one is digital thread of DT-BDVRL in virtual and real fusion analysis, iteration and closed-loop feedback in product full life cycle processes. For different virtual scenes, iterative optimization and verification methods and processes of BDLA models in virtual spaces are established. Moreover, the BDLA results can drive digital twin running in virtual spaces. By this, the BDLA results can be validated iteratively multiple times in virtual spaces. At same time, the BDLA results that run in virtual spaces are synchronized and executed in physical spaces through Industrial Internet platforms, effectively improving the physical execution effect of BDLA models. Finally, the above contents were applied and verified in the actual production case study of power switchgear equipment.|https://doi.org/10.1016/j.jmsy.2020.11.012|https://www.sciencedirect.com/science/article/pii/S0278612520301990||||2021|A digital twin-based big data virtual and real fusion learning reference framework supported by industrial internet towards smart manufacturing|Pei Wang and Ming Luo|article|WANG202116|||Journal of Manufacturing Systems|02786125||58|||||14966|2,310|Q1|70|155|294|8906|2949|288|10,88|57,46|Netherlands|Western Europe|1982-2020|Control and Systems Engineering (Q1); Hardware and Architecture (Q1); Industrial and Manufacturing Engineering (Q1); Software (Q1)|5,413|8.633|0.00561|2010749121|619364890
APIT 2019|Jeju Island, Republic of Korea|Knowledge Management, Data Science, Higher Education Data|4|61–64|Proceedings of the 2019 Asia Pacific Information Technology Conference|Education is an important element towards learning and human development, thus, it is the key towards identifying competencies and better productivity for the workforce. As part of the Commission on Higher Education's (CHED) thrust for improving efficiency and effectiveness by simplifying the collection process for all the stakeholders, the developed system will drastically improve the availability of data for making informed decisions and efficient generation of reports.This research outlines opportunities and challenges associated with the implementation and governance of Big Data in higher education through development and implementation of data analytics tool.|10.1145/3314527.3314537|https://doi.org/10.1145/3314527.3314537|New York, NY, USA|Association for Computing Machinery|9781450366212|2019|Analytical Visualization of Higher Education Institutions' Big Data for Decision Making|Cabanban-Casem, Christianne Lynnette|inproceedings|10.1145/3314527.3314537|||||||||||||||||||||||||||||2013023794|42
|||4|1605–1608||"\"Data quality is one of the most important problems in data management, since dirty data often leads to inaccurate data analytics results and wrong business decisions.Data cleaning exercise often consist of two phases: error detection and error repairing. Error detection techniques can either be quantitative or qualitative; and error repairing is performed by applying data transformation scripts or by involving human experts, and sometimes both.In this tutorial, we discuss the main facets and directions in designing qualitative data cleaning techniques. We present a taxonomy of current qualitative error detection techniques, as well as a taxonomy of current data repairing techniques. We will also discuss proposals for tackling the challenges for cleaning \"\"big data\"\" in terms of scale and distribution.\""|10.14778/3007263.3007320|https://doi.org/10.14778/3007263.3007320||VLDB Endowment||2016|Qualitative Data Cleaning|Chu, Xu and Ilyas, Ihab F.|article|10.14778/3007263.3007320||sep|Proc. VLDB Endow.|21508097|13|9|September 2016||||21100199855|0,946|Q1|134|246|598|12401|3759|566|5,50|50,41|United States|Northern America|2008-2020|Computer Science (miscellaneous) (Q1)|7,198|2.047|0.00891|2013478044|1216159931
||Genomics;Bioinformatics;Databases;Sequential analysis;Runtime;Biological cells;Instruction sets;Genome Data Analysis;Variant Calling;Single Nucleotide Polymorphism;In-Memory Database Technology;Next-Generation Sequencing||27-32|2014 IEEE International Conference on Big Data (Big Data)|Next-generation sequencing enables whole genome sequencing within a few hours at a minimum of cost, entailing advanced medical applications such as personalized treatments. However, this recent technology imposes new challenges to alignment and variant calling as subsequent analysis steps. Compared to former sequencing, both must deal with an increasing amount of data to process at a significantly lower data quality - and are currently not capable of that. In this work, we focus on addressing these challenges for identifying Single Nucleotide Polymorphisms, i.e. SNP calling, in genome data as one subtask of variant calling. We propose the application of a column-store in-memory database for efficient data processing and apply the statistical model that is provided by the Genome Analysis Toolkit's UnifiedGenotyper. Comparisons with the UnifiedGenotyper show that our approach can exploit all computational resources available and accelerates SNP calling up to a factor of 22x.|10.1109/BigData.2014.7004389|||||2014|Towards integrating the detection of genetic variants into an in-memory database|Fähnrich, Cindy and Schapranow, Matthieu-P. and Plattner, Hasso|inproceedings|7004389||Oct|||||||||||||||||||||||||||2016797083|42
https://doi.org/10.1016/j.clinthera.2022.01.012|https://www.sciencedirect.com/science/article/pii/S0149291822000170||||2022|Real-World Evidence for Regulatory Decision-Making: Guidance From Around the World|Leah Burns and Nadege Le Roux and Robert Kalesnik-Orszulak and Jennifer Christian and Mathias Hukkelhoven and Frank Rockhold and John O'Donnell|article|BURNS2022420|||Clinical Therapeutics|01492918|3|44||||||||||||||||||||||||||||||2017746246|42
ICC '16|Cambridge, United Kingdom|Big Environmental and Atmospheric Data, Big Data Mining, Clustering-Based Spatio-Temporal Analysis of Big Data|8||Proceedings of the International Conference on Internet of Things and Cloud Computing|This paper proposes a comprehensive approach for supporting clustering-based spatio-temporal analysis of big atmospheric data via specializing on the interesting applicative setting represented by Greenhouse Gas Emissions (GGEs), a relevant instance of Big Data that empathize the Variety aspect of the well-known 3V Big Data axioms. In particular, in our research we consider GGEs from three EU countries, namely UK, France and Italy. The deriving Big Data Mining model turns to be useful for decision support processes in both the governmental and industrial contexts.|10.1145/2896387.2900326|https://doi.org/10.1145/2896387.2900326|New York, NY, USA|Association for Computing Machinery|9781450340632|2016|Clustering-Based Spatio-Temporal Analysis of Big Atmospheric Data|Cuzzocrea, Alfredo and Gaber, Mohamed Medhat and Lattimer, Staci and Grasso, Giorgio Mario|inproceedings|10.1145/2896387.2900326|74||||||||||||||||||||||||||||2022747075|42
||NEXRAD, Rainfall, Cloud computing, Level II data, Hydrology||69-75||The Iowa Flood Center (IFC) developed a pilot infrastructure to explore rainfall metadata (descriptive statistics) and generate rainfall products over the Iowa domain based on the NEXRAD Level II data directly accessible through cloud storage (e.g., Amazon Web Services). Known as IFC-Cloud-NEXRAD, it resembles the Hydro-NEXRAD portal that provided researchers with ready access to NEXRAD radar data. Taking advantage of the cloud storage benefits (unlimited storage and instant access), IFC-Cloud-NEXRAD reduces the common challenges of most data exploration systems, which often lead to massive data acquisition/ingestion and rapid filling of limited system storage. Its map-based interface allows researchers to select a space-time domain of interest, retrieve and visualize pre-calculated rainfall metadata, and generate radar-derived rainfall products. Because the system provides generalized approaches to compute metadata and process data for rainfall estimation, the framework presented in this study would be readily transferrable to other geographic regions and larger scale applications.|https://doi.org/10.1016/j.envsoft.2019.03.008|https://www.sciencedirect.com/science/article/pii/S1364815218307667||||2019|A pilot infrastructure for searching rainfall metadata and generating rainfall product using the big data of NEXRAD|Bong-Chul Seo and Munsung Keem and Raymond Hammond and Ibrahim Demir and Witold F. Krajewski|article|SEO201969|||Environmental Modelling & Software|13648152||117|||||23295|1,828|Q1|136|223|717|14218|4373|711|5,47|63,76|Netherlands|Western Europe|1997-2020|Ecological Modeling (Q1); Environmental Engineering (Q1); Software (Q1)||||2024422165|665084965
||Deep learning, entity resolution, entity matching, data matching, record linkage|37|||Entity matching is the problem of identifying which records refer to the same real-world entity. It has been actively researched for decades, and a variety of different approaches have been developed. Even today, it remains a challenging problem, and there is still generous room for improvement. In recent years, we have seen new methods based upon deep learning techniques for natural language processing emerge. In this survey, we present how neural networks have been used for entity matching. Specifically, we identify which steps of the entity matching process existing work have targeted using neural networks, and provide an overview of the different techniques used at each step. We also discuss contributions from deep learning in entity matching compared to traditional methods, and propose a taxonomy of deep neural networks for entity matching.|10.1145/3442200|https://doi.org/10.1145/3442200|New York, NY, USA|Association for Computing Machinery||2021|Neural Networks for Entity Matching: A Survey|Barlaug, Nils and Gulla, Jon Atle|article|10.1145/3442200|52|apr|ACM Trans. Knowl. Discov. Data|15564681|3|15|June 2021||||5800173377|0,728|Q1|59|71|169|3729|816|168|4,54|52,52|United States|Northern America|2007-2020|Computer Science (miscellaneous) (Q1)|1,740|2.713|0.00224|2026932358|1302859451
||||16-17|||https://doi.org/10.1016/j.denabs.2015.10.005|https://www.sciencedirect.com/science/article/pii/S0011848615010043||||2016|Clinical research and big data||article|201616|||Dental Abstracts|00118486|1|61|||||||||||||||||||||||2028475314|1120569248
||Intelligent agents;Predictive models;Analytical models;Market research;Conferences;Data mining;Software Agents;Intelligent Agents;Predictive Analytics||3715-3718|2016 3rd International Conference on Computing for Sustainable Global Development (INDIACom)|The arrival of World Wide Web has led to the explosive growth of information on the web. There is a sudden boom in quality of raw data/information, which is commonly referred as Big Data. Very often, this raw information contains very useful insights which are ignored most of the time and are difficult to analyze due to the enormous size of these datasets. The feasibility for human to extract this information from the vast web and build useful application on the top of it, is very low. Hence to create predictive models, there is a huge need for intelligent and autonomous software agents which can procure useful information from the large datasets of raw information. Predictive analytics models can be created from these datasets which can be further used for various applications in security, future prediction etc. This research paper gives an overview of how these software agents will become the most important tools in coming days for building predictive models.||||||2016|Design and development of an intelligent agent based framework for predictive analytics|Bhargava, Deepshikha and Poonia, Ramesh C. and Arora, Upma|inproceedings|7724956||March|||||||||||||||||||||||||||2029124801|42
MiSE '15|Florence, Italy|quality assurance, big data, model-driven engineering|6|78–83|Proceedings of the Seventh International Workshop on Modeling in Software Engineering|Model-driven engineering (MDE) often features quality assurance (QA) techniques to help developers creating software that meets reliability, efficiency, and safety requirements. In this paper, we consider the question of how quality-aware MDE should support data-intensive software systems. This is a difficult challenge, since existing models and QA techniques largely ignore properties of data such as volumes, velocities, or data location. Furthermore, QA requires the ability to characterize the behavior of technologies such as Hadoop/MapReduce, NoSQL, and stream-based processing, which are poorly understood from a modeling standpoint. To foster a community response to these challenges, we present the research agenda of DICE, a quality-aware MDE methodology for data-intensive cloud applications. DICE aims at developing a quality engineering tool chain offering simulation, verification, and architectural optimization for Big Data applications. We overview some key challenges involved in developing these tools and the underpinning models.||||IEEE Press||2015|DICE: Quality-Driven Development of Data-Intensive Cloud Applications|Casale, G. and Ardagna, D. and Artac, M. and Barbier, F. and Nitto, E. Di and Henry, A. and Iuhasz, G. and Joubert, C. and Merseguer, J. and Munteanu, V. I. and P\'{e}rez, J. F. and Petcu, D. and Rossi, M. and Sheridan, C. and Spais, I. and Vladu\v{s}i\v{c}, D.|inproceedings|10.5555/2820489.2820507|||||||||||||||||||||||||||||2029362182|42
||Unified modeling language;Big data;Data models;Computational modeling;Analytical models;Reliability;Software;Big Data;quality assurance;model-driven engineering||78-83|2015 IEEE/ACM 7th International Workshop on Modeling in Software Engineering|Model-driven engineering (MDE) often features quality assurance (QA) techniques to help developers creating software that meets reliability, efficiency, and safety requirements. In this paper, we consider the question of how quality-aware MDE should support data-intensive software systems. This is a difficult challenge, since existing models and QA techniques largely ignore properties of data such as volumes, velocities, or data location. Furthermore, QA requires the ability to characterize the behavior of technologies such as Hadoop/MapReduce, NoSQL, and stream-based processing, which are poorly understood from a modeling standpoint. To foster a community response to these challenges, we present the research agenda of DICE, a quality-aware MDE methodology for data-intensive cloud applications. DICE aims at developing a quality engineering tool chain offering simulation, verification, and architectural optimization for Big Data applications. We overview some key challenges involved in developing these tools and the underpinning models.|10.1109/MiSE.2015.21|||||2015|DICE: Quality-Driven Development of Data-Intensive Cloud Applications|Casale, Giuliano and Ardagna, Danilo and Artac, Matej and Barbier, Franck and Di Nitto, Elisabetta and Henry, Alexis and Iuhasz, Gabriel and Joubert, Christophe and Merseguer, Jose and Munteanu, Victor Ion and Perez, Juan Fernando and Petcu, Dana and Rossi, Matteo and Sheridan, Craig and Spais, Ilias and Vladuic, Daniel|inproceedings|7167407||May||21567891|||||||21100211110|0,197|-|19|0|26|0|79|23|3,04|0,00|United States|Northern America|2012, 2013, 2019|Hardware and Architecture; Software||||2029362182|480564892
||Big Data;Training;Data mining;Cleaning;Sampling methods;Heuristic algorithms;Fault tolerance;data mining;conditional functional dependency;big data;data quality||68-84||Current Conditional Functional Dependency (CFD) discovery algorithms always need a well-prepared training dataset. This condition makes them difficult to apply on large and low-quality datasets. To handle the volume issue of big data, we develop the sampling algorithms to obtain a small representative training set. We design the fault-tolerant rule discovery and conflict-resolution algorithms to address the low-quality issue of big data. We also propose parameter selection strategy to ensure the effectiveness of CFD discovery algorithms. Experimental results demonstrate that our method can discover effective CFD rules on billion-tuple data within a reasonable period.|10.26599/BDMA.2019.9020019|||||2020|Mining conditional functional dependency rules on big data|Li, Mingda and Wang, Hongzhi and Li, Jianzhong|article|8935096||March|Big Data Mining and Analytics|20960654|1|3|||||||||||||||||||||||2030989361|417390997
||Master data, Data quality, ISO 8000, Big data||94-104||During the execution of business processes involving various organizations, Master Data is usually shared and exchanged. It is necessary to keep appropriate levels of quality in these Master Data in order to prevent problems in the business processes. Organizations can be benefitted from having information about the level of quality of master data along with the master data to support decision about the usage of data in business processes is to include information about the level of quality alongside the Master Data. ISO 8000-1x0 specifies how to add this information to the master data messages. From the clauses stated in the various part of standard we developed a reference architecture, enhanced with big data technologies to better support the management of large datasets The main contribution of this paper is a service architecture for Master Data Exchange supporting the requirements stated by the different parts of the standard like the development of a data dictionary with master data terms; a communication protocol; an API to manage the master data messages; and the algorithms in MapReduce to measure the data quality.|https://doi.org/10.1016/j.csi.2016.10.004|https://www.sciencedirect.com/science/article/pii/S0920548916301192||||2017|Towards a service architecture for master data exchange based on ISO 8000 with support to process large datasets|Bibiano Rivas and Jorge Merino and Ismael Caballero and Manuel Serrano and Mario Piattini|article|RIVAS201794|||Computer Standards & Interfaces|09205489||54||SI: New modeling in Big Data|||24303|0,556|Q1|63|36|246|2302|1013|243|3,71|63,94|Netherlands|Western Europe|1985-2020|Law (Q1); Hardware and Architecture (Q2); Software (Q2)||||2038826568|827980402
||social sensing, smart cities, Mobile crowed sensing|21|||A considerable amount of research has addressed Internet of Things and connected communities. It is possible to exploit the sensing capabilities of connected communities, by leveraging the continuously growing use of cloud computing solutions and mobile devices. The pervasiveness of mobile sensors also enables the Mobile Crowd Sensing (MCS) paradigm, which aims at using mobile-embedded sensors to extend monitoring of multiple (environmental) phenomena in expansive urban areas. In this article, we discuss our approach with a cloud-based platform to pave the way for applying crowd sensing in urban scenarios. We have implemented a complete solution for environmental monitoring of several pollutants, like noise, air, electromagnetic fields, and so on in an urban area based on this paradigm. Through extensive experimentation, specifically on noise pollution, we show how the proposed infrastructure exhibits the ability to collect data from connected communities, and enables a seamless support of services needed for improving citizens’ quality of life and eventually helps city decision makers in urban planning.|10.1145/3093895|https://doi.org/10.1145/3093895|New York, NY, USA|Association for Computing Machinery||2017|Crowd-Sourced Data Collection for Urban Monitoring via Mobile Sensors|Longo, Antonella and Zappatore, Marco and Bochicchio, Mario and Navathe, Shamkant B.|article|10.1145/3093895|5|oct|ACM Trans. Internet Technol.|15335399|1|18|February 2018||||||||||||||||||||||2040420670|314651938
||Internet of Things (IoT), Cloud computing, Fog computing, Big data analytics, Energy management, Smart homes||563-573||Internet of Things (IoT) analytics is an essential mean to derive knowledge and support applications for smart homes. Connected appliances and devices inside the smart home produce a significant amount of data about consumers and how they go about their daily activities. IoT analytics can aid in personalizing applications that benefit both homeowners and the ever growing industries that need to tap into consumers profiles. This article presents a new platform that enables innovative analytics on IoT captured data from smart homes. We propose the use of fog nodes and cloud system to allow data-driven services and address the challenges of complexities and resource demands for online and offline data processing, storage, and classification analysis. We discuss in this paper the requirements and the design components of the system. To validate the platform and present meaningful results, we present a case study using a dataset acquired from real smart home in Vancouver, Canada. The results of the experiments show clearly the benefit and practicality of the proposed platform.|https://doi.org/10.1016/j.future.2018.08.040|https://www.sciencedirect.com/science/article/pii/S0167739X18311099||||2019|IoT big data analytics for smart homes with fog and cloud computing|Abdulsalam Yassine and Shailendra Singh and M. Shamim Hossain and Ghulam Muhammad|article|YASSINE2019563|||Future Generation Computer Systems|0167739X||91|||||12264|1,262|Q1|119|798|1935|36054|16877|1868|9,11|45,18|Netherlands|Western Europe|1984-2021|Computer Networks and Communications (Q1); Hardware and Architecture (Q1); Software (Q1)||||2049550019|562237118
||Big data, VRF systems, Operational performance, Statistical analysis||105219||With the increase in the energy consumption of air-conditioning (AC) systems in Chinese residential buildings, the realization of energy savings in AC systems has attracted increasing attention. The variable refrigerant flow (VRF) system is a common AC system for residential buildings in China. In most previous studies on VRF systems, onsite measurements or surveys were conducted to collect operational data. These traditional methods may face various data issues, such as limited sample sizes and invalid data, making them unable to capture the spatial and temporal performance features of VRF systems in residential buildings on a large scale. To fill this gap, with advances in data storage and transmission technology, Big Data methods have been widely used for data collection. In the present study, researchers adopted 16,985 sets of VRF system operation data from China as the database and conducted data analysis for both the spatial and temporal dimensions. Several key indicators were proposed from the two perspectives (spatial and temporal), including the part-space index (PSI), load ratio (LR), use duration (UD), and cooling energy consumption. The main findings were as follows: (1) The “part-time part-space” operation mode of residential VRF systems can be analyzed according to the statistical results of the UD and PSI. (2) An LR of <30% is the main operating condition for VRF systems in residential buildings. (3) Extracted typical LR patterns can reflect different user behavior. The statistical results obtained in this study provide a basis for VRF engineering projects.|https://doi.org/10.1016/j.jobe.2022.105219|https://www.sciencedirect.com/science/article/pii/S2352710222012256||||2022|Investigation of VRF system cooling operation and performance in residential buildings based on large-scale dataset|Hua Liu and Yi Wu and Da Yan and Shan Hu and Mingyang Qian|article|LIU2022105219|||Journal of Building Engineering|23527102||61|||||21100389518|0,974|Q1|39|777|721|38204|4094|720|5,70|49,17|Netherlands|Western Europe|2015-2021|Architecture (Q1); Building and Construction (Q1); Civil and Structural Engineering (Q1); Mechanics of Materials (Q1); Safety, Risk, Reliability and Quality (Q1)|5,990|5.318|0.00753|2050390241|980350148
IDEAS '21|Montreal, QC, Canada|Data management, Systematic mapping, Data lake|5|280–284|Proceedings of the 25th International Database Engineering &amp; Applications Symposium|The computer science community is paying more and more attention to data due to its crucial role in performing analysis and prediction. Researchers have proposed many data containers such as files, databases, data warehouses, cloud systems, and recently data lakes in the last decade. The latter enables holding data in its native format, making it suitable for performing massive data prediction, particularly for real-time application development. Although data lake is well adopted in the computer science industry, its acceptance by the research community is still in its infancy stage. This paper sheds light on existing works for performing analysis and predictions on data placed in data lakes. Our study reveals the necessary data management steps, which need to be followed in a decision process, and the requirements to be respected, namely curation, quality evaluation, privacy-preservation, and prediction. This study aims to categorize and analyze proposals related to each step mentioned above.|10.1145/3472163.3472173|https://doi.org/10.1145/3472163.3472173|New York, NY, USA|Association for Computing Machinery|9781450389914|2021|Data Management in the Data Lake: A Systematic Mapping|Zouari, Firas and Kabachi, Nadia and Boukadi, Khouloud and Ghedira Guegan, Chirine|inproceedings|10.1145/3472163.3472173|||||||||||||||||||||||||||||2050490155|42
SSDBM 2021|Tampa, FL, USA|Knowledge Graphs, Ranking, Question Answering|12|61–72|33rd International Conference on Scientific and Statistical Database Management|The problem of natural language processing over structured data has become a growing research field, both within the relational database and the Semantic Web community, with significant efforts involved in question answering over knowledge graphs (KGQA). However, many of these approaches are either specifically targeted at open-domain question answering using DBpedia, or require large training datasets to translate a natural language question to SPARQL in order to query the knowledge graph. Hence, these approaches often cannot be applied directly to complex scientific datasets where no prior training data is available. In this paper, we focus on the challenges of natural language processing over knowledge graphs of scientific datasets. In particular, we introduce Bio-SODA, a natural language processing engine that does not require training data in the form of question-answer pairs for generating SPARQL queries. Bio-SODA uses a generic graph-based approach for translating user questions to a ranked list of SPARQL candidate queries. Furthermore, Bio-SODA uses a novel ranking algorithm that includes node centrality as a measure of relevance for selecting the best SPARQL candidate query. Our experiments with real-world datasets across several scientific domains, including the official bioinformatics Question Answering over Linked Data (QALD) challenge, as well as the CORDIS dataset of European projects, show that Bio-SODA outperforms publicly available KGQA systems by an F1-score of least 20% and by an even higher factor on more complex bioinformatics datasets.|10.1145/3468791.3469119|https://doi.org/10.1145/3468791.3469119|New York, NY, USA|Association for Computing Machinery|9781450384131|2021|Bio-SODA: Enabling Natural Language Question Answering over Knowledge Graphs without Training Data|Sima, Ana Claudia and Mendes de Farias, Tarcisio and Anisimova, Maria and Dessimoz, Christophe and Robinson-Rechavi, Marc and Zbinden, Erich and Stockinger, Kurt|inproceedings|10.1145/3468791.3469119|||||||||||||||||||||||||||||2050595236|42
||Indexes;Testing;Quantization (signal);Feeds;Blacklisting;Business;Threat Intelligence, Quality Evaluation, User Perspective, Vendor||269-276|2018 17th IEEE International Conference On Trust, Security And Privacy In Computing And Communications/ 12th IEEE International Conference On Big Data Science And Engineering (TrustCom/BigDataSE)|With the widely use of cyber threat intelligence, the influence of security threats and cyber attacks have been relieved and controlled in a degree. More and more users have accepted the conception of threat intelligence and are trying to use threat intelligence in routine security protection. Then, how to choose appropriate threat intelligence vendors and services has become a crucial issue. The present research of threat intelligence evaluation mainly focused on one-sided threat intelligence contents and approaches, which was lack of comprehensiveness and effectiveness. Aiming at this situation, we propose the comprehensive evaluation architecture of threat intelligence in user perspective to evaluate threat intelligence services in several dimensions with quantitative index system. We also carried out typical experiments for threat intelligence data feeds and comprehensive situation to verify the feasibility of proposed method. The results show that the proposed evaluation method has a clear advantage in coverage and partition degree.|10.1109/TrustCom/BigDataSE.2018.00049|||||2018|A Quality Evaluation Method of Cyber Threat Intelligence in User Perspective|Qiang, Li and Zhengwei, Jiang and Zeming, Yang and Baoxu, Liu and Xin, Wang and Yunan, Zhang|inproceedings|8455917||Aug||23249013|||||||||||||||||||||||||2052088210|1841690667
||Social network services;Scalability;Data privacy;Distributed databases;Web services;provenance systems;social provenance data;big provenance data;provenance storage systems;decentralized provenance systems||9-16|2016 12th International Conference on Semantics, Knowledge and Grids (SKG)|Provenance about data derivations in social networks is usually called social data provenance, which helps in the assessment of data quality, resource tracking, and understanding the dissemination of information in social networks. The collection and processing of social data provenance leads to some challenges such as scalability, data quality, and privacy awareness. This study introduces a test suite to evaluate the current state-of-the-art standalone and centralized provenance systems. We conduct performance (responsiveness) and scalability experiments and investigate whether the standalone provenance systems are capable of handling large-size social provenance data. We also propose a software architecture for a decentralized and scalable provenance management system for big social provenance data.|10.1109/SKG.2016.010|||||2016|An Approach to Standalone Provenance Systems for Big Social Provenance Data|Tas, Yucel and Baeth, Mohamed Jehad and Aktas, Mehmet S.|inproceedings|7815071||Aug|||||||||||||||||||||||||||2054264427|42
||Big data, Data analytics, Software development, Software process improvement, Software process assessment||103585||Today, business success is essentially powered by data-centric software. Big data analytics (BDA) grasp the potential of generating valuable insights and empowering businesses to support their strategic decision-making. However, although organizations are aware of BDAs’ potential opportunities, they face challenges to satisfy the BDA-specific processes and integrate them into their daily software development lifecycle. Process capability/ maturity assessment models are used to assist organizations in assessing and realizing the value of emerging capabilities and technologies. However, as a result of the literature review and its analysis, it was observed that none of the existing studies in the BDA domain provides a complete, standardized, and objective capability maturity assessment model. To address this research gap, we focus on developing a BDA process capability assessment model grounded on the well-accepted ISO/IEC 330xx standard series. The proposed model comprises two main dimensions: process and capability. The process dimension covers six BDA-specific processes: business understanding, data understanding, data preparation, model building, evaluation, and deployment and use. The capability dimension has six levels, from not performed to innovating. We conducted case studies in two different organizations to validate the applicability and usability of the proposed model. The results indicate that the proposed model provides significant insights to improve the business value generated by BDA via determining the current capability levels of the organizations' BDA processes, deriving a gap analysis, and creating a comprehensive roadmap for continuous improvement in a standardized way.|https://doi.org/10.1016/j.csi.2021.103585|https://www.sciencedirect.com/science/article/pii/S0920548921000805||||2022|A process assessment model for big data analytics|Mert Onuralp Gökalp and Ebru Gökalp and Kerem Kayabay and Selin Gökalp and Altan Koçyiğit and P. Erhan Eren|article|GOKALP2022103585|||Computer Standards & Interfaces|09205489||80|||||24303|0,556|Q1|63|36|246|2302|1013|243|3,71|63,94|Netherlands|Western Europe|1985-2020|Law (Q1); Hardware and Architecture (Q2); Software (Q2)||||2055564324|827980402
ICSIE '19|Cairo, Egypt|Big Data, Analytics, Challenges, Benefits|4|196–199|Proceedings of the 2019 8th International Conference on Software and Information Engineering|The benefits of deriving useful insights from avalanche of data available everywhere cannot be overemphasized. Big Data analytics can revolutionize the healthcare industry. It can also ensure functional productivity, help forecast and suggest feedbacks to disease outbreaks, enhance clinical practice, and optimize healthcare expenditure which cuts across all stakeholders in healthcare sectors. Notwithstanding these immense capabilities available in the general application of big data; studies on derivation of useful insights from healthcare data that can enhance medical practice have received little academic attention. Therefore, this study highlighted the possibility of making very insightful healthcare outcomes with big data through a simple classification problem which classifies the tendency of individuals towards specific drugs based on personality measures. Our model though trained with less than 2000 samples and with a simple neural network architecture achieved mean accuracies of 76.87% (sd=0.0097) and 75.86% (sd=0.0123) for the 0.15 and 0.05 validation sets respectively. The relatively acceptable performance recorded by our model despite the small dataset could largely be attributed to number of attributes in our dataset. It is essential to uncover some of the many complexities in our societies in relations to healthcare; and through many machine learning architectures like the neural networks these complex relationships can be discovered|10.1145/3328833.3328841|https://doi.org/10.1145/3328833.3328841|New York, NY, USA|Association for Computing Machinery|9781450361057|2019|Big Data in Healthcare: Are We Getting Useful Insights from This Avalanche of Data?|Adenuga, Kayode I. and Muniru, Idris O. and Sadiq, Fatai I. and Adenuga, Rahmat O. and Solihudeen, Muhammad J.|inproceedings|10.1145/3328833.3328841|||||||||||||||||||||||||||||2055567238|42
||Circular economy, Big data, Stakeholder theory, Relational view, Supply chain management, Sustainability||466-474||The business concept of the circular economy (CE) has gained significant momentum among practitioners and researchers alike. However, successful adoption and implementation of this paradigm of managing business remains a challenge. In this article, we build a case for utilizing big data analytics (BDA) as a fundamental basis for informed and data driven decision making in supply chain networks supporting CE. We view this from a stakeholder perspective and argue that a collaborative association among all supply chain members can positively affect CE implementation. We propose a model highlighting the facilitating role of big data analytics for achieving shared sustainability goals. The model is based on integrating thematic categories coming out of 10 semi-structured interviews with key position holders in industry. We argue that mutual support and coordination driven by a stakeholder perspective coupled with holistic information processing and sharing along the entire supply chain network can effectively create a basis for achieving the triple bottom line of economic, ecological and social benefits. The proposed model is useful for managers in that it provides a reference point for aligning activities with the circular economy paradigm. The conceptual model provides a theoretical basis for future empirical research in this domain.|https://doi.org/10.1016/j.techfore.2018.06.030|https://www.sciencedirect.com/science/article/pii/S0040162517314488||||2019|Circular economy and big data analytics: A stakeholder perspective|Shivam Gupta and Haozhe Chen and Benjamin T. Hazen and Sarabjot Kaur and Ernesto D.R. {Santibañez Gonzalez}|article|GUPTA2019466|||Technological Forecasting and Social Change|00401625||144|||||14704|2,226|Q1|117|448|1099|35581|10127|1061|9,01|79,42|United States|Northern America|1970-2020|Applied Psychology (Q1); Business and International Management (Q1); Management of Technology and Innovation (Q1)|21,116|8.593|0.02416|2057318725|1949868303
||Software;Market research;Data mining;Application programming interfaces;Mobile communication;Mobile applications;Technological innovation;Non-functional requirements (NFRs);quality requirements;iOS;Latent Dirichlet allocation (LDA);Stack Overflow||61145-61169||Context: Mobile application developers are getting more concerned due to the importance of quality requirements or non-functional requirements (NFR) in software quality. Developers around the globe are actively asking a question(s) and sharing solutions to the problems related to software development on Stack Overflow (SO). The knowledge shared by developers on SO contains useful information related to software development such as feature requests (functional/non-functional), code snippets, reporting bugs or sentiments. Extracting the NFRs shared by iOS developers on programming Q&A website SO has become a challenge and a less researched area. Objective: To identify and understand the real problems, needs, trends, and the critical NFRs or quality requirements discussed on Stack Overflow related to iOS mobile application development. Method: We extracted and used only the iOS posts data of SO. We applied the well-known statistical topical model Latent Dirichlet Allocation (LDA) to identify the main topics in iOS posts on SO. Then, we labeled the extracted topics with quality requirements or NFRs by using the wordlists to assess the trend, evolution, hot and unresolved NFRs in all iOS discussions. Results: Our findings revealed that the highly frequent topics the iOS developers discussed are related to usability, reliability, and functionality followed by efficiency. Interestingly, the most problematic areas unresolved are also usability, reliability, and functionality though followed by portability. Besides, the evolution trend of each of the six different quality requirements or NFRs over time is depicted through comprehensive visualization. Conclusion: Our first empirical investigation on approximately 1.5 million iOS posts and comments of SO gives insight on comprehending the NFRs in iOS application development through the lens of real-world practitioners.|10.1109/ACCESS.2019.2914429|||||2019|Toward Empirically Investigating Non-Functional Requirements of iOS Developers on Stack Overflow|Ahmad, Arshad and Feng, Chong and Li, Kan and Asim, Syed Mohammad and Sun, Tingting|article|8704713|||IEEE Access|21693536||7|||||21100374601|0,587|Q1|127|18036|24267|771081|116691|24200|4,48|42,75|United States|Northern America|2013-2020|Computer Science (miscellaneous) (Q1); Engineering (miscellaneous) (Q1); Materials Science (miscellaneous) (Q2)|105,968|3.367|0.15396|2059136660|1905633267
||Computer science;Data integrity;User interfaces;Big Data;Information systems;Source Selection;Source Ordering;Data Quality;Big Data Integration;Semantic Similarity||155-164|2020 International Conference on Advanced Computer Science and Information Systems (ICACSIS)|Big data integration gives access to a large number of data sources through a unified user interface. Answers include high-quality data, medium-quality data, and low-quality data. Selecting a subset of high accurate and consistent data sources and ordering them appropriately is critical to obtain as many high-quality answers as possible right after querying few data sources. However, the process of selecting and ordering data sources can be quite complicated and can present several challenges. The main challenge faced during that process is identifying the most adequate data quality factors to consider. In this paper, we present HASSO, a Highly-Automated Source Selection and Ordering System based on data quality factors. To produce consistent and high accurate answers, HASSO identifies, for each data source, its domain, data consistency and data accuracy using the schema matches. To maximize the total number of complete and non-redundant answers returned right after querying a small number of data sources, HASSO orders data sources in terms of their data overlap and in a decreasing order of their overall coverage. Experimental results in real-world domains show that HASSO produces high-quality answers at high speed.|10.1109/ICACSIS51025.2020.9263243|||||2020|HASSO: A Highly-Automated Source Selection and Ordering System Based on Data Quality Factors|Yousfi, Aola and El Yazidi, Moulay Hafid and Zellou, Ahmed|inproceedings|9263243||Oct||23304588|||||||||||||||||||||||||2060016722|85998882
||Deep learning, Big data, IoT security||495-517||Technology has become inevitable in human life, especially the growth of Internet of Things (IoT), which enables communication and interaction with various devices. However, IoT has been proven to be vulnerable to security breaches. Therefore, it is necessary to develop fool proof solutions by creating new technologies or combining existing technologies to address the security issues. Deep learning, a branch of machine learning has shown promising results in previous studies for detection of security breaches. Additionally, IoT devices generate large volumes, variety, and veracity of data. Thus, when big data technologies are incorporated, higher performance and better data handling can be achieved. Hence, we have conducted a comprehensive survey on state-of-the-art deep learning, IoT security, and big data technologies. Further, a comparative analysis and the relationship among deep learning, IoT security, and big data technologies have also been discussed. Further, we have derived a thematic taxonomy from the comparative analysis of technical studies of the three aforementioned domains. Finally, we have identified and discussed the challenges in incorporating deep learning for IoT security using big data technologies and have provided directions to future researchers on the IoT security aspects.|https://doi.org/10.1016/j.comcom.2020.01.016|https://www.sciencedirect.com/science/article/pii/S0140366419315361||||2020|Deep learning and big data technologies for IoT security|Mohamed Ahzam Amanullah and Riyaz Ahamed Ariyaluran Habeeb and Fariza Hanum Nasaruddin and Abdullah Gani and Ejaz Ahmed and Abdul Salam Mohamed Nainar and Nazihah Md Akim and Muhammad Imran|article|AMANULLAH2020495|||Computer Communications|01403664||151|||||13681|0,627|Q1|105|616|599|24961|2424|591|4,08|40,52|Netherlands|Western Europe|1978-2020|Computer Networks and Communications (Q1)|6,725|3.167|0.00513|2060574509|550488617
DEBS '20|Montreal, Quebec, Canada|data validation, resiliency, data pre-processing, stream processing|4|226–229|Proceedings of the 14th ACM International Conference on Distributed and Event-Based Systems|In the last few years, distributed stream processing engines have been on the rise due to their crucial impacts on real-time data processing with guaranteed low latency in several application domains such as financial markets, surveillance systems, manufacturing, smart cities, etc. Stream processing engines are run-time libraries to process data streams without knowing the lower level streaming mechanics. Apache Storm, Apache Flink, Apache Spark, Kafka Streams and Hazelcast Jet are some of the popular stream processing engines. Nowadays, critical systems like energy systems, are interconnected and automated. As a result, these systems are vulnerable to cyber-attacks. In real-world applications, the sensing values come from sensor devices contains missing values, redundant data, data outliers, manipulated data, data failures, etc. Therefore, our system must be resilient to these conditions. In this paper, we present an approach to check if there is any above mentioned conditions by pre-processing data streams using a stream processing engine like Apache Flink which will be updated as a library in future. Then, the pre-processed streams are forwarded to other stream processing engines like Apache Kafka for real stream processing. As a result, data validation, data consistency and integrity for a resilient system can be accomplished before initiating the actual stream processing.|10.1145/3401025.3406443|https://doi.org/10.1145/3401025.3406443|New York, NY, USA|Association for Computing Machinery|9781450380287|2020|Pre-Processing and Data Validation in IoT Data Streams|Baban, Philsy|inproceedings|10.1145/3401025.3406443|||||||||||||||||||||||||||||2061090694|42
||Wind speed;Support vector machines;Biological neural networks;Indexes;Wind turbines;Mathematical model||554-559|2019 IEEE International Conference on Energy Internet (ICEI)|Wind energy is one of the renewable energy sources with a large number of installations in the world. The accuracy of power generation prediction using wind speed data severely challenges the regulation and safe operation of power system. Since there are many time points in the dispatching strategy of power system, which is related to the area condition. It is of great significance for power grid dispatching to be able to timely and accurately predict the generation capacity of wind turbines in a certain period. Due to the randomness and intermittency of wind speed, the accuracy of data quality will be influenced greatly. In this paper, a neural network algorithm based on combination of back propagation (BP) and Newton interpolation mathematical function method is proposed to effectively process wind speed data, so as to predict power generation. BP neural network is a kind of multi-layer feedforward neural network including a hidden layer, which can solve the learning problem of hidden layer connection weight in a multi-layer network. From the perspective of space scale, this paper studies different wind speed data at different heights in the same area. Research results show: compared with the traditional support vector machine method, the accuracy with the proposed method is improved by 3.1%.|10.1109/ICEI.2019.00104|||||2019|Wind power generation forecasting and data quality improvement based on big data with multiple temporal-spatual scale|Qiao, Lin and Chen, Shuo and Bo, Jue and Liu, Sai and Ma, Guiwei and Wang, Haixin and Yang, Junyou|inproceedings|8791314||May|||||||||||||||||||||||||||2061388427|42
WSDM '17|Cambridge, United Kingdom|web mining, predictive modeling, social network analysis|2|821–822|Proceedings of the Tenth ACM International Conference on Web Search and Data Mining|The first international workshop on Mining Actionable Insights from Social Networks (MAISoN'17) is to be held on February 10, 2017; co-located with the Tenth ACM International Web Search and Data Mining (WSDM) Conference in Cambridge, UK. MAISoN'17 aims at bringing together researchers and participants from different disciplines such as computer science, big data mining, machine learning, social network analysis and other related areas in order to identify challenging problems and share ideas, algorithms, and technologies for mining actionable insight from social network data. We organized a workshop program that includes the presentation of eight peer-reviewed papers and keynote talks, which foster discussions around state-of-the-art in social network mining and will hopefully lead to future collaborations and exchanges.|10.1145/3018661.3022759|https://doi.org/10.1145/3018661.3022759|New York, NY, USA|Association for Computing Machinery|9781450346757|2017|Mining Actionable Insights from Social Networksat WSDM 2017|Ensan, Faezeh and Noorian, Zeinab and Bagheri, Ebrahim|inproceedings|10.1145/3018661.3022759|||||||||||||||||||||||||||||2063907954|42
||Smart city, Big data, Internet of things, Smart environments, Cloud computing, Distributed computing||748-758||The expansion of big data and the evolution of Internet of Things (IoT) technologies have played an important role in the feasibility of smart city initiatives. Big data offer the potential for cities to obtain valuable insights from a large amount of data collected through various sources, and the IoT allows the integration of sensors, radio-frequency identification, and Bluetooth in the real-world environment using highly networked services. The combination of the IoT and big data is an unexplored research area that has brought new and interesting challenges for achieving the goal of future smart cities. These new challenges focus primarily on problems related to business and technology that enable cities to actualize the vision, principles, and requirements of the applications of smart cities by realizing the main smart environment characteristics. In this paper, we describe the state-of-the-art communication technologies and smart-based applications used within the context of smart cities. The visions of big data analytics to support smart cities are discussed by focusing on how big data can fundamentally change urban populations at different levels. Moreover, a future business model of big data for smart cities is proposed, and the business and technological research challenges are identified. This study can serve as a benchmark for researchers and industries for the future progress and development of smart cities in the context of big data.|https://doi.org/10.1016/j.ijinfomgt.2016.05.002|https://www.sciencedirect.com/science/article/pii/S0268401216302778||||2016|The role of big data in smart city|Ibrahim Abaker Targio Hashem and Victor Chang and Nor Badrul Anuar and Kayode Adewole and Ibrar Yaqoob and Abdullah Gani and Ejaz Ahmed and Haruna Chiroma|article|HASHEM2016748|||International Journal of Information Management|02684012|5|36|||||15631|2,770|Q1|114|224|382|20318|5853|373|16,16|90,71|United Kingdom|Western Europe|1970, 1986-2021|Artificial Intelligence (Q1); Computer Networks and Communications (Q1); Information Systems (Q1); Information Systems and Management (Q1); Library and Information Sciences (Q1); Management Information Systems (Q1); Marketing (Q1)|12,245|14.098|0.01167|2064981353|747927863
||Computer integrated manufacturing;Mobile communication;IEC standards;Logic gates;Servers;Data models;Asset management;Application virtualization;Virtual reality;Visualization;Standards;Data handling;Data visualization;CIM;Data integration;Big Data||1-5|2014 IEEE PES T&D Conference and Exposition|Modern mobile devices are capable of running sophisticated, network-enabled applications exploiting a variety of sensors on a single low-cost piece of hardware. The electrical industry can benefit from these new platforms to automate existing processes and provide engineers and field crew with access to large amounts of complex data in real-time, anywhere in the world. The development of a standards-based application decouples the mobile client application from a single vendor or existing enterprise system, but requires a complex data integration architecture to support the use and exploitation of large amounts of data spread across multiple existing systems. The integration with a mobile application introduces new challenges when dealing with remote devices where data network communications cannot be relied on, especially under storm conditions, and the devices themselves are at risk of being lost or stolen. Addressing these challenges offers the potential to improve data quality, enable access to accurate, up-to-date information in the field and ultimately save a utility time and money.|10.1109/TDC.2014.6863306|||||2014|Data integration challenges for standards-compliant mobile applications|McMorran, A. W. and Rudd, S. E. and Shand, C. M. and Simmins, J. J. and McCollough, N. and Stewart, E. M.|inproceedings|6863306||April||21608563|||||||87075|0,279|-|65|160|212|2010|182|211|0,86|12,56|United States|Northern America|1994, 1996, 1999-2003, 2005-2006, 2012, 2014, 2016, 2018|Electrical and Electronic Engineering; Energy (miscellaneous); Engineering (miscellaneous)||||2064997176|65589648
||Data models;Data mining;Computational modeling;Hospitals;Analytical models;Big data;Agent-Based Modeling and Simulation (ABMS);Big Data;Data Mining (DM);Decision Support Systems (DSS);Emergency Department (ED);Knowledge Discovery||367-372|2014 International Conference on Future Internet of Things and Cloud|Here a work in progress is reported on within research that aims to obtain knowledge about variables which may influence a hospital emergency department's performance and quality of service. Knowledge discovery will be achieved through the analysis of intensive data generated by the simulation of any possible scenario in the real system. The challenge is to provide knowledge of critical, non-usual or extreme situations. Simulation is the only way to obtain information about these kinds of situations, as it is not possible to test such scenarios in the real system. We show how simulation of the real system through advanced computing is a source of big data, as it allows rapid and massive data generation. The potential of high performance computing makes it possible to generate a very large amount of data within a reasonable time, store this data, then process and analyze it to obtain knowledge. We describe the methodology proposed for this goal, which is based on the use of the simulator as a sensor of the real system, and so as the main source of data. The application of data mining techniques will open the doors to knowledge. To verify that the proposed methodology works, we propose a case study in which the aim is to obtain knowledge from a set of data already available, obtained from the simulation of a reduced set of scenarios of the real system.|10.1109/FiCloud.2014.65|||||2014|Simulation and Big Data: A Way to Discover Unusual Knowledge in Emergency Departments: Work-in-Progress Paper|Bruballa, Eva and Taboada, Manel and Cabrera, Eduardo and Rexachs, Dolores and Luque, Emilio|inproceedings|6984221||Aug|||||||||||||||||||||||||||2069124448|42
||||1624-1638||Advancing a new drug to market requires substantial investments in time as well as financial resources. Crucial bioactivities for drug candidates, including their efficacy, pharmacokinetics (PK), and adverse effects, need to be investigated during drug development. With advancements in chemical synthesis and biological screening technologies over the past decade, a large amount of biological data points for millions of small molecules have been generated and are stored in various databases. These accumulated data, combined with new machine learning (ML) approaches, such as deep learning, have shown great potential to provide insights into relevant chemical structures to predict in vitro, in vivo, and clinical outcomes, thereby advancing drug discovery and development in the big data era.|https://doi.org/10.1016/j.drudis.2020.07.005|https://www.sciencedirect.com/science/article/pii/S1359644620302646||||2020|Advancing computer-aided drug discovery (CADD) by big data and data-driven machine learning modeling|Linlin Zhao and Heather L. Ciallella and Lauren M. Aleksunes and Hao Zhu|article|ZHAO20201624|||Drug Discovery Today|13596446|9|25|||||||||||||||||||||||2071935606|933393058
SBD '19|Amsterdam, Netherlands|data profiling, web tables, key detection, n-ary relations|6||Proceedings of the International Workshop on Semantic Big Data|The Web contains millions of relational HTML tables, which cover a multitude of different, often very specific topics. This rich pool of data has motivated a growing body of research on methods that use web table data to extend local tables with additional attributes or add missing facts to knowledge bases. Nearly all existing approaches for these tasks build upon the assumption that web table data consists of binary relations, meaning that an attribute value depends on a single key attribute, and that the key attribute value is contained in the HTML table. Inspecting randomly chosen tables on the Web, however, quickly reveals that both assumptions are wrong for a large fraction of the tables. In order to better understand the potential of non-binary web table data for downstream applications, this papers analyses a corpus of 5 million web tables originating from 80 thousand different web sites with respect to how many web table attributes are non-binary, what composite keys are required to correctly interpret the semantics of the non-binary attributes, and whether the values of these keys are found in the table itself or need to be extracted from the page surrounding the table. The profiling of the corpus shows that at least 38% of the relations are non-binary. Recognizing these relations requires information from the title or the URL of the web page in 50% of the cases. We find that different websites use keys of varying length for the same dependent attribute, e.g. one cluster of websites presents employment numbers depending on time, another cluster presents them depending on time and profession. By identifying these clusters, we lay the foundation for selecting Web data sources according to the specificity of the keys that are used to determine specific attributes.|10.1145/3323878.3325806|https://doi.org/10.1145/3323878.3325806|New York, NY, USA|Association for Computing Machinery|9781450367660|2019|Profiling the Semantics of N-Ary Web Table Data|Lehmberg, Oliver and Bizer, Christian|inproceedings|10.1145/3323878.3325806|5||||||||||||||||||||||||||||2072208242|42
||compliance checking, process modeling, business process intelligence, performance analysis, Process mining|7|||This introduction to the special issue on Business Process Intelligence (BPI) discusses the relation between data and processes. The recent attention for Big Data illustrates that organizations are aware of the potential of the torrents of data generated by today's information systems. However, at the same time, organizations are struggling to extract value from this overload of data. Clearly, there is a need for data scientists able to transform event data into actionable information. To do this, it is crucial to take a process perspective. The ultimate goal of BPI is not to improve information systems or the recording of data; instead the focus should be in improving the process. For example, we may want to aim at reducing costs, minimizing response times, and ensuring compliance. This requires a “confrontation” between process models and event data. Recent advances in process mining allow us to automatically learn process models showing the bottlenecks from “raw” event data. Moreover, given a normative model, we can use conformance checking to quantify and understand deviations. Automatically learned models may also be used for prediction and recommendation. BPI is rapidly developing as a field linking data science to business process management. This article aims to provide an overview thereby paving the way for the other contributions in this special issue.|10.1145/2685352|https://doi.org/10.1145/2685352|New York, NY, USA|Association for Computing Machinery||2015|Editorial: “Business Process Intelligence: Connecting Data and Processes”|Aalst, Wil Van Der and Zhao, J. Leon and Wang, Harry Jiannan|article|10.1145/2685352|18e|apr|ACM Trans. Manage. Inf. Syst.|2158656X|4|5|March 2015||||||||||||||||||||||2073734335|2097885649
||Social media, Innovation, Systematic review, Framework and research agenda||140-156||The use of social media for innovation requires firms to manage rapid information transfers, big data, and multiway communication. Yet managers lack clear insights on the way social media should be managed and current literature is dispersed across various research streams. In this article, the authors aim to develop a better understanding of how social media use should be leveraged for innovation. To achieve this objective, they build a systematic review of evidence from 177 scientific articles across four key management disciplines. They analyze research perspectives and conceptualizations of social media use for innovation and provide a framework of the drivers, contingencies and outcomes related to this topic. Next, they attempt to identify what is currently known about social media use for innovation. Last, they suggest critical areas for future inquiry on this important subject.|https://doi.org/10.1016/j.jbusres.2022.01.039|https://www.sciencedirect.com/science/article/pii/S0148296322000510||||2022|Social media use: A review of innovation management practices|Marie-Isabelle Muninger and Dominik Mahr and Wafa Hammedi|article|MUNINGER2022140|||Journal of Business Research|01482963||143|||||20550|2,049|Q1|195|878|1342|73221|11507|1315|7,38|83,40|United States|Northern America|1973-2021|Marketing (Q1)|46,935|7.550|0.03523|2074067022|1502892296
dg.o '18|Delft, The Netherlands|use, big data challenges, cross case analysis, big data analytics, public value creation, census big data, electronic census|10||Proceedings of the 19th Annual International Conference on Digital Government Research: Governance in the Data Age|Despite the growing practices in big data and big data analytics use, there is still the paucity of research on links between government big data analytics use and public value creation. This multi-case study of Australia, Ireland, Mexico, and U.S.A. examines the state of big data and big data analytics use in the national census context. The census agencies are at varying stages in digitally transforming their national census process, products and services through assimilating and using big data and big data analytics. The cross-case analysis of government websites and documents identified emerging agency challenges in creating public value in the national census context: (1) big data analytics capability development, (2) cross agency data access and data integration, and (3) data security, privacy &amp; trust. Based on the insights gained, a research model aims to postulate the possible links among challenges, big data/big data analytics use, and public value creation.|10.1145/3209281.3209372|https://doi.org/10.1145/3209281.3209372|New York, NY, USA|Association for Computing Machinery|9781450365260|2018|Census Big Data Analytics Use: International Cross Case Analysis|Chatfield, Akemi Takeoka and Ojo, Adegboyega and Puron-Cid, Gabriel and Reddick, Christopher G.|inproceedings|10.1145/3209281.3209372|10||||||||||||||||||||||||||||2075049069|42
CASCON '19|Toronto, Ontario, Canada|organisational, data science adoption, business practices, legal, challenges|2|384–385|Proceedings of the 29th Annual International Conference on Computer Science and Software Engineering|Data science is an interdisciplinary scientific approach that provides methods to understand and solve problems in an evidence-based manner, using data and experience. Despite the clear benefits from adoption, many firms face challenges, be that legal, organisational, or business practices, when seeking to implement and embed data science within an existing framework. In this workshop, panel and audience members drew on their experiences to elaborate on the challenges encountered when attempting to deploying data science within existing frameworks. Panel and audience members were drawn from business, academia, and think-tanks. For discussion purposes the challenges were grouped within three themes: regulatory; investment; and workforce.|||USA|IBM Corp.||2019|Workshop on Barriers to Data Science Adoption: Why Existing Frameworks Aren't Working|Alexander, Rohan and Lyons, Kelly and Alexopoulos, Michelle and Austin, Lisa|inproceedings|10.5555/3370272.3370329|||||||||||||||||||||||||||||2077103123|42
||Data quality, quality assessment, linked data|32|||The increasing variety of Linked Data on the Web makes it challenging to determine the quality of this data and, subsequently, to make this information explicit to data consumers. Despite the availability of a number of tools and frameworks to assess Linked Data Quality, the output of such tools is not suitable for machine consumption, and thus consumers can hardly compare and rank datasets in the order of fitness for use. This article describes a conceptual methodology for assessing Linked Datasets, and Luzzu; a framework for Linked Data Quality Assessment. Luzzu is based on four major components: (1) an extensible interface for defining new quality metrics; (2) an interoperable, ontology-driven back-end for representing quality metadata and quality problems that can be re-used within different semantic frameworks; (3) scalable dataset processors for data dumps, SPARQL endpoints, and big data infrastructures; and (4) a customisable ranking algorithm taking into account user-defined weights. We show that Luzzu scales linearly against the number of triples in a dataset. We also demonstrate the applicability of the Luzzu framework by evaluating and analysing a number of statistical datasets against a variety of metrics. This article contributes towards the definition of a holistic data quality lifecycle, in terms of the co-evolution of linked datasets, with the final aim of improving their quality.|10.1145/2992786|https://doi.org/10.1145/2992786|New York, NY, USA|Association for Computing Machinery||2016|Luzzu—A Methodology and Framework for Linked Data Quality Assessment|"\"Debattista, Jeremy and Auer, S\"\"{O}ren and Lange, Christoph\""|article|10.1145/2992786|4|oct|J. Data and Information Quality|19361955|1|8|November 2016||||||||||||||||||||||2079568900|833754770
MEDES '20|Virtual Event, United Arab Emirates|Software containers, Big Data workflows, Domain-specific languages|8|76–83|Proceedings of the 12th International Conference on Management of Digital EcoSystems|Big Data processing involves handling large and complex data sets, incorporating different tools and frameworks as well as other processes that help organisations make sense of their data collected from various sources. This set of operations, referred to as Big Data workflows, require taking advantage of the elasticity of cloud infrastructures for scalability. In this paper, we present the design and prototype implementation of a Big Data workflow approach based on the use of software container technologies and message-oriented middleware (MOM) to enable highly scalable workflow execution. The approach is demonstrated in a use case together with a set of experiments that demonstrate the practical applicability of the proposed approach for the scalable execution of Big Data workflows. Furthermore, we present a scalability comparison of our proposed approach with that of Argo Workflows - one of the most prominent tools in the area of Big Data workflows.|10.1145/3415958.3433082|https://doi.org/10.1145/3415958.3433082|New York, NY, USA|Association for Computing Machinery|9781450381154|2020|Scalable Execution of Big Data Workflows Using Software Containers|Dessalk, Yared Dejene and Nikolov, Nikolay and Matskin, Mihhail and Soylu, Ahmet and Roman, Dumitru|inproceedings|10.1145/3415958.3433082|||||||||||||||||||||||||||||2080263354|42
CODASPY '15|San Antonio, Texas, USA|security, privacy, big data|2|279–280|Proceedings of the 5th ACM Conference on Data and Application Security and Privacy|This paper describes the issues surrounding big data security and privacy and provides a summary of the National Science Foundation sponsored workshop on this topic held in Dallas, Texas on September 16-17, 2014. Our goal is to build a community in big data security and privacy to explore the challenging research problems.|10.1145/2699026.2699136|https://doi.org/10.1145/2699026.2699136|New York, NY, USA|Association for Computing Machinery|9781450331913|2015|Big Data Security and Privacy|Thuraisingham, Bhavani|inproceedings|10.1145/2699026.2699136|||||||||||||||||||||||||||||2080755454|42
||Aerosol types, Fine mode fraction, Haze reduction, Satellite remote sensing, Big data analytics||45-59||Spatially contiguous aerosol type grids were rarely available for air quality management in the past. To bridge the gap, we developed an integrated remote sensing and big data analytics framework to generate spatially gap-free aerosol type grids between 2000 and 2020 in China. The effect of emission control via environmental management on haze reduction was fully realized for the first time with the aid of satellite-based gap-free aerosol type data. Daily gap-free aerosol fine mode fraction (FMF) data were first derived via a data-driven regression model based on remote sensing big data. According to empirically determined FMF probability distributions over regions with typical emission sources, aerosols in China were classified into eight major types, including typical/atypical/mixed anthropogenic aerosols, typical/atypical/mixed dust, and typical mixed and multiple modes. The results indicated that the gridded FMF estimates derived in this study agreed well with FMF retrievals from AERONET, with correlation coefficient of 0.81 and root mean square error of 0.13. The long-term variations in major aerosol types showed that in China the territory covered by typical anthropogenic aerosols was reduced from 21.38% to 11.76% over the past two decades, while dust aerosols decreased from 6.99% to 2.15%. The declining trend in anthropogenic aerosols could be attributed to reduced coal consumption and/or favorable dispersion conditions, whereas decreasing dust aerosols were largely associated with increased vegetation cover and/or weakened wind speed in the west. Overall, such advancements provide fresh evidence to improve our understanding of the emission control effect on haze pollution variations in China.|https://doi.org/10.1016/j.isprsjprs.2022.09.001|https://www.sciencedirect.com/science/article/pii/S0924271622002374||||2022|Spatially gap free analysis of aerosol type grids in China: First retrieval via satellite remote sensing and big data analytics|Ke Li and Kaixu Bai and Mingliang Ma and Jianping Guo and Zhengqiang Li and Gehui Wang and Ni-Bin Chang|article|LI202245|||ISPRS Journal of Photogrammetry and Remote Sensing|09242716||193|||||29161|2,960|Q1|138|264|677|16114|7306|668|10,56|61,04|Netherlands|Western Europe|1989-2020|Atomic and Molecular Physics, and Optics (Q1); Computer Science Applications (Q1); Computers in Earth Sciences (Q1); Engineering (miscellaneous) (Q1); Geography, Planning and Development (Q1)|18,026|8.979|0.02145|2081474322|660578442
||Data models;Data integrity;Big Data;Mathematical model;Analytical models;Analytic hierarchy process;Surgery;data quality evaluation;medical;credibility analysis;analytic hierarchy process||940-944|2018 IEEE 4th Information Technology and Mechatronics Engineering Conference (ITOEC)|The current main problem of medical data is low accuracy. Although existing data quality evaluation and audit can meet the needs, network security problem will be more serious in the future. As a result, the data quality evaluation model must include data credibility. This paper proposes the medical big data quality evaluation model based on credibility analysis and Analytic Hierarchy Process(AHP). Firstly, analyze the data credibility. After eliminating the unreliable data, calculate the data quality dimensions respectively. Then obtain the data quality evaluation result by integrating all dimensions with AHP. Through simulation, the data quality evaluation model can effectively identify the data that affects the credibility. The data quality evaluation results are improved after removing the untrusted data, besides the amount of data is reduced, which is applicable to the big data scenario.|10.1109/ITOEC.2018.8740576|||||2018|Medical Data Quality Assessment Model Based on Credibility Analysis|Zan, Songting and Zhang, Xu|inproceedings|8740576||Dec|||||||||||||||||||||||||||2082454757|42
SIGSPATIAL '14|Dallas, Texas|large-scale data, address parsing, record linkage|4|433–436|Proceedings of the 22nd ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems|Record linkage is the task of identifying which records in one or more data collections refer to the same entity, and address is one of the most commonly used fields in databases. Hence, segmentation of the raw addresses into a set of semantic fields is the primary step in this task. In this paper, we present a probabilistic address parsing system based on the Hidden Markov Model. We also introduce several novel approaches to build models for noisy real-world addresses, obtaining 95.6% F-measure. Furthermore, we demonstrate the viability and efficiency of this system for large-scale data by scaling it up to parse billions of addresses with Hadoop.|10.1145/2666310.2666471|https://doi.org/10.1145/2666310.2666471|New York, NY, USA|Association for Computing Machinery|9781450331319|2014|HMM-Based Address Parsing: Efficiently Parsing Billions of Addresses on MapReduce|Li, Xiang and Kardes, Hakan and Wang, Xin and Sun, Ang|inproceedings|10.1145/2666310.2666471|||||||||||||||||||||||||||||2086163286|42
||SMART Standards, knowledge representation, automatic extraction, transfer, 3M model of Duisburg||163-168||"\"Standards are - not directly visible to everyone – omnipresent in nearly every development process. In times of digitalization, where buzzwords such as \"\"connectivity of machines\"\", \"\"artificial intelligence\"\", “big data”, “cloud computing” or “smart factories” are often used, companies are still confronted with problems in handling standards throughout the entire product lifecycle. Today’s way of working with standards is characterized by manual viewing of documents, whereby a user searches for relevant information, such as formulas, and has to transfer this information to his process, method or tool. This manual process results in an increased time, loss of quality due to faulty manual transmission of information, a high adjustment effort for updates of standards and no guarantee for traceability. In order to reduce and minimize errors and needed time for work with information stored within standards, there is a need for a new form of knowledge representation for standards with sufficient data quality to ensure standard-compliant development activities. Consequently, there is a need for machine-actionable standards to ensure autonomous and efficient processes, whereby the effort for preparation is less than the benefit. The question arises how classified standards content can be represented in a machine-actionable way without loss of information. This paper shows a concept for the automatic extraction of standards content and their transfer into a machine-actionable knowledge representation. The concept, which is based on the “3M Framework of Duisburg” and thus answers questions of modularization, modeling and management, consists of six steps \"\"extraction\"\", \"\"modeling\"\", “modification”, \"\"fusion and storage\"\", \"\"provision\"\" and \"\"application\"\", to digitalize existing content, is presented and discussed.\""|https://doi.org/10.1016/j.procir.2021.05.025|https://www.sciencedirect.com/science/article/pii/S2212827121004868||||2021|SMART standards - concept for the automated transfer of standard contents into a machine-actionable form|Dominik Ehring and Janosch Luttmer and Robin Pluhnau and Arun Nagarajah|article|EHRING2021163|||Procedia CIRP|22128271||100||31st CIRP Design Conference 2021 (CIRP Design 2021)|||21100243809|0,683|-|65|1456|3191|30451|8225|3145|2,40|20,91|Netherlands|Western Europe|2012-2020|Control and Systems Engineering; Industrial and Manufacturing Engineering||||2087271173|2127027836
||Internet of things, smart cities, fog computing, sustainability|43|||The Internet of Things (IoT) aims to connect billions of smart objects to the Internet, which can bring a promising future to smart cities. These objects are expected to generate large amounts of data and send the data to the cloud for further processing, especially for knowledge discovery, in order that appropriate actions can be taken. However, in reality sensing all possible data items captured by a smart object and then sending the complete captured data to the cloud is less useful. Further, such an approach would also lead to resource wastage (e.g., network, storage, etc.). The Fog (Edge) computing paradigm has been proposed to counterpart the weakness by pushing processes of knowledge discovery using data analytics to the edges. However, edge devices have limited computational capabilities. Due to inherited strengths and weaknesses, neither Cloud computing nor Fog computing paradigm addresses these challenges alone. Therefore, both paradigms need to work together in order to build a sustainable IoT infrastructure for smart cities. In this article, we review existing approaches that have been proposed to tackle the challenges in the Fog computing domain. Specifically, we describe several inspiring use case scenarios of Fog computing, identify ten key characteristics and common features of Fog computing, and compare more than 30 existing research efforts in this domain. Based on our review, we further identify several major functionalities that ideal Fog computing platforms should support and a number of open challenges toward implementing them, to shed light on future research directions on realizing Fog computing for building sustainable smart cities.|10.1145/3057266|https://doi.org/10.1145/3057266|New York, NY, USA|Association for Computing Machinery||2017|Fog Computing for Sustainable Smart Cities: A Survey|Perera, Charith and Qin, Yongrui and Estrella, Julio C. and Reiff-Marganiec, Stephan and Vasilakos, Athanasios V.|article|10.1145/3057266|32|jun|ACM Comput. Surv.|03600300|3|50|May 2018||||23038|2,079|Q1|163|112|365|15859|6978|365|19,16|141,60|United States|Northern America|1969-2020|Computer Science (miscellaneous) (Q1); Theoretical Computer Science (Q1)|10,790|10.282|0.01438|2087499996|1517405264
||Data fusion, Multimedia Internet of Things, Big data, Quality of Experience, Machine learning, Neural network||1413-1423||With rapid evolution of the Internet of Things (IoT) applications on multimedia, there is an urgent need to enhance the satisfaction level of Multimedia IoT (MIoT) network users. An important and unsolved problem is automatic optimization of Quality of Experience (QoE) through collecting/managing/processing various data from MIoT network. In this paper, we propose an MIoT QoE optimization mechanism leveraging data fusion technology, called QoE optimization via Data Fusion (QoEDF). QoEDF consists of two steps. Firstly, a multimodal data fusion approach is proposed to build a QoE mapping between the uncontrollable user data with the controllable network-related system data. Secondly, an automatic QoE optimization model is built taking fused results, which is different from the traditional way. QoEDF is able to adjust network-related system data automatically so as to achieve optimized user satisfaction. Simulation results show that QoEDF will lead to significant improvements in QoE level as well as be adaptable to dynamic network changes.|https://doi.org/10.1016/j.future.2018.02.046|https://www.sciencedirect.com/science/article/pii/S0167739X17324500||||2018|Improving Quality of Experience in multimedia Internet of Things leveraging machine learning on big data|Xiaohong Huang and Kun Xie and Supeng Leng and Tingting Yuan and Maode Ma|article|HUANG20181413|||Future Generation Computer Systems|0167739X||86|||||12264|1,262|Q1|119|798|1935|36054|16877|1868|9,11|45,18|Netherlands|Western Europe|1984-2021|Computer Networks and Communications (Q1); Hardware and Architecture (Q1); Software (Q1)||||2088529277|562237118
||1/f noise;Mobile communication;Big Data;Accuracy;Mobile Positioning Data;Tourism Statistics||43-48|2017 International Workshop on Big Data and Information Security (IWBIS)|Big Data is a new concept that has become widely popularised in recent years. The revolutionized meaning of information communication technologies and Internet technologies refers to mobile communications which enable individuals to move and generate, transmit and receive different kinds of information. There are many communication options where users can search, interact and share information with other users such as website, social media, online communities blogs, and email called as Digital transformation. In Digital transformation era, over 95 % of travellers today use digital resources. Digital traveler can be as data source for official statistics. One of methods to capture the number of tourist can use Mobile Positioning Data (MPD). This method is considered able to improve the quality of survey data. This article will discuss further how to improve the quality of survey data through Big Data with case studies of MPD users in Indonesian Tourism Statistics.|10.1109/IWBIS.2017.8275101|||||2017|Improving data quality through big data: Case study on big data-mobile positioning data in Indonesia tourism statistics|Setiadi, Yazid and Uluwiyah, Ana|inproceedings|8275101||Sep.|||||||||||||||||||||||||||2091539583|42
KDD '18|London, United Kingdom|big data, graph algorithms, mobile access record resolution, scalable algorithms|9|886–894|Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining|The e-commerce era is witnessing a rapid increase of mobile Internet users. Major e-commerce companies nowadays see billions of mobile accesses every day. Hidden in these records are valuable user behavioral characteristics such as their shopping preferences and browsing patterns. And, to extract these knowledge from the huge dataset, we need to first link records to the corresponding mobile devices. This Mobile Access Records Resolution (MARR) problem is confronted with two major challenges: (1) device identifiers and other attributes in access records might be missing or unreliable; (2) the dataset contains billions of access records from millions of devices. To the best of our knowledge, as a novel challenge industrial problem of mobile Internet, no existing method has been developed to resolve entities using mobile device identifiers in such a massive scale. To address these issues, we propose a SParse Identifier-linkage Graph (SPI-Graph) accompanied with the abundant mobile device profiling data to accurately match mobile access records to devices. Furthermore, two versions (unsupervised and semi-supervised) of Parallel Graph-based Record Resolution (PGRR) algorithm are developed to effectively exploit the advantages of the large-scale server clusters comprising of more than 1,000 computing nodes. We empirically show superior performances of PGRR algorithms in a very challenging and sparse real data set containing 5.28 million nodes and 31.06 million edges from 2.15 billion access records compared to other state-of-the-arts methodologies.|10.1145/3219819.3219916|https://doi.org/10.1145/3219819.3219916|New York, NY, USA|Association for Computing Machinery|9781450355520|2018|Mobile Access Record Resolution on Large-Scale Identifier-Linkage Graphs|Xin, SHEN and Yang, Hongxia and Xian, Weizhao and Ester, Martin and Bu, Jiajun and Wang, Zhongyao and Wang, Can|inproceedings|10.1145/3219819.3219916|||||||||||||||||||||||||||||2093947393|42
||Sparks;Public transportation;Servers;Tuning;Big Data;Structured Query Language;Tools;Analytics;Spark;EMR;Cloud;BigData;Best Practices;Parquet;AWS;Optimization;Tuning||4501-4507|2018 IEEE International Conference on Big Data (Big Data)|Saving nature using Big Data Analytics is a very noble goal. Using New York taxi rides data, we decided to learn how many rides could be consolidated. It was a journey we would like to share. First, we had to choose the platform for calculation between Amazon Athena, Serverless Microservices, SQL or NoSql databases, Hadoop and Spark. Then, we had to find an optimal solution for the platform using assorted tuning and optimization techniques. Although the problem seems to be straight forward, it turned out that the solution is quite challenging because of the input size, data quality, calculation complexities and numerous EMR/Spark tuning options. We have been using New York taxi data from 2009 to 2017 to quantify the rides that can be joined together. The taxi rides were consolidated based on pickup location, pickup time and drop-off location. We have been calculating the percentage of taxi rides that can be joined. The benchmark originally set was rides within five minutes with a pickup and drop-off locations within half a kilometer. Then we started experimenting with different times and locations. We have been using parquet format, parallel Scala collections, compression, filtering, new column introduction, tuning parameters, I/O overhead tuning, bucketing, timeouts and partitioning. Over 1.2 billion rides were processed using Amazon EMR with Spark. We have been optimizing calculation time and processing price. Spark has hundreds of parameters, EMR has over fifty instances to choose from. It was challenging to process our data within reasonable time. We were able to find the optimal Spark queries (plans), tested different types of joins and compared their performances. Also, we were able to compare I/O and in-memory operations during partitioning and large files manipulation (the input file sizes were hundreds of Gigabytes). The results were amazing - we could consolidate around thirty five percent of total rides, saving tons of gas and improving environment and traffic in New York City.|10.1109/BigData.2018.8622378|||||2018|Consolidating billions of Taxi rides with AWS EMR and Spark in the Cloud : Tuning, Analytics and Best Practices|Kaplunovich, Alex and Yesha, Yelena|inproceedings|8622378||Dec|||||||||||||||||||||||||||2094090425|42
||Training;Technological innovation;Employment;Big Data;Prediction algorithms;Biology;Planning;data analysis;majors concerning biology;graduates;employment quality;local normal universities||572-578|2021 2nd International Conference on Artificial Intelligence and Education (ICAIE)|The data of annual reports on the employment quality f local normal universities from 2016 to 2020 were captured from the network with the help of information technology. The data related to basic situation and employment destination of biology graduates in Guangxi Normal University were analyzed and collected by data analytic technology such as Data Mining Algorithms, Data Quality Master Data Management and Predictive Analytic Capabilities. In view of the problems of single employment structure, insufficient employment skills, vague employment planning and low achievement of Innovation and Entrepreneurship, measures such as the employment policy interpretation, the talent training programs adjustment, the employment guidance services system improvement and teaching mode innovation are needed in the purpose of promoting employment, providing useful reference to the further progress of teaching reform and optimizing talent training modes and methods for graduates of biology.|10.1109/ICAIE53562.2021.00127|||||2021|Analysis of Employment Status and Countermeasures of Biology Graduates in Local Normal Universities Based on Big Data Technology—Take the Graduates of Guangxi Normal University From 20l6 to 2020 as an Example|Jie, Lu and Zheng, Su and Qi, Wang and Xiya, Chen|inproceedings|9534570||June|||||||||||||||||||||||||||2094668436|42
iiWAS2021|Linz, Austria|Data Mining, Data Profiling, Incremental Discovery, Functional Dependency|10|400–409|The 23rd International Conference on Information Integration and Web Intelligence|With the advent of Big Data there is an increasing necessity to incrementally mine information from data originating from sensors and other dynamic sources. Thus, it is necessary to devise algorithms capable of mining useful information upon possible evolutions of databases. Among these, there are certainly data profiling info, such as functional dependencies (fd for short), which are particularly useful for data integration and for assessing the quality of data. The incremental scenario requires the definition of search strategies and validation methods able to analyze only the portion of the dataset affected by the last changes. In this paper, we propose a new validation method, which exploits regular expressions and compressed data structures to efficiently verify whether a candidate fd holds on an updated version of the dataset. Experimental results demonstrate the effectiveness of the proposed method on real-world datasets adapted for incremental scenarios, also compared with a baseline incremental fd discovery algorithm.|10.1145/3487664.3487719|https://doi.org/10.1145/3487664.3487719|New York, NY, USA|Association for Computing Machinery|9781450395564|2022|Efficient Discovery of Functional Dependencies from Incremental Databases|Caruccio, Loredana and Cirillo, Stefano and Deufemia, Vincenzo and Polese, Giuseppe|inproceedings|10.1145/3487664.3487719|||||||||||||||||||||||||||||2097655374|42
||Delay prediction, Data Analysis, Machine learning, LightGBM||981-990||Train delays are one of the most important problems in the railway systems across the world, which urges the development of predictive analysis-based approaches to estimate it. In fact, with the advanced big data analysis and machine learning tools and technologies, the train delay-prediction systems can process and extract useful information from the large historical train movement data collected by the railway information system. Besides, accurate prediction of train delays can help train dispatchers make decisions through timetable rescheduling and service reliability improving. We propose, in this manuscript, a machine-learning model that captures the relationship between the arrival delay of passenger trains and the various characteristics of the railway system. We also apply, for the first time, lightGBM regressor based on optimal hyper-parameters to predict train delays. To evaluate the introduced model performance, the latter is compared with that of some other widely used existing models. Its R-squared, RMSE and RME were also compared with those of Support Vector Machine, Random Forest, XGBboost and Artificial Neural Network models. Statistical comparison indicates that the LightGBM outperforms the other models and is the fastest.|https://doi.org/10.1016/j.procs.2021.08.101|https://www.sciencedirect.com/science/article/pii/S1877050921015891||||2021|Train delay prediction in Tunisian railway through LightGBM model|Hassiba Laifa and Raoudha khcherif and Henda Hajjami {Ben Ghezalaa}|article|LAIFA2021981|||Procedia Computer Science|18770509||192||Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 25th International Conference KES2021|||19700182801|0,334|-|76|1907|6483|38511|13722|6359|2,09|20,19|Netherlands|Western Europe|2010-2020|Computer Science (miscellaneous)||||2099140724|2108686752
ICSDE'18|Rabat, Morocco|smart city evaluation, data quality measurement, data, DQSC-MM, maturity model, data quality, Smart city|8|140–147|Proceedings of the 2nd International Conference on Smart Digital Environment|Smart cities provide the ability to improve the quality of the citizen's life. Transformation into a smart city consists of defining the way ICT (Information and Communication Technologies) can be used to improve the weaker aspects of the city and improve the quality of services provided by public sectors (education, health, transportation...). The growth of the urban population implies the growing needs of urban services (health, education...) and resources (water, energy...). ICT can be used to meet the growing population needs and solve many of today's problems in the private and public sectors (health, transportation, school...). Using mobile phones all citizens produce data and information every day and everywhere, this data will be used to improve the quality of services provided by the city. The quality of the generated data presents the key element that will impact the success of the transformation into a smart city.This paper describes the proposed data quality driven smart cities model. The proposed model, called DQSC-MM (Data Quality Driven Smart Cities Maturity Model). DQSC-MM is used to evaluate the maturity of a smart city based on the quality of produced and consumed data. It suggests a way to measure the importance of data quality in a city's transformation into a smart city. The paper describes how the model was conceived, designed and developed. It describes also a JEE application conceived to support DQSC-MM. The developed application provides the ability to measure data quality and use these measurements for smart city evaluation.|10.1145/3289100.3289123|https://doi.org/10.1145/3289100.3289123|New York, NY, USA|Association for Computing Machinery|9781450365079|2018|Data Driven Maturity Model for Assessing Smart Cities|Korachi, Zineb and Bounabat, Bouchaib|inproceedings|10.1145/3289100.3289123|||||||||||||||||||||||||||||2099374821|42
||Big data, Big data analytics, Big data chain, E-government, Governance, Decision-making, Decision-making quality||338-345||Organizations are looking for ways to harness the power of big data (BD) to improve their decision making. Despite its significance the effects of BD on decision-making quality has been given scant attention in the literature. In this paper factors influencing decision-making based on BD are identified using a case study. BD is collected from different sources that have various data qualities and are processed by various organizational entities resulting in the creation of a big data chain. The veracity (manipulation, noise), variety (heterogeneity of data) and velocity (constantly changing data sources) amplified by the size of big data calls for relational and contractual governance mechanisms to ensure BD quality and being able to contextualize data. The case study reveals that taking advantage of big data is an evolutionary process in which the gradually understanding of the potential of big data and the routinization of processes plays a crucial role.|https://doi.org/10.1016/j.jbusres.2016.08.007|https://www.sciencedirect.com/science/article/pii/S0148296316304945||||2017|Factors influencing big data decision-making quality|Marijn Janssen and Haiko {van der Voort} and Agung Wahyudi|article|JANSSEN2017338|||Journal of Business Research|01482963||70|||||20550|2,049|Q1|195|878|1342|73221|11507|1315|7,38|83,40|United States|Northern America|1973-2021|Marketing (Q1)|46,935|7.550|0.03523|2100391757|1502892296
UbiComp '16|Heidelberg, Germany|smartphones, data science, people centric sensing, platforms, data analysis, data collection|6|635–640|Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing: Adjunct|"\"As a base for hypothesis formulation and testing, accurate, timely and reproducible data collection is a challenge for all researchers. Data collection is especially challenging in uncontrolled environments, outside of the lab and when it involves many collaborating disciplines, where the data must serve quality research in all of them. In this paper, we present own \"\"mQoL Smart Lab\"\" for interdisciplinary research efforts on individuals' \"\"Quality of Life\"\" improvement. We present an evolution of our current in-house living lab platform enabling continuous, pervasive data collection from individuals' smartphones. We discuss opportunities for mQoL stemming from developments in machine learning and big data for advanced data analytics in different disciplines, better meeting the requirements put on the platform.\""|10.1145/2968219.2971593|https://doi.org/10.1145/2968219.2971593|New York, NY, USA|Association for Computing Machinery|9781450344623|2016|MQoL Smart Lab: Quality of Life Living Lab for Interdisciplinary Experiments|De Masi, Alexandre and Ciman, Matteo and Gustarini, Mattia and Wac, Katarzyna|inproceedings|10.1145/2968219.2971593|||||||||||||||||||||||||||||2100534875|42
EBIMCS 2021|Hong Kong, China|Quality of sequencing, Classification of gene Sequencer, Machine learning|5|84–88|2021 4th International Conference on E-Business, Information Management and Computer Science|Abstract: Biological sequencing plays a very important role in life science, especially with the improvement of sequencing technology and the development of sequencing instruments, and a large number of biological sequencing quality data are produced every day. Because of different sequencers, the quality of sequencing is different. In the process of sequencing quality control, the model of sequencer can be deduced according to the quality of gene sequence. Therefore, in this paper, five sequencers of Illumina HiSeq series, Illumina HiSeq 2000, Illumina HiSeq 2500, Illumina HiSeq 3000, Illumina HiSeq 4000 and Illumina HiSeq XTen, are selected as the classification objects. Firstly, the sequencing quality data of the five sequencers are preprocessed. Then, the classification model is trained by three machine learning algorithms: decision tree, logistic regression and support vector machine. The experimental results show that the accuracy rates of the three machine learning algorithms are 96.67%, 97.50% and 97.50% respectively. These algorithms are very good to solve the problem of using biological sequencing data quality to classify sequencer.|10.1145/3511716.3511730|https://doi.org/10.1145/3511716.3511730|New York, NY, USA|Association for Computing Machinery|9781450395687|2022|The Classification of Gene Sequencer Based on Machine Learning|Yang, Jie and Cao, Yong|inproceedings|10.1145/3511716.3511730|||||||||||||||||||||||||||||2101308065|42
||Adaptivity, Monitoring, Cloud computing, Big data, Scalability||67-79||Real-time monitoring of cloud resources is crucial for a variety of tasks such as performance analysis, workload management, capacity planning and fault detection. Applications producing big data make the monitoring task very difficult at high sampling frequencies because of high computational and communication overheads in collecting, storing, and managing information. We present an adaptive algorithm for monitoring big data applications that adapts the intervals of sampling and frequency of updates to data characteristics and administrator needs. Adaptivity allows us to limit computational and communication costs and to guarantee high reliability in capturing relevant load changes. Experimental evaluations performed on a large testbed show the ability of the proposed adaptive algorithm to reduce resource utilization and communication overhead of big data monitoring without penalizing the quality of data, and demonstrate our improvements to the state of the art.|https://doi.org/10.1016/j.jpdc.2014.08.007|https://www.sciencedirect.com/science/article/pii/S074373151400149X||||2015|Adaptive, scalable and reliable monitoring of big data on clouds|Mauro Andreolini and Michele Colajanni and Marcello Pietri and Stefania Tosi|article|ANDREOLINI201567|||Journal of Parallel and Distributed Computing|07437315||79-80||Special Issue on Scalable Systems for Big Data Management and Analytics|||25621|0,638|Q1|87|169|592|7166|2473|568|4,72|42,40|United States|Northern America|1984-2021|Computer Networks and Communications (Q1); Hardware and Architecture (Q1); Artificial Intelligence (Q2); Software (Q2); Theoretical Computer Science (Q2)|4,371|3.734|0.00477|2101351609|1083163244
||Resource management;Artificial intelligence;5G mobile communication;Wireless communication;Big Data;Quality of service;Network slicing;5G;beyond 5G (B5G);6G;artificial intelligence (AI);machine learning (ML);network slicing;resource management||58-77||How to explore and exploit the full potential of artificial intelligence (AI) technologies in future wireless communications such as beyond 5G (B5G) and 6G is an extremely hot inter-disciplinary research topic around the world. On the one hand, AI empowers intelligent resource management for wireless communications through powerful learning and automatic adaptation capabilities. On the other hand, embracing AI in wireless communication resource management calls for new network architecture and system models as well as standardized interfaces/protocols/data formats to facilitate the large-scale deployment of AI in future B5G/6G networks. This paper reviews the state-of-art AI-empowered resource management from the framework perspective down to the methodology perspective, not only considering the radio resource (e.g., spectrum) management but also other types of resources such as computing and caching. We also discuss the challenges and opportunities for AI-based resource management to widely deploy AI in future wireless communication networks.|10.23919/JCC.2020.03.006|||||2020|Artificial intelligence-empowered resource management for future wireless communications: A survey|Lin, Mengting and Zhao, Youping|article|9058606||March|China Communications|16735447|3|17|||||19700177325|0,508|Q2|42|227|629|6632|2012|607|3,53|29,22|China|Asiatic Region|2008-2020|Computer Networks and Communications (Q2); Electrical and Electronic Engineering (Q2)|2,891|2.688|0.00373|2101665938|2119267855
||Big data characteristics, Descriptive insight, Predictive insight, Prescriptive insight, Innovation competency||69-84||Grounded in gestalt insight learning theory and organizational learning theory, we collected data from 280 middle and top-level managers to investigate the impact of each big data characteristic (i.e., data volume, data velocity, data variety, and data veracity) on firm innovation competency (i.e., exploitation competency and exploration competency), mediated through data-driven insight generation (i.e., descriptive insight, predictive insight, and prescriptive insight). Findings show that while data velocity, variety, and veracity enhance data-driven insight generation, data volume does not impact it. Additionally, results of the post hoc analysis indicate that while descriptive and predictive insights improve innovation competency, prescriptive insight does not affect it. These results provide interesting and unique theoretical and practical insights.|https://doi.org/10.1016/j.jbusres.2019.07.006|https://www.sciencedirect.com/science/article/pii/S0148296319304138||||2019|Does big data enhance firm innovation competency? The mediating role of data-driven insights|Maryam Ghasemaghaei and Goran Calic|article|GHASEMAGHAEI201969|||Journal of Business Research|01482963||104|||||20550|2,049|Q1|195|878|1342|73221|11507|1315|7,38|83,40|United States|Northern America|1973-2021|Marketing (Q1)|46,935|7.550|0.03523|2103485598|1502892296
||Spark SQL, Inequality join, Selectivity estimation, Incremental, PostgreSQL|26|125–150||Inequality joins, which is to join relations with inequality conditions, are used in various applications. Optimizing joins has been the subject of intensive research ranging from efficient join algorithms such as sort-merge join, to the use of efficient indices such as $$B^+$$B+-tree, $$R^*$$R\'{z}-tree and Bitmap. However, inequality joins have received little attention and queries containing such joins are notably very slow. In this paper, we introduce fast inequality join algorithms based on sorted arrays and space-efficient bit-arrays. We further introduce a simple method to estimate the selectivity of inequality joins which is then used to optimize multiple predicate queries and multi-way joins. Moreover, we study an incremental inequality join algorithm to handle scenarios where data keeps changing. We have implemented a centralized version of these algorithms on top of PostgreSQL, a distributed version on top of Spark SQL, and an existing data cleaning system, Nadeef. By comparing our algorithms against well-known optimization techniques for inequality joins, we show our solution is more scalable and several orders of magnitude faster.|10.1007/s00778-016-0441-6|https://doi.org/10.1007/s00778-016-0441-6|Berlin, Heidelberg|Springer-Verlag||2017|Fast and Scalable Inequality Joins|Khayyat, Zuhair and Lucia, William and Singh, Meghna and Ouzzani, Mourad and Papotti, Paolo and Quian\'{e}-Ruiz, Jorge-Arnulfo and Tang, Nan and Kalnis, Panos|article|10.1007/s00778-016-0441-6||feb|The VLDB Journal|10668888|1|26|February  2017||||13646|0,653|Q1|90|64|117|4824|544|113|4,46|75,38|United States|Northern America|1992-2020|Hardware and Architecture (Q1); Information Systems (Q1)|2,106|2.868|0.0023|2103676100|924310569
BCB '22|Northbrook, Illinois|SMART-on-FHIR, user interface, pediatric patient care, data repository, FHIR, ETL, data interoperability|10||Proceedings of the 13th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics|World-renowned pediatric patient care in scoliosis, craniofacial, orthopedic, and other life-altering conditions is provided at the international Shriners Children's hospital system. The impact of scoliosis can be extreme with significant curvature of the spine that often progresses during childhood periods of growth and development. Gauging the impact of treatment is vital throughout the diagnostic and treatment process and is achieved using radiographic imaging and patient reported feedback surveys. Surgeons from multiple clinical centers have amassed a wealth of patient data from more than 1,000 scoliosis patients. However, these data are difficult to access due to data heterogeneity and poor interoperability between complex hospital systems. These barriers significantly decrease the value of these data to improve patient care. To solve these challenges, we create a generalizable multi-site and multi-modality cloud infrastructure for managing the clinical data of multiple diseases. First, we establish a standardized and secure research data repository using the Fast Health Interoperability Resources (FHIR) standard to harmonize multi-modal clinical data from different hospital sites. Additionally, we develop a SMART-on-FHIR application with a user-friendly graphical user interface (GUI) to enable non-technical users to access the harmonized clinical data. We demonstrate the generalizability of our solution by expanding it to also facilitate craniofacial microsomia and pediatric bone disease imaging research. Ultimately, we present a generalized framework for multi-site, multimodal data harmonization, which can efficiently organize and store data for clinical research to improve pediatric patient care.|10.1145/3535508.3545565|https://doi.org/10.1145/3535508.3545565|New York, NY, USA|Association for Computing Machinery|9781450393867|2022|Development of a Generalizable Multi-Site and Multi-Modality Clinical Data Cloud Infrastructure for Pediatric Patient Care|Hornback, Andrew and Shi, Wenqi and Giuste, Felipe O. and Zhu, Yuanda and Carpenter, Ashley M. and Hilton, Coleman and Bijanki, Vinieth N. and Stahl, Hiram and Gottesman, Gary S. and Purnell, Chad and Iwinski, Henry J. and Wattenbarger, J. Michael and Wang, May D.|inproceedings|10.1145/3535508.3545565|23||||||||||||||||||||||||||||2104361643|42
https://doi.org/10.1039/d2cc03598g|https://www.sciencedirect.com/science/article/pii/S1359734522020882||||2022|Addressing big data challenges in mass spectrometry-based metabolomics|Jian Guo and Huaxu Yu and Shipei Xing and Tao Huan|article|GUO20229979|||Chemical Communications|13597345|72|58||||||||||||||||||||||||||||||2105057776|42
||platform, data contract, cost model, Internet of Things|21|||This article introduces a dynamic cloud-based marketplace of near-realtime human sensing data (MARSA) for different stakeholders to sell and buy near-realtime data. MARSA is designed for environments where information technology (IT) infrastructures are not well developed but the need to gather and sell near-realtime data is great. To this end, we present techniques for selecting data types and managing data contracts based on different cost models, quality of data, and data rights. We design our MARSA platform by leveraging different data transferring solutions to enable an open and scalable communication mechanism between sellers (data providers) and buyers (data consumers). To evaluate MARSA, we carry out several experiments with the near-realtime transportation data provided by people in Ho Chi Minh City, Vietnam, and simulated scenarios in multicloud environments.|10.1145/2883611|https://doi.org/10.1145/2883611|New York, NY, USA|Association for Computing Machinery||2016|MARSA: A Marketplace for Realtime Human Sensing Data|Cao, Tien-Dung and Pham, Tran-Vu and Vu, Quang-Hieu and Truong, Hong-Linh and Le, Duc-Hung and Dustdar, Schahram|article|10.1145/2883611|16|may|ACM Trans. Internet Technol.|15335399|3|16|August 2016||||||||||||||||||||||2105545605|314651938
Big-DAMA '17|Los Angeles, CA, USA|mobile networks, call detail records, human mobility|6|43–48|Proceedings of the Workshop on Big Data Analytics and Machine Learning for Data Communication Networks|"\"The exploitation of cellular network data for studying human mobility has been a popular research topic in the last decade. Indeed, mobile terminals could be considered ubiquitous sensors that allow the observation of human movements on large scale without the need of relying on non-scalable techniques, such as surveys, or dedicated and expensive monitoring infrastructures. In particular, Call Detail Records (CDRs), collected by operators for billing purposes, have been extensively employed due to their rather large availability, compared to other types of cellular data (e.g., signaling). Despite the interest aroused around this topic, the research community has generally agreed about the scarcity of information provided by CDRs: the position of mobile terminals is logged when some kind of activity (calls, SMS, data connections) occurs, which translates in a picture of mobility somehow biased by the activity degree of users. By studying two datasets collected by a Nation-wide operator in 2014 and 2016, we show that the situation has drastically changed in terms of data volume and quality. The increase of flat data plans and the higher penetration of \"\"always connected\"\" terminals have driven up the number of recorded CDRs, providing higher temporal accuracy for users' locations.\""|10.1145/3098593.3098601|https://doi.org/10.1145/3098593.3098601|New York, NY, USA|Association for Computing Machinery|9781450350549|2017|"\"Call Detail Records for Human Mobility Studies: Taking Stock of the Situation in the \"\"Always Connected Era\"\"\""|Fiadino, Pierdomenico and Ponce-Lopez, Victor and Antonio, Juan and Torrent-Moreno, Marc and D'Alconzo, Alessandro|inproceedings|10.1145/3098593.3098601|||||||||||||||||||||||||||||2109710416|42
||Support vector machines;Data integrity;Key performance indicator;Time series analysis;Production;Prediction methods;Big Data;time series prediction;support vector regression;least square approximation||177-180|2022 7th International Conference on Big Data Analytics (ICBDA)|Closely monitoring service performance and making predictions of Key Performance Indicators (KPIs) are critical for Internet-based services. However, fast yet accurate prediction of these seasonal KPIs with various patterns and data quality has been a great challenge. This paper tackles this challenge through a novel approach based on auto-regressive Least Square Twin Support Vector Regression (LS-TSVR). As an improved version of SVR, LS-TSVR can handle big data without any external optimization, and meanwhile, the prediction accuracy is better than that of SVR. For seasonal KPI data in a production dataset, our methods satisfy or approximate a mean average error (MAE) of around 0.013, which is significantly lower than the baseline method.|10.1109/ICBDA55095.2022.9760331|||||2022|The Prediction Method of KPIs by Using LS-TSVR|Wang, Shiyang|inproceedings|9760331||March|||||||||||||||||||||||||||2110167256|42
||||1387-1388|||https://doi.org/10.1016/j.ijforecast.2019.05.004|https://www.sciencedirect.com/science/article/pii/S0169207019301062||||2019|Energy forecasting in the big data world|Tao Hong and Pierre Pinson|article|HONG20191387|||International Journal of Forecasting|01692070|4|35|||||22706|1,268|Q1|96|142|277|5578|1254|261|4,50|39,28|Netherlands|Western Europe|1985-2020|Business and International Management (Q1)|7,207|3.779|0.00612|2111210024|986650009
||ISO, ANSI, ISO 8000, ISO 22745, Semantic Encoding||191-205|Entity Information Life Cycle for Big Data|This chapter provides a discussion of the new International Organization for Standardization (ISO) standards related to the exchange of master data. It includes an in-depth look at the ISO 8000 family of standards, including ISO 8000-110, -120, -130, and -140, and their relationship to the ISO 22745-10, -30, and -40 standards. Also an explanation is given of simple versus strong ISO 8000-110 compliance, and the value proposition for ISO 8000 compliance is discussed.|https://doi.org/10.1016/B978-0-12-800537-8.00011-9|https://www.sciencedirect.com/science/article/pii/B9780128005378000119|Boston|Morgan Kaufmann|978-0-12-800537-8|2015|Chapter 11 - ISO Data Quality Standards for Master Data|John R. Talburt and Yinle Zhou|incollection|TALBURT2015191||||||||||John R. Talburt and Yinle Zhou|||||||||||||||||||2112612000|42
||||377-384||Historically, personalised medicine has been synonymous with pharmacogenomics and oncology. We argue for a new framework for personalised medicine analytics that capitalises on more detailed patient-level data and leverages recent advances in causal inference and machine learning tailored towards decision support applicable to critically ill patients. We discuss how advances in data technology and statistics are providing new opportunities for asking more targeted questions regarding patient treatment, and how this can be applied in the intensive care unit to better predict patient-centred outcomes, help in the discovery of new treatment regimens associated with improved outcomes, and ultimately how these rules can be learned in real-time for the patient.|https://doi.org/10.1016/j.accpm.2018.09.008|https://www.sciencedirect.com/science/article/pii/S2352556818302169||||2019|Big data and targeted machine learning in action to assist medical decision in the ICU|Romain Pirracchio and Mitchell J Cohen and Ivana Malenica and Jonathan Cohen and Antoine Chambaz and Maxime Cannesson and Christine Lee and Matthieu Resche-Rigon and Alan Hubbard|article|PIRRACCHIO2019377|||Anaesthesia Critical Care & Pain Medicine|23525568|4|38|||||21100435142|0,942|Q1|38|192|396|4565|590|179|1,48|23,78|Netherlands|Western Europe|2015-2020|Anesthesiology and Pain Medicine (Q1); Critical Care and Intensive Care Medicine (Q1); Medicine (miscellaneous) (Q2)||||2120643144|1325666609
||Big Data, marketing communication, management, data processing, collection, communication management||5156-5165||The boundaries between the online and offline worlds have become irretrievably blurred, especially as mobile devices have proliferated. As a result, more and more activities are transferred to the Internet. Every activity in the network leaves a trace, which is why the volume of available data is growing rapidly. The amount of increasing information affects all market participants, and the necessity to constantly collect and process large amounts of data becomes an everyday reality. The aim of the article is to present the concept of big data and to indicate examples of the use of big data to manage marketing communication with the environment. It should be emphasized that not only data transfer devices, but also human interaction contribute to the creation of very large data sets. Acquiring and correctly interpreting them plays an important role in market entities in terms of management, including communication management. Contemporary multi-directional communication, including communication in a hypermedia environment, creates new challenges and threats. The article was prepared based on a literature review, research reports and an analysis of secondary sources. It also outlines the practical implications. The considerations provided are the basis for further activities and empirical research.|https://doi.org/10.1016/j.procs.2021.09.293|https://www.sciencedirect.com/science/article/pii/S1877050921020329||||2021|Big Data as a tool helpful in communication management|Agnieszka Smalec|article|SMALEC20215156|||Procedia Computer Science|18770509||192||Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 25th International Conference KES2021|||19700182801|0,334|-|76|1907|6483|38511|13722|6359|2,09|20,19|Netherlands|Western Europe|2010-2020|Computer Science (miscellaneous)||||2127009878|2108686752
||Databases;Data integrity;Image matching;Fingerprint recognition;Big Data;Feature extraction;Convolutional neural networks;fingerprint matching;convolutional neural net-work;fingerprint minutiae points;point-matching||48-52|2022 8th International Conference on Big Data and Information Analytics (BigDIA)|Fingerprint is one of the most widely used biometric features to identify a person. Fingerprint matching is to compare an input fingerprint with fingerprints within the database to find the most similar fingerprints. However, existing fingerprint matching technologies face two major challenges: data quantity and data quality. From the perspective of data quantity, the existing fingerprint database stores hundreds of millions of data, so the traditional iterative matching method is not practical due to huge time consumption. From the data quality point, existing fingerprint products have a small collection area for aesthetics and portability, resulting in a low quality of the captured finger-print image, which seriously affects the accuracy of the matching algorithms. To address the above problems, we present a two-stage fingerprint matching method, which not only improves the efficiency of fingerprint matching, but also ensures the accuracy of fingerprint matching. The first stage utilizes a convolutional neural network trained with triplet loss to extract the overall feature of a fingerprint image. The cosine similarity between features can be used to identify possible fingerprint matches. The second stage compares fingerprint matches acquired in the first stage using a minutiae points comparing algorithm to get a more precise result. The experiment result shows that our method is satisfactory in both speed and accuracy.|10.1109/BigDIA56350.2022.9874011|||||2022|An Efficient and Effective Model for Large Scale Fingerprint Matching|Qideng, Tang and Chaofan, Dai|inproceedings|9874011||Aug||27716902|||||||||||||||||||||||||2127175489|980068981
||Big Data quality in business process, Big Data quality management processes, data cleaning in Big Data, Big Data quality and analytics, data integration in Big Data, frameworks, Data and information quality, and models|15|||Assessing and improving the quality of data are fundamental challenges in Big-Data applications. These challenges have given rise to numerous solutions targeting transformation, integration, and cleaning of data. However, while schema design, data cleaning, and data migration are nowadays reasonably well understood in isolation, not much attention has been given to the interplay between standalone tools in these areas. In this article, we focus on the problem of determining whether the available data-transforming procedures can be used together to bring about the desired quality characteristics of the data in business or analytics processes. For example, to help an organization avoid building a data-quality solution from scratch when facing a new analytics task, we ask whether the data quality can be improved by reusing the tools that are already available, and if so, which tools to apply, and in which order, all without presuming knowledge of the internals of the tools, which may be external or proprietary.Toward addressing this problem, we conduct a formal study in which individual data cleaning, data migration, or other data-transforming tools are abstracted as black-box procedures with only some of the properties exposed, such as their applicability requirements, the parts of the data that the procedure modifies, and the conditions that the data satisfy once the procedure has been applied. As a proof of concept, we provide foundational results on sequential applications of procedures abstracted in this way, to achieve prespecified data-quality objectives, for the use case of relational data and for procedures described by standard relational constraints. We show that, while reasoning in this framework may be computationally infeasible in general, there exist well-behaved cases in which these foundational results can be applied in practice for achieving desired data-quality results on Big Data.|10.1145/3428154|https://doi.org/10.1145/3428154|New York, NY, USA|Association for Computing Machinery||2021|Ensuring Data Readiness for Quality Requirements with Help from Procedure Reuse|Chirkova, Rada and Doyle, Jon and Reutter, Juan|article|10.1145/3428154|15|apr|J. Data and Information Quality|19361955|3|13|September 2021||||||||||||||||||||||2127343963|833754770
||Digital circular economy, Sustainability, Big data analytics, Competitive advantage, Resource-based view, Expert interviews||120957||Digital technologies are growing in importance for accelerating firms’ circular economy transition. However, so far, the focus has primarily been on the technical aspects of implementing these technologies with limited research on the organizational resources and capabilities required for successfully leveraging digital technologies for circular economy. To address this gap, this paper explores the business analytics resources firms should develop and how these should be orchestrated towards a firm-wide capability. The paper proposes a conceptual model highlighting eight business analytics resources that, in combination, build a business analytics capability for the circular economy and how this relates to firms’ circular economy implementation, resource orchestration capability, and competitive performance. The model is based on the results of a thematic analysis of 15 semi-structured expert interviews with key positions in industry. Our approach is informed by and further develops, the theory of the resource-based view and the resource orchestration view. Based on the results, we develop a deeper understanding of the importance of taking a holistic approach to business analytics when leveraging data and analytics towards a more efficient and effective digital-enabled circular economy, the smart circular economy.|https://doi.org/10.1016/j.techfore.2021.120957|https://www.sciencedirect.com/science/article/pii/S0040162521003899||||2021|Towards a business analytics capability for the circular economy|Eivind Kristoffersen and Patrick Mikalef and Fenna Blomsma and Jingyue Li|article|KRISTOFFERSEN2021120957|||Technological Forecasting and Social Change|00401625||171|||||14704|2,226|Q1|117|448|1099|35581|10127|1061|9,01|79,42|United States|Northern America|1970-2020|Applied Psychology (Q1); Business and International Management (Q1); Management of Technology and Innovation (Q1)|21,116|8.593|0.02416|2129281233|1949868303
||Data Warehouse, Big Data, Academic, Hadoop, Higher Education, Analysis||93-99||Nowadays, data warehouse tools and technologies cannot handle the load and analytic process of data into meaningful information for top management. Big data technology should be implemented to extend the existing data warehouse solutions. Universities already collect vast amounts of data so the academic data of university has been growing significantly and become a big academic data. These datasets are rich and growing. University’s top-level management needs tools to produce information from the records. The generated information is expected to support the decision-making process of top-level management. This paper explores how big data technology could be implemented with data warehouse to support decision making process. In this framework, we propose Hadoop as big data analytic tools to be implemented for data ingestion/staging. The paper concludes by outlining future directions relating to the development and implementation of an institutional project on Big Data.|https://doi.org/10.1016/j.procs.2017.12.134|https://www.sciencedirect.com/science/article/pii/S1877050917329022||||2017|Data Warehouse with Big Data Technology for Higher Education|Leo Willyanto Santoso and  Yulia|article|SANTOSO201793|||Procedia Computer Science|18770509||124||4th Information Systems International Conference 2017, ISICO 2017, 6-8 November 2017, Bali, Indonesia|||19700182801|0,334|-|76|1907|6483|38511|13722|6359|2,09|20,19|Netherlands|Western Europe|2010-2020|Computer Science (miscellaneous)||||2132188040|2108686752
|||14|745–758||Disease prediction has the potential to benefit stakeholders such as the government and health insurance companies. It can identify patients at risk of disease or health conditions. Clinicians can then take appropriate measures to avoid or minimize the risk and in turn, improve quality of care and avoid potential hospital admissions. Due to the recent advancement of tools and techniques for data analytics, disease risk prediction can leverage large amounts of semantic information, such as demographics, clinical diagnosis and measurements, health behaviours, laboratory results, prescriptions and care utilisation. In this regard, electronic health data can be a potential choice for developing disease prediction models. A significant number of such disease prediction models have been proposed in the literature over time utilizing large-scale electronic health databases, different methods, and healthcare variables. The goal of this comprehensive literature review was to discuss different risk prediction models that have been proposed based on electronic health data. Search terms were designed to find relevant research articles that utilized electronic health data to predict disease risks. Online scholarly databases were searched to retrieve results, which were then reviewed and compared in terms of the method used, disease type, and prediction accuracy. This paper provides a comprehensive review of the use of electronic health data for risk prediction models. A comparison of the results from different techniques for three frequently modelled diseases using electronic health data was also discussed in this study. In addition, the advantages and disadvantages of different risk prediction models, as well as their performance, were presented. Electronic health data have been widely used for disease prediction. A few modelling approaches show very high accuracy in predicting different diseases using such data. These modelling approaches have been used to inform the clinical decision process to achieve better outcomes.|10.1109/TCBB.2019.2937862|https://doi.org/10.1109/TCBB.2019.2937862|Washington, DC, USA|IEEE Computer Society Press||2021|Use of Electronic Health Data for Disease Prediction: A Comprehensive Literature Review|Hossain, Md. Ekramul and Khan, Arif and Moni, Mohammad Ali and Uddin, Shahadat|article|10.1109/TCBB.2019.2937862||apr|IEEE/ACM Trans. Comput. Biol. Bioinformatics|15455963|2|18|March-April 2021||||||||||||||||||||||2132193389|1878427007
||Social networking (online);Data integrity;Predictive models;Task analysis;Tuning;Context modeling;location-based social networks (LBSNs);missing POI identification;multi-network embedding||1386-1393|2021 IEEE Intl Conf on Parallel & Distributed Processing with Applications, Big Data & Cloud Computing, Sustainable Computing & Communications, Social Computing & Networking (ISPA/BDCloud/SocialCom/SustainCom)|The large volume of data flowing throughout location-based social networks (LBSNs) provides an opportunity for human mobility behavior understanding and prediction. However, data quality issues (e.g., historical check-in POI missing, data sparsity) limit the effectiveness of existing LBSN-oriented studies, e.g., Point-of-Interest (POI) recommendation or prediction. Contrary to previous efforts in next POI recommendation or prediction, we focus on identifying the missing POI which the user has visited at a past specific time and proposed a multi-network Embedding (MNE) method. Specifically, the model jointly captures temporal cyclic effect, user preference and sequence transition influence in a unified way by embedding five relational information graphs into a shared dimensional space from both POI- and category-instance levels. The proposed model also incorporates region-level spatial proximity to explore geographical influence, and derives the ranking score list of candidates for missing POI identification. We conduct extensive experiments to evaluate the performance of our model on two real large-scale datasets, and the experimental results show its superiority over other competitors. Significantly, it also proves that the proposed model can be naturally transferred to general next POI recommendation and prediction tasks with competitive performances.|10.1109/ISPA-BDCloud-SocialCom-SustainCom52081.2021.00189|||||2021|Multi-network Embedding for Missing Point-of-Interest Identification|Wu, Junhang and Hu, Ruimin and Li, Dengshi and Xiao, Yilin and Ren, Lingfei and Hu, Wenyi|inproceedings|9644693||Sep.|||||||||||||||||||||||||||2132778610|42
||Big Data;Multi-agent systems;Companies;Task analysis;Error correction;Data integrity;Real-time systems;Multi-agents;Big Data;Data quality;detection and correction of data errors||1-5|2019 Third International Conference on Intelligent Computing in Data Sciences (ICDS)|The quality of data for decision-making will always be a major factor for companies that want to remain competitors. In addition, the era of Big Data has brought new challenges for the processing, management, storage of data and in particular the challenge represented by the veracity of these data which is one of the 5Vs that characterizes Big Data. This characteristic that defines the quality or reliability of the data and its sources must be verified in the future systems of each company. In this paper, we present an approach that helps to improve the quality of Big Data by the distributed execution of algorithms for detecting and correcting data errors. The idea is to have a multi-agents model for errors detection and correction in big data flow. This model linked to a repository specific to each company. This repository contains the most frequent errors, metadata, error types, error detection algorithms and error correction algorithms. Each agent of this model represents an algorithm and will be deployed in multiple instances when needed. The use of these agents will go through two steps. In the first step, the detection agents and error correction agents manage each flow entering the system in real time. In the second step, all the processed data flows in first step will be a dataset to which the error detection and correction agents are applied in batch in order to process other types of errors. Among architectures who allow this processing type, we have chosen Lambda architecture.|10.1109/ICDS47004.2019.8942297|||||2019|Towards a multi-agents model for errors detection and correction in big data flows|Snineh, Sidi Mohamed and Bouattane, Omar and Youssfi, Mohamed and Daaif, Abdelaziz|inproceedings|8942297||Oct|||||||||||||||||||||||||||2133646755|42
||Data quality, Data interoperability, Social housing, Smart sustainable cities, Business intelligence||358-365||Smart Sustainable Cities (SSC) consist of multiple stakeholders, who must cooperate in order for SSCs to be successful. Housing is an important challenge and in many cities, therefore, a key stakeholder are social housing organisations. This paper introduces a qualitative case study of a social housing provider in the UK who implemented a business intelligence project (a method to assess data networks within an organisation) to increase data quality and data interoperability. Our analysis suggests that creating pathways for different information systems within an organisation to ‘talk to’ each other is the first step. Some of the issues during the project implementation include the lack of training and development, organisational reluctance to change, and the lack of a project plan. The challenges faced by the organisation during this project can be helpful for those implementing SSCs. Currently, many SSC frameworks and models exist, yet most seem to neglect localised challenges faced by the different stakeholders. This paper hopes to help bridge this gap in the SSC research agenda.|https://doi.org/10.1016/j.scs.2018.02.015|https://www.sciencedirect.com/science/article/pii/S2210670717312520||||2018|Data quality and governance in a UK social housing initiative: Implications for smart sustainable cities|Caroline Duvier and P.B. Anand and Crina Oltean-Dumbrava|article|DUVIER2018358|||Sustainable Cities and Society|22106707||39|||||19700194105|1,645|Q1|61|705|1286|43818|10974|1284|8,53|62,15|Netherlands|Western Europe|2011-2020|Civil and Structural Engineering (Q1); Geography, Planning and Development (Q1); Renewable Energy, Sustainability and the Environment (Q1); Transportation (Q1)|14,373|7.587|0.01684|2136719487|1912866754
ICGDA 2020|Marseille, France|Service platform, Quality, Data management|5|57–61|Proceedings of the 2020 3rd International Conference on Geoinformatics and Data Analysis|Quality data of Surveying and mapping is an intuitive reflection of the industry's quality situation and technical development situation. The construction of quality service platform of Surveying and mapping is discussed for the problems existing in the management of surveying and mapping quality data and for the demand for the use of quality data. It discusses the contents, framework and techniques used by the platform. The platform can be used to assist scientific decision-making and improve the service level.|10.1145/3397056.3397078|https://doi.org/10.1145/3397056.3397078|New York, NY, USA|Association for Computing Machinery|9781450377416|2020|Research on Construction of Quality Service Platform of Survey and Mapping|Ge, Juan and Han, Wenli and Zhang, Xunhu and Zhou, Jin|inproceedings|10.1145/3397056.3397078|||||||||||||||||||||||||||||2139767453|42
|||5|18–22||An interview with Paul Wicks, Vice President of Innovation at PatientsLikeMe, a patient network and real-time research platform.|10.1145/2676566|https://doi.org/10.1145/2676566|New York, NY, USA|Association for Computing Machinery||2014|Gathering People to Gather Data|MacLean, Diana Lynn|article|10.1145/2676566||dec|XRDS|15284972|2|21|Winter 2014||||||||||||||||||||||2140214650|613180419
||Big data management, Dynamic capabilities, Big data decision-making capability, Decision-making quality, China||103135||This study examines the antecedents and influence of big data decision-making capabilities on decision-making quality among Chinese firms. We propose that such capabilities are influenced by big data management challenges such as leadership, talent management, technology, and organisational culture. By using primary data from 108 Chinese firms and utilising partial least squares, we tested the antecedents of big data decision-making capability and its impact on decision-making quality. Findings suggest that big data management challenges are the key antecedents of big data decision-making capability. Furthermore, the latter is vital for big data decision-making quality.|https://doi.org/10.1016/j.im.2018.12.003|https://www.sciencedirect.com/science/article/pii/S0378720618302854||||2019|Role of big data management in enhancing big data decision-making capability and quality among Chinese firms: A dynamic capabilities view|Saqib Shamim and Jing Zeng and Syed Muhammad Shariq and Zaheer Khan|article|SHAMIM2019103135|||Information & Management|03787206|6|56|||||12303|2,147|Q1|162|130|248|11385|2482|246|8,94|87,58|Netherlands|Western Europe|1977-2020|Information Systems (Q1); Information Systems and Management (Q1); Management Information Systems (Q1)||||2141298546|1945939487
UnstructureNLP '13|San Francisco, California, USA|natural language processing, information fusion|4|11–14|Proceedings of the 2013 International Workshop on Mining Unstructured Big Data Using Natural Language Processing|Providing a single access point to an information system from multiple documents is helpful for biodiversity researchers as it is true in many fields. It not only saves the time for going back and forth from different sources but also provides the opportunity to generate new information out of the complementary information in different sources and levels of description. This paper investigates the potential of information fusion techniques in biodiversity area since the researchers in this domain desperately need information from different sources to verify their decision. In another sense, there are massive amounts of collections in this area. It is not easy or even possible for the researcher to manually collect information from different places. The proposed system contains 4 steps: Text segmentation and Taxonomic Name Identification, Organ-level and Sub-organ level Information Extraction, Relationship Identification, and Information fusion. Information fusion is based on the seven out of the twenty-four relationships in CST (Cross-document Sentence Theory). We argue that this kind of information fusion system might not only save the researchers the time for going back and forth from different sources but also provides the opportunity to generate new information out of the complementary information in different sources and levels.|10.1145/2513549.2513552|https://doi.org/10.1145/2513549.2513552|New York, NY, USA|Association for Computing Machinery|9781450324151|2013|Information Fusion in Taxonomic Descriptions|Wei, Qin|inproceedings|10.1145/2513549.2513552|||||||||||||||||||||||||||||2144685175|42
|||2|377–378|Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice|||https://doi.org/10.1145/3447404.3447425|New York, NY, USA|Association for Computing Machinery|9781450390293|2021|Ethics and Personal Context|McMenemy, David|inbook|10.1145/3447404.3447425|||||||||1||||||||||||||||||||2144894109|42
||Differential privacy;Analytical models;Power measurement;Laplace equations;Data integrity;Conferences;Companies;power metering data;cluster analysis;differential privacy;Laplacian noise||1-6|2021 IEEE 4th International Electrical and Energy Conference (CIEEC)|Power companies can use the power grid big data platform to cluster analysis of power metering data, which can improve the personalized service quality of power grid companies for different users and discover the power stealing behavior of users to protect the interests of power grid companies. However, in the cluster analysis of power measurement data, the privacy information of power users may also be disclosed. To defend the privacy information of power users, the article applies differential privacy technology to cluster analysis of power metering data to avoid power users’ privacy leakage. First, the article presents the attack model that exists in the cluster analysis of power metering data. Then, the article add Laplacian noise to the power metering data to defend against attacks in the cluster analysis of attackers. Next, to enhance the data availability of noise-added power measurement data in cluster analysis, the article limits noise distance based on the results of the cluster analysis. Experiments show that method proposed in article can guarantee the privacy information of power data during the cluster analysis of power metering data, and ensure the data quality of the power metering data after privacy protection.|10.1109/CIEEC50170.2021.9510797|||||2021|Privacy protection method of power metering data in clustering based on differential privacy|Yan, Xiaowen and Zhou, Yu and Huang, Fuxing and Wang, Xiaofen and Yuan, Peisen|inproceedings|9510797||May|||||||||||||||||||||||||||2144969832|42
