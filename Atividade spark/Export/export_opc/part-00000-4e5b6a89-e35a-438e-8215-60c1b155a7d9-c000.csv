series|location|keywords|numpages|pages|booktitle|abstract|doi|url|address|publisher|isbn|year|title|author|type_publication|ID|articleno|month|journal|issn|number|volume|issue_date|note|edition|editor|Sourceid|SJR|SJR_Best_Quartile|H_index|Total_Docs_2020|Total_Docs_3years|Total_Refs|Total_Cites_3years|Citable_Docs_3years|Cites_Doc_2years|Ref_Doc|Country|Region|Coverage|Categories|Total_Cites|Journal_Impact_Factor|Eigenfactor_Score|id_title|id_issn
||Data Cleaning and Information Extraction;Data Integration and Exchange|||Trends in Cleaning Relational Data: Consistency and Deduplication|"\"Data quality is one of the most important problems in data management, since dirty data often leads to inaccurate data analytics results and wrong business decisions. According to a report by InsightSquared in 2012, poor data across businesses and the government cost the United States economy 3.1 trillion dollars a year. To detect data errors, data quality rules or integrity constraints (ICs) have been proposed as a declarative way to describe legal or correct data instances. Any subset of data that does not conform to the defined rules is considered erroneous, which is also referred to as a violation. Various kinds of data repairing techniques with different objectives have been introduced where algorithms are used to detect subsets of the data that violate the declared integrity constraints, and even to suggest updates to the database such that the new database instance conforms with these constraints. While some of these algorithms aim to minimally change the database, others involve human experts or knowledge bases to verify the repairs suggested by the automatic repeating algorithms. Trends in Cleaning Relational Data: Consistency and Deduplication discusses the main facets and directions in designing error detection and repairing techniques. It proposes a taxonomy of current anomaly detection techniques, including error types, the automation of the detection process, and error propagation. It also sets out a taxonomy of current data repairing techniques, including the repair target, the automation of the repair process, and the update model. It concludes by highlighting current trends in \"\"big data\"\" cleaning.\""|10.1561/1900000045|https://ieeexplore.ieee.org/document/8187386||now|9781680830231|2015||Ilyas, Ihab F. and Chu, Xu|book|8187386|||||||||||||||||||||||||||||42|42
||Environmental informatics, Environmental observation web, Future internet, Cloud computing, Internet of things, Big data, Environmental specific enablers, Volunteered geographic information, Crowdtasking||1-15||This paper investigates the usability of Future Internet technologies (aka “Generic Enablers of the Future Internet”) in the context of environmental applications. The paper incorporates the best aspects of the state-of-the-art in environmental informatics with geospatial solutions and scalable processing capabilities of Internet-based tools. It specifically targets the promotion of the “Environmental Observation Web” as an observation-centric paradigm for building the next generation of environmental applications. In the Environmental Observation Web, the great majority of data are considered as observations. These can be generated from sensors (hardware), numerical simulations (models), as well as by humans (human sensors). Independently from the observation provenance and application scope, data can be represented and processed in a standardised way in order to understand environmental processes and their interdependencies. The development of cross-domain applications is then leveraged by technologies such as Cloud Computing, Internet of Things, Big Data Processing and Analytics. For example, “the cloud” can satisfy the peak-performance needs of applications which may occasionally use large amounts of processing power at a fraction of the price of a dedicated server farm. The paper also addresses the need for Specific Enablers that connect mainstream Future Internet capabilities with sensor and geospatial technologies. Main categories of such Specific Enablers are described with an overall architectural approach for developing environmental applications and exemplar use cases.|https://doi.org/10.1016/j.envsoft.2015.12.015|https://www.sciencedirect.com/science/article/pii/S1364815215301298||||2016|Future Internet technologies for environmental applications|Carlos Granell and Denis Havlik and Sven Schade and Zoheir Sabeur and Conor Delaney and Jasmin Pielorz and Thomas Usländer and Paolo Mazzetti and Katharina Schleidt and Mike Kobernus and Fuada Havlik and Nils Rune Bodsberg and Arne Berre and Jose Lorenzo Mon|article|GRANELL20161|||Environmental Modelling & Software|13648152||78|||||23295|1,828|Q1|136|223|717|14218|4373|711|5,47|63,76|Netherlands|Western Europe|1997-2020|Ecological Modeling (Q1); Environmental Engineering (Q1); Software (Q1)||||838387|665084965
IDEAS '21|Montreal, QC, Canada|Conceptual modeling, Category theory, Multi-model data, Inter-model relationships, Logical models|10|242–251|Proceedings of the 25th International Database Engineering &amp; Applications Symposium|Following the current trend, most of the well-known database systems, being relational, NoSQL, or NewSQL, denote themselves as multi-model. This industry-driven approach, however, lacks plenty of important features of the traditional DBMSs. The primary problem is a design of an optimal multi-model schema and its sufficiently general and efficient representation. In this paper, we provide an overview and discussion of the promising approaches that could potentially be capable of solving these issues, along with a summary of the remaining open problems.|10.1145/3472163.3472267|https://doi.org/10.1145/3472163.3472267|New York, NY, USA|Association for Computing Machinery|9781450389914|2021|Multi-Model Data Modeling and Representation: State of the Art and Research Challenges|Holubova, Irena and Contos, Pavel and Svoboda, Martin|inproceedings|10.1145/3472163.3472267|||||||||||||||||||||||||||||1219242|42
||data valuation, data markets, Datasets|7||||10.1145/3447269|https://doi.org/10.1145/3447269|New York, NY, USA|Association for Computing Machinery||2021|Toward a Complete Data Valuation Process. Challenges of Personal Data|Tufi\c{s}, Mihnea and Boratto, Ludovico|article|10.1145/3447269|20|aug|J. Data and Information Quality|19361955|4|13|December 2021||||||||||||||||||||||1350987|833754770
dg.o '16|Shanghai, China|Linked Data, Linked Enterprise Data, Open Data, XBRL|6|475–480|Proceedings of the 17th International Digital Government Research Conference on Digital Government Research|Linked Data can provide the data according to user's demand, promote the availability of half structured and unstructured data on the network, improve the interoperability of open data. With the development of Linked Data, Linked Enterprise Data becomes a research hotspot. This article draw lessons from foreign investment Linked Data research of listed companies, selected the listed company information and XBRL reports from Shanghai stock exchange and Shenzhen stock exchange, industry information from the China securities regulatory commission and daily stock price from Flush as data source, built investment open data of Chinese listed companies based on Linked Data. This work could promote the internationalization of the Chinese data and the commercial use of open government data, lay the foundation for the global data ecological system, at the same time prepare for the challenge of Shanghai-Hong Kong Stock Connect and Shenzhen-Hong Kong Stock Connect even the challenge of international investment.|10.1145/2912160.2912206|https://doi.org/10.1145/2912160.2912206|New York, NY, USA|Association for Computing Machinery|9781450343398|2016|Constructing Investment Open Data of Chinese Listed Companies Based on Linked Data|Li, Hongqin and Zhai, Jun|inproceedings|10.1145/2912160.2912206|||||||||||||||||||||||||||||9783337|42
||Quality control;Servers;Cloud computing;Big Data;Visualization;Inspection;Databases;Big data platform, Surface Enhanced Raman Spectroscopy, sensor quality control||463-466|2019 IEEE Conference on Multimedia Information Processing and Retrieval (MIPR)|Surface-enhanced Raman spectroscopy (SERS) significantly enhances the Raman scattering by molecules, enabling detection and identification of small quantities of relevant bio-/chemical markers in a wide range of applications. In this paper, we present a big data platform with both a local client and cloud server built for acquiring, processing, visualizing and storing SERS sensor data. The local client controls the hardware (i.e., spectrometer and stage) to collect SERS spectra from HP designed sensors, and offers the options to analyze, visualize and save the spectra with meta-data records, including relevant experimental conditions. The cloud server contains remote databases and web interface for centralized data management to users from different locations. Here we describe how this platform was built and demonstrate its use for automated sensor quality control based on sensor images. Sensor quality control is a common practice, employed in sensor production to select high performing sensors. Image-based approach is a natural way to perform sensor quality control without destructing the sensors. Automating this process using the proposed platform can also reduce the time spent and achieve consistent result by avoiding human visual inspection.|10.1109/MIPR.2019.00093|||||2019|A Big Data Platform for Surface Enhanced Raman Spectroscopy Data with an Application on Image-Based Sensor Quality Control|Zuo, Yiming and Vernica, Rares and Lei, Yang and Barcelo, Steven and Rogacs, Anita|inproceedings|8695373||March|||||||||||||||||||||||||||10486624|42
||Tools;Data models;Containers;Software;Real-time systems;Big Data applications;Big Data Applications;Quality Requirements;Big Data Goal-oriented Requirements Language;Requirements Modelling Tool||5977-5979|2019 IEEE International Conference on Big Data (Big Data)|The development of Big Data applications is not well-explored, to our knowledge. Embracing Big Data in system building, questions arise as to how to elicit, specify, analyse, model, and document Big Data quality requirements. In our ongoing research, we explore a requirements modelling language for Big Data software applications. In this paper, we introduce QualiBD, a modelling tool that implements the proposed goal-oriented requirements language that facilitates the modelling of Big Data quality requirements.|10.1109/BigData47090.2019.9006294|||||2019|QualiBD: A Tool for Modelling Quality Requirements for Big Data Applications|Arruda, Darlan and Madhavji, Nazim H.|inproceedings|9006294||Dec|||||||||||||||||||||||||||10776463|42
||Smart farming, Data analysis, Big data, Machine learning, Digital farming, Predictive farming, Farming industry||103624||The Internet of Things (IoT) and the relevant technologies have had a significant impact on smart farming as a major sub-domain within the field of agriculture. Modern technology supports data collection from IoT devices through several farming processes. The extensive amount of collected smart farming data can be utilized for daily decision making and analysis such as yield prediction, growth analysis, quality maintenance, animal and aquaculture, as well as farm management. This survey focuses on three major aspects of contemporary smart farming. First, it highlights various types of big data generated through smart farming and makes a broad categorization of such data. Second, this paper discusses a comprehensive set of typical applications of big data in smart farming. Third, it identifies and introduces the principal big data and machine learning techniques that are utilized in smart farming data analysis. In doing so, this survey also identifies some of the major, current challenges in smart farming big data analysis.This paper provides a discussion on potential pathways toward more effective smart farming through relevant analytics-guided decision making.|https://doi.org/10.1016/j.compind.2022.103624|https://www.sciencedirect.com/science/article/pii/S0166361522000197||||2022|A survey on smart farming data, applications and techniques|Sandya De Alwis and Ziwei Hou and Yishuo Zhang and Myung Hwan Na and Bahadorreza Ofoghi and Atul Sajjanhar|article|ALWIS2022103624|||Computers in Industry|01663615||138|||||19080|1,432|Q1|100|115|323|6926|3165|319|9,96|60,23|Netherlands|Western Europe|1979-2020|Computer Science (miscellaneous) (Q1); Engineering (miscellaneous) (Q1)|6,018|7.635|0.00584|11268317|605146181
CIKM '21|Virtual Event, Queensland, Australia|user behavior modeling, contrastive curriculum learning|10|3737–3746|Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management|Within online platforms, it is critical to capture the semantics of sequential user behaviors for accurately modeling user interests. However, dynamic characteristics and sparse behaviors make it difficult to train effective user representations for sequential user behavior modeling.Inspired by the recent progress in contrastive learning, we propose a novel Contrastive Curriculum Learning framework for producing effective representations for modeling sequential user behaviors. We make important technical contributions in two aspects, namely data quality and sample ordering. Firstly, we design a model-based data generator by generating high-quality samples confirming to users' attribute information. Given a target user, it can leverage the fused attribute semantics for generating more close-to-real sequences. Secondly, we propose a curriculum learning strategy to conduct contrastive learning via an easy-to-difficult learning process. The core component is a learnable difficulty evaluator, which can score augmented sequences, and schedule them in curriculums. Extensive results on both public and industry datasets demonstrate the effectiveness of our approach on downstream tasks.|10.1145/3459637.3481905|https://doi.org/10.1145/3459637.3481905|New York, NY, USA|Association for Computing Machinery|9781450384469|2021|Contrastive Curriculum Learning for Sequential User Behavior Modeling via Data Augmentation|Bian, Shuqing and Zhao, Wayne Xin and Zhou, Kun and Cai, Jing and He, Yancheng and Yin, Cunxiang and Wen, Ji-Rong|inproceedings|10.1145/3459637.3481905|||||||||||||||||||||||||||||19231447|42
||Data Governance;Data Credibility;Data Traceability||290-294|2020 IEEE Fifth International Conference on Data Science in Cyberspace (DSC)|In the actual project of data sharing and data governance, the problems of data heterogeneity and low data quality have not been solved. Data quality detection based on syntax rules can effectively find data quality problems, but it had not improved the data quality, especially in a variety of heterogeneous data source environments. This paper designs a data governance model based on data traceability, and we can get data feedback and revision through this model. The proposed method considers the different ownership of data and may form a closed-loop data service chain including effective data validation, data tracking, and data revision, data release. The analysis of the case shows that the method is effective to improve the data quality and meet the requirements of data security also.|10.1109/DSC50466.2020.00051|||||2020|A data traceability method to improve data quality in a big data environment|Zhang, Guobao|inproceedings|9172875||July|||||||||||||||||||||||||||20344682|42
||cloud computing, medical care, Body area networks, QoS, healthcare|46|||Body Area Networks (BANs) are becoming increasingly popular and have shown great potential in real-time monitoring of the human body. With the promise of being cost-effective and unobtrusive and facilitating continuous monitoring, BANs have attracted a wide range of monitoring applications, including medical and healthcare, sports, and rehabilitation systems. Most of these applications are real time and life critical and require a strict guarantee of Quality of Service (QoS) in terms of timeliness, reliability, and so on. Recently, there has been a number of proposals describing diverse approaches or frameworks to achieve QoS in BANs (i.e., for different layers or tiers and different protocols). This survey put these individual efforts into perspective and presents a more holistic view of the area. In this regard, this article identifies a set of QoS requirements for BAN applications and shows how these requirements are linked in a three-tier BAN system and presents a comprehensive review of the existing proposals against those requirements. In addition, open research issues, challenges, and future research directions in achieving these QoS in BANs are highlighted.|10.1145/3085580|https://doi.org/10.1145/3085580|New York, NY, USA|Association for Computing Machinery||2017|QoS in Body Area Networks: A Survey|Razzaque, M. A. and Hira, Muta Tah and Dira, Mukta|article|10.1145/3085580|25|aug|ACM Trans. Sen. Netw.|15504859|3|13|August 2017||||4700152843|0,598|Q2|67|44|118|2352|447|117|3,52|53,45|United States|Northern America|2005-2020|Computer Networks and Communications (Q2)|1,365|2.253|9.9E-4|20847787|1384228366
IoT 2019|Bilbao, Spain|agriculture, data privacy, privacy-preserving data analysis|4||Proceedings of the 9th International Conference on the Internet of Things|Today's herd management undergoes a major transformation triggered by the penetration of cheap sensor solutions into cattle farms, and the promise of predictive analytics to detect animal health issues and product-related problems before they occur. The latter is particularly important to prevent disease spread, ensure animal health, animal welfare and product quality. Sensor businesses entering the market tend to build their solutions as end-to-end pipelines spanning sensors, proprietary algorithms, cloud services, and mobile apps. Since data privacy is an important issue in this industry, as a result, disconnected data silos, heterogeneity of APIs, and lack of common standards limit the value the sensor technologies could provide for herd management. In the last few years, researchers and communities proposed a number of data integration architectures to enable exchange between streams of sensor data. This paper surveys the existing efforts and outlines the opportunities they fail to address by treating sensor data as a black box. We discuss alternative solutions to the problem based on privacy-preserving collaborative learning, and provide a set of scenarios to show their benefits for both farmers and businesses.|10.1145/3365871.3365900|https://doi.org/10.1145/3365871.3365900|New York, NY, USA|Association for Computing Machinery|9781450372077|2019|Embracing Opportunities of Livestock Big Data Integration with Privacy Constraints|"\"Papst, Franz and Saukh, Olga and R\"\"{o}mer, Kay and Grandl, Florian and Jakovljevic, Igor and Steininger, Franz and Mayerhofer, Martin and Duda, J\\\"\"{u}rgen and Egger-Danner, Christa\""|inproceedings|10.1145/3365871.3365900|27||||||||||||||||||||||||||||22478255|42
IDEAS '22|Budapest, Hungary|Generative Models, Neural Networks, Synthetic Data|9|94–102|Proceedings of the 26th International Database Engineered Applications Symposium|Generating synthetic data similar to realistic data is a crucial task in data augmentation and data production. Due to the preservation of authentic data distribution, synthetic data provide concealment of sensitive information and therefore enable Big Data acquisition for model training without facing privacy challenges. Nevertheless, the obstacles arise starting with acquiring real-world open-source data to effectively synthesizing new samples as genuine as possible. In this paper, a comparative study is conducted by considering the efficacy of different generative models like Generative Adversarial Networks (GAN), Variational Autoencoder (VAE), Synthetic Minority Oversampling Technique (SMOTE), Data Synthesizer (DS), Synthetic Data Vault with Gaussian Copula (SDV-G), Conditional Generative Adversarial Networks (SDV-GAN), and SynthPop Non-Parametric (SP-NP) approach to synthesize data with regard to various datasets. We used the pairwise correlation and Synthetic Data (SD) metrics as utility measures respectively between real data and generated data for evaluation. Accordingly, this paper investigates the effects of various data generation models, and the processing time of every model is included as one of the evaluation metrics.|10.1145/3548785.3548793|https://doi.org/10.1145/3548785.3548793|New York, NY, USA|Association for Computing Machinery|9781450397094|2022|Synthetic Data Generation: A Comparative Study|Endres, Markus and Mannarapotta Venugopal, Asha and Tran, Tung Son|inproceedings|10.1145/3548785.3548793|||||||||||||||||||||||||||||28067728|42
||Big data analytics, Advanced analysis, Artificial intelligence, Machine learning, Text analytics, Cross-industry processes||100274||We are living in a world where everything computes, everyone and everything is connected and sharing data. Going beyond just capturing and managing data, enterprises are tapping into IoT and Artificial Intelligence (AI) to create insights and intelligence in a revolutionary way that was not possible before. For instance, by analyzing unstructured data (such as text), call centers can extract entities, concepts, themes which can enable them to get faster insights that only few years back was not feasible. Public safety and law enforcement are only few of the examples that benefit from text analytics used to strengthen crime investigation. Sentiment Analysis, Content Classification, Language Detection and Intent Detection are just some of the Text Classification applications. The overall process model of such applications considering the complexity of the unstructured data, can be definitely challenging. In response to the chaotic emerging science of unstructured data analysis, the main goal of this paper is to first contribute to the gap of no existing methodology approach for Text Analytics projects, by introducing a methodology approach based on one of the most widely accepted and used methodology approach of CRISP-DM.|https://doi.org/10.1016/j.bdr.2021.100274|https://www.sciencedirect.com/science/article/pii/S2214579621000915||||2022|Cross-Industry Process Standardization for Text Analytics|Christina G. Skarpathiotaki and Konstantinos E. Psannis|article|SKARPATHIOTAKI2022100274|||Big Data Research|22145796||27|||||21100356018|0,565|Q2|25|11|76|547|324|69|4,06|49,73|United States|Northern America|2014-2020|Computer Science Applications (Q2); Information Systems (Q2); Information Systems and Management (Q2); Management Information Systems (Q2)|586|3.578|0.00126|28167997|1627174784
TEEM'18|Salamanca, Spain|citizen-driven archive, AI, computer vision, Digital Humanities, uncertainty|7|845–851|Proceedings of the Sixth International Conference on Technological Ecosystems for Enhancing Multiculturality|Uncertainties in data, e.g., incomplete data sets, data quality issues or inconsistencies in annotations, are a common phenomenon across disciplines. How to address these issues is context dependent. In this paper, we address uncertainties in the citizen-driven archive Topotheque as a concrete use-case in the Digital Humanities project exploreAT!, and demonstrate, how to deal with uncertainties by benchmarking a set of selected commercial computer vision (CV) tools. The approach aims to enrich Topotheque's data to enable better access, connectivity and analysis for both researchers and citizens. Results show that by applying CV, existing uncertainties are noticeably reduced, but new ones also introduced. Better grounds for semantic structuring are provided, enabling higher connectivity and linking within Topotheque, but also across other data sets. Ultimately, the enrichment of the archive is for the benefit of both researchers and citizens enabled by addressing and tackling apparent uncertainties.|10.1145/3284179.3284322|https://doi.org/10.1145/3284179.3284322|New York, NY, USA|Association for Computing Machinery|9781450365185|2018|Applying Commercial Computer Vision Tools to Cope with Uncertainties in a Citizen-Driven Archive: The Case Study Topothek@exploreAT!|Dorn, Amelie and Wandl-Vogt, Eveline and Palfinger, Thomas and D\'{\i}az, Jos\'{e} Luis Preza and Piringer, Barbara and Schatek, Alexander and Zoubek, Rainer|inproceedings|10.1145/3284179.3284322|||||||||||||||||||||||||||||28552632|42
||Artificial intelligence, Renewable energy, Energy demand, Decision making, Big data, Energy digitization||125834||The energy industry is at a crossroads. Digital technological developments have the potential to change our energy supply, trade, and consumption dramatically. The new digitalization model is powered by the artificial intelligence (AI) technology. The integration of energy supply, demand, and renewable sources into the power grid will be controlled autonomously by smart software that optimizes decision-making and operations. AI will play an integral role in achieving this goal. This study focuses on the use of AI techniques in the energy sector. This study aims to present a realistic baseline that allows researchers and readers to compare their AI efforts, ambitions, new state-of-the-art applications, challenges, and global roles in policymaking. We covered three major aspects, including: i) the use of AI in solar and hydrogen power generation; (ii) the use of AI in supply and demand management control; and (iii) recent advances in AI technology. This study explored how AI techniques outperform traditional models in controllability, big data handling, cyberattack prevention, smart grid, IoT, robotics, energy efficiency optimization, predictive maintenance control, and computational efficiency. Big data, the development of a machine learning model, and AI will play an important role in the future energy market. Our study’s findings show that AI is becoming a key enabler of a complex, new and data-related energy industry, providing a key magic tool to increase operational performance and efficiency in an increasingly cut-throat environment. As a result, the energy industry, utilities, power system operators, and independent power producers may need to focus more on AI technologies if they want meaningful results to remain competitive. New competitors, new business strategies, and a more active approach to customers would require informed and flexible regulatory engagement with the associated complexities of customer safety, privacy, and information security. Given the pace of development in information technology, AI and data analysis, regulatory approvals for new services and products in the new Era of digital energy markets can be enforced as quickly and efficiently as possible.|https://doi.org/10.1016/j.jclepro.2021.125834|https://www.sciencedirect.com/science/article/pii/S0959652621000548||||2021|Artificial intelligence in sustainable energy industry: Status Quo, challenges and opportunities|Tanveer Ahmad and Dongdong Zhang and Chao Huang and Hongcai Zhang and Ningyi Dai and Yonghua Song and Huanxin Chen|article|AHMAD2021125834|||Journal of Cleaner Production|09596526||289|||||19167|1,937|Q1|200|5126|10603|304498|103295|10577|9,56|59,40|United Kingdom|Western Europe|1993-2021|Environmental Science (miscellaneous) (Q1); Industrial and Manufacturing Engineering (Q1); Renewable Energy, Sustainability and the Environment (Q1); Strategy and Management (Q1)|170,352|9.297|0.18299|28576760|1121054297
CAIN '22|Pittsburgh, Pennsylvania||12|205–216|Proceedings of the 1st International Conference on AI Engineering: Software Engineering for AI|The adoption of Artificial Intelligence (AI) in high-stakes domains such as healthcare, wildlife preservation, autonomous driving and criminal justice system calls for a data-centric approach to AI. Data scientists spend the majority of their time studying and wrangling the data, yet tools to aid them with data analysis are lacking. This study identifies the recurrent data quality issues in public datasets. Analogous to code smells, we introduce a novel catalogue of data smells that can be used to indicate early signs of problems or technical debt in machine learning systems. To understand the prevalence of data quality issues in datasets, we analyse 25 public datasets and identify 14 data smells.|10.1145/3522664.3528621|https://doi.org/10.1145/3522664.3528621|New York, NY, USA|Association for Computing Machinery|9781450392754|2022|Data Smells in Public Datasets|Shome, Arumoy and Cruz, Lu\'{\i}s and van Deursen, Arie|inproceedings|10.1145/3522664.3528621|||||||||||||||||||||||||||||29542736|42
||Big Data;Power grids;Indexes;Power measurement;Time measurement;Memory;power big data;multi-source heterogeneous data;spatio-temporal correlation;data storage;multi-dimensional index||2386-2390|2019 IEEE 4th Advanced Information Technology, Electronic and Automation Control Conference (IAEAC)|The operation of complex AC/DC power grid changes rapidly and dynamically, which objectively puts forward higher requirements for on-line analysis, and it is urgent to improve the basic data quality of power grid. Because of low quality and poor synchronization of the basic data of power grid, it is impossible to accurately map the actual operation of the power grid. At the same time, the cross-system data matching degree is low and the data correlation is poor, so it can not support the multi-scale data analysis for all kinds of applications. In this paper, the associated method of multi-source heterogeneous data in the power grid is studied. Combined with big data's access characteristics, big data storage, big data retrieval and artificial intelligence technology, the high-speed data storage and index architecture of power big data are constructed, and a multi-dimensional index reflecting the associated relationship of operating data is established from the dimensions of time, space, application, device and so on. It is easier to analyze multi-source data, to improve the basic data quality of power grid, which provides effective support for accurate data analysis and evaluation of power grid.|10.1109/IAEAC47372.2019.8997699|||||2019|Multi-dimensional Index Construction of Electric Power Multi-source Measurement Data considering Spatio-temporal Correlation|Pan, Lingling and Liu, Jun and Li, Feng|inproceedings|8997699||Dec||23810947||1|||||||||||||||||||||||30061621|1121141125
||||1-5|CIBDA 2022; 3rd International Conference on Computer Information and Big Data Applications|"\"With the development of large data technology, the large variety and quantity of data put forward higher requirements on how to better use the data. The distribution of large data is characterized by \"\"data island\"\", \"\"wide area dispersion\"\", data dispersion, etc. Data types, data relationships and data quality are increasingly different, and contain a large number of unlabeled data, data sparse areas and domain knowledge. In order to solve the problem of multi-source heterogeneous data fusion, this paper innovatively introduces the Federal Learning algorithm, builds a Federal Learning Classifier, aggregates multiple fusion models of the same kind of data, and achieves the iterative optimization of data fusion without uploading data. The simulation results show that this algorithm model has certain advantages in data fusion stability and model iteration convergence speed.\""||||||2022|Research on data fusion method based on Federated learning|Yuan, Jianan and Huo, Chao and Zhang, Ganghong and Gao, Jian|inproceedings|9899089||March|||||||||||||||||||||||||||31275846|42
https://doi.org/10.1016/j.outlook.2016.11.021|https://www.sciencedirect.com/science/article/pii/S0029655416303967||||2017|Big data science: A literature review of nursing research exemplars|Bonnie L. Westra and Martha Sylvia and Elizabeth F. Weinfurter and Lisiane Pruinelli and Jung In Park and Dianna Dodd and Gail M. Keenan and Patricia Senk and Rachel L. Richesson and Vicki Baukner and Christopher Cruz and Grace Gao and Luann Whittenburg and Connie W. Delaney|article|WESTRA2017549|||Nursing Outlook|00296554|5|65||||||||||||||||||||||||||||||34947859|42
CIKM '19|Beijing, China|ontology, relation extraction, enterprise knowledge management, entity recognition, knowledge graph|2|2965–2966|Proceedings of the 28th ACM International Conference on Information and Knowledge Management|Data driven Knowledge Graph is rapidly adapted by different societies. Many open domain and specific domain knowledge graphs have been constructed, and many industries have benefited from knowledge graph. Currently, enterprise related knowledge graph is classified as specific domain, but the applications span from solving a narrow specific problem to Enterprise Knowledge Management system. With the digital transform of traditional industry, Enterprise knowledge becomes more and more complicated, it involves knowledge from common domain, multiple specific domains, and corporate-specific in general. This tutorial provides an overview of current Enterprise Knowledge Graph(EKG). It distinguishes the EKG from specific domain according to the knowledge it covers, and provides the examples to illustrate the difference between EKG and specific domain KG. The tutorial further summarizes EKG into three types: Specific Business Task Enterprise KG, Specific Business Unit Enterprise KG and Cross Business Unit Enterprise KG, and illustrates the characteristics, steps, challenges, and future research in constructing and consuming of each of these three types of EKG .|10.1145/3357384.3360314|https://doi.org/10.1145/3357384.3360314|New York, NY, USA|Association for Computing Machinery|9781450369763|2019|Enterprise Knowledge Graph From Specific Business Task to Enterprise Knowledge Management|Duan, Rong and Xiao, Yanghua|inproceedings|10.1145/3357384.3360314|||||||||||||||||||||||||||||39001193|42
||Data Visualization, Dimensionality Reduction, Parallel Processing, Hadoop, Machine Learning, Big Data||112-121||The amount of data created by people, machines and corporations around the world is growing every year. Thanks to innovations such as the Internet of Things, this trend will continue, giving rise to the creation of Big Data. Data visualization leverages principles of visual psychology to help stakeholders identify patterns, trends and correlations that might go undetected in text-based or spreadsheet data. The return on investment (ROI) of big data visualization is well-documented in numerous studies and use cases. However, to achieve ROI from analytics investments, key insights must be uncovered, understood and communicated. Synthesizing huge quantities of data into key insights grows more challenging as data volumes and varieties increase. To address visualization challenges posed by big and high-dimensional data, this paper explores algorithms and techniques that compress the amount of data and/or reduce the number of attributes to be analyzed and visualized. Specifically, this paper examines applying dimensionality reduction and data compression algorithms to reduce attributes, tuples and data points returned to the visualization. By reducing data returned to the visualization, trends, patterns and correlations are easier to view and visualization tool performance is optimized.|https://doi.org/10.1016/j.procs.2018.10.308|https://www.sciencedirect.com/science/article/pii/S1877050918319811||||2018|Visualizing High Dimensional and Big Data|Amy Genender-Feltheimer|article|GENENDERFELTHEIMER2018112|||Procedia Computer Science|18770509||140||Cyber Physical Systems and Deep Learning Chicago, Illinois November 5-7, 2018|||19700182801|0,334|-|76|1907|6483|38511|13722|6359|2,09|20,19|Netherlands|Western Europe|2010-2020|Computer Science (miscellaneous)||||39679325|2108686752
||Big data, Prediction, Case studies, Explanation, Elections, Weather||96-104||Has the rise of data-intensive science, or ‘big data’, revolutionized our ability to predict? Does it imply a new priority for prediction over causal understanding, and a diminished role for theory and human experts? I examine four important cases where prediction is desirable: political elections, the weather, GDP, and the results of interventions suggested by economic experiments. These cases suggest caution. Although big data methods are indeed very useful sometimes, in this paper's cases they improve predictions either limitedly or not at all, and their prospects of doing so in the future are limited too.|https://doi.org/10.1016/j.shpsa.2019.09.002|https://www.sciencedirect.com/science/article/pii/S0039368119300652||||2020|Big data and prediction: Four case studies|Robert Northcott|article|NORTHCOTT202096|||Studies in History and Philosophy of Science Part A|00393681||81|||||11600154632|0,615|Q1|37|95|150|5051|229|134|1,22|53,17|United Kingdom|Western Europe|1970-1971, 1974-1978, 1980-1981, 1983-1986, 1988, 1990-2020|History (Q1); History and Philosophy of Science (Q1)||||42338857|1900527991
https://doi.org/10.1016/j.cmpb.2021.106293|https://www.sciencedirect.com/science/article/pii/S0169260721003679||||2021|Management of medical and health big data based on integrated learning-based health care system: A review and comparative analysis|Yuguang Ye and Jianshe Shi and Daxin Zhu and Lianta Su and Jianlong Huang and Yifeng Huang|article|YE2021106293|||Computer Methods and Programs in Biomedicine|01692607||209||||||||||||||||||||||||||||||47459595|42
||Data integrity;Measurement;Warranties;Metadata;Cleaning;Big Data;Heterogeneous Data Sets;Data Quality;Metadata;Data Cleaning;Data Quality Assessment||155-162|2017 IEEE International Conference on Internet of Things (iThings) and IEEE Green Computing and Communications (GreenCom) and IEEE Cyber, Physical and Social Computing (CPSCom) and IEEE Smart Data (SmartData)|Every industry has significant data output as a product of their working process, and with the recent advent of big data mining and integrated data warehousing it is the case for a robust methodology for assessing the quality for sustainable and consistent processing. In this paper a review is conducted on Data Quality (DQ) in multiple domains in order to propose connections between their methodologies. This critical review suggests that within the process of DQ assessment of heterogeneous data sets, not often are they treated as separate types of data in need of an alternate data quality assessment framework. We discuss the need for such a directed DQ framework and the opportunities that are foreseen in this research area and propose to address it through degrees of heterogeneity.|10.1109/iThings-GreenCom-CPSCom-SmartData.2017.28|||||2017|Towards a Data Quality Framework for Heterogeneous Data|Micic, Natasha and Neagu, Daniel and Campean, Felician and Zadeh, Esmaeil Habib|inproceedings|8276745||June|||||||||||||||||||||||||||48265929|42
||ZigBee;Meters;Wireless communication;Automation;Smart meters;Monitoring;Sensors;Losses;smart meters;AMI;wireless technique;ZigBee technology;NTL||1-6|2020 International Conference on Artificial Intelligence, Big Data, Computing and Data Communication Systems (icABCD)|Utility services are experiencing a common problem of power losses, which impose a significant impact on their annual budget. Practically, power losses consist of technical losses and non-technical losses. Technical losses are due to operations and aging of infrastructure, while nontechnical losses (NTL) are due to non-metered energy. The focus is on managing non-technical losses using an automation wireless method. The wireless ZigBee technique is proposed and further investigated for communication failure over long distances while solving the problem of stealing electricity. Advance-metering infrastructure (AMI) technique and smart meters are feasible for system integration; that is why they are chosen to be part of this study. The success of the study depends on quality data of the Utility, meaning the more accurate the data, the easier the analysis of outliers. The operation and planning of revenue protection contain a large amount of data that needs to be worked on, so data mining assists in that regard. Then the load profiling method assists in illustrating the variation in demand/electricalload over a specific time. This is a preliminary investigation using a wireless communication technique as a viable solution in curbing electricity theft. The uniqueness of the proposed ZigBee system is that it recognizes the everyday act of stealing electricity through tempering with the meter box and tapping of the supply.|10.1109/icABCD49160.2020.9183812|||||2020|Curbing Electricity Theft Using Wireless Technique with Communication Constraints|Tshikomba, Salome C. and Estrice, Milton and Ojo, Evans and Davidson, Innocent E|inproceedings|9183812||Aug|||||||||||||||||||||||||||50505614|42
||Mobile crowdsensing, Smart contracts, Data aggregation, Incentive mechanism, dApp, IPFS||103483||The crowd's power, combined with the sensing capabilities of smart mobile de-vices, has resulted in the emergence of a revolutionary data acquisition paradigm known as Mobile Crowdsensing. In exchange for rewards, mobile users collect and share location-specific data values. However, most existing crowdsensing systems built on traditional centralized architectures are highly prone to attacks, intrusions, single point of failure, manipulations, and low reliability. Recently, decentralized blockchain technologies are being applied in mobile crowdsensing systems to ensure workers' privacy, data privacy, and the quality of sensed data at a low service fee. By leveraging blockchain technology, this paper inherits the advantages of the public blockchain without the need for any trusted third-parties. We propose a smart contract-based privacy-preserving data aggregation and quality assessment protocol to infer reliable aggregated results and estimate data quality, wherein, we design a fair quality-driven incentive mechanism to distribute rewards based on the data quality. The protocol ensures a secure, cost-optimal, and reliable aggregation and estimation of the sensed data quality on the public blockchain without disclosing the sensed data's and participants' privacy. We adopt Interplanetary File Systems to offset the blockchain's expensive storage costs. Experiments were conducted using real-world datasets which were implemented on a full-stack on-chain and off-chain decentralized application on the Ethereum blockchain. The experimental results demonstrate our design is highly efficient in achieving privacy-preserving data aggregation and significantly reduces on-chain computation costs.|https://doi.org/10.1016/j.jnca.2022.103483|https://www.sciencedirect.com/science/article/pii/S1084804522001291||||2022|Towards a privacy-preserving smart contract-based data aggregation and quality-driven incentive mechanism for mobile crowdsensing|Ruiyun Yu and Ann Move Oguti and Dennis Reagan Ochora and Shuchen Li|article|YU2022103483|||Journal of Network and Computer Applications|10848045||207|||||||||||||||||||||||55370741|2040591560
||entity resolution, Blocking, filtering|42|||Entity Resolution (ER), a core task of Data Integration, detects different entity profiles that correspond to the same real-world object. Due to its inherently quadratic complexity, a series of techniques accelerate it so that it scales to voluminous data. In this survey, we review a large number of relevant works under two different but related frameworks: Blocking and Filtering. The former restricts comparisons to entity pairs that are more likely to match, while the latter identifies quickly entity pairs that are likely to satisfy predetermined similarity thresholds. We also elaborate on hybrid approaches that combine different characteristics. For each framework we provide a comprehensive list of the relevant works, discussing them in the greater context. We conclude with the most promising directions for future work in the field.|10.1145/3377455|https://doi.org/10.1145/3377455|New York, NY, USA|Association for Computing Machinery||2020|Blocking and Filtering Techniques for Entity Resolution: A Survey|Papadakis, George and Skoutas, Dimitrios and Thanos, Emmanouil and Palpanas, Themis|article|10.1145/3377455|31|mar|ACM Comput. Surv.|03600300|2|53|March 2021||||23038|2,079|Q1|163|112|365|15859|6978|365|19,16|141,60|United States|Northern America|1969-2020|Computer Science (miscellaneous) (Q1); Theoretical Computer Science (Q1)|10,790|10.282|0.01438|59711183|1517405264
||Big data, Data quality, Data quality assessment, Data curation, Data standardization||270-283||Data quality assessment has gained attention in the recent years since more and more companies and medical centers are highlighting the importance of an automated framework to effectively manage the quality of their big data. Data cleaning, also known as data curation, lies in the heart of the data quality assessment and is a key aspect prior to the development of any data analytics services. In this work, we present the objectives, functionalities and methodological advances of an automated framework for data curation from a medical perspective. The steps towards the development of a system for data quality assessment are first described along with multidisciplinary data quality measures. A three-layer architecture which realizes these steps is then presented. Emphasis is given on the detection and tracking of inconsistencies, missing values, outliers, and similarities, as well as, on data standardization to finally enable data harmonization. A case study is conducted in order to demonstrate the applicability and reliability of the proposed framework on two well-established cohorts with clinical data related to the primary Sjögren's Syndrome (pSS). Our results confirm the validity of the proposed framework towards the automated and fast identification of outliers, inconsistencies, and highly-correlated and duplicated terms, as well as, the successful matching of more than 85% of the pSS-related medical terms in both cohorts, yielding more accurate, relevant, and consistent clinical data.|https://doi.org/10.1016/j.compbiomed.2019.03.001|https://www.sciencedirect.com/science/article/pii/S0010482519300733||||2019|Medical data quality assessment: On the development of an automated framework for medical data curation|Vasileios C. Pezoulas and Konstantina D. Kourou and Fanis Kalatzis and Themis P. Exarchos and Aliki Venetsanopoulou and Evi Zampeli and Saviana Gandolfo and Fotini Skopouli and Salvatore {De Vita} and Athanasios G. Tzioufas and Dimitrios I. Fotiadis|article|PEZOULAS2019270|||Computers in Biology and Medicine|00104825||107|||||17957|0,884|Q1|94|383|936|19977|5223|923|5,59|52,16|United Kingdom|Western Europe|1970-2020|Computer Science Applications (Q1); Health Informatics (Q2)|9,751|4.589|0.01186|60367680|1746679698
AIAM2021|Manchester, United Kingdom||4|1197–1200|2021 3rd International Conference on Artificial Intelligence and Advanced Manufacture|The rapid development of science and technology has promoted the rapid development and wide application of CT(computer Technology), especially the emergence of BDT(big data technology) in recent years. The integration and improvement of CT and it has promoted great changes in all aspects of society, and these changes are beneficial, and they have brought us great convenience and help Help. For the education industry, under the background of BDT, the integration of CT and modern teaching system can provide new development thinking and new direction for the reform of modern education. In order to study what effect the combination of the two will bring, this paper selects two universities and their students as the experimental research objects to explore how the modern teaching system will be innovated and developed under the effect of this new technology. The experimental results show that the students of a university who have applied CT in the modern teaching system have a high degree of satisfaction, and the percentage of those who are satisfied has reached 67%. Moreover, the score of teaching and research group of a university is relatively high, and the highest score is 95.|10.1145/3495018.3495364|https://doi.org/10.1145/3495018.3495364|New York, NY, USA|Association for Computing Machinery|9781450385046|2022|Optimization of Modern Teaching System with Computer Technology under the Background of Big Data|Ding, Gaohu|inproceedings|10.1145/3495018.3495364|||||||||||||||||||||||||||||63080981|42
||Electrical engineering;Data integrity;Education;Clustering algorithms;Big Data;Parallel processing;Data warehouses;English Teaching;Post Competency;Clustering Algorithm;System Assessment||887-891|2021 IEEE International Conference on Advances in Electrical Engineering and Computer Applications (AEECA)|With the rapid development of big data, user data information is increasingly perfect, data warehouse integration is more reasonable, and data quality is constantly improving, so the value of data is increasing. Based on parallel processing of K-means clustering algorithm, this paper extracts ability constraint information, integrates K-means clustering algorithm, clusters and integrates various index parameters of post competency. From the final experimental results, this method improves the execution efficiency and accuracy compared with other methods, and can be used in practice.|10.1109/AEECA52519.2021.9574197|||||2021|Competency Evaluation System of English Teaching Post based on K-Means Clustering Algorithm|Xue, Lian|inproceedings|9574197||Aug|||||||||||||||||||||||||||65061570|42
||Sleep-disordered breathing, Big data, Management, Sleep apnea||241-255|||https://doi.org/10.1016/j.jsmc.2016.01.009|https://www.sciencedirect.com/science/article/pii/S1556407X1630008X||||2016|The Role of Big Data in the Management of Sleep-Disordered Breathing|Rohit Budhiraja and Robert Thomas and Matthew Kim and Susan Redline|article|BUDHIRAJA2016241|||Sleep Medicine Clinics|1556407X|2|11||Novel Approaches to the Management of Sleep-Disordered Breathing|||17500155127|0,989|Q1|39|58|179|3875|513|147|2,71|66,81|United Kingdom|Western Europe|2006-2020|Clinical Psychology (Q1); Medicine (miscellaneous) (Q1); Neurology (clinical) (Q2); Neuropsychology and Physiological Psychology (Q2); Psychiatry and Mental Health (Q2)||||65836692|415339853
CCGRID '16|Cartagena, Columbia||7|649–655|Proceedings of the 16th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing|A big data analytics workflow is long and complex, with many programs, tools and scripts interacting together. In general, in modern organizations there is a significant amount of big data analytics processing performed outside a database system, which creates many issues to manage and process big data analytics workflows. In general, data preprocessing is the most time-consuming task in a big data analytics workflow. In this work, we defend the idea of preprocessing, computing models and scoring data sets inside a database system. In addition, we discuss recommendations and experiences to improve big data analytics workflows by pushing data preprocessing (i.e. data cleaning, aggregation and column transformation) into a database system. We present a discussion of practical issues and common solutions when transforming and preparing data sets to improve big data analytics workflows. As a case study validation, based on experience from real-life big data analytics projects, we compare pros and cons between running big data analytics workflows inside and outside the database system. We highlight which tasks in a big data analytics workflow are easier to manage and faster when processed by the database system, compared to external processing.|10.1109/CCGrid.2016.63|https://doi.org/10.1109/CCGrid.2016.63||IEEE Press|9781509024520|2016|Managing Big Data Analytics Workflows with a Database System|Ordonez, Carlos and Garc\'{\i}a-Garc\'{\i}a, Javier|inproceedings|10.1109/CCGrid.2016.63|||||||||||||||||||||||||||||66040420|42
||Knowledge-based dynamic capabilities view, Big data capabilities, Knowledge management, Sustainability marketing, Social media, Big data strategy||22-38||To address the unsolved problem of the mechanism underlying the effect of big data analytics capabilities on competitive advantage and performance, this study combined quantitative and qualitative methods to test the examined framework. The results of 257 questionnaires from hotel marketing managers and 19 semistructured interviews, confirm that big data analytics capabilities develop from big data strategies and knowledge management and enhance competitive advantage and performance through sustainability marketing. Moreover, social media enhance sustainability marketing and competitive advantage and performance. The original findings of the current research contribute to the development of big data, sustainability marketing, and social media.|https://doi.org/10.1016/j.jhtm.2022.02.026|https://www.sciencedirect.com/science/article/pii/S1447677022000389||||2022|Role of big data capabilities in enhancing competitive advantage and performance in the hospitality sector: Knowledge-based dynamic capabilities view|Jeou-Shyan Horng and Chih-Hsing Liu and Sheng-Fang Chou and Tai-Yi Yu and Da-Chian Hu|article|HORNG202222|||Journal of Hospitality and Tourism Management|14476770||51|||||||||||||||||||||||69581566|1534495294
ICBDT '18|Hangzhou, China|software project, software quality, earned value analysis, function point method|8|101–108|Proceedings of 2018 International Conference on Big Data Technologies|Earned value method is an important tool to evaluate and control the schedule and cost of the project. It is widely used in engineering construction projects, but rarely used in software projects. Due to the characteristics of the software project, the accuracy of the calculation of the basic parameters of the earned value analysis is low, which leads to the fact that the credibility of the result of the earned value analysis becomes very low or even meaningless. In order to make Earned value method applied to software projects better , IFPUG function points method is used to measure the actual completion of software projects, then used earned value method on the basis of IFPUG function point method. This can improved the accuracy of earned value analysis better. In order to analyze the cost and schedule of software projects better, the quality factors of software are also taken into account in the analysis of earned value, this can monitor the cost and schedule of software projects accurately.|10.1145/3226116.3226135|https://doi.org/10.1145/3226116.3226135|New York, NY, USA|Association for Computing Machinery|9781450364270|2018|Quality Earned Value Analysis Based on IFPUG Method in Software Project|Huang, Huijun and Zheng, Jiguang|inproceedings|10.1145/3226116.3226135|||||||||||||||||||||||||||||70009260|42
||Big Data, Fusion, High-performance analytics, Visualization||14-20|Application of Big Data for National Security|Threats to local, national, and global public security are continually evolving, and for those tasked with preventing and responding to these threats, the amount of potentially useful data can often seem overwhelming. What compounds this Big Data issue is the fact that we are living in a time of global economic austerity in which national security and law enforcement agencies need to become better at exploiting information while managing the demands of ever-shrinking budgets. This chapter looks at how, by using the latest software tools and techniques for data fusion and high-performance analytics, agencies can automate traditional labor-intensive tasks, gain a holistic view of information that originates from multiple sources, and extract valuable intelligence in a timely and more efficient manner.|https://doi.org/10.1016/B978-0-12-801967-2.00002-1|https://www.sciencedirect.com/science/article/pii/B9780128019672000021||Butterworth-Heinemann|978-0-12-801967-2|2015|Chapter 2 - Drilling into the Big Data Gold Mine: Data Fusion and High-Performance Analytics for Intelligence Professionals|Rupert Hollin|incollection|HOLLIN201514||||||||||Babak Akhgar and Gregory B. Saathoff and Hamid R. Arabnia and Richard Hill and Andrew Staniforth and Petra Saskia Bayerl|||||||||||||||||||72056557|42
ICBCT 2019|Honolulu, HI, USA|Blockchain, Data marketplaces, Smart Contracts, Data quality, Security|5|55–59|Proceedings of the 2019 International Conference on Blockchain Technology|"\"In the digital Economy 'Data is the new oil. In the last decade technology has disrupted every filed imaginable. One such booming technology is Blockchain. A blockchain is essentially a distributed database of records or public ledger of all transactions or digital events that have been executed and shared among participating parties. And once entered, the information is immutable. Ongoing projects and prior work in the fields of big data, data mining and data science has revealed how relevant data can be used to enhance products and services. There are uncountable applications and advantages of relevant data. The most valuable companies of today treat data as a commodity, which they trade and earn revenues.But use of relevant data has also drawn attention by the other non-conventional organizations and domains. To facilitate such trading, data marketplaces have emerged. In this paper we present a global data marketplace for users to easily buy and share data. The main focus of this research is to have a central data sharing platform for the recycling industry. This paper is a part of the research project \"\"Recycling 4.0\"\" which is focusing on sustainably improving the recycling process through exchange of information. We identify providing secure platform, data integrity and data quality as some major challenges for a data marketplace. In this paper we also explore how global data marketplace could be implemented using blockchain and similar technologies.\""|10.1145/3320154.3320165|https://doi.org/10.1145/3320154.3320165|New York, NY, USA|Association for Computing Machinery|9781450362689|2019|Blockchain Technology as an Approach for Data Marketplaces|Lawrenz, Sebastian and Sharma, Priyanka and Rausch, Andreas|inproceedings|10.1145/3320154.3320165|||||||||||||||||||||||||||||72271065|42
CSAI 2017|Jakarta, Indonesia|Metric, Measurement, Software, Big Data Analytics|5|265–269|Proceedings of the 2017 International Conference on Computer Science and Artificial Intelligence|Big data is defined as a very large data set (volume), velocity and variety. Big data analytics systems must be supports for parallel processing and large storage. The problem of this research is how to identify measurement metric based on big data analytics system characteristic. One device that support big data platform is Hadoop. Measurement is a process for assigning values or symbols to the attributes of an entity. The purpose of measurement is to distinguish between entities one to another. Indicator for software measurement represented with a metric. The aim of this research is to proposes some measurement metric for big data analytics system. This research using UML exactly a class diagram in system modelling to identify the measurement metric. Both of dynamic and static metric is proposed as solution to measure big data analytics system. Result for this researh are some measurement ndicator both of dynamic and static metric based on class diagram for big data analytics.|10.1145/3168390.3168425|https://doi.org/10.1145/3168390.3168425|New York, NY, USA|Association for Computing Machinery|9781450353922|2017|Measurement Metric Proposed For Big Data Analytics System|Samosir, Ridha Sefina and Hendric, Harco Leslie and Gaol, Ford Lumban and Abdurachman, Edi and Soewito, Benfano|inproceedings|10.1145/3168390.3168425|||||||||||||||||||||||||||||76141351|42
||Organizations;Parallel processing;Computer architecture;Security;Privacy;big data;data analytic;data quality;SMEs;business intelligence;cloud computing||1-6|2016 6th International Conference on Computer and Knowledge Engineering (ICCKE)|Wisdom aligns with technology is the key factor for sustainable business development. By increasing amount of public and private data, organizations need to find new solutions to manage data and information which lead to knowledge, better decision making, and value. In the big data-bang, smart organization surfing on-line technology and start planning big data strategy. However, many organizations do not yet have a big data strategy. A challenge facing SMEs is that they may not have the same capacity as large companies to analysis new data sets. Also, traditional data processing tools are not capable for SMEs decision making because of volume, velocity and variety if data. For address this problem we need new leveraging technology, tools and talent. SMEs which have risen to leveraging the value of big data are using advantage of cloud computing and open-source software to realize various goals. The main goal of this investment is about value as a new concept in a big data era. In this study, we focus on emerging trends and future requirement: technology and tools for SMEs.|10.1109/ICCKE.2016.7802106|||||2016|Leveraging big data technology for small and medium-sized enterprises (SMEs)|Kalan, Reza Shokri and Ünalir, Murat Osman|inproceedings|7802106||Oct|||||||||||||||||||||||||||77139543|42
||||495-498||Welcome to this special issue of the Future Generation Computer Systems (FGCS) journal. The special issue compiles seven technical contributions that significantly advance the state-of-the-art in exploration of Internet of Things (IoT) generated big data using semantic web techniques and technologies.|https://doi.org/10.1016/j.future.2017.06.032|https://www.sciencedirect.com/science/article/pii/S0167739X17313912||||2017|A note on exploration of IoT generated big data using semantics|Rajiv Ranjan and Dhavalkumar Thakker and Armin Haller and Rajkumar Buyya|article|RANJAN2017495|||Future Generation Computer Systems|0167739X||76|||||12264|1,262|Q1|119|798|1935|36054|16877|1868|9,11|45,18|Netherlands|Western Europe|1984-2021|Computer Networks and Communications (Q1); Hardware and Architecture (Q1); Software (Q1)||||78218258|562237118
||Big data, Precision healthcare, Personalized healthcare, Big Data analytics, Telepsychiatry, Deep learning||35-49|Big Data in Psychiatry #x0026; Neurology|Big data technologies enable correlation of multiple data sources into a coherent view. Big data and Big Data analytics have been used in public health, electronic consultation (e-consultation), real-time telediagnosis, precision healthcare, and personalized healthcare. e-Consultation is one aspect of telemedicine related to remote communication between medical specialists and clinicians, or clinicians and patients. It is generally implemented via the Internet or mobile communication devices (e.g., smartphone) and often generates big data. The concepts, characteristics, methods, emerging technologies, and software platforms or tools of big data and Big Data analytics are introduced in this chapter. Big data and applications in general healthcare are presented. Specifically, big data in precision healthcare and personalized healthcare are introduced. Challenges of big data and Big Data analytics in personalized healthcare are also outlined.|https://doi.org/10.1016/B978-0-12-822884-5.00017-9|https://www.sciencedirect.com/science/article/pii/B9780128228845000179||Academic Press|978-0-12-822884-5|2021|Chapter 2 - Big data in personalized healthcare|Lidong Wang and Cheryl Alexander|incollection|WANG202135||||||||||Ahmed A. Moustafa|||||||||||||||||||79957371|42
||Industries;Cloud computing;Visualization;5G mobile communication;Wireless networks;Data integrity;Computer architecture;5G network;vision detection;communication structure;edge computing;information systems||328-332|2021 IEEE 6th International Conference on Big Data Analytics (ICBDA)|The emergence of a large number of real-time data putforward higher requirements on network transmission technology. The new edge computing cloud technology based on 5G network has become an important research direction of vision detection. However, for the industrial users, they still confuse the architecture of the non-public 5G network (NPN) and misunderstand the data quality of service (QOS). In order to overcome the unstable network structure of 5G for vision detection in industry in a limited bandwidth, achieve high-quality transmission of detection image, and obtain intelligent optimal results, has become an urgent problem to be solved. This paper proposes the network configuration and mode, also design a intelligent edge computing cloud based on 5G scheme. In the ends, an vision detection architecture case has been developed on the 5G communication structure and verified visual detection application scene design its feasibility purpose in the wireless network.|10.1109/ICBDA51983.2021.9402999|||||2021|The Edge Computing Cloud Architecture Based on 5G Network for Industrial Vision Detection|Lu, Tielin and Fan, Zitian and Lei, Yue and Shang, Yujia and Wang, Chunxi|inproceedings|9402999||March|||||||||||||||||||||||||||80628408|42
CCGrid '18|Washington, District of Columbia||7|675–681|Proceedings of the 18th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing|"\"This paper introduces a general framework for supporting data-driven privacy-preserving big data management in distributed environments, such as emerging Cloud settings. The proposed framework can be viewed as an alternative to classical approaches where the privacy of big data is ensured via security-inspired protocols that check several (protocol) layers in order to achieve the desired privacy. Unfortunately, this injects considerable computational overheads in the overall process, thus introducing relevant challenges to be considered. Our approach instead tries to recognize the \"\"pedigree\"\" of suitable summary data representatives computed on top of the target big data repositories, hence avoiding computational overheads due to protocol checking. We also provide a relevant realization of the framework above, the so-called Data-dRIven aggregate-PROvenance privacy-preserving big Multidimensional data (DRIPROM) framework, which specifically considers multidimensional data as the case of interest.\""|10.1109/CCGRID.2018.00100|https://doi.org/10.1109/CCGRID.2018.00100||IEEE Press|9781538658154|2018|Pedigree-Ing Your Big Data: Data-Driven Big Data Privacy in Distributed Environments|Cuzzocrea, Alfredo and Damiani, Ernesto|inproceedings|10.1109/CCGRID.2018.00100|||||||||||||||||||||||||||||81091606|42
||expertise location, data analytics., social media|19|66–84||Knowing who is an expert on social media is a challenging yet important task, especially in a world where misleading information is commonplace and where social media is an important information source for knowledge seekers. In this paper we investigate expertise heuristics by comparing features of experts versus non-experts in big data settings. We employ a large set of features to classify experts and non-experts using data collected on two social media platform (Twitter and reddit). Our results show a good ability to predict who is an expert, especially using language-based features, validating that heuristics can be developed to differentiate experts from novices organically, based on social media use. Our results contribute to the development of expertise location and identification systems as well as our understanding on how experts present themselves on social media.|10.1145/3353401.3353406|https://doi.org/10.1145/3353401.3353406|New York, NY, USA|Association for Computing Machinery||2019|Recognizing Experts on Social Media: A Heuristics-Based Approach|Horne, Benjamin D. and Nevo, Dorit and Adal\i{}, Sibel|article|10.1145/3353401.3353406||jul|SIGMIS Database|00950033|3|50|August 2019||||4900152206|0,797|Q1|57|26|87|1770|193|76|1,70|68,08|United States|Northern America|1969-2020|Computer Networks and Communications (Q1); Management Information Systems (Q1)|772|1.828|5.2E-4|83051279|456565911
||Big Data, Network Filesystems, Synthetic Aperture Radar, Ice Sheet Data||1504-1513||We introduce the Forward Observer system, which is designed to provide data assurance in field data acquisition while receiving significant amounts (several terabytes per flight) of Synthetic Aperture Radar data during flights over the polar regions, which provide unique requirements for developing data collection and processing systems. Under polar conditions in the field and given the difficulty and expense of collecting data, data retention is absolutely critical. Our system provides a storage and analysis cluster with software that connects to field instruments via standard protocols, replicates data to multiple stores automatically as soon as it is written, and provides pre-processing of data so that initial visualizations are available immediately after collection, where they can provide feedback to researchers in the aircraft during the flight.|https://doi.org/10.1016/j.procs.2015.05.340|https://www.sciencedirect.com/science/article/pii/S1877050915011485||||2015|Big Data on Ice: The Forward Observer System for In-flight Synthetic Aperture Radar Processing|Richard Knepper and Matthew Standish and Matthew Link|article|KNEPPER20151504|||Procedia Computer Science|18770509||51||International Conference On Computational Science, ICCS 2015|||19700182801|0,334|-|76|1907|6483|38511|13722|6359|2,09|20,19|Netherlands|Western Europe|2010-2020|Computer Science (miscellaneous)||||86142108|2108686752
||context;big data||1-1|2015 IEEE International Conference on Pervasive Computing and Communication Workshops (PerCom Workshops)|In pervasive computing research and literature, context has mostly been seen as an information source for applications that adapt their behavior according to the current situation of their user or their (often physical) environment. This adaptation could be the change of the user interface, the performance of actions (like sending messages or triggering actuators), or the change of used resources (like network bandwidth or processing power). To determine relevant situations, many heterogeneous data sources could be used, ranging from sensor data over mined patterns in files to explicit user input. Since most sensors are not perfect, context quality has to be considered. And since many context-aware applications are mobile, the set of data sources may change during runtime. According to the widely used definition by Anind Dey, context can be “any information that can be used to characterize the situation of an entity”. In the past years, we have seen a significant increase in the so-called “big data” domain, in research, technology, and industrial usage. The desire to analyze, gain knowledge and use more and more data it in new ways is rising in a way that resemble a gold rush. Data is the new oil. Beside applications like predictive maintenance of machines or optimization of industrial processes, a main target for big data analyses are humans - in their roles as travelers, current or potential clients, or application users. We could say that big data is “any information that can be used to characterize the situation of a user”, and relate these approaches to what have been done in context modelling and reasoning. This gets even clearer when these analyses leave the virtual world (e.g., client behavior in web shops) and enter the real world (e.g., client behavior in retail). In addition to the ambiguities of the analysis itself that only leads to predictions with a limited probability, sensor data quality becomes an issue: the sensor data might be inaccurate, outdated or conflicting with other observations or physical laws; in addition, sensor data processing algorithms like object classification or tracking might lead to ambiguous results. In this talk, we will shortly review these two domains and derive what could be learned for context-aware applications. A special focus will be given on quality of context on all semantic levels, and how the improper consideration of quality issues can lead to dangerous digital prejudices.|10.1109/PERCOMW.2015.7133983|||||2015|Keynote: Context, big data, and digital prejudices|Nicklas, Daniela|inproceedings|7133983||March|||||||||||||||||||||||||||90112535|42
ICCSE'19|Jinan, China|Knowledge discovery, Data mining, Data management, Decision support systems, Society survey|9|195–203|Proceedings of the 4th International Conference on Crowd Science and Engineering|The General Society Survey(GSS) is a kind of government-funded survey which aims at examining the Socio-economic status, quality of life, and structure of contemporary society. GSS dataset is regarded as one of the authoritative source for the government and organization practitioners to make data-driven policies. The previous analytic approaches for GSS dataset are designed by combining expert knowledges and simple statistics. In this paper, we proposed a comprehensive data management and data mining approach for GSS datasets. The approach is designed to be operated in a two-phase manner: a data management phase which can improve the quality of GSS data by performing attribute preprocessing and filter-based attribute selection; a data mining phase which can extract hidden knowledges from the dataset by performing data mining analysis including prediction analysis, classification analysis, association analysis and clustering analysis. By leveraging the power of data mining techniques, our proposed approach can explore knowledges in a fine-grained manner with minimum human interference. Experiments on Chinese General Social Survey dataset are conducted at the end to evaluate the performance of our approach.|10.1145/3371238.3371269|https://doi.org/10.1145/3371238.3371269|New York, NY, USA|Association for Computing Machinery|9781450376402|2019|Comprehensive Data Management and Analytics for General Society Survey Dataset|Pan, Zhiwen and Zhao, Shuangye and Pacheco, Jesus and Zhang, Yuxin and Song, Xiaofan and Chen, Yiqiang and Dai, Lianjun and Zhang, Jun|inproceedings|10.1145/3371238.3371269|||||||||||||||||||||||||||||90524018|42
||Indoor PM, Infiltration factor, Indoor/outdoor ratio, Beijing||839-847||Due to time- and expense- consuming of conventional indoor PM2.5 (particulate matter with aerodynamic diameter of less than 2.5 μm) sampling, the sample size in previous studies was generally small, which leaded to high heterogeneity in indoor PM2.5 exposure assessment. Based on 4403 indoor air monitors in Beijing, this study evaluated indoor PM2.5 exposure from 15th March 2016 to 14th March 2017. Indoor PM2.5 concentration in Beijing was estimated to be 38.6 ± 18.4 μg/m3. Specifically, the concentration in non-heating season was 34.9 ± 15.8 μg/m3, which was 24% lower than that in heating season (46.1 ± 21.2 μg/m3). A significant correlation between indoor and ambient PM2.5 (p < 0.05) was evident with an infiltration factor of 0.21, and the ambient PM2.5 contributed approximately 52% and 42% to indoor PM2.5 for non-heating and heating seasons, respectively. Meanwhile, the mean indoor/outdoor (I/O) ratio was estimated to be 0.73 ± 0.54. Finally, the adjusted PM2.5 exposure level integrating the indoor and outdoor impact was calculated to be 46.8 ± 27.4 μg/m3, which was approximately 42% lower than estimation only relied on ambient PM2.5 concentration. This study is the first attempt to employ big data from commercial air monitors to evaluate indoor PM2.5 exposure and risk in Beijing, which may be instrumental to indoor PM2.5 pollution control.|https://doi.org/10.1016/j.envpol.2018.05.030|https://www.sciencedirect.com/science/article/pii/S0269749118307681||||2018|Using big data from air quality monitors to evaluate indoor PM2.5 exposure in buildings: Case study in Beijing|JinXing Zuo and Wei Ji and YuJie Ben and Muhammad Azher Hassan and WenHong Fan and Liam Bates and ZhaoMin Dong|article|ZUO2018839|||Environmental Pollution|02697491||240|||||23916|2,136|Q1|227|2109|4204|129684|35383|4169|8,04|61,49|United Kingdom|Western Europe|1970-1980, 1986-2020|Health, Toxicology and Mutagenesis (Q1); Medicine (miscellaneous) (Q1); Pollution (Q1); Toxicology (Q1)|84,491|8.071|0.07982|92846989|354628351
||Big data, Data pooling, Personalized medicine, Radiotherapy, Thyroid, Cancer management||73-78||The big data approach offers a powerful alternative to Evidence-based medicine. This approach could guide cancer management thanks to machine learning application to large-scale data. Aim of the Thyroid CoBRA (Consortium for Brachytherapy Data Analysis) project is to develop a standardized web data collection system, focused on thyroid cancer. The Metabolic Radiotherapy Working Group of Italian Association of Radiation Oncology (AIRO) endorsed the implementation of a consortium directed to thyroid cancer management and data collection. The agreement conditions, the ontology of the collected data and the related software services were defined by a multicentre ad hoc working-group (WG). Six Italian cancer centres were firstly started the project, defined and signed the Thyroid COBRA consortium agreement. Three data set tiers were identified: Registry, Procedures and Research. The COBRA-Storage System (C-SS) appeared to be not time-consuming and to be privacy respecting, as data can be extracted directly from the single centre's storage platforms through a secured connection that ensures reliable encryption of sensible data. Automatic data archiving could be directly performed from Image Hospital Storage System or the Radiotherapy Treatment Planning Systems. The C-SS architecture will allow “Cloud storage way” or “distributed learning” approaches for predictive model definition and further clinical decision support tools development. The development of the Thyroid COBRA data Storage System C-SS through a multicentre consortium approach appeared to be a feasible tool in the setup of complex and privacy saving data sharing system oriented to the management of thyroid cancer and in the near future every cancer type.|https://doi.org/10.1016/j.ejim.2018.02.012|https://www.sciencedirect.com/science/article/pii/S0953620518300621||||2018|A new standardized data collection system for interdisciplinary thyroid cancer management: Thyroid COBRA|Luca Tagliaferri and Carlo Gobitti and Giuseppe Ferdinando Colloca and Luca Boldrini and Eleonora Farina and Carlo Furlan and Fabiola Paiar and Federica Vianello and Michela Basso and Lorenzo Cerizza and Fabio Monari and Gabriele Simontacchi and Maria Antonietta Gambacorta and Jacopo Lenkowicz and Nicola Dinapoli and Vito Lanzotti and Renzo Mazzarotto and Elvio Russi and Monica Mangoni|article|TAGLIAFERRI201873|||European Journal of Internal Medicine|09536205||53|||||26617|0,894|Q2|71|384|907|8947|1904|557|2,14|23,30|Netherlands|Western Europe|1989-2020|Internal Medicine (Q2)|7,083|4.487|0.00933|96302001|794196092
CHI '19|Glasgow, Scotland Uk|needs analysis, law enforcement, qualitative, human trafficking|14|1–14|Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems|In working to rescue victims of human trafficking, law enforcement officers face a host of challenges. Working in complex, layered organizational structures, they face challenges of collaboration and communication. Online information is central to every phase of a human-trafficking investigation. With terabytes of available data such as sex work ads, policing is increasingly a big-data research problem. In this study, we interview sixteen law enforcement officers working to rescue victims of human trafficking to try to understand their computational needs. We highlight three major areas where future work in human-computer interaction can help. First, combating human trafficking requires advances in information visualization of large, complex, geospatial data, as victims are frequently forcibly moved across jurisdictions. Second, the need for unified information databases raises critical research issues of usable security and privacy. Finally, the archaic nature of information systems available to law enforcement raises policy issues regarding resource allocation for software development.|10.1145/3290605.3300561|https://doi.org/10.1145/3290605.3300561|New York, NY, USA|Association for Computing Machinery|9781450359702|2019|Understanding Law Enforcement Strategies and Needs for Combating Human Trafficking|Deeb-Swihart, Julia and Endert, Alex and Bruckman, Amy|inproceedings|10.1145/3290605.3300561|||||||||||||||||||||||||||||99931652|42
CIKM '20|Virtual Event, Ireland|dirty source, approximate match, cep, heterogeneous source, complex event processing, low-quality data|4|3237–3240|Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management|Pattern matching is an important task in the field of Complex Event Processing (CEP). However, exact event pattern matching methods could suffer from low hit rate and loss for meaningful events identification due to the heterogeneous and dirty sources in the big data era. Since both events and patterns could be imprecise, the actual event trace may have different event names as well as structures from the pre-defined pattern. The low-quality data even intensifies the difficulty of matching. In this work, we propose to learn embedding representations for patterns and event traces separately and calculate their similarity as the scores for approximate matching.|10.1145/3340531.3418506|https://doi.org/10.1145/3340531.3418506|New York, NY, USA|Association for Computing Machinery|9781450368599|2020|Approximate Event Pattern Matching over Heterogeneous and Dirty Sources|Huang, Ruihong|inproceedings|10.1145/3340531.3418506|||||||||||||||||||||||||||||101906417|42
||||298-299|||https://doi.org/10.1016/j.jcrc.2019.09.005|https://www.sciencedirect.com/science/article/pii/S0883944119313723||||2019|The electronic medical record: Big data, little information?|Walter Verbrugghe and Kirsten Colpaert|article|VERBRUGGHE2019298|||Journal of Critical Care|08839441||54|||||||||||||||||||||||103189457|1415751128
||Data integrity;Redundancy;Databases;Dynamic programming;Education;Remuneration;Organizations;Science and Technology Cloud;data quality;data redundancy;missing value processing||319-323|2018 IEEE 3rd International Conference on Big Data Analysis (ICBDA)|This paper analyzes and summarizes the data quality problems in the Shaanxi Science and Technology Resource Coordination Center “Science and Technology Cloud” project. These two major problems about scientific and technological information data quality are verified. One is data redundancy caused by organizations' name abbreviation and the other is partial scientific and technological information data missing. This paper designs and implements solutions to the problems in “Science and Technology Cloud” project. This paper extracts 15643 data from scientific and technical talent pool and scientific literature library. The experimental results verify the effectiveness and feasibility of the solution of data redundancy and data missing.|10.1109/ICBDA.2018.8367700|||||2018|Verification method of data quality in science and technology cloud in Shaanxi province|Yu, Bin and Zhang, Chen and Tang, ZhouHua and Sun, JiangYan|inproceedings|8367700||March|||||||||||||||||||||||||||103295327|42
IHC 2018|Bel\'{e}m, Brazil|Systematic Mapping Review, Human-Data Interaction, Human-Computer Interaction|12||Proceedings of the 17th Brazilian Symposium on Human Factors in Computing Systems|"\"Big Data, e-Science and Internet of Things have contributed to increase the production, processing and storage of data, changing the way people deal and live with data. Although the problem is not new, the \"\"human aspect\"\" of data and the possible impact of Human-Data Interaction (HDI) in human life have been explored and discussed as an emerging research area. On the one hand, HDI offers plenty of opportunities for research and development, and on the other hand it demands characterization, grounding, critical discussions, empirical results and thinking tools to support research and practice. This paper presents a Systematic Mapping of Literature on HDI in Computer Science, identifying the different definitions for the area, elements or objects of investigation, contexts of application, stakeholders, etc. Based on 28 selected papers, results point out to a lack of definition or agreement on what HDI is, but suggest that there are different aspects that can characterize it, and allow identifying concerns and objects of study, such as privacy, ownership and transparency. Results suggest a demand for theoretical and methodological frameworks to support the understanding, design and evaluation of HDI via computing systems.\""|10.1145/3274192.3274219|https://doi.org/10.1145/3274192.3274219|New York, NY, USA|Association for Computing Machinery|9781450366014|2018|Human Data-Interaction: A Systematic Mapping|Strey, Mateus Rambo and Pereira, Roberto and de Castro Salgado, Luciana C.|inproceedings|10.1145/3274192.3274219|27||||||||||||||||||||||||||||110154634|42
|||2|407–408|Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice|||https://doi.org/10.1145/3447404.3447427|New York, NY, USA|Association for Computing Machinery|9781450390293|2021|Ethics and Adaptive Touch Interfaces|McMenemy, David|inbook|10.1145/3447404.3447427|||||||||1||||||||||||||||||||111579253|42
|||8|58–65||Featuring the various dimensions of data management, it guides organizations through implementation fundamentals.|10.1145/3210752|https://doi.org/10.1145/3210752|New York, NY, USA|Association for Computing Machinery||2018|Framework for Implementing a Big Data Ecosystem in Organizations|Orenga-Rogl\'{a}, Sergio and Chalmeta, Ricardo|article|10.1145/3210752||dec|Commun. ACM|00010782|1|62|January 2019||||||||||||||||||||||112576378|647144465
||data driven sustainable enterprise, fuzzy unordered induction algo, big data analytics, condition-based maintenance, machine learning techniques, backward feature elimination||428-439||Smart manufacturing refers to a future-state of manufacturing and it can lead to remarkable changes in all aspects of operations through minimizing energy and material usage while simultaneously maximizing sustainability enabling a futuristic more digitalized scenario of manufacturing. This research develops a big data analytics framework that optimizes the maintenance schedule through condition-based maintenance (CBM) optimization and also improves the prediction accuracy to quantify the remaining life prediction uncertainty. Through effective utilization of condition monitoring and prediction information, CBM would enhance equipment reliability leading to reduction in maintenance cost. The proposed framework uses a CBM optimization method that utilizes a new linguistic interval-valued fuzzy reasoning method for predicting the information. The proposed big data analytics framework in our study for estimating the uncertainty based on backward feature elimination and fuzzy unordered rule induction algorithm prediction errors, is an innovative contribution to the remaining life prediction field. Our paper elaborates on the basic underlying structure of CBM system that is defined by transaction matrix and the threshold value of failure probability. We developed this framework for analysing the CBM policy cost more accurately and to find the probabilistic threshold values of covariate that corresponds to the lowest price of predictive maintenance cost. The experimental results are performed on a big dataset which is generated from a sophisticated simulator of a gas turbine propulsion plant. A comparative analysis confirms that the method used in the proposed framework outpaces the classical methods in terms of classification accuracy and other statistical performance evaluation metrics.|https://doi.org/10.1016/j.jocs.2017.06.006|https://www.sciencedirect.com/science/article/pii/S1877750316305129||||2018|A big data driven sustainable manufacturing framework for condition-based maintenance prediction|Ajay Kumar and Ravi Shankar and Lakshman S. Thakur|article|KUMAR2018428|||Journal of Computational Science|18777503||27|||||19700174607|0,704|Q1|46|123|485|5550|2325|465|4,97|45,12|Netherlands|Western Europe|2010-2020|Computer Science (miscellaneous) (Q1); Modeling and Simulation (Q2); Theoretical Computer Science (Q2)|3,198|3.976|0.00489|112841487|185640112
||Big Data, medical and healthcare Big Data, Big Data Analytics, databases, healthcare information systems||85-112|Applications of Big Data in Healthcare|In the era of big data, a huge volume of heterogeneous healthcare and medical data are generated daily. These heterogeneous data, that are stored in diverse data formats, have to be integrated and stored in a standard way and format to perform suitable efficient and effective data analysis and visualization. These data, which are generated from different sources such as mobile devices, sensors, lab tests, clinical notes, social media, demographics data, diverse omics data, etc., can be structured, semistructured, or unstructured. These varieties of data structures require these big data to be stored not only in the standard relational databases but also in NoSQL databases. To provide effective data analysis, suitable classification and standardization of big data in medicine and healthcare are necessary, as well as excellent design and implementation of healthcare information systems. Regarding the security and privacy of the patient’s data, we suggest employing suitable data governance policies. Additionally, we suggest choosing of proper software development frameworks, tools, databases, in-database analytics, stream computing and data mining algorithms (supervised, unsupervised and semisupervised) to reveal valuable knowledge and insights from these healthcare and medical big data. Ultimately we propose the development of not only patient-oriented but also decision- and population-centric healthcare information systems.|https://doi.org/10.1016/B978-0-12-820203-6.00005-9|https://www.sciencedirect.com/science/article/pii/B9780128202036000059||Academic Press|978-0-12-820203-6|2021|4 - Healthcare and medical Big Data analytics|Blagoj Ristevski and Snezana Savoska|incollection|RISTEVSKI202185||||||||||Ashish Khanna and Deepak Gupta and Nilanjan Dey|||||||||||||||||||114837676|42
IUI '22|Helsinki, Finland||22|323–344|27th International Conference on Intelligent User Interfaces|Much of the reported work on personas suffers from the lack of empirical evidence. To address this issue, we introduce Persona Analytics (PA), a system that tracks how users interact with data-driven personas. PA captures users’ mouse and gaze behavior to measure users’ interaction with algorithmically generated personas and use of system features for an interactive persona system. Measuring these activities grants an understanding of the behaviors of a persona user, required for quantitative measurement of persona use to obtain scientifically valid evidence. Conducting a study with 144 participants, we demonstrate how PA can be deployed for remote user studies during exceptional times when physical user studies are difficult, if not impossible.|10.1145/3490099.3511144|https://doi.org/10.1145/3490099.3511144|New York, NY, USA|Association for Computing Machinery|9781450391443|2022|Developing Persona Analytics Towards Persona Science|Salminen, Joni and Jung, Soon-Gyo and Jansen, Bernard|inproceedings|10.1145/3490099.3511144|||||||||||||||||||||||||||||118923611|42
CHI EA '18|Montreal QC, Canada|information understanding, collaborative work, sensemaking|7|1–7|Extended Abstracts of the 2018 CHI Conference on Human Factors in Computing Systems|Sensemaking is a common activity in the analysis of a large or complex amount of information. This active area of HCI research asks how DO people come to understand such difficult sets of information? The information workplace is increasing dominated by high velocity, high volume, complex information streams. At the same time, understanding how sensemaking operates has become an urgent need in an era of increasingly unreliable news and information sources. While there has been a huge amount of work in this space, the research involved is scattered over a number of different domains with differing approaches. This workshop will focus on the most recent work in sensemaking, the activities, technologies and behaviors that people do when making sense of their complex information spaces. In the second part of the workshop we will work to synthesize a cross-disciplinary view of how sensemaking works in people, along with the human behaviors, biases, proclivities, and technologies required to support it.|10.1145/3170427.3170636|https://doi.org/10.1145/3170427.3170636|New York, NY, USA|Association for Computing Machinery|9781450356213|2018|Sensemaking in a Senseless World: 2018 Workshop Abstract|Russell, Daniel M. and Convertino, Gregorio and Kittur, Aniket and Pirolli, Peter and Watkins, Elizabeth Anne|inproceedings|10.1145/3170427.3170636|||||||||||||||||||||||||||||120335828|42
ICBDM 2020|Manchester, United Kingdom|Business Process Model, Quality Requirements, Non-Functional Requirements, Use Case, System Model, Business Models, Business/IT Alignment|6|84–89|Proceedings of the 2020 International Conference on Big Data in Management|Non-Functional Requirements (NFR) are defined as the desired quality requirements, such as availability, that restrict software product being developed where some external restrictions may apply. Since information systems have been introduced, organizations in the business world align their functional activities with systems without paying attention to quality-based alignment. Few research works have been conducted in order to classify and integrate the NFR with business or system models. But these classifications and integrations are only confined to either the business side or the system side, which in turn have caused in having a gap in mapping the classifications between the two sides. Because business models and system models mutually affect each other in many ways, their NFR integration and classification should be aligned with each other. Having a NFR alignment-based classification between business and information systems contributes to assist the stakeholders in reflecting the quality requirements at the business side for a particular task on the related tasks integrated with NFRs at the systems side. Also having an alignment-oriented classification contributes to trace quality/NFR-based changes from the business organization to its systems and vice versa.In this research, we propose a NFR classification for aligning quality requirements in business with their NFRs in information systems. The work in business side is represented through business process models designed using Business Process Model and Notation (BPMN) where the use case models represents the system side in this research. The proposed classification is demonstrated in both business and systems using the academic advising and registration case study at Applied Science University in Jordan.|10.1145/3437075.3437091|https://doi.org/10.1145/3437075.3437091|New York, NY, USA|Association for Computing Machinery|9781450375061|2021|Non-Functional Requirements Classification for Aligning Business with Information Systems|Majthoub, Manar and Odeh, Yousra and Hijjawi, Mohammed|inproceedings|10.1145/3437075.3437091|||||||||||||||||||||||||||||124548763|42
|||2|339–340|Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice|||https://doi.org/10.1145/3447404.3447423|New York, NY, USA|Association for Computing Machinery|9781450390293|2021|Ethics and Secure Gestures|McMenemy, David|inbook|10.1145/3447404.3447423|||||||||1||||||||||||||||||||125405480|42
||Big data analytics, Big data project’s governance methodology, Big data architecture, Data governance methodology, Big data project’s team, Big data telecommunications use cases||2758-2770||With the upsurge of data traffic due to the change in customer behavior towards the use of telecommunications services, fostered by the current global health situation (mainly due to Covid-19), the telecommunications operators have a golden opportunity to create new sources of revenues using Big Data Analytics (BDA) solutions. Looking to setting up a BDA project, we faced several challenges, notably, in terms of choice of the technical solution from the plethora of the existing tools, and the choice of the governance methodologies for governing the project and the data. The majority of research documents related to the telecommunications industry have not addressed BDA project implementation from start to finish. The purpose of this study focuses on a BDA telecommunications project, namely, Project’s Governance, Architecture, Data Governance and the BDA Project’s Team. The last part of this study presents useful BDA use cases, in terms of applications enabling revenue creation and cost optimization. It appears that this work will facilitate the implementation of BDA projects, and enable telecommunications operators to have a better understanding about the fundamental aspects to be focused on. It is therefore, a study that will contribute positively toward such goal.|https://doi.org/10.1016/j.jksuci.2020.11.024|https://www.sciencedirect.com/science/article/pii/S131915782030553X||||2022|Big data analytics in telecommunications: Governance, architecture and use cases|Mohamed Zouheir Kastouni and Ayoub {Ait Lahcen}|article|KASTOUNI20222758|||Journal of King Saud University - Computer and Information Sciences|13191578|6, Part A|34|||||||||||||||||||||||126615752|1257083324
|||8|60–67||The dark data extraction or knowledge base construction (KBC) problem is to populate a SQL database with information from unstructured data sources including emails, webpages, and pdf reports. KBC is a long-standing problem in industry and research that encompasses problems of data extraction, cleaning, and integration. We describe DeepDive, a system that combines database and machine learning ideas to help develop KBC systems. The key idea in DeepDive is that statistical inference and machine learning are key tools to attack classical data problems in extraction, cleaning, and integration in a unified and more effective manner. DeepDive programs are declarative in that one cannot write probabilistic inference algorithms; instead, one interacts by defining features or rules about the domain. A key reason for this design choice is to enable domain experts to build their own KBC systems. We present the applications, abstractions, and techniques of DeepDive employed to accelerate construction of KBC systems.|10.1145/2949741.2949756|https://doi.org/10.1145/2949741.2949756|New York, NY, USA|Association for Computing Machinery||2016|DeepDive: Declarative Knowledge Base Construction|De Sa, Christopher and Ratner, Alex and R\'{e}, Christopher and Shin, Jaeho and Wang, Feiran and Wu, Sen and Zhang, Ce|article|10.1145/2949741.2949756||jun|SIGMOD Rec.|01635808|1|45|March 2016||||13622|0,372|Q2|142|39|81|808|127|57|1,67|20,72|United States|Northern America|1969, 1973-1978, 1981-2020|Information Systems (Q2); Software (Q2)|1,819|0.775|7.5E-4|127195301|962972343
ICISPC 2017|Penang, Malaysia|GIS-Geographic Information System, KSF-Key Success Factors, TNB-Tenaga Nasional Berhad|5|138–142|Proceedings of the International Conference on Imaging, Signal Processing and Communication|Tenaga Nasional Berhad (TNB) being one of the largest utilities in the Southeast Asia has embarked on enriching their GIS solutions suite for its business operations. A distribution station was chosen as a pilot project to run the business processes using GIS. The successful implementation is to be measured through its impact on the station day-today operations. Key success factors (KSF) were defined and will be measured with reference to component of GIS. The outcome of the measurement will guide the implementation of GIS nation-wide in TNB Distribution, Malaysia. This paper is aimed at providing insights for utilities who are keen in identifying those success factors and methodology of measuring the success of the GIS implementation.|10.1145/3132300.3132305|https://doi.org/10.1145/3132300.3132305|New York, NY, USA|Association for Computing Machinery|9781450352895|2017|Locating Success Within A Geographic Information System|Yatim, Ir. Fazilah Mat and Majid, Zulkepli and Amerudin, Shahabuddin|inproceedings|10.1145/3132300.3132305|||||||||||||||||||||||||||||130349674|42
https://doi.org/10.1016/j.infsof.2020.106448|https://www.sciencedirect.com/science/article/pii/S0950584920301981||||2021|Big Data analytics in Agile software development: A systematic mapping study|Katarzyna Biesialska and Xavier Franch and Victor Muntés-Mulero|article|BIESIALSKA2021106448|||Information and Software Technology|09505849||132||||||||||||||||||||||||||||||132335391|42
ICC '16|Cambridge, United Kingdom|Warehousing Big Data, Protecting Big Data, Big Data Analytics, Big Data|7||Proceedings of the International Conference on Internet of Things and Cloud Computing|This paper proposes a comprehensive critical survey on the issues of warehousing and protecting big data, which are recognized as critical challenges of emerging big data research. Indeed, both are critical aspects to be considered in order to build truly, high-performance and highly-flexible big data management systems. We report on state-of-the-art approaches, methodologies and trends, and finally conclude by providing open problems and challenging research directions to be considered by future efforts.|10.1145/2896387.2900335|https://doi.org/10.1145/2896387.2900335|New York, NY, USA|Association for Computing Machinery|9781450340632|2016|Warehousing and Protecting Big Data: State-Of-The-Art-Analysis, Methodologies, Future Challenges|Cuzzocrea, Alfredo|inproceedings|10.1145/2896387.2900335|14||||||||||||||||||||||||||||135615446|42
Computer Aided Chemical Engineering||Virtual sensor, soft-sensor, big data quality prediction, hashing-based just-in-time modeling||1681-1686|14th International Symposium on Process Systems Engineering|In recent years, the just-in-time (JIT) predictive models have attracted considerable attention due to their ability to prevent degradation of prediction accuracy. However, one of their practical limitations is expensive computation, which becomes a major factor that prevents them from being used for big data quality prediction. This is because the JIT modeling methods need to update the local regression model using the relevant samples that are searched through the lineal scan of the database during online operation. To solve this issue, the present work proposes a novel hashing-based JIT (HbJIT) modeling method that is suitable for big data quality prediction. In HbJIT, a family of locality-sensitive hash functions is firstly used to hash big data into a set of buckets, in which similar samples are grouped on themselves. During online prediction, HbJIT looks up multiple buckets that have a high probability of containing similar samples of a query object through the intelligent probing scheme, uses the data objects in the buckets as the candidate set of the results, and then filters the candidate objects using a linear scan. After filtering, the most relevant samples are used to construct the local regression model to yield the prediction of the query object. By integrating the multi-probe hashing strategy into the JIT learning framework, HbJIT can not only deal with process nonlinearity and time-varying characteristics but also is applicable to large-scale industrial processes. Experimental results on real-world dataset have demonstrated that the proposed HbJIT is time-efficient in processing large-scale datasets, and greatly reduces the online prediction time without compromising on the prediction accuracy.|https://doi.org/10.1016/B978-0-323-85159-6.50280-3|https://www.sciencedirect.com/science/article/pii/B9780323851596502803||Elsevier||2022|Hashing-based just-in-time learning for big data quality prediction|Xinmin Zhang and Jiang Zhai and Zhihuan Song and Yuan Li|incollection|ZHANG20221681||||15707946||49||||Yoshiyuki Yamashita and Manabu Kano|11200153545||-|25|345|1662|3359|1080|0|0,73|9,74|Netherlands|Western Europe|1997, 2000-2019|Chemical Engineering (miscellaneous); Computer Science Applications||||136234836|2206423
||Artificial intelligence, Data mining, Knowledge bases, Nursing||229-234||Based on the concept and research status of big data, we analyze and examine the importance of constructing the knowledge system of nursing science for the development of the nursing discipline in the context of big data and propose that it is necessary to establish big data centers for nursing science to share resources, unify language standards, improve professional nursing databases, and establish a knowledge system structure.|https://doi.org/10.1016/j.ijnss.2019.03.001|https://www.sciencedirect.com/science/article/pii/S2352013218305507||||2019|The application of big data and the development of nursing science: A discussion paper|Ruifang Zhu and Shifan Han and Yanbing Su and Chichen Zhang and Qi Yu and Zhiguang Duan|article|ZHU2019229|||International Journal of Nursing Sciences|23520132|2|6|||||21100469749|0,703|Q1|16|87|229|2854|526|199|2,62|32,80|Singapore|Asiatic Region|2014-2020|Nursing (miscellaneous) (Q1)||||139678474|1461537998
||data reconstruction, online participant selection, Compressive mobile crowdsensing|29|||A mobile crowdsensing (MCS) platform motivates employing participants from the crowd to complete sensing tasks. A crucial problem is to maximize the profit of the platform, i.e., the charge of a sensing task minus the payments to participants that execute the task. In this article, we improve the profit via the data reconstruction method, which brings new challenges, because it is hard to predict the reconstruction quality due to the dynamic features and mobility of participants. In particular, two Profit-driven Online Participant Selection (POPS) problems under different situations are studied in our work: (1) for S-POPS, the sensing cost of the different parts within the target area is the Same. Two mechanisms are designed to tackle this problem, including the ProSC and ProSC+. An exponential-based quality estimation method and a repetitive cross-validation algorithm are combined in the former mechanism, and the spatial distribution of selected participants are further discussed in the latter mechanism; (2) for V-POPS, the sensing cost of different parts within the target area is Various, which makes it the NP-hard problem. A heuristic mechanism called ProSCx is proposed to solve this problem, where the searching space is narrowed and both the participant quantity and distribution are optimized in each slot. Finally, we conduct comprehensive evaluations based on the real-world datasets. The experimental results demonstrate that our proposed mechanisms are more effective and efficient than baselines, selecting the participants with a larger profit for the platform.|10.1145/3342515|https://doi.org/10.1145/3342515|New York, NY, USA|Association for Computing Machinery||2019|Towards Profit Optimization During Online Participant Selection in Compressive Mobile Crowdsensing|Chen, Yueyue and Guo, Deke and Bhuiyan, MD Zakirul Alam and Xu, Ming and Wang, Guojun and Lv, Pin|article|10.1145/3342515|38|aug|ACM Trans. Sen. Netw.|15504859|4|15|November 2019||||4700152843|0,598|Q2|67|44|118|2352|447|117|3,52|53,45|United States|Northern America|2005-2020|Computer Networks and Communications (Q2)|1,365|2.253|9.9E-4|144435235|1384228366
WOSC '19|Davis, CA, USA|file systems, materials science, metadata extraction, serverless, data lakes|6|43–48|Proceedings of the 5th International Workshop on Serverless Computing|"\"The use and reuse of scientific data is ultimately dependent on the ability to understand what those data represent, how they were captured, and how they can be used. In many ways, data are only as useful as the metadata available to describe them. Unfortunately, due to growing data volumes, large and distributed collaborations, and a desire to store data for long periods of time, scientific \"\"data lakes\"\" quickly become disorganized and lack the metadata necessary to be useful to researchers. New automated approaches are needed to derive metadata from scientific files and to use these metadata for organization and discovery. Here we describe one such system, Xtract, a service capable of processing vast collections of scientific files and automatically extracting metadata from diverse file types. Xtract relies on function as a service models to enable scalable metadata extraction by orchestrating the execution of many, short-running extractor functions. To reduce data transfer costs, Xtract can be configured to deploy extractors centrally or near to the data (i.e., at the edge). We present a prototype implementation of Xtract and demonstrate that it can derive metadata from a 7 TB scientific data repository.\""|10.1145/3366623.3368140|https://doi.org/10.1145/3366623.3368140|New York, NY, USA|Association for Computing Machinery|9781450370387|2019|Serverless Workflows for Indexing Large Scientific Data|Skluzacek, Tyler J. and Chard, Ryan and Wong, Ryan and Li, Zhuozhao and Babuji, Yadu N. and Ward, Logan and Blaiszik, Ben and Chard, Kyle and Foster, Ian|inproceedings|10.1145/3366623.3368140|||||||||||||||||||||||||||||144978864|42
CHI '15|Seoul, Republic of Korea|human-robotic interaction, robotics, design probes, formative inquiry, sustainable hci, energy audits, thermography|10|1993–2002|Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems|The building sector accounts for 41% of primary energy consumption in the US, contributing an increasing portion of the country's carbon dioxide emissions. With recent sensor improvements and falling costs, auditors are increasingly using thermography-infrared (IR) cameras-to detect thermal defects and analyze building efficiency. Research in automated thermography has grown commensurately, aimed at reducing manual labor and improving thermal models. Though promising, we could find no prior work exploring the professional auditor's perspectives of thermography or reactions to emerging automation. To address this gap, we present results from two studies: a semi-structured interview with 10 professional energy auditors, which includes design probes of five automated thermography scenarios, and an observational case study of a residential audit. We report on common perspectives, concerns, and benefits related to thermography and summarize reactions to our automated scenarios. Our findings have implications for thermography tool designers as well as researchers working on automated solutions in robotics, computer science, and engineering.|10.1145/2702123.2702528|https://doi.org/10.1145/2702123.2702528|New York, NY, USA|Association for Computing Machinery|9781450331456|2015|Understanding the Role of Thermography in Energy Auditing: Current Practices and the Potential for Automated Solutions|Mauriello, Matthew Louis and Norooz, Leyla and Froehlich, Jon E.|inproceedings|10.1145/2702123.2702528|||||||||||||||||||||||||||||146902743|42
||Crowdsourcing, Citizen science, Smart farming, Participatory approaches, Big data, ICT, Data collection||126-138||Crowdsourcing, understood as outsourcing tasks or data collection by a large group of non-professionals, is increasingly used in scientific research and operational applications. In this paper, we reviewed crowdsourcing initiatives in agricultural science and farming activities and further discussed the particular characteristics of this approach in the field of agriculture. On-going crowdsourcing initiatives in agriculture were analysed and categorised according to their crowdsourcing component. We identified eight types of agricultural data and information that can be generated from crowdsourcing initiatives. Subsequently we described existing methods of quality control of the crowdsourced data. We analysed the profiles of potential contributors in crowdsourcing initiatives in agriculture, suggested ways for increasing farmers’ participation, and discussed the on-going initiatives in the light of their target beneficiaries. While crowdsourcing is reported to be an efficient way of collecting observations relevant to environmental monitoring and contributing to science in general, we pointed out that crowdsourcing applications in agriculture may be hampered by privacy issues and other barriers to participation. Close connections with the farming sector, including extension services and farm advisory companies, could leverage the potential of crowdsourcing for both agricultural research and farming applications. This paper coins the term of farmsourcing asa professional crowdsourcing strategy in farming activities and provides a source of recommendations and inspirations for future collaborative actions in agricultural crowdsourcing.|https://doi.org/10.1016/j.compag.2017.08.026|https://www.sciencedirect.com/science/article/pii/S0168169917300479||||2017|Crowdsourcing for agricultural applications: A review of uses and opportunities for a farmsourcing approach|Julien Minet and Yannick Curnel and Anne Gobin and Jean-Pierre Goffart and François Mélard and Bernard Tychon and Joost Wellens and Pierre Defourny|article|MINET2017126|||Computers and Electronics in Agriculture|01681699||142|||||30441|1,208|Q1|115|648|1300|28725|9479|1298|7,27|44,33|Netherlands|Western Europe|1985-2020|Agronomy and Crop Science (Q1); Animal Science and Zoology (Q1); Computer Science Applications (Q1); Forestry (Q1); Horticulture (Q1)|17,657|5.565|0.01646|149854273|1531073408
https://doi.org/10.1016/j.medin.2018.06.002|https://www.sciencedirect.com/science/article/pii/S0210569118301827||||2019|Big data and machine learning in critical care: Opportunities for collaborative research|Antonio {Núñez Reiz} and Fernando {Martínez Sagasti} and Manuel {Álvarez González} and Antonio {Blesa Malpica} and Juan Carlos {Martín Benítez} and Mercedes {Nieto Cabrera} and Ángela {del Pino Ramírez} and José Miguel {Gil Perdomo} and Jesús {Prada Alonso} and Leo Anthony Celi and Miguel Ángel {Armengol de la Hoz} and Rodrigo Deliberato and Kenneth Paik and Tom Pollard and Jesse Raffa and Felipe Torres and Julio Mayol and Joan Chafer and Arturo {González Ferrer} and Ángel Rey and Henar {González Luengo} and Giuseppe Fico and Ivana Lombroni and Liss Hernandez and Laura López and Beatriz Merino and María Fernanda Cabrera and María Teresa Arredondo and María Bodí and Josep Gómez and Alejandro Rodríguez and Miguel {Sánchez García}|article|NUNEZREIZ201952|||Medicina Intensiva|02105691|1|43||||||||||||||||||||||||||||||150343392|42
ICBDC '18|Shenzhen, China|Data quality, Distributed database system, CRC, Data inconsistency|6|38–43|Proceedings of the 3rd International Conference on Big Data and Computing|Most existing systems suffer from data quality problems. Data quality has been affected by many factors such as manual operation, software problems and hardware problems, especially data inconsistencies. As an important carrier of data, database system plays an important role in distributed systems. In order to reduce the impact of data inconsistency on data quality in distributed database systems, we design and implement a multi-source data inconsistency detection and repair method based on CRC algorithm.The idea of the proposed techniques is to use the rolling checksum in the rsync algorithm. In the process of data inconsistency detection, the method divides the table into chunks and calculates the checksums of data chunks in parallel for multiple data tables to detect and repair the inconsistent data. The experimental results show that the detection effect of this method is consistent with that of the traditional method which comparing source data with target data. The detection rate is as high as 99%, but it performs better than the traditional method, and the running time is reduced by about 20%.|10.1145/3220199.3220206|https://doi.org/10.1145/3220199.3220206|New York, NY, USA|Association for Computing Machinery|9781450364263|2018|Multi: Source Data Inconsistency Detection and Repair Based on CRC Algorithm|Xie, Dingding and Li, Junyi and Yuan, Lening and Peng, Peng|inproceedings|10.1145/3220199.3220206|||||||||||||||||||||||||||||156199208|42
ICCA 2020|Dhaka, Bangladesh|Bitcoin, Ethereum, Web 3.0, Decentralised Web, Encryption, Data Privacy, DApp, Cryptography, Smart Contracts, Blockchain, Server vulnerabilities, Peer-To-Peer Network, Whisper|7||Proceedings of the International Conference on Computing Advancements|Throughout this paper, we try to describe with blockchain technology the decentralization of the internet. A decentralized network that encourages the internet to operate from the smartphone or tablet of anybody instead of centralized servers. A decentralized implementation would be based on a peer-to-peer network that is dependent on a user community. Their machines connected to the internet will host the network, not a community of more powerful servers. Each site would be distributed across thousands of nodes on various devices. The data is therefore not contained, owned by private storage facilities. There is therefore no central point to hack, and no way for an oligarchy of entities to take control of it. A proposed alternative was formed based on a systematic literature review that demonstrates that Internet decentralization is what this modern technology needs in order to address not only the weaknesses of current servers including server down issue, hacking and data manipulation or single point of failure, but also to prevent companies from monetizing the data of citizens through their server and to market them to the advertisers.|10.1145/3377049.3377083|https://doi.org/10.1145/3377049.3377083|New York, NY, USA|Association for Computing Machinery|9781450377782|2020|Secured Blockchain Based Decentralised Internet: A Proposed New Internet|Chowdhury, S. M. Habibul Mursaleen and Jahan, Ferdous and Sara, Sarawat Murtaza and Nandi, Dip|inproceedings|10.1145/3377049.3377083|8||||||||||||||||||||||||||||158355065|42
||Data quality, Sustainability, Energy policy, Transition, EU Eurobarometer||102813||Ensuring data quality is a pivotal and resurgent issue in research. The ascending importance of natural resource and energy sustainability, development and transition in social science scholarship has led to a soaring of databases exploring detailed dynamics. Nevertheless, international organisations and development agencies have not necessarily managed to reply effectively to the arising needs, which stimulates a request for reliable environmental and socioeconomic data. This perspective paper aims at highlighting key flaws in institutional data quality from sustainability, development and transition science, focusing on energy. To this end, the 2019 Eurobarometer 91.4 survey about Europeans' attitudes toward the EU energy policy is examined. Indeed, the EU Eurobarometer collocates amongst the most pertinent databases though is affected by a number of data frailties. As potentially biased information may then be used to formulate assumably inappropriate policy recommendations, this article foresees a set of solutions to address this relevant matter, proposing a reform in the survey administration methods.|https://doi.org/10.1016/j.erss.2022.102813|https://www.sciencedirect.com/science/article/pii/S2214629622003164||||2022|Misleading intentions? Questioning the effectiveness and biases of Eurobarometer data for energy sustainability, development and transition research|Andrea Gatto and Demetrio Panarello|article|GATTO2022102813|||Energy Research & Social Science|22146296||93|||||21100325067|2,313|Q1|63|419|801|33308|5449|801|6,86|79,49|United Kingdom|Western Europe|2014-2020|Energy Engineering and Power Technology (Q1); Fuel Technology (Q1); Nuclear Energy and Engineering (Q1); Renewable Energy, Sustainability and the Environment (Q1); Social Sciences (miscellaneous) (Q1)||||159240839|812617055
||Big data, Precision medicine, Personalized healthcare, Data science, Big data analytics||69-94|Big Data in Psychiatry #x0026; Neurology|“Big data” is a term that has been used often in the past decade to describe datasets that are extremely large and complex so that traditional software is unable to store and analyze them in an accurate way. It can refer to “long data,” “wide data,” and both. Big data is of increasing importance in healthcare as well: new methods dedicated to improving data collection, storage, cleaning, processing, and interpretation for medical research continue to be developed. Exploiting new tools and methods to extract meaning from large volume information has the potential to drive real change in clinical practice, and combining this novel data-driven research with the classical hypothesis-driven research will have a large impact on personalized healthcare. However, significant challenges remain. Here we discuss the challenges (and possible solutions) posed to biomedical research by our increasing ability to collect, store, and analyze large datasets. Important challenges include: (1) the need for standardization of data content, format, and clinical definitions, adhering to the FAIR guiding principles; (2) the need for collaborative networks with sharing of both data and expertise, for example through a federated approach; (3) stricter privacy and ethics regulations, in particular the GDPR in the European Union; and (4) a need to reconsider how and when analytic methodology (data science) is taught to medical researchers. Overcoming these challenges will help to make a success of the use of big data in medical and translational research.|https://doi.org/10.1016/B978-0-12-822884-5.00016-7|https://www.sciencedirect.com/science/article/pii/B9780128228845000167||Academic Press|978-0-12-822884-5|2021|Chapter 4 - Challenges and solutions for big data in personalized healthcare|Tim Hulsen|incollection|HULSEN202169||||||||||Ahmed A. Moustafa|||||||||||||||||||159295679|42
|||3||||10.1145/3005395|https://doi.org/10.1145/3005395|New York, NY, USA|Association for Computing Machinery||2016|Editorial: Special Issue on Web Data Quality|Bizer, Christian and Dong, Luna and Ilyas, Ihab and Vidal, Maria-Esther|article|10.1145/3005395|1|nov|J. Data and Information Quality|19361955|1|8|November 2016||||||||||||||||||||||163720064|833754770
https://doi.org/10.1016/j.artmed.2019.101704|https://www.sciencedirect.com/science/article/pii/S0933365717301781||||2019|Machine learning and big data: Implications for disease modeling and therapeutic discovery in psychiatry|Andy M.Y. Tai and Alcides Albuquerque and Nicole E. Carmona and Mehala Subramanieapillai and Danielle S. Cha and Margarita Sheko and Yena Lee and Rodrigo Mansur and Roger S. McIntyre|article|TAI2019101704|||Artificial Intelligence in Medicine|09333657||99||||||||||||||||||||||||||||||163879491|42
BDET 2018|Chengdu, China|insurance industry, big data platform, time and space data, platform architecture, Financial technology|5|31–35|Proceedings of the 2018 International Conference on Big Data Engineering and Technology|With the rise of the concept of financial technology, financial and technology gradually in-depth integration, scientific and technological means to become financial product innovation, improve financial efficiency and reduce financial transaction costs an important driving force. In this context, the new technology platform is from the business philosophy, business model, technical means, sales, internal management and other dimensions to re-shape the financial industry. In this paper, the existing big data platform architecture technology innovation, adding space-time data elements, combined with the insurance industry for practical analysis, put forward a meaningful product circle and customer circle.|10.1145/3297730.3297743|https://doi.org/10.1145/3297730.3297743|New York, NY, USA|Association for Computing Machinery|9781450365826|2018|Big Data Platform Architecture under The Background of Financial Technology: In The Insurance Industry As An Example|Liu, Yi and Peng, Jiawen and Yu, Zhihao|inproceedings|10.1145/3297730.3297743|||||||||||||||||||||||||||||166404606|42
||||1-3|||https://doi.org/10.1016/j.fss.2018.05.022|https://www.sciencedirect.com/science/article/pii/S0165011418302987||||2018|Preface: Special Issue on Big Data|Sadok Ben Yahia and Anne Laurent and Gabriella Pasi|article|YAHIA20181|||Fuzzy Sets and Systems|01650114||348||SI: Fuzzy Approaches to Big Data|||26529|0,902|Q1|169|333|570|11642|2262|560|3,72|34,96|Netherlands|Western Europe|1978-2020|Artificial Intelligence (Q1); Logic (Q1)|17,883|3.343|0.00737|171445781|2044953503
||Simulation, Big Data, Data issues, Semantic validation, Supply chain management, Industry 4.0||101985||Simulation stands out as an appropriate method for the Supply Chain Management (SCM) field. Nevertheless, to produce accurate simulations of Supply Chains (SCs), several business processes must be considered. Thus, when using real data in these simulation models, Big Data concepts and technologies become necessary, as the involved data sources generate data at increasing volume, velocity and variety, in what is known as a Big Data context. While developing such solution, several data issues were found, with simulation proving to be more efficient than traditional data profiling techniques in identifying them. Thus, this paper proposes the use of simulation as a semantic validator of the data, proposed a classification for such issues and quantified their impact in the volume of data used in the final achieved solution. This paper concluded that, while SC simulations using Big Data concepts and technologies are within the grasp of organizations, their data models still require considerable improvements, in order to produce perfect mimics of their SCs. In fact, it was also found that simulation can help in identifying and bypassing some of these issues.|https://doi.org/10.1016/j.simpat.2019.101985|https://www.sciencedirect.com/science/article/pii/S1569190X19301182||||2020|On the use of simulation as a Big Data semantic validator for supply chain management|António AC Vieira and Luís MS Dias and Maribel Y Santos and Guilherme AB Pereira and José A Oliveira|article|VIEIRA2020101985|||Simulation Modelling Practice and Theory|1569190X||98|||||12189|0,554|Q2|69|132|316|5805|1227|312|4,02|43,98|Netherlands|Western Europe|2002-2021|Hardware and Architecture (Q2); Modeling and Simulation (Q2); Software (Q2)|3,547|3.272|0.00315|173172853|1046810433
|||4|2901–2904||The amount of time-series data that is generated has exploded due to the growing popularity of Internet of Things (IoT) devices and applications. These applications require efficient management of the time-series data on both the edge and cloud side that support high throughput ingestion, low latency query and advanced time series analysis. In this demonstration, we present Apache IoTDB managing time-series data to enable new classes of IoT applications. IoTDB has both edge and cloud versions, provides an optimized columnar file format for efficient time-series data storage, and time-series database with high ingestion rate, low latency queries and data analysis support. It is specially optimized for time-series oriented operations like aggregations query, down-sampling and sub-sequence similarity search. An edge-to-cloud time-series data management application is chosen to demonstrate how IoTDB handles time-series data in real-time and supports advanced analytics by integrating with Hadoop and Spark. An end-to-end IoT data management solution is shown by integrating IoTDB with PLC4x, Calcite, and Grafana.|10.14778/3415478.3415504|https://doi.org/10.14778/3415478.3415504||VLDB Endowment||2020|Apache IoTDB: Time-Series Database for Internet of Things|Wang, Chen and Huang, Xiangdong and Qiao, Jialin and Jiang, Tian and Rui, Lei and Zhang, Jinrui and Kang, Rong and Feinauer, Julian and McGrail, Kevin A. and Wang, Peng and Luo, Diaohan and Yuan, Jun and Wang, Jianmin and Sun, Jiaguang|article|10.14778/3415478.3415504||sep|Proc. VLDB Endow.|21508097|12|13|August 2020||||21100199855|0,946|Q1|134|246|598|12401|3759|566|5,50|50,41|United States|Northern America|2008-2020|Computer Science (miscellaneous) (Q1)|7,198|2.047|0.00891|173908881|1216159931
||Machine learning, Internet of Things, Smart data, Smart City||161-175||Rapid developments in hardware, software, and communication technologies have facilitated the emergence of Internet-connected sensory devices that provide observations and data measurements from the physical world. By 2020, it is estimated that the total number of Internet-connected devices being used will be between 25 and 50 billion. As these numbers grow and technologies become more mature, the volume of data being published will increase. The technology of Internet-connected devices, referred to as Internet of Things (IoT), continues to extend the current Internet by providing connectivity and interactions between the physical and cyber worlds. In addition to an increased volume, the IoT generates big data characterized by its velocity in terms of time and location dependency, with a variety of multiple modalities and varying data quality. Intelligent processing and analysis of this big data are the key to developing smart IoT applications. This article assesses the various machine learning methods that deal with the challenges presented by IoT data by considering smart cities as the main use case. The key contribution of this study is the presentation of a taxonomy of machine learning algorithms explaining how different techniques are applied to the data in order to extract higher level information. The potential and challenges of machine learning for IoT data analytics will also be discussed. A use case of applying a Support Vector Machine (SVM) to Aarhus smart city traffic data is presented for a more detailed exploration.|https://doi.org/10.1016/j.dcan.2017.10.002|https://www.sciencedirect.com/science/article/pii/S235286481730247X||||2018|Machine learning for internet of things data analysis: a survey|Mohammad Saeid Mahdavinejad and Mohammadreza Rezvan and Mohammadamin Barekatain and Peyman Adibi and Payam Barnaghi and Amit P. Sheth|article|MAHDAVINEJAD2018161|||Digital Communications and Networks|23528648|3|4|||||21100823476|1,082|Q1|26|77|105|3226|881|96|8,81|41,90|China|Asiatic Region|2015-2020|Communication (Q1); Computer Networks and Communications (Q1); Hardware and Architecture (Q1)|823|6.797|0.00138|179127825|493706778
||Data management, data quality, decision making, data analysis||532-537||Today, as technologies mature and people are encouraged to contribute data to organizations’ databases, more transactions are being captured than ever before. Meanwhile, improvements in data storage technologies have made the cost of evaluating, selecting, and destroying legacy data considerably greater than simply letting it accumulate. On the one hand, the excess of stored data has considerably increased the opportunities to interrelate and analyze them, while the moderate enthusiasm generated by data warehousing and data mining in the 1990s has been replaced by a rampant euphoria about big data and data analytics. But, is this as wonderful as seems? This paper presents a risk analysis of Big Data and Big Data Analytics based on a review of quality factors.|https://doi.org/10.1016/j.procs.2019.11.052|https://www.sciencedirect.com/science/article/pii/S1877050919317521||||2019|Risk Analysis of Using Big Data in Computer Sciences|Jesus Silva and Omar Bonerge {Pineda Lezama} and Ligia Romero and Darwin Solano and Claudia Fernández|article|SILVA2019532|||Procedia Computer Science|18770509||160||The 10th International Conference on Emerging Ubiquitous Systems and Pervasive Networks (EUSPN-2019) / The 9th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare (ICTH-2019) / Affiliated Workshops|||19700182801|0,334|-|76|1907|6483|38511|13722|6359|2,09|20,19|Netherlands|Western Europe|2010-2020|Computer Science (miscellaneous)||||179719792|2108686752
ICIAI 2020|Xiamen, China|equipment fault diagnosis, CPS, traffic safety, High-speed railway|8|147–154|Proceedings of the 2020 the 4th International Conference on Innovation in Artificial Intelligence|"\"From the view of high-speed railway traffic safety, this paper establishes an intelligent management platform for operation safety equipment based on CPS for \"\"person-equipment-environment\"\", and designs a framework of traffic safety system composed of perception control hardware, Internet of Things, cognitive decision-making and information services. The deep fusion of information system and traffic safety equipment is discussed, and the fault diagnosis method of driving equipment based on complex sensing technology is given, such as intelligent identification, online monitoring and ubiquitous sensing of the characteristics of safety protection equipment. Through the application of equipment fault diagnosis, it realizes the rapid retrieval and active collection of safety information, provides early warning and auxiliary decision-making, big data analysis and prediction, and improves the traffic safety.\""|10.1145/3390557.3394127|https://doi.org/10.1145/3390557.3394127|New York, NY, USA|Association for Computing Machinery|9781450376587|2020|Research on Intelligent Management Platform of Highspeed Railway Traffic Safety Equipment Based on CPS|Zhan, Lin and Junhua, Zhao and Fan, Li and Zhifei, Wang|inproceedings|10.1145/3390557.3394127|||||||||||||||||||||||||||||180674377|42
||Support vector machines;Heuristic algorithms;Data analysis;Aerodynamics;Cybernetics;Big data;Machine learning algorithms;Data analysis;Data correction;Support vector machine;Data Mining||820-824|2016 International Conference on Machine Learning and Cybernetics (ICMLC)|Data quality plays an important role in modern intelligent information system and is crucial to any data analysis task. Many imperfection-handling techniques avoid overfitting or simply remove offending portions of the data. Data correction can help to retain and recover as much information as possible from the original data resources. In this paper, we proposed a novel technique based on polynomial smooth support vector machine. The quadratic polynomial and the first degree of polynomial as the support vector machine smooth functions are investigated. At the same time, the function was used as smooth function to calculate compensation values. In order to show the procedures of our algorithm, some necessary steps need to be considered. Firstly, the original data are normalized, so as to eliminate experimental effects of dimensional problems. Secondly, the three different kinds of smooth functions need to be analysed mathematically. The difference measure are calculated to make sure the results of correction through different data correction models. The results of given noised data sets can show that the proposed the data correction method based on polynomial smooth support vector machine is effectiveness.|10.1109/ICMLC.2016.7872993|||||2016|A dynamic data correction algorithm based on polynomial smooth support vector machine|Pu, Dong-Mei and Gao, Da-Qi and Yuan, Yu-Bo|inproceedings|7872993||July||21601348||2|||||20300195054|0,149|-|18|0|227|0|114|219|0,50|0,00|United States|Northern America|2011, 2012, 2013, 2014, 2019|Artificial Intelligence; Computational Theory and Mathematics; Computer Networks and Communications; Human-Computer Interaction||||188922051|1337514328
SIGSPATIAL '18|Seattle, Washington|utility networks, spatial databases, GIS, graphs and networks, graph algorithms|10|249–258|Proceedings of the 26th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems|Given a utility network and one or more starting points that define where analysis should begin, the problem of analyzing utility networks entails assembling a subset of network elements that meet some specified criteria. Analyzing utility network data has several applications and provides tremendous business value to utilities. For example, analysis may answer questions about the current state of the network (e.g., what valves need to be closed to shut off water flow to a location of a pipe leak), help to design future facilities (e.g., how many houses are fed by a transformer and can the transformer supply another house without overloading its capacity?), and help to organize business practices (e.g., create circuit maps for work crews to facilitate damage assessment after an ice storm). Analyzing utility networks is a challenging problem due to 1) the size of the data, which could have many tens of millions of network elements per utility, and billions of elements at the nationwide or continental scale, 2) modeling and analyzing utility assets at high fidelity (level of detail), and 3) the different analysis requirements across utility domains (e.g., water, wastewater, sewer, district heating, gas, electric, fiber, and telecom). This paper describes the trace framework for utility network analysis that has been implemented in ArcGIS Pro 2.1/ArcGIS Enterprise 10.6. The trace framework features algorithms in a services-based architecture for addressing analysis tasks across a wide array of utility domains. Previous approaches have focused on solving specific problems in specific domains whereas the trace framework provides a more general, scalable solution. We present experiments that demonstrate the scalability of the trace framework and a case study that highlights its value in performing a wide variety of analytics on utility networks.|10.1145/3274895.3274899|https://doi.org/10.1145/3274895.3274899|New York, NY, USA|Association for Computing Machinery|9781450358897|2018|A Trace Framework for Analyzing Utility Networks: A Summary of Results (Industrial Paper)|Oliver, Dev and Hoel, Erik G.|inproceedings|10.1145/3274895.3274899|||||||||||||||||||||||||||||190357901|42
||Digital environment, Digital society, Digital government, Digital trust||||Digital trust is born with the evolution of digital society. It is an inescapable topic in the digital society and it is developed from traditional interpersonal trust and institutional trust and has been extensively used in the Internet space. At present, the research on digital trust is rare. Based on the Trust Theory and the Expectation Confirmation Theory, this paper puts forward an integration model with user satisfaction as the intermediary variable. Besides, this paper develops a set of scales for evaluating digital trust combined with maturity scales and points out that digital trust consists of digital cognitive trust and emotional trust. This paper assumed that user perception and user expectation indirectly affect digital trust through user satisfaction and used SPSS 23.0 to do reliability, validity test, and exploratory factor analysis. The results found that user satisfaction plays a mediating role by fitting, evaluating, and optimizing the structural equation model with AMOS23.0. User satisfaction is a partial intermediary between user perception and digital trust, and it is the complete intermediary between user expectation and digital trust. These results demonstrate two things. Firstly, In the digital society, the construction of users' digital trust is based on users' satisfaction. The government should provide diversified and high-quality e-government services as far as possible. Secondly, digital trust is directly or indirectly affected by user perception and user expectation. The government should build a safe, green, and harmonious digital environment for users and make e-government services consistent with users' expectation.|10.1145/3543860|https://doi.org/10.1145/3543860|New York, NY, USA|Association for Computing Machinery||2022|Digital Trust and the Reconstruction of Trust in the Digital Society: An Integrated Model Based on Trust Theory and Expectation Confirmation Theory|Guo, Yuanyuan|article|10.1145/3543860||aug|Digit. Gov.: Res. Pract.|2691199X||||Just Accepted|||||||||||||||||||||191474722|141116223
UbiComp '16|Heidelberg, Germany|crowd sensing, deployment experiences, data analytics, social computing, cloud computing, crowdsourcing, planet-scale measurement|4|1275–1278|Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing: Adjunct|The recent advances of mobile devices, online social networks, and the emergence of the Internet of Things have driven the corresponding data collection and analytics to planetary scale. It is, thus, essential to provide a forum to discuss the technical advances, share the lessons, experiences, and challenges associated with real-world large-scale deployment. The 7th International Workshop on Hot Topics in Planet-Scale Measurement (HotPlanet '16) is to provide such a forum for the researchers and practitioners in the fields mentioned above. By bringing together the experts in these fields, and through thoughtful discussions and valuable sharing, HotPlanet '16 aims to advance the work in these fields forward.|10.1145/2968219.2985840|https://doi.org/10.1145/2968219.2985840|New York, NY, USA|Association for Computing Machinery|9781450344623|2016|The 7th International Workshop on Hot Topics in Planet-Scale Measurement (HotPlanet '16)|Hui, Pan and Ou, Zhonghong and Zhang, Yanyong and Striegel, Aaron D|inproceedings|10.1145/2968219.2985840|||||||||||||||||||||||||||||192042291|42
HPCCT &amp; BDAI '20|Qingdao, China|Hard compressed video, DeepFake detection, Deep Learning, Super-resolution reconstruction|6|98–103|Proceedings of the 2020 4th High Performance Computing and Cluster Technologies Conference &amp; 2020 3rd International Conference on Big Data and Artificial Intelligence|The DeepFake video detection method based on convolutional neural networks has a poor performance in the dataset of hard compressed DeepFake video. And a large number of false tests will occur to the real data. To solve this problem, a networks model detection method for super-resolution reconstruction of DeepFake video is proposed. First of all, the face area of real data is processed by Gaussian blur, which is converted into negative data, and the real data and processing data are input into neural network for training. Then the residual network is used for super-resolution reconstruction of test data. Finally, the trained model is used to test the video after super-resolution reconstruction. Experiments show that the proposed method can reduce the false detection rate and improve the accuracy in detection of single frames.|10.1145/3409501.3409542|https://doi.org/10.1145/3409501.3409542|New York, NY, USA|Association for Computing Machinery|9781450375603|2020|A Detection Method for DeepFake Hard Compressed Videos Based on Super-Resolution Reconstruction Using CNN|Hongmeng, Zhang and Zhiqiang, Zhu and Lei, Sun and Xiuqing, Mao and Yuehan, Wang|inproceedings|10.1145/3409501.3409542|||||||||||||||||||||||||||||193141120|42
||Big data, Cognitive computing, Literature review, Resource based View, Institutional theory||78-89||Analysis of data by humans can be a time-consuming activity and thus use of sophisticated cognitive systems can be utilized to crunch this enormous amount of data. Cognitive computing can be utilized to reduce the shortcomings of the concerns faced during big data analytics. The aim of the study is to provide readers a complete understanding of past, present and future directions in the domain big data and cognitive computing. A systematic literature review has been adopted for this study by using the Scopus, DBLP and Web of Science databases. The work done in the field of big data and cognitive computing is currently at the nascent stage and this is evident from the publication record. The characteristics of cognitive computing, namely observation, interpretation, evaluation and decision were mapped to the five V’s of big data namely volume, variety, veracity, velocity and value. Perspectives which touch all these parameters are yet to be widely explored in existing literature.|https://doi.org/10.1016/j.ijinfomgt.2018.06.005|https://www.sciencedirect.com/science/article/pii/S0268401218304110||||2018|Big data with cognitive computing: A review for the future|Shivam Gupta and Arpan Kumar Kar and Abdullah Baabdullah and Wassan A.A. Al-Khowaiter|article|GUPTA201878|||International Journal of Information Management|02684012||42|||||15631|2,770|Q1|114|224|382|20318|5853|373|16,16|90,71|United Kingdom|Western Europe|1970, 1986-2021|Artificial Intelligence (Q1); Computer Networks and Communications (Q1); Information Systems (Q1); Information Systems and Management (Q1); Library and Information Sciences (Q1); Management Information Systems (Q1); Marketing (Q1)|12,245|14.098|0.01167|194446313|747927863
||unknown unknowns, Aggregate query processing, crowdsourcing, species estimation|37|||It is common practice for data scientists to acquire and integrate disparate data sources to achieve higher quality results. But even with a perfectly cleaned and merged data set, two fundamental questions remain: (1) Is the integrated data set complete? and (2) What is the impact of any unknown (i.e., unobserved) data on query results?In this work, we develop and analyze techniques to estimate the impact of the unknown data (a.k.a., unknown unknowns) on simple aggregate queries. The key idea is that the overlap between different data sources enables us to estimate the number and values of the missing data items. Our main techniques are parameter-free and do not assume prior knowledge about the distribution; we also propose a parametric model that can be used instead when the data sources are imbalanced. Through a series of experiments, we show that estimating the impact of unknown unknowns is invaluable to better assess the results of aggregate queries over integrated data sources.|10.1145/3167970|https://doi.org/10.1145/3167970|New York, NY, USA|Association for Computing Machinery||2018|Estimating the Impact of Unknown Unknowns on Aggregate Query Results|Chung, Yeounoh and Mortensen, Michael Lind and Binnig, Carsten and Kraska, Tim|article|10.1145/3167970|3|mar|ACM Trans. Database Syst.|03625915|1|43|March 2018||||||||||||||||||||||194780568|1726010902
||Big data, Crowdsourcing or crowdsourced, Data discovery, Data discovery, Data science, Database, Dryad, Environmental health, Interdisciplinary, Machine readable, Metadata, Remote sensing, Social media||11-20|Encyclopedia of Ecology (Second Edition)|The use of data repositories for parameterizing ecological models and storing model runs is becoming more common, yet often these data archives do not contain the appropriate metadata, nor are they maintained for others to use. Data archiving and sharing are additional steps in the scientific process that add value to a researcher׳s work, and more importantly, facilitate transparency and repeatability of a researcher׳s work. Historically, peer-reviewed publications did not allow for the full presentation of underlying datasets, which were only shared through personal contact with a scientist. However, with the expanding use of “supporting online material” (SOM) files that accompany digital publication there is an increased expectation that even large datasets can be made accessible to readers. Thus, researchers are faced with the additional task of becoming their own archivist and depositing data in a repository where it can be used by others. This article introduces basic concepts in data archiving and sharing, including major digital repositories for life science data, commonly used digital file formats, and why metadata is an essential element to successful data sharing when machine-readable data is increasingly used in large-scale studies.|https://doi.org/10.1016/B978-0-12-409548-9.10557-3|https://www.sciencedirect.com/science/article/pii/B9780124095489105573|Oxford|Elsevier|978-0-444-64130-4|2019|Big Data for Ecological Models|Marin M. Kress|incollection|KRESS201911|||||||||Second Edition|Brian Fath|||||||||||||||||||195550633|42
AIAM2021|Manchester, United Kingdom||4|2605–2608|2021 3rd International Conference on Artificial Intelligence and Advanced Manufacture|Under the background of typical task orientation in higher vocational education, it is an inevitable trend to carry out teaching reform of GNSS Survey for engineering survey major in higher vocational education. This paper breaks the traditional theoretical teaching system, reconstructs it from the aspects of teaching content selection, combination of theory and practice teaching system, considers the actual situation of students' weak theoretical foundation in higher vocational colleges, and the core technical ability needed for employment and post, optimizes the curriculum structure, integrates the theoretical knowledge system and practice teaching links, and meets the requirements of project-oriented curriculum standards.|10.1145/3495018.3501149|https://doi.org/10.1145/3495018.3501149|New York, NY, USA|Association for Computing Machinery|9781450385046|2022|Computer Assisted Design and Practice of GNSS Survey Course of Typical Tasks through Cloud Computing and Big Data Technology|Tang, Yan|inproceedings|10.1145/3495018.3501149|||||||||||||||||||||||||||||196647243|42
UbiComp '13 Adjunct|Zurich, Switzerland|smart cities, value chain., big data, software architecture|4|1359–1362|Proceedings of the 2013 ACM Conference on Pervasive and Ubiquitous Computing Adjunct Publication|The domain of inquiry of this research is the collection, organization, integration, distribution and consumption of knowledge derived from urban open data, and how it can be best offered to application cities' stakeholders through a software middleware. We argue that the extensive investigation proposed in this research will contribute to a growing body of knowledge about data integration and application in smart cities, and offer opportunities to re-think an integrated urban infrastructure.|10.1145/2494091.2499223|https://doi.org/10.1145/2494091.2499223|New York, NY, USA|Association for Computing Machinery|9781450322157|2013|A Middleware Framework for Urban Data Management|Romualdo-Suzuki, Larissa and Finkelstein, Anthony and Gann, David|inproceedings|10.1145/2494091.2499223|||||||||||||||||||||||||||||197655704|42
ICBDT 2020|Qingdao, China|North China, Deep learning, LSTM, Hourly temperature prediction|4|93–96|Proceedings of the 2020 3rd International Conference on Big Data Technologies|Accurate prediction of temperature is an important part of fine weather forecast services (such as heating energy consumption in winter, Winter Olympic Games, etc.). Therefore, the accurate prediction of hourly temperature is very significant in the management of human health and the decision-making of government. In this study, a long short term (LSTM) memory model was proposed and used to predict the next hour's temperature in mega-cites in North China. It was fully considered for the historic temperature and meteorological condition. As a result, the predictor secured a fast and accurate prediction performance by fully reflecting the long-term historic process of input time series data through LSTM. The meteorological data from Beijing, Tianjin, Shijiazhuang and Taiyuan which represents the mega-cites of North China during October 1 to December 31 in 2016-2018 were used to verify the validity of the proposed method. In conclusion, the proposed method was proved to have a good prediction performance in cooling and turning warming processes, making up for the poor performance of turning weather prediction in the previous research. It confirmed that the forward supplement LSTM model has the best prediction ability for hourly temperature prediction in Beijing among mega-cites in North China. The results also indicate great potential of the machine learning method in improving local weather forecast and the potential to serve the 2022 Winter Olympics.|10.1145/3422713.3422718|https://doi.org/10.1145/3422713.3422718|New York, NY, USA|Association for Computing Machinery|9781450387859|2020|Deep Learning-Based Hourly Temperature Prediction: A Case Study of Mega-Cites in North China|Qi, Yajie and Guo, Chunwei|inproceedings|10.1145/3422713.3422718|||||||||||||||||||||||||||||199049753|42
||Wearable Sensor, Accelerometry, Automatic Synchronization, Time Synchronization, Temporal Drift, Video, Wearable Camera|26|||The development and validation of computational models to detect daily human behaviors (e.g., eating, smoking, brushing) using wearable devices requires labeled data collected from the natural field environment, with tight time synchronization of the micro-behaviors (e.g., start/end times of hand-to-mouth gestures during a smoking puff or an eating gesture) and the associated labels. Video data is increasingly being used for such label collection. Unfortunately, wearable devices and video cameras with independent (and drifting) clocks make tight time synchronization challenging. To address this issue, we present the Window Induced Shift Estimation method for Synchronization (SyncWISE) approach. We demonstrate the feasibility and effectiveness of our method by synchronizing the timestamps of a wearable camera and wearable accelerometer from 163 videos representing 45.2 hours of data from 21 participants enrolled in a real-world smoking cessation study. Our approach shows significant improvement over the state-of-the-art, even in the presence of high data loss, achieving 90% synchronization accuracy given a synchronization tolerance of 700 milliseconds. Our method also achieves state-of-the-art synchronization performance on the CMU-MMAC dataset.|10.1145/3411824|https://doi.org/10.1145/3411824|New York, NY, USA|Association for Computing Machinery||2020|SyncWISE: Window Induced Shift Estimation for Synchronization of Video and Accelerometry from Wearable Sensors|Zhang, Yun C. and Zhang, Shibo and Liu, Miao and Daly, Elyse and Battalio, Samuel and Kumar, Santosh and Spring, Bonnie and Rehg, James M. and Alshurafa, Nabil|article|10.1145/3411824|107|sep|Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.||3|4|September 2020||||||||||||||||||||||201235335|42
||Training;Analytical models;Systematics;Databases;Data integrity;Biological system modeling;Education;Educational data;Data quality;Statistics||363-367|2021 IEEE 4th International Conference on Information Systems and Computer Aided Education (ICISCAE)|With the development of Internet and information technology, data has become an important asset related to the development prospects of society and all walks of life. At present, there are many quality problems in the use of educational data, which has brought great obstacles to exerting the value of educational data. Only by using scientific statistical methods, obtaining real, objective, comprehensive, scientific and effective basic data, and carrying out systematic and comprehensive analysis on the obtained data, can we give full play to its command and decision-making role. The development of big data and artificial intelligence technology provides new ideas for the analysis and evaluation of educational data quality, and is committed to restoring the overall picture of the education system and promoting the change of regional educational ecology. In this paper, an educational data quality analysis model based on multiple constraint model is proposed, which classifies the data in the database, divides the information in the database into several different categories according to the data characteristics, and establishes a quality management system for educational data in universities, so as to effectively improve the quality of educational data.|10.1109/ICISCAE52414.2021.9590700|||||2021|Research on the Construction of Educational Data Quality Model Based on Multiple Constraints Model|Du, Jinming|inproceedings|9590700||Sep.|||||||||||||||||||||||||||201632851|42
WWW '22|Virtual Event, Lyon, France|datasets, text tagging, neural networks, gaze detection|5|94–98|Companion Proceedings of the Web Conference 2022|With the rapid growth of video resources on the Internet, text-video retrieval has become a common requirement. Scholars handled text-video retrieval tasks with two-broad-category: concept-based methods and neural semantics match networks. Besides deep neural semantics matching models, some scholars mined queries and videos relationships from click-graphs, which express the users’ implicit judgments on relevance relations. However, bad generalization of click-based or concept-based models hardly capture semantic information from short queries, which stunt existing methods to fully utilize the methods to enhance the IR performance. In this paper, we propose a framework ETHGS to combine the abilities of concept-based, click-based and semantic-based models in IR and publish a new video retrieval dataset QVT from a real-world video search engine. In ETHGS, we make use of tags (i.e. concept) to construct a heterogeneous graph to alleviate the sparsity of click-through data. And we also overcome the problem of long-tailed query representation without graph information by fusing tag embeddings to represent queries. ETHGS leverages semantic embeddings to review deviant semantic information from graph nodes information. Finally, we evaluate our model ETHGS on the QVT.|10.1145/3487553.3524214|https://doi.org/10.1145/3487553.3524214|New York, NY, USA|Association for Computing Machinery|9781450391306|2022|Semantic IR Fused Heterogeneous Graph Model in Tag-Based Video Search|Gao, Liming and Liao, Dongliang and Li, Gongfu and Xu, Jin and Zhuo, Hankz Hankui|inproceedings|10.1145/3487553.3524214|||||||||||||||||||||||||||||202466445|42
||Big data, Smart farming, Value, Farmers, Cyber-physical-social systems, Activity theory||100297||Big data represent a pioneering development in the field of agriculture. By producing intuition, intelligence, and insights, these data have the potential to recast conventional process-driven agriculture, plotting the course for a smarter, data-driven farming. However, many open issues about the use of big data in agriculture remain unanswered. In this work, conceptualizing smart agricultural systems as cyber-physical-social systems, and building upon activity theory, we aim at highlighting some key questions that need to be addressed. To our view, big data constitute a tool reciprocally produced by all the actors involved in the agrifood supply chains. The constant flux of this tool and the intricate nature of the interactions among the actors who share it complicate the translation of big data into value. Moreover, farmers’ limited capacity to deal with data complexity, along with their dual role as producers and users of big data, impedes the institutionalization of this tool at the farm level. Although the approach used left us with more questions than answers, we suggest that unraveling the institutional arrangements that govern value co-creation, capturing the motivations of farmers and other actors, and detailing the direct and indirect effects that big data (and the technologies used to generate them) have in farms are important preconditions for setting forth rules that facilitate the extraction and equal exchange of value from big data.|https://doi.org/10.1016/j.njas.2019.04.003|https://www.sciencedirect.com/science/article/pii/S1573521418302197||||2019|Key questions on the use of big data in farming: An activity theory approach|Evagelos D. Lioutas and Chrysanthi Charatsari and Giuseppe {La Rocca} and Marcello {De Rosa}|article|LIOUTAS2019100297|||NJAS - Wageningen Journal of Life Sciences|15735214||90-91|||||130185|1,023|Q1|43|25|86|1259|392|85|5,11|50,36|Netherlands|Western Europe|1988, 1996-2003, 2005-2020|Agronomy and Crop Science (Q1); Animal Science and Zoology (Q1); Development (Q1); Food Science (Q1); Plant Science (Q1)||||203360630|689068658
||role and responsibility analysis, business management, data analytics, chief data officer (CDO)|9|4–12||While the number of organizations creating the role of Chief Data Officer (CDO) is increasing each year, the nature of the role is still emerging. CDO management responsibilities can vary widely from company to company. The study focuses on the various management responsibilities of the CDO role and their commonalities across organizations. After collecting and analyzing CDO job description from 411 organizations, we came to the following conclusions. Data analytics and business management are the most often cited and thus the most important management responsibilities for the CDO. Second is the management of data quality and data governance programs. Third, the CDO should keep abreast of new information technologies that could help firms design and execute an enterprise data strategy that coordinates the firm's business intelligence processes, leads to the development of new products, and acquires new customers through new data media.|||Evansville, IN, USA|Consortium for Computing Sciences in Colleges||2018|Chief Data Officer (CDO) Role and Responsibility Analysis|Nie, Yu and Talburt, John and Li, Xinming and Xiao, Zhongdong|article|10.5555/3204979.3204980||may|J. Comput. Sci. Coll.|19374771|5|33|May 2018||||||||||||||||||||||206273353|1795572528
dg.o 2019|Dubai, United Arab Emirates|Data protection, GDPR, Criminal justice data, Open data, Privacy, Microdata, Justice domain data|10|314–323|Proceedings of the 20th Annual International Conference on Digital Government Research|To enhance the transparency, accountability and efficiency of the Dutch Ministry of Justice and Security, the ministry has set up an open data program to proactively stimulate sharing its (publicly funded) data sets with the public. Disclosure of personal data is considered as one of the main threats for data opening. In this contribution we argue that, according to Dutch laws, the criminal data within the Dutch justice domain are sensitive data in GDPR terms and that the criminal data can only be opened if these sensitive data are transformed to have no personal information. Subsequently, having no personal information in data sets is related to two GDPR concepts: the data being anonymous in its GDPR sense or the data being pseudonymized in its GDPR sense. These two GDPR concepts, i.e., being anonymous data or pseudonymized data in a GDPR sense, can be distinguished in our setting based on whether the data controller cannot or can revert the data protection process, respectively. (Note that the terms anonymous and pseudonymized are interpreted differently in the technical domain.) We examine realizing these GDPR concepts with the Statistical Disclosure Control (SDC) technology and subsequently argue that pseudonymized data in a GDPR sense delivers a better data utility than the other. At the end, we present a number of the consequences of adopting either of these concepts, which can inform legislators and policymakers to define their strategy for opening privacy sensitive microdata sets, like those pertaining to the Dutch criminal justice domain.|10.1145/3325112.3325222|https://doi.org/10.1145/3325112.3325222|New York, NY, USA|Association for Computing Machinery|9781450372046|2019|Opening Privacy Sensitive Microdata Sets in Light of GDPR|S. Bargh, Mortaza and Meijer, Ronald and Vink, Marco and van Den Braak, Susan and Schirm, Walter and Choenni, Sunil|inproceedings|10.1145/3325112.3325222|||||||||||||||||||||||||||||206953187|42
IDEAS '16|Montreal, QC, Canada|discovery algorithm, approximate match, functional dependency, Database integration|9|53–61|Proceedings of the 20th International Database Engineering &amp; Applications Symposium|Functional dependencies (fds) express important relationships among data, which can be used for several goals, including schema normalization and data cleansing. However, to solve several issues in emerging application domains, such as the identification of data inconsistencies or patterns of semantically related data, it has been necessary to relax the fd definition through the introduction of approximations in data comparison and/or validity. Moreover, while fds were originally specified at design time, with the availability of massive data and computational power many algorithms have been devised to automatically discover them from data, including algorithms for discovering some types of relaxed fds. In this paper we present a technique that exploits lattice-based algorithms for the discovery of fds from data, in order to detect relaxed fds. Moreover, we introduce an algorithm to determine a proper distance threshold for a given relaxed fd holding over the entire database.|10.1145/2938503.2938519|https://doi.org/10.1145/2938503.2938519|New York, NY, USA|Association for Computing Machinery|9781450341189|2016|On the Discovery of Relaxed Functional Dependencies|Caruccio, Loredana and Deufemia, Vincenzo and Polese, Giuseppe|inproceedings|10.1145/2938503.2938519|||||||||||||||||||||||||||||208210235|42
PervasiveHealth'19|Trento, Italy|Cough, mHealth, Digital Biomarkers, Breathlessness, Crowdsourced Annotation, Breathing, Data Quality|10|179–188|Proceedings of the 13th EAI International Conference on Pervasive Computing Technologies for Healthcare|Proliferation of sensors embedded in smartphones and smartwatches helps capture rich dataset for machine learning algorithms to extract meaningful digital bio-markers on consumer devices for monitoring disease progression and treatment response. However, development and validation of machine learning algorithms depend on gathering high fidelity sensor data and reliable ground-truth. We conduct a study, called mLungStudy, with 131 subjects with varying pulmonary conditions to collect mobile sensor data including audio, accelerometer, gyroscope using a smartphone and a smartwatch, in order to extract pulmonary biomarkers such as breathing, coughs, spirometry, and breathlessness. Our study shows that commonly used breathing ground-truth data from chestband may not always be reliable as a gold-standard. Our analysis shows that breathlessness biomarkers such as pause time and pause frequency from 2.15 minutes of audio can be as reliable as those extracted from 5 minutes' worth of speech data. This finding can be useful for future studies to trade-off between the reliability of breathlessness data and patient comfort in generating continuous speech data. Furthermore, we use crowdsourcing techniques to annotate pulmonary sound events for developing signal processing and machine learning algorithms. In this paper, we highlight several practical challenges to collect and annotate physiological data and acoustic symptoms from chronic pulmonary patients and ways to improve data quality. We show that the waveform visualization of the audio signal improves annotation quality which leads to a 6.59% increase in cough classification accuracy and a 6% increase in spirometry event classification accuracy. Findings from this study inform future studies focusing on developing explainable machine learning models to extract pulmonary digital bio-markers using mobile sensors.|10.1145/3329189.3329204|https://doi.org/10.1145/3329189.3329204|New York, NY, USA|Association for Computing Machinery|9781450361262|2019|Towards Reliable Data Collection and Annotation to Extract Pulmonary Digital Biomarkers Using Mobile Sensors|Rahman, Md Mahbubur and Nathan, Viswam and Nemati, Ebrahim and Vatanparvar, Korosh and Ahmed, Mohsin and Kuang, Jilong|inproceedings|10.1145/3329189.3329204|||||||||||||||||||||||||||||210239380|42
||Big data, User-Generated content, Latent Dirichlet Allocation, Topic modeling, Market research, Qualitative data||115-124||User-generated content, such as online product reviews, is a valuable source of consumer insight. Such unstructured big data is generated in real-time, is easily accessed, and contains messages consumers want managers to hear. Analyzing such data has potential to revolutionize market research and competitive analysis, but how can the messages be extracted? How can the vast amount of data be condensed into insights to help steer businesses’ strategy? We describe a non-proprietary technique that can be applied by anyone with statistical training. Latent Dirichlet Allocation (LDA) can analyze huge amounts of text and describe the content as focusing on unseen attributes in a specific weighting. For example, a review of a graphic novel might be analyzed to focus 70% on the storyline and 30% on the graphics. Aggregating the content from numerous consumers allows us to understand what is, collectively, on consumers’ minds, and from this we can infer what consumers care about. We can even highlight which attributes are seen positively or negatively. The value of this technique extends well beyond the CMO's office as LDA can map the relative strategic positions of competitors where they matter most: in the minds of consumers.|https://doi.org/10.1016/j.bushor.2015.10.001|https://www.sciencedirect.com/science/article/pii/S0007681315001408||||2016|Uncovering the message from the mess of big data|Neil T. Bendle and Xin (Shane) Wang|article|BENDLE2016115|||Business Horizons|00076813|1|59|||||20209|2,174|Q1|87|64|278|2475|2066|227|6,68|38,67|United Kingdom|Western Europe|1957-2020|Business and International Management (Q1); Marketing (Q1)|7,443|6.361|0.00571|215748688|847373672
||Building heat demand, Big data, Large scale modelling, Bottom-up modelling, GIS, Climate data, Spatio-temporal modelling||277-290||Building heat demand is responsible for a significant share of the total global final energy consumption. Building stock models with a high spatio-temporal resolution are a powerful tool to investigate the effects of new building policies aimed at increasing energy efficiency, the introduction of new heating technologies or the integration of buildings within an energy system based on renewable energy sources. Therefore, building stock models have to be able to model the improvements and variation of used materials in buildings. In this paper, we propose a method based on generalized large-scale geographic information system (GIS) to model building heat demand of large regions with a high temporal resolution. In contrast to existing building stock models, our approach allows to derive the envelope of all buildings from digital elevation models and to model location dependent effects such as shadowing due to the topography and climate conditions. We integrate spatio-temporal climate data for temperature and solar radiation to model climate effects of complex terrain. The model is validated against a database containing the measured energy demand of 1845 buildings of the city of St. Gallen, Switzerland and 120 buildings of the Alpine village of Zernez, Switzerland. The proposed model is able to assess and investigate large regions by using spatial data describing natural and anthropogenic land features. The validation resulted in an average goodness of fit (R2) of 0.6.|https://doi.org/10.1016/j.apenergy.2017.10.041|https://www.sciencedirect.com/science/article/pii/S030626191731454X||||2017|Big data GIS analysis for novel approaches in building stock modelling|René Buffat and Andreas Froemelt and Niko Heeren and Martin Raubal and Stefanie Hellweg|article|BUFFAT2017277|||Applied Energy|03062619||208|||||28801|3,035|Q1|212|1729|5329|100144|56804|5304|10,59|57,92|United Kingdom|Western Europe|1975-2020|Building and Construction (Q1); Energy (miscellaneous) (Q1); Management, Monitoring, Policy and Law (Q1); Mechanical Engineering (Q1)|122,712|9.746|0.15329|215935753|892883729
||Artificial intelligence, Big data, Intelligent medicine, Data collection, Data storage, Data annotation, Data management||||Medical artificial intelligence (AI) and big data technology have rapidly advanced in recent years, and they are now routinely used for image-based diagnosis. China has a massive amount of medical data. However, a uniform criteria for medical data quality have yet to be established. Therefore, this review aimed to develop a standardized and detailed set of quality criteria for medical data collection, storage, annotation, and management related to medical AI. This will greatly improve the process of medical data resource sharing and the use of AI in clinical medicine.|https://doi.org/10.1016/j.imed.2021.11.002|https://www.sciencedirect.com/science/article/pii/S2667102621001200||||2021|Standardization of collection, storage, annotation, and management of data related to medical artificial intelligence|Yahan Yang and Ruiyang Li and Yifan Xiang and Duoru Lin and Anqi Yan and Wenben Chen and Zhongwen Li and Weiyi Lai and Xiaohang Wu and Cheng Wan and Wei Bai and Xiucheng Huang and Qiang Li and Wenrui Deng and Xiyang Liu and Yucong Lin and Pisong Yan and Haotian Lin|article|YANG2021|||Intelligent Medicine|26671026|||||||||||||||||||||||||217603446|714385722
SIGMOD '21|Virtual Event, China|streaming processing, real-time infrastructure|14|2503–2516|Proceedings of the 2021 International Conference on Management of Data|Uber's business is highly real-time in nature. PBs of data is continuously being collected from the end users such as Uber drivers, riders, restaurants, eaters and so on everyday. There is a lot of valuable information to be processed and many decisions must be made in seconds for a variety of use cases such as customer incentives, fraud detection, machine learning model prediction. In addition, there is an increasing need to expose this ability to different user categories, including engineers, data scientists, executives and operations personnel which adds to the complexity. In this paper, we present the overall architecture of the real-time data infrastructure and identify three scaling challenges that we need to continuously address for each component in the architecture. At Uber, we heavily rely on open source technologies for the key areas of the infrastructure. On top of those open-source software, we add significant improvements and customizations to make the open-source solutions fit in Uber's environment and bridge the gaps to meet Uber's unique scale and requirements. We then highlight several important use cases and show their real-time solutions and tradeoffs. Finally, we reflect on the lessons we learned as we built, operated and scaled these systems.|10.1145/3448016.3457552|https://doi.org/10.1145/3448016.3457552|New York, NY, USA|Association for Computing Machinery|9781450383431|2021|Real-Time Data Infrastructure at Uber|Fu, Yupeng and Soman, Chinmay|inproceedings|10.1145/3448016.3457552|||||||||||||||||||||||||||||220384119|42
|||6|44–49||"\"Intel has moved to a collaboration model with universities consisting of \"\"Science and Technology Centers\"\" (ISTCs). These are located at a \"\"hub\"\" university with participation from other universities, contain embedded Intel personnel, and are focused on some research theme. Intel held a national competition for a 5th Science and Technology center in 2012 and selected a proposal from M.I.T. with a theme of \"\"Big Data\"\". This paper presents the big data vision of this technology center and the execution plan for the first few years.\""|10.1145/2481528.2481537|https://doi.org/10.1145/2481528.2481537|New York, NY, USA|Association for Computing Machinery||2013|"\"Intel \"\"Big Data\"\" Science and Technology Center Vision and Execution Plan\""|Stonebraker, Michael and Madden, Sam and Dubey, Pradeep|article|10.1145/2481528.2481537||may|SIGMOD Rec.|01635808|1|42|March 2013||||13622|0,372|Q2|142|39|81|808|127|57|1,67|20,72|United States|Northern America|1969, 1973-1978, 1981-2020|Information Systems (Q2); Software (Q2)|1,819|0.775|7.5E-4|222584672|962972343
ICCAD '15|Austin, TX, USA|semiconductor, manufacturing, Big data, analytics|5|776–780|Proceedings of the IEEE/ACM International Conference on Computer-Aided Design|"\"Big data analytics is the latest spotlight with all the glare of fame ranging from media coverage to booming start-up companies to eye-catching merges and acquisitions. On the contrary, the $336 billion industry of semiconductor was seen as an \"\"old-fashioned\"\" business, with fading interests from the best and brightest among young graduates and engineers. How will modern big data analytics help the semiconductor industry walk through this transition? This paper answers this question via a number of practical but challenging problems arising from semiconductor manufacturing process. We show that many existing machine learning algorithms are not well positioned to solve these problems, and novel techniques involving temporal, structural and hierarchical properties need to be developed to solve these problems.\""||||IEEE Press|9781467383899|2015|"\"Modern Big Data Analytics for \"\"Old-Fashioned\"\" Semiconductor Industry Applications\""|Zhu, Yada and Xiong, Jinjun|inproceedings|10.5555/2840819.2840927|||||||||||||||||||||||||||||223589608|42
||Renal failure, Risk prediction, Electronic health record, Health big data, Machine learning||100234||Renal failure is a fatal disease raising global concerns. Previous risk models for renal failure mostly rely on the diagnosis of chronic kidney disease, which lacks obvious clinical symptoms and thus is mostly undiagnosed, causing significant omission of high-risk patients. In this paper, we proposed a framework to predict the risk of renal failure directly from a big data repository of chronic disease population without prerequisite diagnosis of chronic kidney disease. The electronic health records of 42,256 patients with hypertension or diabetes in Shenzhen Health Information Big Data Platform were collected, with 398 suffered from renal failure during a 3-year follow-up. Five state-of-the-art machine learning methods are utilized to build risk prediction models of renal failure for chronic disease population. Extensive experimental results show that the proposed framework achieves quite well performance. Particularly, the XGBoost obtains the best performance with an area under receiving-operating-characteristics curve (AUC) of 0.9139. By analyzing the effect of risk factors, we identified that serum creatine, age, urine acid, systolic blood pressure, and blood urea nitrogen are the top five factors associated with renal failure risk. Compared with existing models, our model can be deployed into routine chronic disease management procedures and enable more preemptive, widely-covered screening of renal risks, which would in turn reduce the damage caused by the disease through timely intervention.|https://doi.org/10.1016/j.bdr.2021.100234|https://www.sciencedirect.com/science/article/pii/S2214579621000514||||2021|Risk Prediction of Renal Failure for Chronic Disease Population Based on Electronic Health Record Big Data|Yujie Yang and Ye Li and Runge Chen and Jing Zheng and Yunpeng Cai and Giancarlo Fortino|article|YANG2021100234|||Big Data Research|22145796||25|||||21100356018|0,565|Q2|25|11|76|547|324|69|4,06|49,73|United States|Northern America|2014-2020|Computer Science Applications (Q2); Information Systems (Q2); Information Systems and Management (Q2); Management Information Systems (Q2)|586|3.578|0.00126|226400715|1627174784
||Deep learning;Cloud computing;Data preprocessing;Big Data;Synchronization;Reliability;Task analysis;Data Quality;Data Preprocessing;Sensor Data;Edge Computing;Data Management||3522-3531|2020 IEEE International Conference on Big Data (Big Data)|Sensor data whether collected for machine learning, deep learning or other applications must be preprocessed to fit input requirements or improve performance and accuracy. Data preparation is an expensive, resource consuming and complex phase often performed centrally on raw data for a specific application. The dataflow between the edge and the cloud can be enhanced in terms of efficiency, reliability and lineage by preprocessing the datasets closer to their data sources. We propose a dedicated data preprocessing framework that distributes preprocessing tasks between a cloud stage and two edge stages to create a dataflow with progressively improving quality. The framework handles heterogenous data and dynamic preprocessing plans simultaneously targeting diverse applications and use cases from different domains. Each stage autonomously executes sensor specific preprocessing plans in parallel while synchronizing the progressive execution and dynamic updates of the preprocessing plans with the other stages. Our approach minimizes the workload on central infrastructures and reduces the resources used for transferring raw data from the edge. We also demonstrate that preprocessing data can be sensor specific rather than application specific and thus can be performed prior to knowing a specific application.|10.1109/BigData50022.2020.9377900|||||2020|Synchronized Preprocessing of Sensor Data|Tawakuli, Amal and Kaiser, Daniel and Engel, Thomas|inproceedings|9377900||Dec|||||||||||||||||||||||||||227577026|42
||Blockchain, Big data analytic, Hyperledger fabric, Hadoop, MapReduce, Docker, PIVT||100081||The demand for electricity is increasing exponentially day by day, especially with the arrival of electric vehicles. In the smart community neighborhood project, electricity should be produced at the household or community level and sold or bought according to the demands. Since the actors can produce, sell, and buy according to the demands, thus the name prosumers. ICT solutions can contribute to this in several ways, such as machine learning for analyzing the household data for customer demand and peak hours for the usage of electricity, blockchain as a trustworthy platform for selling or buying, data hub, and ensuring data security and privacy of prosumers. TOTEM: Token for controlled computation is a framework that allows users to analyze the data without moving the data from the data owner's environment. It also ensures the data security and privacy of the data. Here, in this article, we will show the importance of the TOTEM architecture in the EnergiX project and how the extended version of TOTEM can be efficiently merged with the demands of the current and similar projects.|https://doi.org/10.1016/j.bcra.2022.100081|https://www.sciencedirect.com/science/article/pii/S2096720922000227||||2022|Integrating big data and blockchain to manage energy smart grids—TOTEM framework|Dhanya Therese Jose and Jørgen Holme and Antorweep Chakravorty and Chunming Rong|article|JOSE2022100081|||Blockchain: Research and Applications|20967209|3|3|||||||||||||||||||||||227643326|986772320
|||35|105–139|Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice|||https://doi.org/10.1145/3447404.3447412|New York, NY, USA|Association for Computing Machinery|9781450390293|2021|DSP Basics|Alexander, Jason and Thanh Vi, Chi|inbook|10.1145/3447404.3447412|||||||||1||||||||||||||||||||228036682|42
||Internet-scale network, Dense subgraph mining, Low carbon, Multiple features, Biologically-aware||257-262||In the modern society, there are a rich number of low-carbon enterprises that the explicitly/implicitly collaborated. Effectively understanding the mechanism of their complex cooperative relationships is becoming an urgent and significant problem in information processing and management. Traditionally, these cooperation behavior are analyzed in a holistic and non-quantitative way, where the complicated relationships among various enterprises cannot be well represented. In this work, we propose to understand the low-carbon entrepreneurs’ cooperation by leveraging a massive-scale dense subgraph mining technique, based on which an evolutionary graphical model is built to dynamically represent such complex relationships. More specifically, given million-scale low-carbon enterprises, we first extract multiple biologically-aware features (e.g., production value and carbon emission) to represent each of them. Based on this, a massive-scale affinity network is constructed to characterize the relationships among these enterprises. Based on this, an efficient subgraph mining algorithm (called graph shift) is deployed to discover the neighbors for each enterprise. Finally, based on the discovered neighbors of each enterprise, we can build a graphical model to represent the relationships among explicitly/implicitly-connected enterprises. The flows of multiple attributes (benefit exchange and resources swap) can be modeled effectively. To demonstrate the usefulness of our method, we manually label the attributes of 20,000 enterprises, based on which a classification model is trained by encoding the neighboring attributes of each enterprise. Comparative results have clearly demonstrated the competitiveness of our method. Moreover, visualization results can reveal the effectiveness of our method in uncovering the intrinsic distributions/correlations of million-scale enterprises.|https://doi.org/10.1016/j.future.2021.01.002|https://www.sciencedirect.com/science/article/pii/S0167739X21000042||||2021|Massive-scale carbon pollution control and biological fusion under big data context|Yi Liu and Jie Xu and Weijie Yi|article|LIU2021257|||Future Generation Computer Systems|0167739X||118|||||12264|1,262|Q1|119|798|1935|36054|16877|1868|9,11|45,18|Netherlands|Western Europe|1984-2021|Computer Networks and Communications (Q1); Hardware and Architecture (Q1); Software (Q1)||||230177691|562237118
ICAIIS 2021|Chongqing, China||10||2021 2nd International Conference on Artificial Intelligence and Information Systems||10.1145/3469213.3470424|https://doi.org/10.1145/3469213.3470424|New York, NY, USA|Association for Computing Machinery|9781450390200|2021|A Building Integrated Control Platform Oriented Towards Intelligent Building|Yan, Feng|inproceedings|10.1145/3469213.3470424|217||||||||||||||||||||||||||||231023686|42
||Points of interest, Population, Random forests, Nighttime light, China||936-946||Remote sensing image products (e.g. brightness of nighttime lights and land cover/land use types) have been widely used to disaggregate census data to produce gridded population maps for large geographic areas. The advent of the geospatial big data revolution has created additional opportunities to map population distributions at fine resolutions with high accuracy. A considerable proportion of the geospatial data contains semantic information that indicates different categories of human activities occurring at exact geographic locations. Such information is often lacking in remote sensing data. In addition, the remarkable progress in machine learning provides toolkits for demographers to model complex nonlinear correlations between population and heterogeneous geographic covariates. In this study, a typical type of geospatial big data, points-of-interest (POIs), was combined with multi-source remote sensing data in a random forests model to disaggregate the 2010 county-level census population data to 100 × 100 m grids. Compared with the WorldPop population dataset, our population map showed higher accuracy. The root mean square error for population estimates in Beijing, Shanghai, Guangzhou, and Chongqing for this method and WorldPop were 27,829 and 34,193, respectively. The large under-allocation of the population in urban areas and over-allocation in rural areas in the WorldPop dataset was greatly reduced in this new population map. Apart from revealing the effectiveness of POIs in improving population mapping, this study promises the potential of geospatial big data for mapping other socioeconomic parameters in the future.|https://doi.org/10.1016/j.scitotenv.2018.12.276|https://www.sciencedirect.com/science/article/pii/S0048969718351489||||2019|Improved population mapping for China using remotely sensed and points-of-interest data within a random forests model|Tingting Ye and Naizhuo Zhao and Xuchao Yang and Zutao Ouyang and Xiaoping Liu and Qian Chen and Kejia Hu and Wenze Yue and Jiaguo Qi and Zhansheng Li and Peng Jia|article|YE2019936|||Science of The Total Environment|00489697||658|||||25349|1,795|Q1|244|6929|13278|455970|106837|13125|7,96|65,81|Netherlands|Western Europe|1970, 1972-2021|Environmental Chemistry (Q1); Environmental Engineering (Q1); Pollution (Q1); Waste Management and Disposal (Q1)|210,143|7.963|0.23082|231751511|2019676356
||Business;Petri nets;Engineering drawings;Indexes;Routing;Sun;Data mining;Data repairing;event data processing;petri net||2943-2957||For various entering and transmission issues raised by human or system, missing events often occur in event data, which record execution logs of business processes. Without recovering the missing events, applications such as provenance analysis or complex event processing built upon event data are not reliable. Following the minimum change discipline in improving data quality, it is also rational to find a recovery that minimally differs from the original data. Existing recovery approaches fall short of efficiency owing to enumerating and searching over all of the possible sequences of events. In this paper, we study the efficient techniques for recovering missing events. According to our theoretical results, the recovery problem appears to be NP-hard. Nevertheless, advanced indexing, pruning techniques are developed to further improve the recovery efficiency. The experimental results demonstrate that our minimum recovery approach achieves high accuracy, and significantly outperforms the state-of-the-art technique for up to five orders of magnitudes improvement in time performance.|10.1109/TKDE.2016.2594785|||||2016|Efficient Recovery of Missing Events|Wang, Jianmin and Song, Shaoxu and Zhu, Xiaochen and Lin, Xuemin and Sun, Jiaguang|article|7523405||Nov|IEEE Transactions on Knowledge and Data Engineering|15582191|11|28|||||||||||||||||||||||231767679|1598944404
||Agriculture;Big Data;Production;Sensors;Systematics;Sociology;Statistics;big data;smart agriculture;data driven;precision agriculture;smart farming||60-65|2019 2nd International Conference on Artificial Intelligence and Big Data (ICAIBD)|Increasing agricultural production is top most solution in the face of rapid population growth through digitalization of agriculture by using most developed technology like big data. There is a long debate on the application of big data in agriculture. This study is an attempt to explore the suitability of the big data technologies for increasing production and improving quality in agriculture. The study uses an extensive review of current research works and studies in agriculture for exploring the best and compatible practices which can help farmers at field level for increasing production and improving quality. This study reveals a number of available big data technologies and practices in agriculture for solving the current problems and challenges at field level. A conceptual model is developed for proper implementation of available big data technologies at farmer's field level. The study highlights data generation procedure, availability of technology, availability of hardware, software, data collection techniques, method of analysis and suitability of application of big data technologies for smart agriculture. The article explores that there are still some challenges exists in this field as a new domain in agriculture like privacy of data, data quality, availability, initial investment, infrastructure and related expertise. The study suggests that government initiatives, public-private partnership, openness of data, financial investment and regional basis research work are necessary for implementing the big data technologies in agriculture at large scale.|10.1109/ICAIBD.2019.8836982|||||2019|Big Data Driven Smart Agriculture: Pathway for Sustainable Development|Islam Sarker, Md Nazirul and Wu, Min and Chanthamith, Bouasone and Yusufzada, Shaheen and Li, Dan and Zhang, Jie|inproceedings|8836982||May|||||||||||||||||||||||||||233545680|42
||||646-656||Brain activity is a dynamic combination of the responses to sensory inputs and its own spontaneous processing. Consequently, such brain activity is continuously changing whether or not one is focusing on an externally imposed task. Previously, we have introduced an analysis method that allows us, using Hidden Markov Models (HMM), to model task or rest brain activity as a dynamic sequence of distinct brain networks, overcoming many of the limitations posed by sliding window approaches. Here, we present an advance that enables the HMM to handle very large amounts of data, making possible the inference of very reproducible and interpretable dynamic brain networks in a range of different datasets, including task, rest, MEG and fMRI, with potentially thousands of subjects. We anticipate that the generation of large and publicly available datasets from initiatives such as the Human Connectome Project and UK Biobank, in combination with computational methods that can work at this scale, will bring a breakthrough in our understanding of brain function in both health and disease.|https://doi.org/10.1016/j.neuroimage.2017.06.077|https://www.sciencedirect.com/science/article/pii/S1053811917305487||||2018|Discovering dynamic brain networks from big data in rest and task|Diego Vidaurre and Romesh Abeysuriya and Robert Becker and Andrew J. Quinn and Fidel Alfaro-Almagro and Stephen M. Smith and Mark W. Woolrich|article|VIDAURRE2018646|||NeuroImage|10538119||180||Brain Connectivity Dynamics|||||||||||||||||||||238559533|2006600757
|||10|902–911||Feature selection (FS) is one of the fundamental data processing techniques in various machine learning algorithms, especially for classification of healthcare data. However, it is a challenging issue due to the large search space. Binary Particle Swarm Optimization (BPSO) is an efficient evolutionary computation technique, and has been widely used in FS. In this paper, we proposed a Confidence-based and Cost-effective feature selection (CCFS) method using BPSO to improve the performance of healthcare data classification. Specifically, first, CCFS improves search effectiveness by developing a new updating mechanism that designs the feature confidence to explicitly take into account the fine-grained impact of each dimension in the particle on the classification performance. The feature confidence is composed of two measurements: the correlation between feature and categories, and historically selected frequency of each feature. Second, considering the fact that the acquisition costs of different features are naturally different, especially for medical data, and should be fully taken into account in practical applications, besides the classification performance, the feature cost and the feature reduction ratio are comprehensively incorporated into the design of fitness function. The proposed method has been verified in various UCI public datasets and compared with various benchmark schemes. The thoroughly experimental results show the effectiveness of the proposed method, in terms of accuracy and feature selection cost.|10.1109/TCBB.2019.2903804|https://doi.org/10.1109/TCBB.2019.2903804|Washington, DC, USA|IEEE Computer Society Press||2019|CCFS: A Confidence-Based Cost-Effective Feature Selection Scheme for Healthcare Data Classification|Chen, Yiyuan and Wang, Yufeng and Cao, Liang and Jin, Qun|article|10.1109/TCBB.2019.2903804||mar|IEEE/ACM Trans. Comput. Biol. Bioinformatics|15455963|3|18|May-June 2021||||||||||||||||||||||238894949|1878427007
https://doi.org/10.1053/j.semvascsurg.2022.09.002|https://www.sciencedirect.com/science/article/pii/S089579672200062X||||2022|Big data: Using databases and registries|Jean Jacob-Brassard and Charles {de Mestral}|article|JACOBBRASSARD2022|||Seminars in Vascular Surgery|08957967||||||||||||||||||||||||||||||||242633853|42
|||23|433–455|Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice|||https://doi.org/10.1145/3447404.3447430|New York, NY, USA|Association for Computing Machinery|9781450390293|2021|Authors’ Biographies/Index||inbook|10.1145/3447404.3447430|||||||||1||||||||||||||||||||242884795|42
|||16|323–338|Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice|||https://doi.org/10.1145/3447404.3447422|New York, NY, USA|Association for Computing Machinery|9781450390293|2021|Secure Gestures—Case Study 4|Liu, Can and Lindqvist, Janne|inbook|10.1145/3447404.3447422|||||||||1||||||||||||||||||||247581775|42
||Big data architecture, Action research, Case study research, Blockchain adoption, Supply chain management||102452||Blockchain appears to still be nascent in its growth and a relatively untapped asset. This research investigates the need of blockchain in Industry 4.0 environment from Big Data perspective in supply chain management. The research method used in this study involves a combination of an Action Research method and Case Study research. More specifically, the action research method was applied in two industry case studies that implemented and tested the designed architecture in a global logistics environment. Case Study A examined the blockchain application in cross-border cargo movements whereas Case Study B investigated the application in a liquid chemical logistics company serving to petroleum industries. Our research analysis has identified that the Case A subject had disconnected systems and services for blockchain wherein the big data interactions had failed (failure case). Whereas in Case B, the company has achieved nearly 25% increase in revenue through its customer service after the blockchain implementation and thereby reduction in paperwork and carbon emissions (success case). This research contributes to the advancement of the body of knowledge to big data and blockchain by identifying key implementation guideline and issues for blockchain in supply chain management. Further, action-based research coupled with a case study approach has been used to evaluate the application aspects of the architecture's scalability and functionality of bigdata and blockchain in supply chain management.|https://doi.org/10.1016/j.omega.2021.102452|https://www.sciencedirect.com/science/article/pii/S030504832100061X||||2021|Big data driven supply chain design and applications for blockchain: An action research using case study approach|Balan Sundarakani and Aneesh Ajaykumar and Angappa Gunasekaran|article|SUNDARAKANI2021102452|||Omega|03050483||102|||||21915|2,500|Q1|142|196|363|10104|2965|361|7,55|51,55|United Kingdom|Western Europe|1973-2020|Information Systems and Management (Q1); Management Science and Operations Research (Q1); Strategy and Management (Q1)||||248027058|61180080
||Big Data;Optimization;Data models;Quality assessment;Big Data;Data Quality Evaluation;Data Quality Rules Discovery;Big Data Pre-Processing||498-501|2017 IEEE International Congress on Big Data (BigData Congress)|In the Big Data Era, data is the core for any governmental, institutional, and private organization. Efforts were geared towards extracting highly valuable insights that cannot happen if data is of poor quality. Therefore, data quality (DQ) is considered as a key element in Big data processing phase. In this stage, low quality data is not penetrated to the Big Data value chain. This paper, addresses the data quality rules discovery (DQR) after the evaluation of quality and prior to Big Data pre-processing. We propose a DQR discovery model to enhance and accurately target the pre-processing activities based on quality requirements. We defined, a set of pre-processing activities associated with data quality dimensions (DQD's) to automatize the DQR generation process. Rules optimization are applied on validated rules to avoid multi-passes pre-processing activities and eliminates duplicate rules. Conducted experiments showed an increased quality scores after applying the discovered and optimized DQR's on data.|10.1109/BigDataCongress.2017.73|||||2017|Big Data Pre-Processing: Closing the Data Quality Enforcement Loop|Taleb, Ikbal and Serhani, Mohamed Adel|inproceedings|8029366||June|||||||||||||||||||||||||||248513621|42
ICIT 2020|Xi'an, China|signaling data, map matching, road networks, human mobility|6|94–99|2020 The 8th International Conference on Information Technology: IoT and Smart City|Cellular signaling data is a valuable and abundant data source to explore human mobility. Yet challenges remain to restore movement routes from signaling data due to its coarse positioning information. We propose an efficient map matching method based on road network topology. First, a customized spatial-temporal clustering algorithm ST-DBSCAN was employed to find stationary point clusters, which were later used to segment trips into sub-trips. The search space was then clipped with a fixed buffer zone along the line that connects the whole trip. Two optional strategies were provided to find the best matching routes with distance costs. Experiments on real-world data showed that both strategies achieved high map matching accuracies (88.2% and 94.3%). With Deep Mode, the method reached higher accuracy, while with longer computation time. The proposed method has the potential in solving practical problems, in the sense that it could be easily parallelized to deal with mass data.|10.1145/3446999.3447017|https://doi.org/10.1145/3446999.3447017|New York, NY, USA|Association for Computing Machinery|9781450388559|2021|A Map Matching Method for Restoring Movement Routes with Cellular Signaling Data|Wang, Mo and Wang, Jing and Song, Yulun|inproceedings|10.1145/3446999.3447017|||||||||||||||||||||||||||||253832924|42
||trustworthiness, DeFacto, benchmark, fact checking, linked data, data quality, exploratory data analysis|26|||Among different characteristics of knowledge bases, data quality is one of the most relevant to maximize the benefits of the provided information. Knowledge base quality assessment poses a number of big data challenges such as high volume, variety, velocity, and veracity. In this article, we focus on answering questions related to the assessment of the veracity of facts through Deep Fact Validation (DeFacto), a triple validation framework designed to assess facts in RDF knowledge bases. Despite current developments in the research area, the underlying framework faces many challenges. This article pinpoints and discusses these issues and conducts a thorough analysis of its pipeline, aiming at reducing the error propagation through its components. Furthermore, we discuss recent developments related to this fact validation as well as describing advantages and drawbacks of state-of-the-art models. As a result of this exploratory analysis, we give insights and directions toward a better architecture to tackle the complex task of fact-checking in knowledge bases.|10.1145/3177873|https://doi.org/10.1145/3177873|New York, NY, USA|Association for Computing Machinery||2018|Toward Veracity Assessment in RDF Knowledge Bases: An Exploratory Analysis|Esteves, Diego and Rula, Anisa and Reddy, Aniketh Janardhan and Lehmann, Jens|article|10.1145/3177873|16|feb|J. Data and Information Quality|19361955|3|9|September 2017||||||||||||||||||||||260173989|833754770
||Machine learning algorithms;Social networking (online);Soft sensors;Training data;Big Data;Programming;Sparks;Big data;Machine Learning;Apache Spark;Apache Mahout;Map Reduce programming||1-8|2022 International Congress on Human-Computer Interaction, Optimization and Robotic Applications (HORA)|A wide range of disparate variety of heterogeneous and even disparate data sources has been integrated into the computer science research principles through the assistance of Artificial Intelligence (AI) and Machine Learning (ML) delivering excellent results in accuracy and data quality. Even so, taking machine learning approaches to very broad and dynamic datasets is both logistically and computationally costly. Some people are generating so much data regularly, which means that advanced analytics platforms have greater importance than ever before. Spark Machine Learning has extensive capabilities for a variety of ML activities such as regression, grouping, dimension reduction, and extraction of rules. While it has numerous datasets of high quality and consistency of structure. The specification of the MapReduce programming model was intended for the massive density of large-volume computation. It accomplishes this by splitting the job into several well-defined sub-sized units. There are various sets of data mining frameworks that link to a distributed device. Hadoop serves as the infrastructure for the distributed system. Several resources claim that Mahout will scale to handle massive datasets. This article discusses and demonstrates the use of algorithms and their significant impact on Bigdata, as well as a comparison between their implementations among Apache Spark and Apache Mahout.|10.1109/HORA55278.2022.9799848|||||2022|Machine learning algorithms in Bigdata Analysis and its applications: A review|Khoshaba, Farah and Kareem, Shahab and Awla, Hoshang and Mohammed, Chnar|inproceedings|9799848||June|||||||||||||||||||||||||||260707768|42
||Fault Diagnosis, High-Speed Train, Big Data, Cloud Computing, Edge Computing||309-314||High-speed trains are very fast (e.g. 350km/h) and operate at high traffic density, so once a fault has occurred, the consequences are disastrous. In order to better control the train operational status by timely and rapid detection of faults, we need new methods to handle and analyze the huge volumes of high-speed railway data. In this paper, we propose a novel framework and platform for high-speed train fault diagnosis based on big data technologies. The framework aims to allow researchers to focus on fault detection algorithm development and on-line application, with all the complexities of big data import, storage, management, and realtime use handled transparently by the framework. The framework uses a combination of cloud computing and edge computing and a two-level architecture that handles the massive data of train operations. The platform uses Hadoop as its basic framework and combines HDFS, HBase, Redis and MySQL database as the data storage framework. A lossless data compression method is presented to reduce the data storage space and improve data storage efficiency. In order to support various types of data analysis tasks for fault diagnosis and prognosis, the framework integrates online computation, off-line computation, stream computation, real-time computation and so on. Moreover, the platform provides fault diagnosis and prognosis as services to users and a simple case study is given to further illustrate how the basic functions of the platform are implemented.|https://doi.org/10.1016/j.ifacol.2018.09.318|https://www.sciencedirect.com/science/article/pii/S2405896318320007||||2018|A Platform for Fault Diagnosis of High-Speed Train based on Big Data⁎⁎Project supported by the National Natural Science Foundation, China(61490704, 61440015) and the National High-Tech. R&D Program, China (No. 2015AA043802).|Quan Xu and Peng Zhang and Wenqin Liu and Qiang Liu and Changxin Liu and Liangyong Wang and Anthony Toprac and S. {Joe Qin}|article|XU2018309|||IFAC-PapersOnLine|24058963|18|51||10th IFAC Symposium on Advanced Control of Chemical Processes ADCHEM 2018|||21100456158|0,308|Q3|72|115|8407|1913|9863|8351|1,13|16,63|Austria|Western Europe|2002-2019|Control and Systems Engineering (Q3)||||262475721|676980763
||Maintenance, (Big) data analysis, Off-shore wind turbines, Decision making||160-165||Planning and scheduling for wind farms play a critical role in the costs of maintenance. The use and analysis of field data or so-called Product Use Information (PUI) to improve maintenance activities and to reduce the costs has gained attention in the recent years. The product use data consist of sources such as measure of sensors on the turbines, the alarms information or signals from the condition monitoring, Supervisory Control and Data Acquisition (SCADA) systems, which are currently used in maintenance activities. However, those data have the potential to offer alternative solutions to improve processes and provide better decisions, by transforming them into actionable knowledge. In order to make the right decision it is important to understand, which PUI data source and which data analysis methods, are suitable for what kind of decision making task. The aim of this study is to discover, how analysis of PUI can help in the maintenance processes of off-shore wind power. The techniques from the field of big data analytics for analyzing the PUI are here addressed. The results of this study contain suggestions on the basis of algorithms of data analytics, suitable for each decision type.|https://doi.org/10.1016/j.procir.2016.09.026|https://www.sciencedirect.com/science/article/pii/S221282711630960X||||2017|Data Driven Decision Making in Planning the Maintenance Activities of Off-shore Wind Energy|Elaheh Gholamzadeh Nabati and Klaus-Dieter Thoben|article|NABATI2017160|||Procedia CIRP|22128271||59||Proceedings of the 5th International Conference in Through-life Engineering Services Cranfield University, 1st and 2nd November 2016|||21100243809|0,683|-|65|1456|3191|30451|8225|3145|2,40|20,91|Netherlands|Western Europe|2012-2020|Control and Systems Engineering; Industrial and Manufacturing Engineering||||265348381|2127027836
KDD '14|New York, New York, USA|truth discovery, record linkage, source reliability, entity profiling|10|1146–1155|Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining|The rapid growth of information sources on the Web has intensified the problem of data quality. In particular, the same real world entity may be described by different sources in various ways with overlapping information, and possibly conflicting or even erroneous values. In order to obtain a more complete and accurate picture for a real world entity, we need to collate the data records that refer to the entity, as well as correct any erroneous values. We observe that these two tasks are often tightly coupled: rectifying erroneous values will facilitate data collation, while linking similar records provides us with a clearer view of the data and additional evidence for error correction. In this paper, we present a framework called Comet that interleaves record linkage with error correction, taking into consideration the source reliabilities on various attributes. The proposed framework first utilizes confidence based matching to discriminate records in terms of ambiguity and source reliability. Then it performs adaptive matching to reduce the impact of erroneous values. Experiment results demonstrate that Comet outperforms the state-of-the-art techniques and is able to build complete and accurate profiles for real world entities.|10.1145/2623330.2623685|https://doi.org/10.1145/2623330.2623685|New York, NY, USA|Association for Computing Machinery|9781450329569|2014|Entity Profiling with Varying Source Reliabilities|Li, Furong and Lee, Mong Li and Hsu, Wynne|inproceedings|10.1145/2623330.2623685|||||||||||||||||||||||||||||266314114|42
||Big Data;Data integrity;Data analysis;Social network services;Data preprocessing;Internet;Big Data;Big data processing;Data Quality;Recommendation system;Prediction system||1-7|2017 IEEE SmartWorld, Ubiquitous Intelligence & Computing, Advanced & Trusted Computed, Scalable Computing & Communications, Cloud & Big Data Computing, Internet of People and Smart City Innovation (SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI)|With the rapid development of social networks, Internet of things, Cloud computing as well as other technologies, big data age is arriving. The increasing number of data has brought great value to the public and enterprises. Meanwhile how to manage and use big data better has become the focus of all walks of life. The 4V characteristics of big data have brought a lot of issues to the big data processing. The key to big data processing is to solve data quality issue, and to ensure data quality is a prerequisite for the successful application of big data technique. In this paper, we use recommendation systems and prediction systems as typical big data applications, and try to find out the data quality issues during data collection, data preprocessing, data storage and data analysis stages of big data processing. According to the elaboration and analysis of the proposed issues, the corresponding solutions are also put forward. Finally, some open problems to be solved in the future are also raised.|10.1109/UIC-ATC.2017.8397554|||||2017|Data quality in big data processing: Issues, solutions and open problems|Zhang, Pengcheng and Xiong, Fang and Gao, Jerry and Wang, Jimin|inproceedings|8397554||Aug|||||||||||||||||||||||||||266809268|42
||Big data, Wearable and implantable computing, Accelerometer||25-29||Big Data is increasingly prevalent in science and data analysis. We provide a short tutorial for adapting to these changes and making the necessary adjustments to the academic culture to keep Biostatistics truly impactful in scientific research.|https://doi.org/10.1016/j.spl.2018.02.014|https://www.sciencedirect.com/science/article/pii/S0167715218300592||||2018|A practical guide to big data|Ekaterina Smirnova and Andrada Ivanescu and Jiawei Bai and Ciprian M. Crainiceanu|article|SMIRNOVA201825|||Statistics & Probability Letters|01677152||136||The role of Statistics in the era of big data|||14794|0,576|Q2|66|241|862|4066|1014|860|1,17|16,87|Netherlands|Western Europe|1982-2021|Statistics and Probability (Q2); Statistics, Probability and Uncertainty (Q2)||||266904119|1688735168
||IoNT, Security, Big data, Design factors||357-368||Internet of Nano-Things (IoNT) overcomes critical difficulties and additionally open doors for wearable sensor based huge information examination. Conventional computing and/or communication systems do not offer enough flexibility and adaptability to deal with the gigantic amount of assorted information nowadays. This creates the need for legitimate components that can efficiently investigate and communicate the huge data while maintaining security and quality of service. In addition, while developing the ultra-wide Heterogeneous Networks (HetNets) associated with the ongoing Big Data project and 5G-based IoNT, it is required to resolve the emerging difficulties as well. Accordingly, these difficulties and other relevant design issues have been comprehensively reported in this survey. It mainly focuses on security issues and associated intelligence to be considered while managing these issues.|https://doi.org/10.1016/j.future.2019.08.009|https://www.sciencedirect.com/science/article/pii/S0167739X19301074||||2020|Intelligence and security in big 5G-oriented IoNT: An overview|Fadi Al-Turjman|article|ALTURJMAN2020357|||Future Generation Computer Systems|0167739X||102|||||12264|1,262|Q1|119|798|1935|36054|16877|1868|9,11|45,18|Netherlands|Western Europe|1984-2021|Computer Networks and Communications (Q1); Hardware and Architecture (Q1); Software (Q1)||||268428127|562237118
SIGMOD '21|Virtual Event, China|data profiling, python, data exploration, exploratory data analysis, statistical modeling, data preparation|10|2271–2280|Proceedings of the 2021 International Conference on Management of Data|Exploratory Data Analysis (EDA) is a crucial step in any data science project. However, existing Python libraries fall short in supporting data scientists to complete common EDA tasks for statistical modeling. Their API design is either too low level, which is optimized for plotting rather than EDA, or too high level, which is hard to specify more fine-grained EDA tasks. In response, we propose DataPrep.EDA, a novel task-centric EDA system in Python. DataPrep.EDA allows data scientists to declaratively specify a wide range of EDA tasks in different granularity with a single function call. We identify a number of challenges to implement DataPrep.EDA, and propose effective solutions to improve the scalability, usability, customizability of the system. In particular, we discuss some lessons learned from using Dask to build the data processing pipelines for EDA tasks and describe our approaches to accelerate the pipelines. We conduct extensive experiments to compare DataPrep.EDA with Pandas-profiling, the state-of-the-art EDA system in Python. The experiments show that DataPrep.EDA significantly outperforms Pandas-profiling in terms of both speed and user experience. DataPrep.EDA is open-sourced as an EDA component of DataPrep: https://github.com/sfu-db/dataprep.|10.1145/3448016.3457330|https://doi.org/10.1145/3448016.3457330|New York, NY, USA|Association for Computing Machinery|9781450383431|2021|DataPrep.EDA: Task-Centric Exploratory Data Analysis for Statistical Modeling in Python|Peng, Jinglin and Wu, Weiyuan and Lockhart, Brandon and Bian, Song and Yan, Jing Nathan and Xu, Linghao and Chi, Zhixuan and Rzeszotarski, Jeffrey M. and Wang, Jiannan|inproceedings|10.1145/3448016.3457330|||||||||||||||||||||||||||||270197152|42
||Big data, Data mining, Social media, Social networks, Social-based frameworks and applications||45-59||Big data has become an important issue for a large number of research areas such as data mining, machine learning, computational intelligence, information fusion, the semantic Web, and social networks. The rise of different big data frameworks such as Apache Hadoop and, more recently, Spark, for massive data processing based on the MapReduce paradigm has allowed for the efficient utilisation of data mining methods and machine learning algorithms in different domains. A number of libraries such as Mahout and SparkMLib have been designed to develop new efficient applications based on machine learning algorithms. The combination of big data technologies and traditional machine learning algorithms has generated new and interesting challenges in other areas as social media and social networks. These new challenges are focused mainly on problems such as data processing, data storage, data representation, and how data can be used for pattern mining, analysing user behaviours, and visualizing and tracking data, among others. In this paper, we present a revision of the new methodologies that is designed to allow for efficient data mining and information fusion from social media and of the new applications and frameworks that are currently appearing under the “umbrella” of the social networks, social media and big data paradigms.|https://doi.org/10.1016/j.inffus.2015.08.005|https://www.sciencedirect.com/science/article/pii/S1566253515000780||||2016|Social big data: Recent achievements and new challenges|Gema Bello-Orgaz and Jason J. Jung and David Camacho|article|BELLOORGAZ201645|||Information Fusion|15662535||28|||||||||||||||||||||||272481181|1192976563
||Extended Kalman filter, Fixed-coupon bond prices, Arbitrage-free Nelson–Siegel model||26-46||Nearly all studies that analyze the term structure of interest rates take a two-step approach. First, actual bond prices are summarized by interpolated synthetic zero-coupon yields, and second, some of these yields are used as the source data for further empirical examination. In contrast, we consider the advantages of a one-step approach that directly analyzes the universe of bond prices. To illustrate the feasibility and desirability of the one-step approach, we compare arbitrage-free dynamic term structure models estimated using both approaches. We also provide a simulation study showing that a one-step approach can extract the information in large panels of bond prices and avoid any arbitrary noise introduced from a first-stage interpolation of yields.|https://doi.org/10.1016/j.jeconom.2019.04.019|https://www.sciencedirect.com/science/article/pii/S0304407619300740||||2019|Term Structure Analysis with Big Data: One-Step Estimation Using Bond Prices|Martin M. Andreasen and Jens H.E. Christensen and Glenn D. Rudebusch|article|ANDREASEN201926|||Journal of Econometrics|03044076|1|212||Big Data in Dynamic Predictive Econometric Modeling|||28973|3,769|Q1|159|241|418|11045|1497|408|3,38|45,83|Netherlands|Western Europe|1973-2020|Applied Mathematics (Q1); Economics and Econometrics (Q1); History and Philosophy of Science (Q1)|25,569|2.388|0.02085|274608773|128950970
||Training;Cloud computing;Data integrity;Noise reduction;Big Data;Robustness;Classification algorithms;classification;label noise;granular ball||102-106|2022 7th International Conference on Cloud Computing and Big Data Analytics (ICCCBDA)|This paper presents a granular ball denoising method (GBD) which can effectively improve the accuracy and robustness of classification algorithm. GBD method first uses the self-adaptive hypersphere to cover the data space, then eliminates the data not contained in the hypersphere and the noise data in the sphere, and finally uses only the hypersphere data for training, so as to reduce the sample data and improve the data quality. Experiments show that using the data obtained by this method for training can greatly improve the accuracy of the classification algorithm. In addition, because the hypersphere has good adaptive ability, GBD also has excellent robustness. The experiment shows that GBD sampling data training can still get good results after adding noise to the data set. Therefore, GBD is an efficient and robust denoising method.|10.1109/ICCCBDA55098.2022.9778872|||||2022|Research on Classification Label Denoising Algorithm Based on Granular Ball|Kong, Weiyu and Wu, Yanmin and Qi, Jinli and Chen, Yanyi|inproceedings|9778872||April|||||||||||||||||||||||||||274940586|42
||||||Data quality is one of the most important problems in data management, since dirty data often leads to inaccurate data analytics results and incorrect business decisions. Poor data across businesses and the U.S. government are reported to cost trillions of dollars a year. Multiple surveys show that dirty data is the most common barrier faced by data scientists. Not surprisingly, developing effective and efficient data cleaning solutions is challenging and is rife with deep theoretical and engineering problems.This book is about data cleaning, which is used to refer to all kinds of tasks and activities to detect and repair errors in the data. Rather than focus on a particular data cleaning task, we give an overview of the endto- end data cleaning process, describing various error detection and repair methods, and attempt to anchor these proposals with multiple taxonomies and views. Specifically, we cover four of the most common and important data cleaning tasks, namely, outlier detection, data transformation, error repair (including imputing missing values), and data deduplication. Furthermore, due to the increasing popularity and applicability of machine learning techniques, we include a chapter that specifically explores how machine learning techniques are used for data cleaning, and how data cleaning is used to improve machine learning models.This book is intended to serve as a useful reference for researchers and practitioners who are interested in the area of data quality and data cleaning. It can also be used as a textbook for a graduate course. Although we aim at covering state-of-the-art algorithms and techniques, we recognize that data cleaning is still an active field of research and therefore provide future directions of research whenever appropriate.|||New York, NY, USA|Association for Computing Machinery|9781450371520|2019|Data Cleaning|Ilyas, Ihab F. and Chu, Xu|book|10.1145/3310205|||||||||||||||||||||||||||||278569514|42
||Medical big data, Data sharing, Information security, Cooperation mechanism, Medical data acquisition, Medical data centre||103149||The construction of medical big data includes several problems that need to be solved, such as integration and data sharing of many heterogeneous information systems, efficient processing and analysis of large-scale medical data with complex structure or low degree of structure, and narrow application range of medical data. Therefore, medical big data construction is not only a simple collection and application of medical data but also a complex systematic project. This paper introduces China's experience in the construction of a regional medical big data ecosystem, including the overall goal of the project; establishment of policies to encourage data sharing; handling the relationship between personal privacy, information security, and information availability; establishing a cooperation mechanism between agencies; designing a polycentric medical data acquisition system; and establishing a large data centre. From the experience gained from one of China's earliest established medical big data projects, we outline the challenges encountered during its development and recommend approaches to overcome these challenges to design medical big data projects in China more rationally. Clear and complete top-level design of a project requires to be planned in advance and considered carefully. It is essential to provide a culture of information sharing and to facilitate the opening of data, and changes in ideas and policies need the guidance of the government. The contradiction between data sharing and data security must be handled carefully, that is not to say data openness could be abandoned. The construction of medical big data involves many institutions, and high-level management and cooperation can significantly improve efficiency and promote innovation. Compared with infrastructure construction, it is more challenging and time-consuming to develop appropriate data standards, data integration tools and data mining tools.|https://doi.org/10.1016/j.jbi.2019.103149|https://www.sciencedirect.com/science/article/pii/S153204641930067X||||2019|Experience and reflection from China’s Xiangya medical big data project|Bei Li and Jianbin Li and Yuqiao Jiang and Xiaoyun Lan|article|LI2019103149|||Journal of Biomedical Informatics|15320464||93|||||||||||||||||||||||280540878|568426795
||Big Data;NoSQL databases;Transforms;Scalability;Servers;Tools;Relational databases;NoSQL;big data;data cleansing||69042-69057||In an era of super computing, data is increasing exponentially requiring more proficiency from the available technologies of data storage, data processing, and analysis. Such continuous massive growth of structured and unstructured data is referred to as a “Big data”. The processing and storage of big data through a conventional technique is not possible. Due to improved proficiency of Big Data solution in handling data, such as NoSQL caused the developers in the previous decade to start preferring big data databases, such as Apache Cassandra, Oracle, and NoSQL. NoSQL is a modern database technology that is designed to provide scalability to support voluminous data, leading to the rise of NoSQL as the most viable database solution. These modern databases aim to overcome the limitations of relational databases such as unlimited scalability, high performance, data modeling, data distribution, and continuous availability. These days, the larger enterprises need to shift NoSQL databases due to their more flexible models. It is a great challenge for business organizations and enterprises to transform their existing databases to NoSQL databases considering heterogeneity and complexity in relational data. In addition, with the emergence of big data, data cleansing has become a great challenge. In this paper, we proposed an approach that has two modules: data transformation and data cleansing module. The first phase is the transformation of a relational database to Oracle NoSQL database through model transformation. The second phase provides data cleansing ability to improve data quality and prepare it for big data analytics. The experiments show the proposed approach successfully transforms the relational database to a big data database and improve data quality.|10.1109/ACCESS.2019.2916912|||||2019|Intelligent Data Engineering for Migration to NoSQL Based Secure Environments|Ramzan, Shabana and Bajwa, Imran Sarwar and Ramzan, Bushra and Anwar, Waheed|article|8715359|||IEEE Access|21693536||7|||||21100374601|0,587|Q1|127|18036|24267|771081|116691|24200|4,48|42,75|United States|Northern America|2013-2020|Computer Science (miscellaneous) (Q1); Engineering (miscellaneous) (Q1); Materials Science (miscellaneous) (Q2)|105,968|3.367|0.15396|281393513|1905633267
||Data quality, Information system, Petri net, Optimization model, Process model||113381||The low quality of data in information systems poses enormous risks to business operations and decision making. In this paper, a single-period resource allocation problem for controlling the information system's data quality problem is considered. We develop a Data-Quality-Petri net to capture the process through which data quality problem generates, propagates, and accumulates in the information system. The net considers not only the factors leading to the production of the data quality problem by the data operation nodes and the data flow structure, but also the data transfer ratio of the nodes. Then, we propose a nonlinear programming optimization model with control resource constraints. The result of the model provides an optimal strategy to allocate resources for minimizing the expected data quality problem of an information system. Further, we examine the impact of the data flow structure on optimal resource allocation. The result shows that the optimal resource input level for a data operation node is proportional to its potential for downstream propagation. A warehouse management system of an e-commerce company is utilized to illustrate the model. Our study provides a method for data managers to control the information system's data quality problem by employing a process perspective.|https://doi.org/10.1016/j.dss.2020.113381|https://www.sciencedirect.com/science/article/pii/S0167923620301366||||2020|Minimizing the data quality problem of information systems: A process-based method|Qi Liu and Gengzhong Feng and Xi Zhao and Wenlong Wang|article|LIU2020113381|||Decision Support Systems|01679236||137|||||21933|1,564|Q1|151|115|342|7152|2672|338|7,04|62,19|Netherlands|Western Europe|1985-2020|Arts and Humanities (miscellaneous) (Q1); Developmental and Educational Psychology (Q1); Information Systems (Q1); Information Systems and Management (Q1); Management Information Systems (Q1)|13,580|5.795|0.0081|281538329|1234879127
||Data quality;Soft sensors;Big data||1185-1191|2019 IEEE 17th International Conference on Industrial Informatics (INDIN)|Data preparation for data mining in industrial applications is a key success factor which requires considerable repeated efforts. Although the required activities need to be repeated in very similar fashion across many projects, details of their implementation differ and require both application understanding and experience. As a result, data preparation is done by data mining experts with a strong domain background and a good understanding of the characteristics of the data to be analyzed. Experts with these profiles usually have an engineering background and no strong expertise in distributed programming or big data technology. Unfortunately, the amount of data can be so large that distributed algorithms are required to allow for inspection of results and iteration of preparation steps. This contribution introduces an interactive data preparation workflow for signal data from chemical plants enabling domain experts without background in distributed computing and extensive programming experience to leverage the power of big data technologies.|10.1109/INDIN41052.2019.8972078|||||2019|Data Preparation for Data Mining in Chemical Plants using Big Data|Borrison, Reuben and Kloepper, Benjamin and Mullen, Jennifer|inproceedings|8972078||July||2378363X||1|||||||||||||||||||||||282303148|546782495
https://doi.org/10.1016/j.emj.2022.07.001|https://www.sciencedirect.com/science/article/pii/S0263237322000883||||2022|Artificial intelligence and HRM: HR managers’ perspective on decisiveness and challenges|Aleksandar Radonjić and Henrique Duarte and Nádia Pereira|article|RADONJIC2022|||European Management Journal|02632373||||||||||||||||||||||||||||||||285371413|42
CHI EA '18|Montreal QC, Canada|design, soft law, labeling, privacy, quality standards, responsible innovation, codes of conduct|8|1–8|Extended Abstracts of the 2018 CHI Conference on Human Factors in Computing Systems|With HCI, researchers conduct studies in interdisciplinary projects involving massive volume of data, artificial intelligence and machine learning capabilities. Awareness of the responsibility is emerging as a key concern for the HCI community.This Community will be impacted by the General Data Protection Regulation (GDPR) [5], that will enter into force on the 25th of May 2018. From that date, each data controller and data processor will face an increase of its legal obligations (in particular its accountability) under certain conditions.The GDPR encourages the adoption of Soft Law mechanisms, approved by the national competent authority on data protection, to demonstrate the compliance to the Regulation. Approved Guidelines, Codes of Conducts, Labeling, Marks and Seals dedicated to data protection, as well as certification mechanisms are some of the options proposed by the GDPR.There may be discrepancies between the realities of HCI fieldwork and the formal process of obtaining Soft Law approval by Competent Authorities dedicated to data protection. Given these issues, it is important for researchers to reflect on legal and ethical encounters in HCI research as a community.This workshop will provide a forum for researchers to share experiences about Soft Law they have put in place to increase Trust, Transparency and Accountability among the shareholders. These discussions will be used to develop a white paper of practical Soft Law mechanisms (certification, labeling, marks, seals...) emerging in HCI research with the aim to demonstrate that the GDPR may be an opportunity for the HCI community.|10.1145/3170427.3170632|https://doi.org/10.1145/3170427.3170632|New York, NY, USA|Association for Computing Machinery|9781450356213|2018|The General Data Protection Regulation: An Opportunity for the HCI Community?|Thelisson, Eva and Sharma, Kshitij and Salam, Hanan and Dignum, Virginia|inproceedings|10.1145/3170427.3170632|||||||||||||||||||||||||||||286589758|42
CSAE 2021|Sanya, China|Digital twin, Assembly shop-floor, Global monitoring, 3D visual monitoring|7||The 5th International Conference on Computer Science and Application Engineering|Aiming at the requirements of rapid response and production efficiency improvement in the assembly shop-floor, a three-dimensional (3D) visual and global monitoring method for the assembly shop-floor based on the digital twin is proposed. The paper analyzes the monitoring objectives, objects, and methods of assembly shop-floor, and constructs a global monitoring framework of digital twin-based assembly shop-floor. Then, three key technologies of realizing global monitoring shop-floor are described: current-time data perception and collection, current-time information-driven digital twin generation, and state monitoring and optimization based on digital twin. Finally, a global monitoring prototype system is designed and developed to verify the effectiveness of the proposed method.|10.1145/3487075.3487147|https://doi.org/10.1145/3487075.3487147|New York, NY, USA|Association for Computing Machinery|9781450389853|2021|Digital Twin-Based Three-Dimensional Visual and Global Monitoring of Assembly Shop-Floor|Zhang, Jiapeng and Zhuang, Cunbo and Liu, Jianhua and Yuan, Kun and Zhang, Jin and Liu, Juan|inproceedings|10.1145/3487075.3487147|72||||||||||||||||||||||||||||286753367|42
ICETT 2021|Macau, China|AI Education, Smart Classroom, Oral Assessment, Education Informatization, Adaptive Learning|8|28–35|2021 7th International Conference on Education and Training Technologies|This article first explained the definition of AI in education (AIEd) and reported findings regarding the current development of AIEd industry in the Chinese context. The research design is a context-specific case study using the supply and demand theoretical framework. From a demand-side perspective, the author made an in-depth analysis of the specific AI applications employed in different educational scenarios, including the automated speaking assessment system, the content-based image retrieval system, adaptive learning system, AI-supported classrooms, and AI-assisted campus safety system. For the supply analysis of the AIEd industry, this article summarized key AIEd industry chains and technologies currently widely used in China, obtaining the industry market scale through data collected from different sources. In addition, the iFLYTEK company, as a typical enterprise in the AIEd industry, was taken as a medium to conduct a case analysis. The employment of various AI applications in smart classrooms, smart exams, and smart terminals were comprehensively discussed. In a nutshell, this article discussed the development status and future trends of Chinese AIEd industry, with an aim to offer suggestions and implications for education practitioners.|10.1145/3463531.3463536|https://doi.org/10.1145/3463531.3463536|New York, NY, USA|Association for Computing Machinery|9781450389662|2021|A Study on the Current Development of Artificial Intelligence in Education Industry in China|Wan, Xinxin|inproceedings|10.1145/3463531.3463536|||||||||||||||||||||||||||||290050734|42
ATIP '12|Buona Vista, Singapore||46||Proceedings of the ATIP/A*CRC Workshop on Accelerator Technologies for High-Performance Computing: Does Asia Lead the Way?||||SGP|A*STAR Computational Resource Centre|9781450316446|2012|Accelerate High Throughput Analysis for Genome Sequencing with GPU|Wang, BingQiang and See, Simon|inproceedings|10.5555/2346696.2346710|11||||||||||||||||||||||||||||290674773|42
HotMobile '14|Santa Barbara, California||6||Proceedings of the 15th Workshop on Mobile Computing Systems and Applications|In this paper we explore a scalable data collection methodology that simultaneously achieves low cost and a high degree of control. We use popular online crowdsourcing platforms to recruit 63 subjects for a 90-day data collection that resulted in over 75K hours of data. The total cost of data collection was dramatically lower than for alternative methodologies, with total subject compensation under $3.5K US, and a total of less than 10 hours/week spent by researchers managing the study. At the same time, our methodology enhances control and enables richer study protocols by allowing direct contact with subjects. We were able to conduct surveys, exchange messages, and debug remotely with feedback from subjects. In addition to reporting on study details, we also discuss interesting findings and offer lessons learned.|10.1145/2565585.2565608|https://doi.org/10.1145/2565585.2565608|New York, NY, USA|Association for Computing Machinery|9781450327428|2014|Crowdsourced Mobile Data Collection: Lessons Learned from a New Study Methodology|Welbourne, Evan and Wu, Pang and Bao, Xuan and Munguia-Tapia, Emmanuel|inproceedings|10.1145/2565585.2565608|2||||||||||||||||||||||||||||292328069|42
||||103237|||https://doi.org/10.1016/j.im.2019.103237|https://www.sciencedirect.com/science/article/pii/S0378720619310687||||2020|Big data and business analytics: A research agenda for realizing business value|Patrick Mikalef and Ilias O. Pappas and John Krogstie and Paul A. Pavlou|article|MIKALEF2020103237|||Information & Management|03787206|1|57||Big data and business analytics: A research agenda for realizing business value|||12303|2,147|Q1|162|130|248|11385|2482|246|8,94|87,58|Netherlands|Western Europe|1977-2020|Information Systems (Q1); Information Systems and Management (Q1); Management Information Systems (Q1)||||292471504|1945939487
||Big Data, OLAP, Multidimensional model, Indexes, Partitioning, Cost estimation||336-356||In the recent years the problems of using generic storage (i.e., relational) techniques for very specific applications have been detected and outlined and, as a consequence, some alternatives to Relational DBMSs (e.g., HBase) have bloomed. Most of these alternatives sit on the cloud and benefit from cloud computing, which is nowadays a reality that helps us to save money by eliminating the hardware as well as software fixed costs and just pay per use. On top of this, specific querying frameworks to exploit the brute force in the cloud (e.g., MapReduce) have also been devised. The question arising next tries to clear out if this (rather naive) exploitation of the cloud is an alternative to tuning DBMSs or it still makes sense to consider other options when retrieving data from these settings. In this paper, we study the feasibility of solving OLAP queries with Hadoop (the Apache project implementing MapReduce) while benefiting from secondary indexes and partitioning in HBase. Our main contribution is the comparison of different access plans and the definition of criteria (i.e., cost estimation) to choose among them in terms of consumed resources (namely CPU, bandwidth and I/O).|https://doi.org/10.1016/j.is.2014.09.005|https://www.sciencedirect.com/science/article/pii/S0306437914001458||||2015|Tuning small analytics on Big Data: Data partitioning and secondary indexes in the Hadoop ecosystem|Oscar Romero and Victor Herrero and Alberto Abelló and Jaume Ferrarons|article|ROMERO2015336|||Information Systems|03064379||54|||||12305|0,547|Q2|85|100|271|4739|1027|255|3,39|47,39|United Kingdom|Western Europe|1975-2021|Hardware and Architecture (Q2); Information Systems (Q2); Software (Q2)|2,604|2.309|0.00331|292823758|303930735
Advances in Computers||NoSQL, Big data system, Storage solution, Bivariate analysis, Cluster analysis, Classification||||With the explosion of social media, the Web, Internet of Things, and the proliferation of smart devices, large amounts of data are being generated each day. However, traditional data management technologies are increasingly inadequate to cope with this growth in data. NoSQL has become increasingly popular as this technology can provide consistent, scalable and available solutions for the ever-growing heterogeneous data. Recent years have seen growing applications shifting from traditional data management systems to NoSQL solutions. However, there is limited in-depth literature reporting on NoSQL storage technologies for big graph and their applications in various fields. This chapter fills this gap by conducting a comprehensive study of 80 state-of-the-art NoSQL technologies. In this chapter, we first present a feature analysis of the NoSQL solutions and then generate a data set of the investigated solutions for further analysis in order to better understand and select the technologies. We perform a clustering analysis to segment the NoSQL solutions, compare the classified solutions based on their storage data models and Brewer's CAP theorem, and examine big graph applications in six specific domains. To help users select appropriate NoSQL solutions, we have developed a decision tree model and a web-based user interface to facilitate this process. In addition, the significance, challenges, applications and categories of storage technologies are discussed as well.|https://doi.org/10.1016/bs.adcom.2021.09.006|https://www.sciencedirect.com/science/article/pii/S0065245821000590||Elsevier||2022|Bivariate, cluster, and suitability analysis of NoSQL solutions for big graph applications|Samiya Khan and Xiufeng Liu and Syed Arshad Ali and Mansaf Alam|incollection|KHAN2022||||00652458|||||||23080|1,236|Q1|33|36|79|1113|290|3|4,58|30,92|United States|Northern America|1960-1962, 1964, 1966-1967, 1969-1973, 1975-2000, 2002-2020|Computer Science (miscellaneous) (Q1)|598|2.655|5.2E-4|293458523|1405939739
||data quality, classification, big data, deep learning, data streams, machine learning, label noise||||Class label noise is a critical component of data quality that directly inhibits the predictive performance of machine learning algorithms. While many data-level and algorithm-level methods exist for treating label noise, the challenges associated with big data call for new and improved methods. This survey addresses these concerns by providing an extensive literature review on treating label noise within big data. We begin with an introduction to the class label noise problem and traditional methods for treating label noise. Next, we present 30 methods for treating class label noise in a range of big data contexts, i.e. high volume, high variety, and high velocity problems. The surveyed works include distributed solutions capable of operating on data sets of arbitrary sizes, deep learning techniques for large-scale data sets with limited clean labels, and streaming techniques for detecting class noise in the presence of concept drift. Common trends and best practices are identified in each of these areas, implementation details are reviewed, empirical results are compared across studies when applicable, and references to 17 open-source projects and programming packages are provided. An emphasis on label noise challenges, solutions, and empirical results as they relate to big data distinguishes this work as a unique contribution that will inspire future research and guide machine learning practitioners.|10.1145/3492546|https://doi.org/10.1145/3492546|New York, NY, USA|Association for Computing Machinery||2022|A Survey on Classifying Big Data with Label Noise|Johnson, Justin M and Khoshgoftaar, Taghi M|article|10.1145/3492546||apr|J. Data and Information Quality|19361955||||Just Accepted|||||||||||||||||||||293489000|833754770
||Feature extraction;Data analysis;Data integrity;Data models;Machine learning algorithms;Context modeling;Task analysis;Data analysis;data context;data quality;data consistency;machine learning;word embeddings;approximate dependencies;mutual information;apache hadoop;apache spark||1-6|2019 IEEE/ACS 16th International Conference on Computer Systems and Applications (AICCSA)|Data analysis is a demanding task that involves extracting deep insights hidden in data. Many businesses enforce data analysis irrespective of the domain, as it is crucial in minimizing developmental risks. Raw data cannot be used to perform any analysis as poor-quality data leads to erroneous decision-making. This makes data quality assessment a necessary function before data analysis. Data quality is a multi-dimensional factor that affects the analysis in multiple ways. Among all the dimensions, consistency is one of the most critical dimensions to assess. Context of data plays an important role in consistency assessment, as the records are inherently related within a dataset. Existing studies are computationally expensive and do not consider the context of data. In this paper, we propose a comprehensive context-aware data consistency assessment tool that uses machine learning to evaluate the consistency of data. Our model was developed on Apache Hadoop and Apache Spark to support big data, as well as to boost some computationally intensive algorithms.|10.1109/AICCSA47632.2019.9035250|||||2019|Assessing Context-Aware Data Consistency|Mylavarapu, Goutam and Viswanathan, K. Ashwin and Thomas, Johnson P.|inproceedings|9035250||Nov||21615330|||||||21100198533|0,164|-|18|59|517|1383|308|507|0,62|23,44|United States|Northern America|2001, 2011, 2013, 2019|Computer Networks and Communications; Computer Science Applications; Control and Systems Engineering; Electrical and Electronic Engineering; Hardware and Architecture; Signal Processing||||293497837|894215892
||Digital phenotyping, Big data, Mental health, Data mining, Information fusion||290-307||The landscape of mental health has undergone tremendous changes within the last two decades, but the research on mental health is still at the initial stage with substantial knowledge gaps and the lack of precise diagnosis. Nowadays, big data and artificial intelligence offer new opportunities for the screening and prediction of mental problems. In this review paper, we outline the vision of digital phenotyping of mental health (DPMH) by fusing the enriched data from ubiquitous sensors, social media and healthcare systems, and present a broad overview of DPMH from sensing and computing perspectives. We first conduct a systematical literature review and propose the research framework, which highlights the key aspects related with mental health, and discuss the challenges elicited by the enriched data for digital phenotyping. Next, five key research strands including affect recognition, cognitive analytics, behavioral anomaly detection, social analytics, and biomarker analytics are unfolded in the psychiatric context. Finally, we discuss various open issues and the corresponding solutions to underpin the digital phenotyping of mental health.|https://doi.org/10.1016/j.inffus.2019.04.001|https://www.sciencedirect.com/science/article/pii/S1566253518305244||||2019|A survey on big data-driven digital phenotyping of mental health|Yunji Liang and Xiaolong Zheng and Daniel D. Zeng|article|LIANG2019290|||Information Fusion|15662535||52|||||||||||||||||||||||297160643|1192976563
||Big data, Predictive analytics, Supply chain management||592-598||Big data and predictive analytics (BDPA) tools and methodologies are leveraged by businesses in many ways to improve operational and strategic capabilities, and ultimately, to positively impact corporate financial performance. BDPA has become crucial for managing supply chain functions, where data intensive processes can be vastly improved through its effective use. BDPA has also become a competitive necessity for the management of supply chains, with practitioners and scholars focused almost entirely on how BDPA is used to increase economic measures of performance. There is limited understanding, however, as to how BDPA can impact other aspects of the triple bottom-line, namely environmental and social sustainability outcomes. Indeed, this area is in immediate need of attention from scholars in many fields including industrial engineering, supply chain management, information systems, business analytics, as well as other business and engineering disciplines. The purpose of this article is to motivate such research by proposing an agenda based in well-established theory. This article reviews eight theories that can be used by researchers to examine and clarify the nature of BDPA’s impact on supply chain sustainability, and presents research questions based upon this review. Scholars can leverage this article as the basis for future research activity, and practitioners can use this article as a means to understand how company-wide BDPA initiatives might impact measures of supply chain sustainability.|https://doi.org/10.1016/j.cie.2016.06.030|https://www.sciencedirect.com/science/article/pii/S036083521630225X||||2016|Big data and predictive analytics for supply chain sustainability: A theory-driven research agenda|Benjamin T. Hazen and Joseph B. Skipper and Jeremy D. Ezell and Christopher A. Boone|article|HAZEN2016592|||Computers & Industrial Engineering|03608352||101|||||18164|1,315|Q1|128|641|1567|31833|9597|1557|5,97|49,66|United Kingdom|Western Europe|1976-2020|Computer Science (miscellaneous) (Q1); Engineering (miscellaneous) (Q1)||||300238017|1798521593
WIMS 2020|Biarritz, France|aviation, datasets, services, ontology, queries|12|21–32|Proceedings of the 10th International Conference on Web Intelligence, Mining and Semantics|The management of aviation data is a great challenge in the aviation industry, as they are complex and can be derived from heterogeneous data sources. To handle this challenge, ontologies can be applied to facilitate the modelling of the data across multiple data sources. This paper presents an aviation domain ontology, the ICARUS ontology, which aims at facilitating the semantic description and integration of information resources that represent the various assets of the ICARUS platform and their use. To present the functionality and usability of the proposed ontology, we present the results of querying the ontology using SPARQL queries through three use case scenarios. As shown from the evaluation, the ICARUS ontology enables the integration and reasoning over multiple sources of heterogeneous aviation-related data, the semantic description of metadata produced by ICARUS, and their storage in a knowledge-base which is dynamically updated and provides access to its contents via SPARQL queries.|10.1145/3405962.3405983|https://doi.org/10.1145/3405962.3405983|New York, NY, USA|Association for Computing Machinery|9781450375429|2020|The ICARUS Ontology: A General Aviation Ontology Developed Using a Multi-Layer Approach|Stefanidis, Dimosthenis and Christodoulou, Chrysovalantis and Symeonidis, Moysis and Pallis, George and Dikaiakos, Marios and Pouis, Loukas and Orphanou, Kalia and Lampathaki, Fenareti and Alexandrou, Dimitrios|inproceedings|10.1145/3405962.3405983|||||||||||||||||||||||||||||308565332|42
||Cyber security, semantics of data, privacy, data breach, data aggregation|33|||If the mantra “data is the new oil” of our digital economy is correct, then data leak incidents are the critical disasters in the online society. The initial goal of our research was to present a comprehensive database of data breaches of personal information that took place in 2018 and 2019. This information was to be drawn from press reports, industry studies, and reports from regulatory agencies across the world. This article identified the top 430 largest data breach incidents among more than 10,000 data breach incidents.In the process, we encountered many complications, especially regarding the lack of standardization of reporting. This article should be especially interesting to the readers of JDIQ because it describes both the range of data quality and consistency issues found as well as what was learned from the database created.The database that was created, available at https://www.databreachdb.com, shows that the number of data records breached in those top 430 incidents increased from around 4B in 2018 to more than 22B in 2019. This increase occurred despite the strong efforts from regulatory agencies across the world to enforce strict rules on data protection and privacy, such as the General Data Protection Regulation (GDPR) that went into effect in Europe in May 2018. Such regulatory effort could explain the reason why there is such a large number of data breach cases reported in the European Union when compared to the U.S. (more than 10,000 data breaches publicly reported in the U.S. since 2018, while the EU reported more than 160,0001 data breaches since May 2018). However, we still face the problem of an excessive number of breach incidents around the world.This research helps to understand the challenges of proper visibility of such incidents on a global scale. The results of this research can help government entities, regulatory bodies, security and data quality researchers, companies, and managers to improve the data quality of data breach reporting and increase the visibility of the data breach landscape around the world in the future.|10.1145/3439873|https://doi.org/10.1145/3439873|New York, NY, USA|Association for Computing Machinery||2021|Developing a Global Data Breach Database and the Challenges Encountered|Neto, Nelson Novaes and Madnick, Stuart and Paula, Anchises Moraes G. De and Borges, Natasha Malara|article|10.1145/3439873|3|jan|J. Data and Information Quality|19361955|1|13|March 2021||||||||||||||||||||||310136060|833754770
||Rough sets;Data analysis;Approximation algorithms;Big Data;Data integrity;Tools;rough set;incomplete data;missing values;approximations;matrix||206-211|2018 19th IEEE/ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD)|The veracity related with data quality such as incomplete, imprecise and inconsistent data creates a major challenge to data mining and data analysis. Rough set theory provides a special tool for handling the imprecise and incomplete data in information systems. However, the existing rough set based incomplete data analysis methods may not be able to handle large amount of data within the acceptable time. This paper focuses on speeding up the incomplete data analysis. The computation of the lower and upper approximations is a vital step for improving the performance of rough set based data analysis process. In this paper, the lower and upper approximations are characterized as matrix-represented approximations. The resulting approximations are exploited as inputs for data analysis method LERS (Learning from Examples based on Rough Set) used with LEM2 (Learning from Examples Module, Version2) rule induction algorithm. Then, this paper provides a set of experiments on missing datasets with different missing percent. The experimental results on incomplete or missing datasets from UCI Machine Learning Repository show that the proposed system effectively reduces the computational time in comparison with the existing system.|10.1109/SNPD.2018.8441160|||||2018|Speeding up Incomplete Data Analysis using Matrix-Represented Approximations|Soe, Thin Thin and Min, Myat Myat|inproceedings|8441160||June|||||||||||||||||||||||||||313528989|42
||||19-21|||https://doi.org/10.1016/j.cjca.2019.09.018|https://www.sciencedirect.com/science/article/pii/S0828282X19312826||||2020|The Role of Physicians in the Era of Big Data|George A. Heckman and John P. Hirdes and Robert S. McKelvie|article|HECKMAN202019|||Canadian Journal of Cardiology|0828282X|1|36|||||||||||||||||||||||317044245|639451577
||Big data, Social media, Machine learning, Analytics||417-428||Big data analytics has recently emerged as an important research area due to the popularity of the Internet and the advent of the Web 2.0 technologies. Moreover, the proliferation and adoption of social media applications have provided extensive opportunities and challenges for researchers and practitioners. The massive amount of data generated by users using social media platforms is the result of the integration of their background details and daily activities. This enormous volume of generated data known as “big data” has been intensively researched recently. A review of the recent works is presented to obtain a broad perspective of the social media big data analytics research topic. We classify the literature based on important aspects. This study also compares possible big data analytics techniques and their quality attributes. Moreover, we provide a discussion on the applications of social media big data analytics by highlighting the state-of-the-art techniques, methods, and the quality attributes of various studies. Open research challenges in big data analytics are described as well.|https://doi.org/10.1016/j.chb.2018.08.039|https://www.sciencedirect.com/science/article/pii/S074756321830414X||||2019|Social media big data analytics: A survey|Norjihan Abdul Ghani and Suraya Hamid and Ibrahim Abaker {Targio Hashem} and Ejaz Ahmed|article|GHANI2019417|||Computers in Human Behavior|07475632||101|||||19419|2,108|Q1|178|385|1597|26867|14501|1573|7,83|69,78|United Kingdom|Western Europe|1985-2021|Arts and Humanities (miscellaneous) (Q1); Human-Computer Interaction (Q1); Psychology (miscellaneous) (Q1)|45,035|6.829|0.05973|317389233|265090421
||big data, data processing, Data stream, public transport|30|||In this work, we describe an urban Internet of Things (IoT) architecture, grounded in big data patterns and focused on the needs of cities and their key stakeholders. First, the architecture of the dedicated platform USE4IoT (Urban Service Environment for the Internet of Things), which gathers and processes urban big data and extends the Lambda architecture, is proposed. We describe how the platform was used to make IoT an enabling technology for intelligent transport planning. Moreover, key data processing components vital to provide high-quality IoT data streams in a near-real-time manner are defined. Furthermore, tests showing how the IoT platform described in this study provides a low-latency analytical environment for smart cities are included.|10.1145/3396850|https://doi.org/10.1145/3396850|New York, NY, USA|Association for Computing Machinery||2020|IoT Architecture for Urban Data-Centric Services and Applications|Luckner, Marcin and Grzenda, Maciej and Kunicki, Robert and Legierski, Jaroslaw|article|10.1145/3396850|29|jul|ACM Trans. Internet Technol.|15335399|3|20|August 2020||||||||||||||||||||||317462386|314651938
||Data integrity;Machine learning;Pipelines;Cleaning;Libraries;Buildings;Data quality;machine learning;data cleaning;scalability;automation;data science||2913-2922|2019 IEEE International Conference on Big Data (Big Data)|Fueled with growth in the fields of Internet of Things (IoT) and Big Data, data has become one of the most valuable assets in today's world. While we are leveraging this data for analyzing complex systems using machine learning and deep learning, a considerable amount of time and effort is spent on addressing data quality issues. If undetected, data quality issues can cause large deviations in the analysis, misleading data scientists. To ease the effort of identifying and addressing data quality challenges, we introduce DQA, a scalable, automated and interactive data quality advisor. In this paper, we describe the DQA framework, provide detailed description of its components and the benefits of integrating it in a data science process. We propose a programmatic approach for implementing the data quality framework which automatically generates dynamic executable graphs for performing data validations fine-tuned for a given dataset. We discuss the use of DQA to build a library of validation checks common to many applications. We provide insight into how DQA addresses many persistence and usability issues which currently make data cleaning a laborious task for data scientists. Finally, we provide a case study of how DQA is implemented in a realworld system and describe the benefits realized.|10.1109/BigData47090.2019.9006187|||||2019|DQA: Scalable, Automated and Interactive Data Quality Advisor|Shrivastava, Shrey and Patel, Dhaval and Bhamidipaty, Anuradha and Gifford, Wesley M. and Siegel, Stuart A. and Ganapavarapu, Venkata Sitaramagiridharganesh and Kalagnanam, Jayant R.|inproceedings|9006187||Dec|||||||||||||||||||||||||||318778613|42
CHIIR '18|New Brunswick, NJ, USA|repository, intertactive information retrieval, evaluation|4|382–385|Proceedings of the 2018 Conference on Human Information Interaction &amp; Retrieval|The goal of this workshop is to serve as a starting point for a community-driven effort to design and implement a platform for the collection, organization, maintenance, and sharing of resources for IIR experimentation. As in all scientific endeavors, progress in IIR research is contingent on the ability to build on previous ideas, approaches, and resources. However, we believe there to be a number of barriers to reproducibility and re-use of resources in IIR research: the fragmentary nature of how the community»s resources are organized, the lack of awareness of their existence, documentation and organization of the resources, the nature of the typical research publication cycle, and the effort required to make such resources available. We believe that an online platform dedicated to the collection and organization of IIR resources could be a promising way of overcoming these barriers. The workshop therefore aims to serve both as a brainstorming opportunity about the shape this iRepository should take, as well as a way of building support in the community for its implementation.|10.1145/3176349.3176901|https://doi.org/10.1145/3176349.3176901|New York, NY, USA|Association for Computing Machinery|9781450349253|2018|Workshop on Barriers to Interactive IR Resources Re-Use|"\"Bogers, Toine and G\"\"{a}de, Maria and Freund, Luanne and Hall, Mark and Koolen, Marijn and Petras, Vivien and Skov, Mette\""|inproceedings|10.1145/3176349.3176901|||||||||||||||||||||||||||||319944727|42
||Metadata;Social networking (online);Databases;Search engines;Interoperability;Internet;Libraries;Scientific Data;Metadata;Elasticsearch;Social Network Analysis||208-215|2020 8th International Conference in Software Engineering Research and Innovation (CONISOFT)|There are a number of challenges associated to metadata in its different applications including data quality, data acquisition, computing resources, interoperability, and discoverability. This work presents an approach to structure metadata of scientific information for social network analysis based on an academic case study from scientific articles published by universities, to evaluate the area of risk assessment. Studying metadata for scientists' social networks helps identify authors' relevance based on their position within the network. By using Elasticsearch (ES) and Python technologies, this work addresses big data analysis issues related to data structure and volume, given ES full-text search engine capabilities for indexing and searching data, and Python's processing support. The data is obtained from the ArnetMiner (Aminer) open scientific database providing a fresh overview of scientific records up to January 2019. From a sample of 64,070 publications, a total of 45, 000 relations are graphed in a co-authorship network. Through the computation of network centrality measures, this work identifies central-positioned authors, clusters of research, and their affiliations. The results show that degree centrality is an important measure to identify prominent scientists in this co-authorship network, and closeness and betweenness centralities together are dominant measures to pinpoint the key players in the flow of information within the network. We conclude that the application of this approach allows rapid full-text search, visualizing dense co-authorship networks, and identifying central authors through centrality metrics. The results presented in this work can help researchers or research groups identify key research collaborators, multi-disciplinary areas, and international stakeholders.|10.1109/CONISOFT50191.2020.00038|||||2020|A Metadata Application Profile to Structure a Scientific Database for Social Network Analysis (SNA)|López-Acosta, Araceli and García-Hernández, Alejandra and Vázquez-Reyes, Sodel and Mauricio-González, Alejandro|inproceedings|9307781||Nov|||||||||||||||||||||||||||324767088|42
||GEOSS, Big Data, Multidisciplinary systems, Earth System Science, Research infrastructures, Interoperability, Cloud systems||1-26||There are many expectations and concerns about Big Data in the sector of Earth Observation. It is necessary to understand whether Big Data is a radical shift or an incremental change for the existing digital infrastructures. This manuscript explores the impact of Big Data dimensionalities (commonly known as ‘V’ axes: volume, variety, velocity, veracity, visualization) on the Global Earth Observation System of Systems (GEOSS) and particularly its common digital infrastructure (i.e. the GEOSS Common Infrastructure). GEOSS is a global and flexible network of content providers allowing decision makers to access an extraordinary range of data and information. GEOSS is a pioneering framework for global and multidisciplinary data sharing in the EO realm. The manuscript introduces and discusses the general GEOSS strategies to address Big Data challenges, focusing on the cloud-based discovery and access solutions. A final section reports the results of the scalability and flexibility performance tests.|https://doi.org/10.1016/j.envsoft.2015.01.017|https://www.sciencedirect.com/science/article/pii/S1364815215000481||||2015|Big Data challenges in building the Global Earth Observation System of Systems|Stefano Nativi and Paolo Mazzetti and Mattia Santoro and Fabrizio Papeschi and Max Craglia and Osamu Ochiai|article|NATIVI20151|||Environmental Modelling & Software|13648152||68|||||23295|1,828|Q1|136|223|717|14218|4373|711|5,47|63,76|Netherlands|Western Europe|1997-2020|Ecological Modeling (Q1); Environmental Engineering (Q1); Software (Q1)||||326295450|665084965
ITCC 2021|Guangzhou, China||7|73–79|2021 3rd International Conference on Information Technology and Computer Communications|Buildings in remote sensing images have large scale differences and complex shapes. And there are often distractors with visual features similar to buildings in complex scenes. The traditional methods used to extract buildings are limited by the ability of feature representation, resulting in low accuracy and low universality. The semantic segmentation network based on the Encoder-Decoder structure can automatically learn multi-level building feature representation from the data set, and achieve end-to-end building extraction. UNet is a typical semantic segmentation Encoder-Decoder network, but UNet cannot explore enough building information. Small buildings are easy to be missed, large buildings with complex colors and shapes are incompletely extracted, boundary segmentation is inaccurate. And the network is easily affected by roads, trees, shadows and other distractors. Therefore, this article improves UNet and proposes a multi-scale Encoder-Decoder network to learn multi-scale and distinguishable features to better identify buildings and backgrounds. We experiment with the improved network and the classic U-Net on two data sets, and show that the multi-scale Encoder-Decoder network can effectively improve the accuracy of building extraction.|10.1145/3473465.3473478|https://doi.org/10.1145/3473465.3473478|New York, NY, USA|Association for Computing Machinery|9781450389884|2021|Multi Scale UNet Encoder-Decoder Network for Building Extraction|Sun, Xiyan and Xiao, Yu and Ji, Yuanfa and Huang, Jianhua and Bai, Yang|inproceedings|10.1145/3473465.3473478|||||||||||||||||||||||||||||330149050|42
SAC '22|Virtual Event|federated data network, electronic health records, deep learning, comorbidity, lab measurements|9|627–635|Proceedings of the 37th ACM/SIGAPP Symposium on Applied Computing|Vast quantities of electronic patient medical data are currently being collated and processed in large federated data repositories. For instance, TriNetX, Inc., a global health research network, has access to more than 300 million patients, sourced from healthcare organizations, biopharmaceutical companies, and contract research organizations. As such, pipelines that are able to algorithmically extract huge quantities of patient data from multiple modalities present opportunities to leverage machine learning and deep learning approaches with the possibility of generating actionable insight. In this work, we present a modular, semi-automated end-to-end machine and deep learning pipeline designed to interface with a federated network of structured patient data. This proof-of-concept pipeline is disease-agnostic, scalable, and requires little domain expertise and manual feature engineering in order to quickly produce results for the case of a user-defined binary outcome event. We demonstrate the pipeline's efficacy with three different disease workflows, with high discriminatory power achieved in all cases.|10.1145/3477314.3507007|https://doi.org/10.1145/3477314.3507007|New York, NY, USA|Association for Computing Machinery|9781450387132|2022|Configuring a Federated Network of Real-World Patient Health Data for Multimodal Deep Learning Prediction of Health Outcomes|Haudenschild, Christian and Vaickus, Louis and Levy, Joshua|inproceedings|10.1145/3477314.3507007|||||||||||||||||||||||||||||330563510|42
ICIST '20|Lecce, Italy|Industrial data space, Industry 4.0, Predictive maintenance, Collaborative business process, Blockchain, FIWARE|11||Proceedings of the 10th International Conference on Information Systems and Technologies|In the context of Industry 4.0, the manufacturing related processes have shifted from conventional processes within one organization to collaborative processes cross different organizations, for example, product design processes, manufacturing processes, and maintenance processes across different factories and enterprises. The application of Internet of things, i.e. smart devices and sensors increases collection and availability of diverse data. Advanced technologies such as big data analytics and cloud computing offer new opportunities for effective optimization of manufacturing related processes, e.g. predictive maintenance. Predictive maintenance provides a detailed examination of the detection, location and diagnosis of faults in related machineries using various analyses. RAMI4.0 is a framework for thinking about the various efforts that constitute Industry 4.0. It spans the entire product life cycle &amp; value stream axis, hierarchical structure axis and functional classification axis. The Industrial Data Space (now International Data Space) is a virtual data space using standards and common governance models to facilitate the secure exchange and easy linkage of data in business ecosystems. It thereby provides a basis for creating and using smart services and innovative business processes, while at the same time ensuring digital sovereignty of data owners. This paper looks at how to support predictive maintenance in the context of Industry 4.0? Especially, applying RAMI 4.0 architecture supports the predictive maintenance using FIWARE framework, which leads to deal with data exchanging among different organizations with different security requirements as well as modularizing of related functions.|10.1145/3447568.3448537|https://doi.org/10.1145/3447568.3448537|New York, NY, USA|Association for Computing Machinery|9781450376556|2021|Predictive Maintenance in Industry 4.0|Sang, Go Muan and Xu, Lai and de Vrieze, Paul and Bai, Yuewei and Pan, Fangyu|inproceedings|10.1145/3447568.3448537|29||||||||||||||||||||||||||||331192468|42
||Power systems;Cleaning;Clustering algorithms;Big Data;Sparks;Data acquisition;Algorithm design and analysis;electric power senser data;data cleaning;k-means clustering;outlier;Spark||430-435|2017 14th International Symposium on Pervasive Systems, Algorithms and Networks & 2017 11th International Conference on Frontier of Computer Science and Technology & 2017 Third International Symposium of Creative Computing (ISPAN-FCST-ISCC)|With the development of Smart Grid Technology, more and more electric power sensor data are utilized in various electric power systems. To guarantee the effectiveness of such systems, it is necessary to ensure the quality of electric power sensor data, especially when the scale of electric power sensor data is large. In the field of large-scale electric power sensor data cleaning, the computational efficiency and accuracy of data cleaning are two vital requirements. In order to satisfy these requirements, this paper presents an electric power sensor data oriented data cleaning solution, which is composed of a data cleaning framework and a data cleaning method. Based on Hadoop, the given framework is able to support large-scale electric power sensor data acquisition, storage and processing. Meanwhile, the proposed method which achieves outlier detection and reparation is implemented on the basis of a time-relevant k-means clustering algorithm in Spark. The feasibility and effectiveness of the proposed method is evaluated on a data set which originates from charging piles. Experimental results show that the proposed data cleaning method is able to improve the data quality of electric power sensor data by finding and repairing most outliers. For large-scale electric power sensor data, the proposed data cleaning method has high parallel performance and strong scalability.|10.1109/ISPAN-FCST-ISCC.2017.29|||||2017|An Electric Power Sensor Data Oriented Data Cleaning Solution|Liu, He and Chen, Jiangqi and Huang, Fupeng and Li, Han|inproceedings|8121809||June||2375527X|||||||||||||||||||||||||331377225|675400532
||Maintenance engineering;Data mining;Cleaning;Measurement;Time series analysis;Context;Information management;Data glitches;Big Data;missing values;outliers;network analysis;Earth Mover Distance||226-233|2012 IEEE 12th International Conference on Data Mining Workshops|Data quality issues have special implications in network data. Data glitches are propagated rapidly along pathways dictated by the hierarchy and topology of the network. In this paper, we use temporal data from a vast data network to study data glitches and their effect on network monitoring tasks such as anomaly detection. We demonstrate the consequences of cleaning the data, and develop targeted and customized cleaning strategies by exploiting the network hierarchy.|10.1109/ICDMW.2012.125|||||2012|Effect of Data Repair on Mining Network Streams|Loh, Ji Meng and Dasu, Tamraparni|inproceedings|6406445||Dec||23759259|||||||||||||||||||||||||334152633|1446126935
||Internet of things, IoT, Big data, Data quality, Outliers Detection, DBSCAN, RDDs||131-138||Internet of Things (IoT) is a fundamental concept of a new technology that will be promising and significant in various fields. IoT is a vision that allows things or objects equipped with sensors, actuators, and processors to talk and communicate with each other over the internet to achieve a meaningful goal. Unfortunately, one of the major challenges that affect IoT is data quality and uncertainty, as data volume increases noise, inconsistency and redundancy increases within data and causes paramount issues for IoT technologies. And since IoT is considered to be a massive quantity of heterogeneous networked embedded devices that generate big data, then it is very complex to compute and analyze such massive data. So this paper introduces a new model named NRDD-DBSCAN based on DBSCAN algorithm and using resilient distributed datasets (RDDs) to detect outliers that affect the data quality of IoT technologies. NRDD-DBSCAN has been applied on three different datasets of N-dimensions (2-D, 3-D, and 25-D) and the results were promising. Finally, comparisons have been made between NRDD-DBSCAN and previous models such as RDD-DBSCAN model and DBSCAN algorithm, and these comparisons proved that NRDD-DBSCAN solved the low dimensionality issue of RDD-DBSCAN model and also solved the fact that DBSCAN algorithm cannot handle IoT data. So the conclusion is that NRDD-DBSCAN proposed model can detect the outliers that exist in the datasets of N-dimensions by using resilient distributed datasets (RDDs), and NRDD-DBSCAN can enhance the quality of data exists in IoT applications and technologies.|https://doi.org/10.1016/j.eij.2019.12.001|https://www.sciencedirect.com/science/article/pii/S1110866519301616||||2020|Detection outliers on internet of things using big data technology|Haitham Ghallab and Hanan Fahmy and Mona Nasr|article|GHALLAB2020131|||Egyptian Informatics Journal|11108665|3|21|||||19700182731|0,728|Q1|34|47|59|1655|415|59|6,94|35,21|Egypt|Africa/Middle East|2010-2020|Information Systems (Q1); Computer Science Applications (Q2); Management Science and Operations Research (Q2)|820|3.943|9.3E-4|336162223|2097874567
CHI EA '16|San Jose, California, USA|analytics, mythology, values, business intelligence|7|2341–2347|Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems|"\"We present results from a case study of the use of business intelligence (BI) systems in a human services organization. In their organizational trajectory towards a \"\"culture of data,\"\" our informants perceived four values associated with BI: data-driven, predictive and proactive, shared accountability, and inquisitive. Each value corresponds to a mythology of big data and BI. For each, we highlight the ways in which the enactment of the mythology is problematized by disconnects between aggregate and drill-down views of data that often impede the desired actionability. Our findings contribute initial empirical evidence of the ways in which the epistemological biases of BI systems influence organizations. We suggest design implications for better enabling data-driven decision making.\""|10.1145/2851581.2892379|https://doi.org/10.1145/2851581.2892379|New York, NY, USA|Association for Computing Machinery|9781450340823|2016|Mythologies of Business Intelligence|Verma, Nitya and Voida, Amy|inproceedings|10.1145/2851581.2892379|||||||||||||||||||||||||||||337298217|42
|||21|566–586||Analysis of RNA-sequence (RNA-seq) data is widely used in transcriptomic studies and it has many applications. We review RNA-seq data analysis from RNA-seq reads to the results of differential expression analysis. In addition, we perform a descriptive comparison of tools used in each step of RNA-seq data analysis along with a discussion of important characteristics of these tools. A taxonomy of tools is also provided. A discussion of issues in quality control and visualization of RNA-seq data is also included along with useful tools. Finally, we provide some guidelines for the RNA-seq data analyst, along with research issues and challenges which should be addressed.|10.1109/TCBB.2018.2873010|https://doi.org/10.1109/TCBB.2018.2873010|Washington, DC, USA|IEEE Computer Society Press||2020|Differential Expression Analysis of RNA-Seq Reads: Overview, Taxonomy, and Tools|Chowdhury, Hussain Ahmed and Bhattacharyya, Dhruba Kumar and Kalita, Jugal Kumar|article|10.1109/TCBB.2018.2873010||apr|IEEE/ACM Trans. Comput. Biol. Bioinformatics|15455963|2|17|March-April 2020||||||||||||||||||||||339256399|1878427007
||Autoimmune diseases, Digital technology, Big data, Delivery of health care, Telemedicine||102864||The past decade has seen tremendous development in digital health, including in innovative new technologies such as Electronic Health Records, telemedicine, virtual visits, wearable technology and sophisticated analytical tools such as artificial intelligence (AI) and machine learning for the deep-integration of big data. In the field of rare connective tissue diseases (rCTDs), these opportunities include increased access to scarce and remote expertise, improved patient monitoring, increased participation and therapeutic adherence, better patient outcomes and patient empowerment. In this review, we discuss opportunities and key-barriers to improve application of digital health technologies in the field of autoimmune diseases. We also describe what could be the fully digital pathway of rCTD patients. Smart technologies can be used to provide real-world evidence about the natural history of rCTDs, to determine real-life drug utilization, advanced efficacy and safety data for rare diseases and highlight significant unmet needs. Yet, digitalization remains one of the most challenging issues faced by rCTD patients, their physicians and healthcare systems. Digital health technologies offer enormous potential to improve autoimmune rCTD care but this potential has so far been largely unrealized due to those significant obstacles. The need for robust assessments of the efficacy, affordability and scalability of AI in the context of digital health is crucial to improve the care of patients with rare autoimmune diseases.|https://doi.org/10.1016/j.autrev.2021.102864|https://www.sciencedirect.com/science/article/pii/S1568997221001361||||2021|Digital health, big data and smart technologies for the care of patients with systemic autoimmune diseases: Where do we stand?|Hugo Bergier and Loïc Duron and Christelle Sordet and Lou Kawka and Aurélien Schlencker and François Chasset and Laurent Arnaud|article|BERGIER2021102864|||Autoimmunity Reviews|15689972|8|20|||||20689|2,621|Q1|122|177|465|12128|3509|408|7,66|68,52|Netherlands|Western Europe|2002-2020|Immunology (Q1); Immunology and Allergy (Q1)|13,493|9.754|0.01481|342492205|263467543
||||1652-1654|||https://doi.org/10.1016/j.scib.2019.09.011|https://www.sciencedirect.com/science/article/pii/S2095927319305523||||2019|Opportunities and challenges of using big data for global health|Peng Jia and Hong Xue and Shiyong Liu and Hao Wang and Lijian Yang and Therese Hesketh and Lu Ma and Hongwei Cai and Xin Liu and Yaogang Wang and Youfa Wang|article|JIA20191652|||Science Bulletin|20959273|22|64|||||21100405003|1,983|Q1|112|365|833|12806|5639|616|7,21|35,08|Netherlands|Western Europe|2015-2020|Multidisciplinary (Q1)|8,832|11.780|0.0164|343855102|895863808
COINS '19|Crete, Greece|data generation, data characteristics, data storage, Big Data|6|19–24|Proceedings of the International Conference on Omni-Layer Intelligent Systems|Currently Big Data is the biggest buzzword, and definitely, we believe that Big Data is changing the world. Some researchers say Big Data will be even bigger buzzword than the Internet. With fast-growing computing resources, information and knowledge a new digital globe has emerged. Information is being created and stored at a fast rate and is being accessed by a vast range of applications through scientific computing, commercial workloads, and social media. In 2018, over 28 billion devices globally, are connected to the internet. In 2020, more than 50 billion smart appliances will be connected worldwide and internet traffic flow will be 92 times greater than it was in 2005. The usage of such a massive number of connected devices not only increase the data volume but also the velocity of data addition with speed of light on fiber optic and various wireless networks. This fast generation of enormous data creates numerous threats and challenges. There exist various approaches that are addressing issues and challenges of Big Data with the theory of Vs such as 3 V's, 5 V's, 7 V's etc. The objective of this work is to explore and investigate the status of the current Big Data domain. Further, a comprehensive overview of Big Data, its characteristics, opportunities, issues, and challenges have been explored and described with the help of 51 V's. The outcome of this research will help in understanding the Big Data in a systematic way.|10.1145/3312614.3312623|https://doi.org/10.1145/3312614.3312623|New York, NY, USA|Association for Computing Machinery|9781450366403|2019|The 51 V's Of Big Data: Survey, Technologies, Characteristics, Opportunities, Issues and Challenges|Khan, Nawsher and Naim, Arshi and Hussain, Mohammad Rashid and Naveed, Quadri Noorulhasan and Ahmad, Naim and Qamar, Shamimul|inproceedings|10.1145/3312614.3312623|||||||||||||||||||||||||||||344202722|42
||electric vehicle, lithium-ion power battery, modeling, battery management, bigdata, deeplearning||168-173||Battery is the bottleneck technology of electric vehicles. The complex chemical reactions inside the battery are difficult to monitor directly. The establishment of a precise mathematical model for the battery is of great significance in ensuring the secure and stable operation of the battery management system. First of all, a data cleaning method based on machine learning is put forward, which is applicable to the characteristics of big data from batteries in electric vehicles. Secondly, this paper establishes a lithium-ion battery model based on deep learning algorithm and the error of model based on different algorithms is compared. The data of electric buses are used for validating the effectiveness of the model. The result shows that the data cleaning method achieves good results, in the case of the terminal voltage missing, the mean absolute percentage error of filling is within 4%, and the battery modeling method in this paper is able to simulate the battery characteristics accurately, and the mean absolute percentage error of the terminal voltage estimation is within 2.5%.|https://doi.org/10.1016/j.egypro.2018.12.046|https://www.sciencedirect.com/science/article/pii/S1876610218313419||||2019|Lithium-ion battery modeling based on Big Data|Shuangqi Li and Jianwei Li and Hongwen He and Hanxiao Wang|article|LI2019168|||Energy Procedia|18766102||159||Renewable Energy Integration with Mini/Microgrid|||17700156736|0,474|-|81|0|7789|0|15629|7685|1,89|0,00|United Kingdom|Western Europe|2009-2019|Energy (miscellaneous)||||344975680|1992938878
||software reference architecture, quality assurance, Big Data, requirements engineering, Big Data systems, software engineering|39|||Big Data Systems (BDSs) are an emerging class of scalable software technologies whereby massive amounts of heterogeneous data are gathered from multiple sources, managed, analyzed (in batch, stream or hybrid fashion), and served to end-users and external applications. Such systems pose specific challenges in all phases of software development lifecycle and might become very complex by evolving data, technologies, and target value over time. Consequently, many organizations and enterprises have found it difficult to adopt BDSs. In this article, we provide insight into three major activities of software engineering in the context of BDSs as well as the choices made to tackle them regarding state-of-the-art research and industry efforts. These activities include the engineering of requirements, designing and constructing software to meet the specified requirements, and software/data quality assurance. We also disclose some open challenges of developing effective BDSs, which need attention from both researchers and practitioners.|10.1145/3408314|https://doi.org/10.1145/3408314|New York, NY, USA|Association for Computing Machinery||2020|Big Data Systems: A Software Engineering Perspective|Davoudian, Ali and Liu, Mengchi|article|10.1145/3408314|110|sep|ACM Comput. Surv.|03600300|5|53|September 2021||||23038|2,079|Q1|163|112|365|15859|6978|365|19,16|141,60|United States|Northern America|1969-2020|Computer Science (miscellaneous) (Q1); Theoretical Computer Science (Q1)|10,790|10.282|0.01438|346198155|1517405264
||data-mined, interpretability, Soft-margin SVM, knowledge, black-box||||The lack of interpretability often makes black-box models challenging to be applied in many practical domains. For this reason, the current work, from the black-box model input port, proposes to incorporate data-mined knowledge into the black-box soft-margin SVM model to enhance accuracy and interpretability. The concept and incorporation mechanism of data-mined knowledge are successively developed, based on which a partially interpretable soft-margin SVM (pTsm-SVM) optimization model is designed and then solved through reformulating the optimization problem as standard quadratic programming. An algorithm for mining linear positive (negative) class knowledge from general data sets is also proposed, which generates a linear two-dimensional discriminative rule with specificity (sensitivity) equal to 1 and the highest possible sensitivity (specificity) among all two-dimensional feature spaces. The knowledge-integrated pTsm-SVM works by achieving a good trade-off among the “large margin”, “high specificity”, and “high sensitivity”. Our experimental results on eight UCI datasets demonstrate the superiority of the proposed pTsm-SVM over the standard soft-margin SVM both in terms of accuracy and interpretability.|10.1145/3548775|https://doi.org/10.1145/3548775|New York, NY, USA|Association for Computing Machinery||2022|Incorporation of Data-Mined Knowledge into Black-Box SVM for Interpretability|Chen, Shaohan and Gao, Chuanhou and Zhang, Ping|article|10.1145/3548775||jul|ACM Trans. Intell. Syst. Technol.|21576904||||Just Accepted|||||||||||||||||||||347578486|273436860
|||9|61–69|||10.1145/2070736.2070750|https://doi.org/10.1145/2070736.2070750|New York, NY, USA|Association for Computing Machinery||2011|A Call to Arms: Revisiting Database Design|Badia, Antonio and Lemire, Daniel|article|10.1145/2070736.2070750||nov|SIGMOD Rec.|01635808|3|40|September 2011||||13622|0,372|Q2|142|39|81|808|127|57|1,67|20,72|United States|Northern America|1969, 1973-1978, 1981-2020|Information Systems (Q2); Software (Q2)|1,819|0.775|7.5E-4|349999902|962972343
||Big data, FastText, Recurrent neural networks, LSTM, BiLSTM, GRU, Natural language processing, Sentiment analysis, Social big data analytics||102122||Big data generated by social media stands for a valuable source of information, which offers an excellent opportunity to mine valuable insights. Particularly, User-generated contents such as reviews, recommendations, and users’ behavior data are useful for supporting several marketing activities of many companies. Knowing what users are saying about the products they bought or the services they used through reviews in social media represents a key factor for making decisions. Sentiment analysis is one of the fundamental tasks in Natural Language Processing. Although deep learning for sentiment analysis has achieved great success and allowed several firms to analyze and extract relevant information from their textual data, but as the volume of data grows, a model that runs in a traditional environment cannot be effective, which implies the importance of efficient distributed deep learning models for social Big Data analytics. Besides, it is known that social media analysis is a complex process, which involves a set of complex tasks. Therefore, it is important to address the challenges and issues of social big data analytics and enhance the performance of deep learning techniques in terms of classification accuracy to obtain better decisions. In this paper, we propose an approach for sentiment analysis, which is devoted to adopting fastText with Recurrent neural network variants to represent textual data efficiently. Then, it employs the new representations to perform the classification task. Its main objective is to enhance the performance of well-known Recurrent Neural Network (RNN) variants in terms of classification accuracy and handle large scale data. In addition, we propose a distributed intelligent system for real-time social big data analytics. It is designed to ingest, store, process, index, and visualize the huge amount of information in real-time. The proposed system adopts distributed machine learning with our proposed method for enhancing decision-making processes. Extensive experiments conducted on two benchmark data sets demonstrate that our proposal for sentiment analysis outperforms well-known distributed recurrent neural network variants (i.e., Long Short-Term Memory (LSTM), Bidirectional Long Short-Term Memory (BiLSTM), and Gated Recurrent Unit (GRU)). Specifically, we tested the efficiency of our approach using the three different deep learning models. The results show that our proposed approach is able to enhance the performance of the three models. The current work can provide several benefits for researchers and practitioners who want to collect, handle, analyze and visualize several sources of information in real-time. Also, it can contribute to a better understanding of public opinion and user behaviors using our proposed system with the improved variants of the most powerful distributed deep learning and machine learning algorithms. Furthermore, it is able to increase the classification accuracy of several existing works based on RNN models for sentiment analysis.|https://doi.org/10.1016/j.ipm.2019.102122|https://www.sciencedirect.com/science/article/pii/S0306457319305163||||2020|Towards a real-time processing framework based on improved distributed recurrent neural network variants with fastText for social big data analytics|Badr {Ait Hammou} and Ayoub {Ait Lahcen} and Salma Mouline|article|AITHAMMOU2020102122|||Information Processing & Management|03064573|1|57|||||||||||||||||||||||350132427|1769516999
ICEGOV '17|New Delhi AA, India|Forest Rights, Women Land Rights, India, Big Data, SDGs|9|127–135|Proceedings of the 10th International Conference on Theory and Practice of Electronic Governance|Demands for production and dissemination of reliable data is growing with increasing demand from public policies to monitor, compare and improve global and national developmental status and targets. Implementation of intentionally agreed commitments like Millennium Development Goals (MDGs), Sustainable development Goals (SDGs) are influencing data production and availability, and the development of national statistical capacities. They also trigger challenges and opportunities in production of internationally comparable data to induce fair comparability among nations. Being a signatory to major international treaties, India has considerably improved data production, accessibility and availability over the years to ensure proper alignment of national level statistics and induce international comparison. However, very little efforts have been made to assess India's progress around data production and dissemination around growingly important land governance. This assessment attempts to identify key opportunities and challenges at the country level to improve data availability, access, timeliness and quality.India has made many progressive reforms around land laws and institutions to make land governance more inclusive and equitable; however its assessment with respect to global best practices through World Bank's Land Governance Assessment Framework (LGAF) indicate the need of improvements around different land dimensions. Movement towards good land governance outcomes is incumbent upon robust and regular monitoring mechanism of land indicators across spatial (viz. administrative boundaries, land being a state subject in India) and temporal scales.India has traditions of collecting, maintaining and reporting land information through nation-wide surveys, census, administrative and judicial reports/ databases. Its flagship program Digital India Land Record Modernization Program (DILRMP), has been supporting universal digitization of spatial and textual land records by the states. Together, these administrative and survey-derived datasets provide seamless opportunity for routine generation of data on key land indicators at low cost on a regularbasis. Land is a state subject in India. Monitoring and reporting land-indicators at state levels would help in systematically discovering and identifying good practice that can then be documented and disseminated across states, manage change, and gradually move towards a more performance-based approach to improving land governance in India. However, there have been lack of institutionalized attempts, so far, to report land-indicators at national scale.We have tried to assess the state of data in India, particularly to track and report two critical land governance indicators viz. women land rights and forest rights, critical to ensure equity and sustainability in terms of public policy. With UN's SDG, defining similar indicators, we also attempt aligning them around SDG indicators. Status of these two parameters were analyzed using nation-wide datasets collecting whole population data, through legitimate institutions following robust processes and reporting them open access.Census (human population) data and Forest Survey of India (FSI) data were used to assess village-wise forest areas eligible for recognition of rights under India's historic Forest Rights Act, 2005. Using the FSI data and meta-analysis of census data, we calculated the estimated population (150 million including 90 million tribal) living in villages that have forest land within administrative revenue boundaries, potential area (40 million ha) that can be recognized under FRA and number of villages (0.17 million) that are eligible to initiate the claim. These data were made available across administrative boundaries of state, district and village, providing opportunities for relevant Government Ministries at Central and State level and civil society to expedite the forest rights recognition under India's largest land reform process.In order to assess women's land rights (WLR) in India in the context of the SDGs, after examining the existing data sets, we used Agricultural Census data, conducted by Government of India every fifth year following the guidelines of World Census on Agriculture (WCA). Using Agricultural census data, we have developed atlas of women land rights (based on operational holdings) in India with state and district wise granularity with further disaggregation across ethnicity (caste) and other socio-economic parameters. The study also attempted to analyze the link between the inter-regional and temporal variability of WLR and relevant policies and legal-institutional frameworks among the states to see if the correlations can better inform public policy and also induce healthy competition among states to appreciate and follow best practices. This paper presents the process, methodology and results of the data-analysis for these two land indicators while delving into the scope and challenges of dealing with existing and upcoming big datasets in India to report the land governance indicators and the potential policy spinoffs.|10.1145/3047273.3047296|https://doi.org/10.1145/3047273.3047296|New York, NY, USA|Association for Computing Machinery|9781450348256|2017|Using Administrative Data for Monitoring and Improving Land Policy and Governance in India|Choudhury, Pranab Ranjan and Behera, Manoj Kumar|inproceedings|10.1145/3047273.3047296|||||||||||||||||||||||||||||350777915|42
||Industrial big data, Mechanical performances prediction, Lower upper bound estimation, Broad learning system, Hot-rolling||104-114||Industrial big data technology has become one of the important driving forces to intelligent manufacturing in the steel industry. In this study, the characteristics of data in steel production are analyzed and an industrial big data platform for steeling process is developed to extract the quality-related parameters. A data-driven approach to construct prediction intervals (PIs) of mechanical performances for hot-rolling strips is proposed to represent the uncertainty and reliability of the prediction results. The proposed method employs a new manifold visualization method, SLISEMAP, to reduce the feature dimensions with interpretability, utilizes lower upper bound estimation (LUBE) method to obtain the PIs, in which the broad learning system (BLS) is used as the basic training network model and the artificial bee colony (ABC) algorithm is applied to optimize the weighting parameters of BLS under the LUBE framework. A hot-rolling steel coil dataset consisting of 39 variables and 1335 coil samples is used to validate the proposed method. Two Delta-based approaches, namely back propagation neural network (BPNN) and extreme learning machine (ELM); and three LUBE-based approaches, namely ABC-BPNN, ABC-ELM, and ABC-support vector regression (SVR) are compared with the proposed method. Results show that the proposed ABC-BLS in LUBE is effective and efficient in constructing the PIs with a higher coverage probability and a narrower interval width.|https://doi.org/10.1016/j.jmsy.2022.08.014|https://www.sciencedirect.com/science/article/pii/S027861252200142X||||2022|Industrial big data-driven mechanical performance prediction for hot-rolling steel using lower upper bound estimation method|Gongzhuang Peng and Yinliang Cheng and Yufei Zhang and Jian Shao and Hongwei Wang and Weiming Shen|article|PENG2022104|||Journal of Manufacturing Systems|02786125||65|||||14966|2,310|Q1|70|155|294|8906|2949|288|10,88|57,46|Netherlands|Western Europe|1982-2020|Control and Systems Engineering (Q1); Hardware and Architecture (Q1); Industrial and Manufacturing Engineering (Q1); Software (Q1)|5,413|8.633|0.00561|351167022|619364890
||Electricity grids, Analytics, Big data, Decision-making||106788||This paper provides a survey of big data analytics applications and associated implementation issues. The emphasis is placed on applications that are novel and have demonstrated value to the industry, as illustrated using field data and practical applications. The paper reflects on the lessons learned from initial implementations, as well as ideas that are yet to be explored. The various data science trends treated in the literature are outlined, while experiences from applying them in the electricity grid setting are emphasized to pave the way for future applications. The paper ends with opportunities and challenges, as well as implementation goals and strategies for achieving impactful outcomes.|https://doi.org/10.1016/j.epsr.2020.106788|https://www.sciencedirect.com/science/article/pii/S0378779620305915||||2020|Big data analytics for future electricity grids|Mladen Kezunovic and Pierre Pinson and Zoran Obradovic and Santiago Grijalva and Tao Hong and Ricardo Bessa|article|KEZUNOVIC2020106788|||Electric Power Systems Research|03787796||189|||||16044|0,845|Q1|122|643|1165|20949|5369|1158|4,25|32,58|Netherlands|Western Europe|1977-2021|Electrical and Electronic Engineering (Q1); Energy Engineering and Power Technology (Q1)|13,115|3.414|0.01287|352301898|609246082
SIGCSE '18|Baltimore, Maryland, USA|computing education, ethics, data science, big data|6|952–957|Proceedings of the 49th ACM Technical Symposium on Computer Science Education|Data science is a new field that integrates aspects of computer science, statistics and information management. As a new field, ethical issues a data scientist may encounter have received little attention to date, and ethics training within a data science curriculum has received even less attention. To address this gap, this article explores the different codes of conduct and ethics frameworks related to data science. We compare this analysis with the results of a systematic literature review focusing on ethics in data science. Our analysis identified twelve key ethics areas that should be included within a data science ethics curriculum. Our research notes that none of the existing codes or frameworks covers all of the identified themes. Data science educators and program coordinators can use our results as a way to identify key ethical concepts that can be introduced within a data science program.|10.1145/3159450.3159483|https://doi.org/10.1145/3159450.3159483|New York, NY, USA|Association for Computing Machinery|9781450351034|2018|Key Concepts for a Data Science Ethics Curriculum|Saltz, Jeffrey S. and Dewar, Neil I. and Heckman, Robert|inproceedings|10.1145/3159450.3159483|||||||||||||||||||||||||||||352880637|42
CIKM '16|Indianapolis, Indiana, USA|localization, regression models, telco big data|10|439–448|Proceedings of the 25th ACM International on Conference on Information and Knowledge Management|It is still challenging in telecommunication (telco) industry to accurately locate mobile devices (MDs) at city-scale using the measurement report (MR) data, which measure parameters of radio signal strengths when MDs connect with base stations (BSs) in telco networks for making/receiving calls or mobile broadband (MBB) services. In this paper, we find that the widely-used location based services (LBSs) have accumulated lots of over-the-top (OTT) global positioning system (GPS) data in telco networks, which can be automatically used as training labels for learning accurate MR-based positioning systems. Benefiting from these telco big data, we deploy a context-aware coarse-to-fine regression (CCR) model in Spark/Hadoop-based telco big data platform for city-scale localization of MDs with two novel contributions. First, we design map-matching and interpolation algorithms to encode contextual information of road networks. Second, we build a two-layer regression model to capture coarse-to-fine contextual features in a short time window for improved localization performance. In our experiments, we collect 108 GPS-associated MR records in the centroid of Shanghai city with 12 x 11 square kilometers for 30 days, and measure four important properties of real-world MR data related to localization errors: stability, sensitivity, uncertainty and missing values. The proposed CCR works well under different properties of MR data and achieves a mean error of 110m and a median error of $80m$, outperforming the state-of-art range-based and fingerprinting localization methods.|10.1145/2983323.2983345|https://doi.org/10.1145/2983323.2983345|New York, NY, USA|Association for Computing Machinery|9781450340731|2016|City-Scale Localization with Telco Big Data|Zhu, Fangzhou and Luo, Chen and Yuan, Mingxuan and Zhu, Yijian and Zhang, Zhengqing and Gu, Tao and Deng, Ke and Rao, Weixiong and Zeng, Jia|inproceedings|10.1145/2983323.2983345|||||||||||||||||||||||||||||355387312|42
||interorganizational networks, information visualization, Data triangulation, business ecosystem|32|||Business ecosystems consist of a heterogeneous and continuously evolving set of entities that are interconnected through a complex, global network of relationships. However, there is no well-established methodology to study the dynamics of this network. Traditional approaches have primarily utilized a single source of data of relatively established firms; however, these approaches ignore the vast number of relevant activities that often occur at the individual and entrepreneurial levels. We argue that a data-driven visualization approach, using both institutionally and socially curated datasets, can provide important complementary, triangulated explanatory insights into the dynamics of interorganizational networks in general and business ecosystems in particular. We develop novel visualization layouts to help decision makers systemically identify and compare ecosystems. Using traditionally disconnected data sources on deals and alliance relationships (DARs), executive and funding relationships (EFRs), and public opinion and discourse (POD), we empirically illustrate our data-driven method of data triangulation and visualization techniques through three cases in the mobile industry Google’s acquisition of Motorola Mobility, the coopetitive relation between Apple and Samsung, and the strategic partnership between Nokia and Microsoft. The article concludes with implications and future research opportunities.|10.1145/2724730|https://doi.org/10.1145/2724730|New York, NY, USA|Association for Computing Machinery||2015|Understanding Business Ecosystem Dynamics: A Data-Driven Approach|"\"Basole, Rahul C. and Russell, Martha G. and Huhtam\"\"{a}ki, Jukka and Rubens, Neil and Still, Kaisa and Park, Hyunwoo\""|article|10.1145/2724730|6|jun|ACM Trans. Manage. Inf. Syst.|2158656X|2|6|July 2015||||||||||||||||||||||357495572|2097885649
||Artificial intelligence, Functional genomics, Genomics, Proteomics, Epigenomics, Transcriptomics, Epitranscriptomics, Metabolomics, Machine learning, Deep learning||5762-5790||We review the current applications of artificial intelligence (AI) in functional genomics. The recent explosion of AI follows the remarkable achievements made possible by “deep learning”, along with a burst of “big data” that can meet its hunger. Biology is about to overthrow astronomy as the paradigmatic representative of big data producer. This has been made possible by huge advancements in the field of high throughput technologies, applied to determine how the individual components of a biological system work together to accomplish different processes. The disciplines contributing to this bulk of data are collectively known as functional genomics. They consist in studies of: i) the information contained in the DNA (genomics); ii) the modifications that DNA can reversibly undergo (epigenomics); iii) the RNA transcripts originated by a genome (transcriptomics); iv) the ensemble of chemical modifications decorating different types of RNA transcripts (epitranscriptomics); v) the products of protein-coding transcripts (proteomics); and vi) the small molecules produced from cell metabolism (metabolomics) present in an organism or system at a given time, in physiological or pathological conditions. After reviewing main applications of AI in functional genomics, we discuss important accompanying issues, including ethical, legal and economic issues and the importance of explainability.|https://doi.org/10.1016/j.csbj.2021.10.009|https://www.sciencedirect.com/science/article/pii/S2001037021004311||||2021|AI applications in functional genomics|Claudia Caudai and Antonella Galizia and Filippo Geraci and Loredana {Le Pera} and Veronica Morea and Emanuele Salerno and Allegra Via and Teresa Colombo|article|CAUDAI20215762|||Computational and Structural Biotechnology Journal|20010370||19|||||21100318415|1,908|Q1|45|357|255|28093|2022|255|7,89|78,69|Sweden|Western Europe|2012-2020|Biochemistry (Q1); Biophysics (Q1); Biotechnology (Q1); Computer Science Applications (Q1); Genetics (Q1); Structural Biology (Q2)|3,620|7.271|0.00677|360385662|175296720
||Data quality assessment, Data quality control, Information quality, Benchmarking, Production planning, control||583-591||Industrial enterprises rely on prediction of market behavior, monitoring of performance measures, evaluation of production processes and other data analyses to support strategic and operational decisions. However, although an adequate data quality (DQ) is essential for any data analysis and several methodologies for DQ assessment exist, not all organizations consider DQ in decision-making processes. E.g., inaccurate and delayed data acquisition leads to imprecise master data and poor knowledge of machine utilization. While these aspects should influence production planning and control, current approaches to data evaluation are too complex to use them on a-day-to-day basis. In this paper, we propose a methodology that simplifies the execution of DQ evaluations and improves the understandability of its results. One of its main concerns is to make DQ assessment usable to small and medium-sized enterprises (SME). The approach takes selected, context related structured or semi-structured data as input and uses a set of generic test criteria applicable to different tasks and domains. It combines data and domain driven aspects and can be partly executed automated and without context specific domain knowledge. The results of the assessment can be summarized into quality dimensions and used for benchmarking. The methodology is validated using data from the enterprise resource planning (ERP) and manufacturing execution system (MES) of a sheet metal manufacturer covering a year of time. The particular application aims at calculating logistic key performance indicators. Based on these conditions, data requirements are defined and the available data is evaluated considering domain specific characteristics.|https://doi.org/10.1016/j.promfg.2019.02.114|https://www.sciencedirect.com/science/article/pii/S2351978919301477||||2019|Data quality assessment for improved decision-making: a methodology for small and medium-sized enterprises|Lisa C. Günther and Eduardo Colangelo and Hans-Hermann Wiendahl and Christian Bauer|article|GUNTHER2019583|||Procedia Manufacturing|23519789||29||“18th International Conference on Sheet Metal, SHEMET 2019”“New Trends and Developments in Sheet Metal Processing”|||21100792109|0,504|Q2|43|1390|3628|27760|8346|3572|1,79|19,97|Netherlands|Western Europe|2015-2020|Artificial Intelligence (Q2); Industrial and Manufacturing Engineering (Q2)||||361637670|896540749
ScienceCloud '14|Vancouver, BC, Canada|infrastructure as a service, scalable systems, platform as a service, cloud programming models, cloud computing, map reduce|8|1–8|Proceedings of the 5th ACM Workshop on Scientific Cloud Computing|Microsoft Research is now in its fourth year of awarding Windows Azure cloud resources to the academic community. As of April 2014, over 200 research projects have started. In this paper we review the results of this effort to date. We also characterize the computational paradigms that work well in public cloud environments and those that are usually disappointing. We also discuss many of the barriers to successfully using commercial cloud platforms in research and ways these problems can be overcome.|10.1145/2608029.2608030|https://doi.org/10.1145/2608029.2608030|New York, NY, USA|Association for Computing Machinery|9781450329118|2014|Science in the Cloud: Lessons from Three Years of Research Projects on Microsoft Azure|Gannon, Dennis and Fay, Dan and Green, Daron and Takeda, Kenji and Yi, Wenming|inproceedings|10.1145/2608029.2608030|||||||||||||||||||||||||||||362399374|42
||Boolean expressions, publish/subscribe, data structure, complex event processing|47|||BE-Tree is a novel dynamic data structure designed to efficiently index Boolean expressions over a high-dimensional discrete space. BE Tree-copes with both high-dimensionality and expressiveness of Boolean expressions by introducing an effective two-phase space-cutting technique that specifically utilizes the discrete and finite domain properties of the space. Furthermore, BE-Tree employs self-adjustment policies to dynamically adapt the tree as the workload changes. Moreover, in BE-Tree, we develop two novel cache-conscious predicate evaluation techniques, namely, lazy and bitmap evaluations, that also exploit the underlying discrete and finite space to substantially reduce BE-Tree's matching time by up to 75%BE-Tree is a general index structure for matching Boolean expression which has a wide range of applications including (complex) event processing, publish/subscribe matching, emerging applications in cospaces, profile matching for targeted web advertising, and approximate string matching. Finally, the superiority of BE-Tree is proven through a comprehensive evaluation with state-of-the-art index structures designed for matching Boolean expressions.|10.1145/2487259.2487260|https://doi.org/10.1145/2487259.2487260|New York, NY, USA|Association for Computing Machinery||2013|Analysis and Optimization for Boolean Expression Indexing|Sadoghi, Mohammad and Jacobsen, Hans-Arno|article|10.1145/2487259.2487260|8|jul|ACM Trans. Database Syst.|03625915|2|38|June 2013||||||||||||||||||||||367070173|1726010902
WebSci '13|Paris, France|data extraction, media studies, research tool, social network analysis, social networking services, Facebook|10|346–355|Proceedings of the 5th Annual ACM Web Science Conference|This paper describes Netvizz, a data collection and extraction application that allows researchers to export data in standard file formats from different sections of the Facebook social networking service. Friendship networks, groups, and pages can thus be analyzed quantitatively and qualitatively with regards to demographical, post-demographical, and relational characteristics. The paper provides an overview over analytical directions opened up by the data made available, discusses platform specific aspects of data extraction via the official Application Programming Interface, and briefly engages the difficult ethical considerations attached to this type of research.|10.1145/2464464.2464475|https://doi.org/10.1145/2464464.2464475|New York, NY, USA|Association for Computing Machinery|9781450318891|2013|Studying Facebook via Data Extraction: The Netvizz Application|Rieder, Bernhard|inproceedings|10.1145/2464464.2464475|||||||||||||||||||||||||||||368827509|42
ICACCI '12|Chennai, India|information-retrieval, document clustering, contradiction analysis|4|102–105|Proceedings of the International Conference on Advances in Computing, Communications and Informatics|Contradiction Analysis is one of the popular text-mining operations in which a document whose content is contradictory to the theme of a set of documents is identified. It is a means to identifying Outlier documents that do not confirm to the overall sense conveyed by other documents. Most of the existing techniques perform document-level comparisons, ignoring the sentence-level semantics, often leading to loss of vital information. Applications in domains like Defence and Healthcare require high levels of accuracy and identification of micro-level contradictions are vital. In this paper, we propose an algorithm for identifying contradictory documents using sentence-level clustering technique along with an optimization feature. A novel visualization scheme is also suggested to present the results to an end-user.|10.1145/2345396.2345413|https://doi.org/10.1145/2345396.2345413|New York, NY, USA|Association for Computing Machinery|9781450311960|2012|An Algorithm for Fuzzy-Based Sentence-Level Document Clustering for Micro-Level Contradiction Analysis|Mehta, R. Vasanth Kumar and Sankarasubramaniam, B. and Rajalakshmi, S.|inproceedings|10.1145/2345396.2345413|||||||||||||||||||||||||||||369765386|42
CBMI '22|Graz, Austria|Safety Management, 3D Reconstruction, Digital Twins, Structural Health Monitoring, Computer Vision|8|103–110|Proceedings of the 19th International Conference on Content-Based Multimedia Indexing|In the construction domain, Digital twins are mostly used for facilities management of buildings, but their applications are still very limited. The virtualization of buildings and bridges in the last 15 years in the form of Building or Bridge Information Models is clearly identified as the starting point for the DTs. The industry has erected a frame with semantically rich 3D reference models that are now heavily enriched with visual sensor data captured on construction sites. This article provides an overview of the research and current practices of computer vision methods in the construction industry and presents typical examples of their applications for 3D reconstruction, safety management and structural monitoring for quality assurance. It then highlights the dominant achievements presented in the literature and concludes with the challenges and research directions applicable to digital twins that need to be addressed and exploited in the future.|10.1145/3549555.3549594|https://doi.org/10.1145/3549555.3549594|New York, NY, USA|Association for Computing Machinery|9781450397209|2022|A Survey for Image Based Methods in Construction: From Images to Digital Twins|Koulalis, Ilias and Dourvas, Nikolaos and Triantafyllidis, Theocharis and Ioannidis, Konstantinos and Vrochidis, Stefanos and Kompatsiaris, Ioannis|inproceedings|10.1145/3549555.3549594|||||||||||||||||||||||||||||372386587|42
|||6|63–68|||10.1145/2536669.2536682|https://doi.org/10.1145/2536669.2536682|New York, NY, USA|Association for Computing Machinery||2013|Data Centric Research at the University of Queensland|Zhou, Xiaofang and Sadiq, Shazia|article|10.1145/2536669.2536682||oct|SIGMOD Rec.|01635808|3|42|September 2013||||13622|0,372|Q2|142|39|81|808|127|57|1,67|20,72|United States|Northern America|1969, 1973-1978, 1981-2020|Information Systems (Q2); Software (Q2)|1,819|0.775|7.5E-4|373631763|962972343
ICICSE '20|Male, Maldives|Machine learning, Semiconductor manufacturing, Advanced process control, Data analytics, Virtual metrology|3|109–111|Proceedings of the 2020 International Conference on Internet Computing for Science and Engineering|In this article, the authors attempt to describe the core quality inspection during semiconductor manufacturing in terms of production efficiency and yield. Special focus is therefore given to photolithography, which is the most critical step for the fabrication of wafer patterns in front-end processes. Further, machine learning approaches are demonstrated and their applicability in semiconductor manufacturing industry is discussed. Also, a technical concept regarding virtual metrology for advanced process control in semiconductor production is introduced as a potential utilization case. Finally, current status and future trends in technology as well as application are summarized based on authors' perspective in the concluding section.|10.1145/3424311.3424326|https://doi.org/10.1145/3424311.3424326|New York, NY, USA|Association for Computing Machinery|9781450377348|2020|Application of Machine Learning for Process Control in Semiconductor Manufacturing|Wang, Lei and Wang, Yang|inproceedings|10.1145/3424311.3424326|||||||||||||||||||||||||||||377965601|42
dg.o '18|Delft, The Netherlands|data preparedness, framework, humanitarian sector, data ecosystem, governance|9||Proceedings of the 19th Annual International Conference on Digital Government Research: Governance in the Data Age|"\"The incidence of natural disasters worldwide is increasing. As a result, a growing number of people is in need of humanitarian support, for which limited resources are available. This requires an effective and efficient prioritization of the most vulnerable people in the preparedness phase, and the most affected people in the response phase of humanitarian action. Data-driven models have the potential to support this prioritization process. However, the applications of these models in a country requires a certain level of data preparedness. To achieve this level of data preparedness on a large scale we need to know how to facilitate, stimulate and coordinate data-sharing between humanitarian actors. We use a data ecosystem perspective to develop success criteria for establishing a \"\"humanitarian data ecosystem\"\". We first present the development of a general framework with data ecosystem governance success criteria based on a systematic literature review. Subsequently, the applicability of this framework in the humanitarian sector is assessed through a case study on the \"\"Community Risk Assessment and Prioritization toolbox\"\" developed by the Netherlands Red Cross. The empirical evidence led to the adaption the framework to the specific criteria that need to be addressed when aiming to establish a successful humanitarian data ecosystem.\""|10.1145/3209281.3209326|https://doi.org/10.1145/3209281.3209326|New York, NY, USA|Association for Computing Machinery|9781450365260|2018|A Framework for Strengthening Data Ecosystems to Serve Humanitarian Purposes|Haak, Elise and Ubacht, Jolien and Van den Homberg, Marc and Cunningham, Scott and Van den Walle, Bartel|inproceedings|10.1145/3209281.3209326|85||||||||||||||||||||||||||||379258977|42
ICEGOV '17|New Delhi AA, India|counterfactuals, ex-post policy evaluation, Big data, data linkage|4|228–231|Proceedings of the 10th International Conference on Theory and Practice of Electronic Governance|"\"The collection and analysis of relevant data for evaluating public policies is not a straightforward task. An important type of such studies is the so-called ex-post evaluation. The main objective of ex-post evaluations is to determine to what extent a realized intervention is successful in tackling a societal challenge, e.g., youth unemployment. At a first glance an obvious method is to collect some baseline measurements for a set of relevant variables, apply the intervention for a while and collect the new measurement values for the same set of variables. Then, comparing the measurement values of the variables before and after the intervention provides an insight into the extent of successfulness of the intervention. This, however, is only true if the \"\"ceteris paribus\"\" condition holds. In practice it is infeasible to enforce this condition for societal challenges. Often, after having the baseline measurements, several phenomena emerge that may impact the new measurements without being taken into account. This makes it difficult to determine how much of the measured differences between the values of the variables before and after the intervention should be attributed to the emerging phenomena (or the so-called counterfactuals) and how much of the differences can be attributed to the applied intervention.This paper discusses how exploiting big data may contribute to the task of elucidating the influences of counterfactuals (and interventions) in ex-post evaluation studies. The paper proposes a framework to utilize big data for accounting for the impact of emerging phenomena in ex-post evaluation studies.\""|10.1145/3047273.3047377|https://doi.org/10.1145/3047273.3047377|New York, NY, USA|Association for Computing Machinery|9781450348256|2017|Exploiting Big Data for Evaluation Studies|Netten, Niels and Bargh, Mortaza S. and Choenni, Sunil and Meijer, Ronald|inproceedings|10.1145/3047273.3047377|||||||||||||||||||||||||||||380254845|42
|||8|27–34||"\"There is an increasing amount of structure on the Web as a result of modern Web languages, user tagging and annotation, emerging robust NLP tools, and an ever growing volume of linked data. These meaningful, semantic, annotations hold the promise to significantly enhance information access, by enhancing the depth of analysis of today's systems. The goal of the ESAIR'14 workshop remained to advance the general research agenda on this core problem, with an explicit focus on one of the most challenging aspects to address in the coming years. The main remaining challenge is on the user's side---the potential of rich document annotations can only be realized if matched by more articulate queries exploiting these powerful retrieval cues---and a more dynamic approach is emerging by exploiting new forms of query autosuggest. How can the query suggestion paradigm be used to encourage searcher to articulate longer queries, with concepts and relations linking their statement of request to existing semantic models? How do entity results and social network data in \"\"graph search\"\" change the classic division between searchers and information and lead to extreme personalization---are you the query? How to leverage transaction logs and recommendation, and how adaptive should we make the system? What are the privacy ramifications and the UX aspects---how to not creep out users.There was a strong feeling that we made substantial progress. Specifically, the discussion contributed to our understanding of the way forward. First, for notable (head, shoulder, but not tail) entities in semantic search we have reached the level of quality at minimal costs allowing for deployment in major web search engines---the dream has become a reality. Second, entity detection is moving fast into domain specific, personal, and business domains, and has become a vital component for a range of applications. Third, semantic web has exchanged logic for machine learning approaches, and machine learning is the natural unification of semantic web and information retrieval approaches.\""|10.1145/2795403.2795412|https://doi.org/10.1145/2795403.2795412|New York, NY, USA|Association for Computing Machinery||2015|Report on the Seventh Workshop on Exploiting Semantic Annotations in Information Retrieval (ESAIR'14)|Alonso, Omar and Kamps, Jaap and Karlgren, Jussi|article|10.1145/2795403.2795412||jun|SIGIR Forum|01635840|1|49|June 2015||||||||||||||||||||||381506811|1301506833
|||9|52–60||"\"The panel on data(base) education at VLDB2021 [13] drew attention to important challenges in choosing how database classes are constructed for students in a world where data is being used in novel and impactful settings. This paper aims to present one view of a process for making these pedagogy decisions. We don't aim to present a best-possible design of the subject, rather we want to illuminate the space of possibilities, to encourage reasoned choices rather than simply teaching the subject as it was previously offered, or spending time on the latest innovations without considering the \"\"opportunity cost\"\" of doing so. We hope to guide the perplexed instructor or departmental curriculum committee.\""|10.1145/3552490.3552504|https://doi.org/10.1145/3552490.3552504|New York, NY, USA|Association for Computing Machinery||2022|Teaching about Data and Databases: Why, What, How?|"\"Fekete, Alan D. and R\"\"{o}hm, Uwe\""|article|10.1145/3552490.3552504||jul|SIGMOD Rec.|01635808|2|51|June 2022||||13622|0,372|Q2|142|39|81|808|127|57|1,67|20,72|United States|Northern America|1969, 1973-1978, 1981-2020|Information Systems (Q2); Software (Q2)|1,819|0.775|7.5E-4|384013526|962972343
||Electric vehicles, SOH, User behavior, LWLR, LSTM||102867||State of health (SOH) of lithium-ion battery pack directly determines the driving mileage and output power of the electric vehicle. With the development of big data storage and analysis technology, using big data to off-line estimate battery pack SOH is more feasible than before. This paper proposes a SOH estimation method based on real data of electric vehicles concerning user behavior. The charging capacity is calculated by historical charging data, and locally weighted linear regression (LWLR) algorithm is used to qualitatively characterize the capacity decline trend. The health features are extracted from historical operating data, maximal information coefficient (MIC) algorithm is used to measure the correlation between health features and capacity. Then, long and short-term memory (LSTM)-based neural network will further learn the nonlinear degradation relationship between capacity and health features. Bayesian optimization algorithm is used to ensure the generalization of the model when different electric vehicles produce different user behaviors. The estimation method is validated by the 300 days historical dataset from 100 vehicles with different driving behavior. The results indicates that the maximum relative error of estimating SOH is 0.2%.|https://doi.org/10.1016/j.est.2021.102867|https://www.sciencedirect.com/science/article/pii/S2352152X21005892||||2021|State-of-health estimation based on real data of electric vehicles concerning user behavior|Zhigang He and Xiaoyu Shen and Yanyan Sun and Shichao Zhao and Bin Fan and Chaofeng Pan|article|HE2021102867|||Journal of Energy Storage|2352152X||41|||||21100400826|1,088|Q1|42|860|831|44642|5383|828|6,87|51,91|Netherlands|Western Europe|2015-2020|Electrical and Electronic Engineering (Q1); Energy Engineering and Power Technology (Q1); Renewable Energy, Sustainability and the Environment (Q2)|7,765|6.583|0.00926|387215686|593635875
BDCA'17|Tetouan, Morocco|middleware, linked, open data, business discovery, data mining, knowledge discovery in databases, smart city, business intelligence|6||Proceedings of the 2nd International Conference on Big Data, Cloud and Applications|The extent to which data mining tools are able to make efficient use of an open data oriented strategy in a smart city is limited. In a sense that it is not fully automated, incompatible or has to be supervised. These sets of tools may offer the possibility to import a dataset in a certain predefined standardized format, still, they do not make it a part of their workflow and algorithms in a fully unsupervised manner (i.e without ongoing human guidance). In a departure from previous research works, in this paper, we present a middleware architecture that exploits open data as background knowledge by acting as a bridge between data mining tools and open data resources.|10.1145/3090354.3090382|https://doi.org/10.1145/3090354.3090382|New York, NY, USA|Association for Computing Machinery|9781450348522|2017|Exploiting Open Data to Improve the Business Intelligence &amp; Business Discovery Experience|El Bacha, Oussama and Jmad, Othmane and El Bouzekri El Idrissi, Younes and Hmina, Nabil|inproceedings|10.1145/3090354.3090382|27||||||||||||||||||||||||||||390361198|42
||Big data, Pikalert, Road weather, Surface transportation, Pavement condition, Weather forecasts||100071||Operating modern multi-modal surface transportation systems are becoming increasingly automated and driven by decision support systems. One aspect necessary for successful, safe, reliable, and efficient operation of any transportation network is real-time and forecasted weather and pavement condition information. Providing such information requires an adaptive system capable of blending large amounts of observational and model data that arrives quickly, in disparate formats and times, and blends and optimizes their use via expert systems and machine-learning algorithms. Quality control of the data is also essential, and historical data is required to both develop expert-based empirical algorithms and train machine learning models. This paper reports on the open-source Pikalert® system that brings together weather information and real-time data from connected vehicles to provide crucial information to enhance the safety and efficiency of surface transportation systems. This robust framework can be applied to a diverse array of user community specifications and is designed to rapidly ingest more, unique data sets as they become available. Ultimately, the developmental framework of this system will provide critical environmental information necessary to promote the development, growth, refinement, and expanded adoption of automated and connected multi-modal vehicular systems globally.|https://doi.org/10.1016/j.trip.2019.100071|https://www.sciencedirect.com/science/article/pii/S2590198219300703||||2019|An adaptive big data weather system for surface transportation|Amanda R. Siems-Anderson and Curtis L. Walker and Gerry Wiener and William P. Mahoney and Sue Ellen Haupt|article|SIEMSANDERSON2019100071|||Transportation Research Interdisciplinary Perspectives|25901982||3|||||21100943534|0,383|Q2|10|171|64|8370|114|62|1,78|48,95|United Kingdom|Western Europe|2019-2020|Automotive Engineering (Q2); Civil and Structural Engineering (Q3); Management Science and Operations Research (Q3); Transportation (Q3)||||390597695|2103768449
ICEME 2020|Beijing, China|Urban brain, Data, Urban cockpit|4|70–73|2020 The 11th International Conference on E-Business, Management and Economics|The urban cockpit will comprehensively perceive and process all kinds of data in the city operation, establish the data chassis of the smart city, objectively, comprehensively and multi dimensionally display the operation situation of the city, and carry out early warning, prediction and scientific disposal of outstanding problems and emergencies in the city operation. Moreover, in the near future, with the introduction and use of 5G, artificial intelligence, big data, data Luan Sheng and edge computing in the city brain project, the city cockpit will also give the city managers and visitors a better and more beautiful feeling in the display effect (such as immersion and three-dimensional), thus accelerating the promotion and landing of the city brain project and promoting social governance Intelligent and professional, improve the level of comprehensive city governance, and change the transformation and upgrading of the city from extensive to precise and refined.|10.1145/3414752.3414800|https://doi.org/10.1145/3414752.3414800|New York, NY, USA|Association for Computing Machinery|9781450388016|2020|The Significance of Urban Cockpit for Urban Brain Construction|Huang, Qibao and Huang, Yiqi|inproceedings|10.1145/3414752.3414800|||||||||||||||||||||||||||||393050558|42
||Pediatric oncology, Pediatric cancer, Big data, Data sharing, Data science, Informatics||56-64||Pediatric cancer is a rare disease with a low annual incidence, which presents a significant challenge in being able to collect enough data to fuel clinical discoveries. Big data registry trials hold promise to advance the study of pediatric cancers by allowing for the combination of traditional randomized controlled trials with the power of larger cohort sizes. The emergence of big data resources and data-sharing initiatives are becoming transformative for pediatric cancer diagnosis and treatment. This review discusses the uses of big data in pediatric cancer, existing pediatric cancer registry initiatives and research, the challenges in harmonizing these data to improve accessibility for study, and building pediatric data commons and other important future endeavors.|https://doi.org/10.1053/j.seminoncol.2020.02.006|https://www.sciencedirect.com/science/article/pii/S0093775420300063||||2020|Using big data in pediatric oncology: Current applications and future directions|Ajay Major and Suzanne M. Cox and Samuel L. Volchenboum|article|MAJOR202056|||Seminars in Oncology|00937754|1|47||Pediatric Oncology|||13120|1,812|Q1|133|51|150|2969|690|124|4,21|58,22|United Kingdom|Western Europe|1974-2020|Hematology (Q1); Oncology (Q1)|5,713|4.929|0.00455|393115729|1514255128
VizSec '14|Paris, France|defense, visualization, cyber security|8|33–40|Proceedings of the Eleventh Workshop on Visualization for Cyber Security|"\"What does it take to be a successful visualization in cyber security? This question has been explored for some time, resulting in many potential solutions being developed and offered to the cyber security community. However, when one reflects upon the successful visualizations in this space they are left wondering where all those offerings have gone. Excel and Grep are still the kings of cyber security defense tools; there is a great opportunity to help in this domain, yet many visualizations fall short and are not utilized.In this paper we present seven challenges, informed by two user studies, to be considered when developing a visualization for cyber security purposes. Cyber security visualizations must go beyond isolated solutions and \"\"pretty picture\"\" visualizations in order to impact users. We provide an example prototype that addresses the challenges with a description of how they are met. Our aim is to assist in increasing utility and adoption rates for visualization capabilities in cyber security.\""|10.1145/2671491.2671497|https://doi.org/10.1145/2671491.2671497|New York, NY, USA|Association for Computing Machinery|9781450328265|2014|7 Key Challenges for Visualization in Cyber Network Defense|Best, Daniel M. and Endert, Alex and Kidwell, Daniel|inproceedings|10.1145/2671491.2671497|||||||||||||||||||||||||||||395487686|42
|Singapore, Singapore||||||||New York, NY, USA|Association for Computing Machinery|9781450338455|2016|ASE 2016: Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering||proceedings|10.1145/2970276|||||||||||||||||||||||||||||396873282|42
CHI EA '14|Toronto, Ontario, Canada|user oriented design, wellbeing, data analysis|4|95–98|CHI '14 Extended Abstracts on Human Factors in Computing Systems|Sustaining our health and wellbeing requires lifelong efforts for prevention and healthy living. Continuously observing ourselves is one of the fundamental measures to be taken. While many devices support monitoring and quantifying our health behavior and health state, they all are facing the same trade-off: the higher the data quality is the higher are the efforts of data acquisition. However, for lifelong use, minimizing efforts for the user is crucial. Nowadays, few devices find a good balance between cost and value. In this interdisciplinary workshop we discuss how this trade-off can be approached by addressing three topics: understanding the user's information needs, exploring options for data acquisition, and discussing potential designs for life-long use.|10.1145/2559206.2560469|https://doi.org/10.1145/2559206.2560469|New York, NY, USA|Association for Computing Machinery|9781450324748|2014|Beyond Quantified Self: Data for Wellbeing|Meyer, Jochen and Simske, Steven and Siek, Katie A. and Gurrin, Cathal G. and Hermens, Hermie|inproceedings|10.1145/2559206.2560469|||||||||||||||||||||||||||||398832542|42
||information system education, computer science education, problem-based learning, natural language processing, NLP, big data text analytics, machine learning, deep learning||18-43||Natural language processing (NLP) covers a large number of topics and tasks related to data and information management, leading to a complex and challenging teaching process. Meanwhile, problem-based learning is a teaching technique specifically designed to motivate students to learn efficiently, work collaboratively, and communicate effectively. With this aim, we developed a problem-based learning course for both undergraduate and graduate students to teach NLP. We provided student teams with big data sets, basic guidelines, cloud computing resources, and other aids to help different teams in summarizing two types of big collections: Web pages related to events, and electronic theses and dissertations (ETDs). Student teams then deployed different libraries, tools, methods, and algorithms to solve the task of big data text summarization. Summarization is an ideal problem to address learning NLP since it involves all levels of linguistics, as well as many of the tools and techniques used by NLP practitioners. The evaluation results showed that all teams generated coherent and readable summaries. Many summaries were of high quality and accurately described their corresponding events or ETD chapters, and the teams produced them along with NLP pipelines in a single semester. Further, both undergraduate and graduate students gave statistically significant positive feedback, relative to other courses in the Department of Computer Science. Accordingly, we encourage educators in the data and information management field to use our approach or similar methods in their teaching and hope that other researchers will also use our data sets and synergistic solutions to approach the new and challenging tasks we addressed.|https://doi.org/10.2478/dim-2020-0003|https://www.sciencedirect.com/science/article/pii/S2543925122000572||||2020|Teaching Natural Language Processing through Big Data Text Summarization with Problem-Based Learning|Liuqing Li and Jack Geissinger and William A. Ingram and Edward A. Fox|article|LI202018|||Data and Information Management|25439251|1|4|||||||||||||||||||||||399046126|529068339
||Big Data, Analysis, Informed skepticism, Questioning||63-80||Big Data is now readily available for analysis, and analysts trained to conduct effective analysis in this area are in high demand. However, there is a dearth of discussion in the literature related to identifying the important cognitive skills required for accountants to conduct effective Big Data analysis. Here we argue that accountants need to approach Big Data analysis as informed skeptics, being ever ready to challenge the analysis by asking good questions in appropriate topical areas. These areas include understanding the limits of measurement and representation, the subjectiveness of insight, the challenges of statistics and integrating data sets, and the effects of underdetermination and inductive reasoning. Accordingly, we develop a framework and an illustrative example to facilitate the training of accounting students to become informed skeptics in the era of Big Data by explaining the conceptual relevance of each of the topical areas to Big Data analysis. In addition, example questions are identified that accountants conducting Big Data analysis should be asking regarding each topic. Further, for each topic, references to additional resources are provided that students can access to learn more about effectively conducting Big Data analysis.|https://doi.org/10.1016/j.jaccedu.2016.12.007|https://www.sciencedirect.com/science/article/pii/S0748575116301051||||2017|The need for ‘skeptical’ accountants in the era of Big Data|Earl McKinney and Charles J. Yoos and Ken Snead|article|MCKINNEY201763|||Journal of Accounting Education|07485751||38||Special Issue on Big Data|||29852|0,931|Q1|35|24|73|831|227|69|2,76|34,63|United Kingdom|Western Europe|1983-2020|Accounting (Q1); Education (Q1)||||399603804|1852131396
||information quality, Data quality, text mining|28|||Research in data and information quality has made significant strides over the last 20 years. It has become a unified body of knowledge incorporating techniques, methods, and applications from a variety of disciplines including information systems, computer science, operations management, organizational behavior, psychology, and statistics. With organizations viewing “Big Data”, social media data, data-driven decision-making, and analytics as critical, data quality has never been more important. We believe that data quality research is reaching the threshold of significant growth and a metamorphosis from focusing on measuring and assessing data quality—content—toward a focus on usage and context. At this stage, it is vital to understand the identity of this research area in order to recognize its current state and to effectively identify an increasing number of research opportunities within. Using Latent Semantic Analysis (LSA) to analyze the abstracts of 972 peer-reviewed journal and conference articles published over the past 20 years, this article contributes by identifying the core topics and themes that define the identity of data quality research. It further explores their trends over time, pointing to the data quality dimensions that have—and have not—been well-studied, and offering insights into topics that may provide significant opportunities in this area.|10.1145/2996198|https://doi.org/10.1145/2996198|New York, NY, USA|Association for Computing Machinery||2017|From Content to Context: The Evolution and Growth of Data Quality Research|Shankaranarayanan, G. and Blake, Roger|article|10.1145/2996198|9|jan|J. Data and Information Quality|19361955|2|8|February 2017||||||||||||||||||||||401600774|833754770
|||6|58–63|||10.1145/3511666|https://doi.org/10.1145/3511666|New York, NY, USA|Association for Computing Machinery||2022|Energy Informatics: Key Elements for Tomorrow's Energy System|Schmeck, Hartmut and Monti, Antonello and Hagenmeyer, Veit|article|10.1145/3511666||mar|Commun. ACM|00010782|4|65|April 2022||||||||||||||||||||||403770888|647144465
||Multi-criteria decision-making, Veracity in ‘big’ data, Data quality assessment, Data quality handling||541-554||Decision support systems aim to help a decision maker with selecting the option from a set of available options that best meets her or his needs. In multi-criteria based decision support approaches, a suitability degree is computed for each option, reflecting how suitable that option is considering the preferences of the decision maker. Nowadays, it becomes more and more common that data of different quality, originating from different data sets and different data providers have to be integrated and processed in order to compute the suitability degrees. Also, data sets can be very large such that their data become commonly prone to incompleteness, imprecision and uncertainty. Hence, not all data used for decision making can be trusted to the same extent and consequently, neither the results of computations with such data can be trusted to the same extent. For this reason, data quality assessment becomes an important aspect of a decision making process. To correctly inform the users, it is essential to communicate not only the computed suitability degrees of the available options, but also the confidence about these suitability degrees as can be derived from data quality assessment. In this paper a novel multi-dimensional approach for data quality assessment in multi-criteria decision making, supporting the computation of associated confidence degrees, is presented. Providing confidence information adds an extra dimension to the decision making process and leads to more soundly decisions. The added value of the approach is illustrated with aspects of a geographic decision making process.|https://doi.org/10.1016/j.ins.2017.09.008|https://www.sciencedirect.com/science/article/pii/S0020025517309337||||2018|Handling veracity in multi-criteria decision-making: A multi-dimensional approach|Guy {De Tré} and Robin {De Mol} and Antoon Bronselaer|article|DETRE2018541|||Information Sciences|00200255||460-461|||||15134|1,524|Q1|184|928|2184|40007|17554|2168|7,89|43,11|United States|Northern America|1968-2021|Artificial Intelligence (Q1); Computer Science Applications (Q1); Control and Systems Engineering (Q1); Information Systems and Management (Q1); Software (Q1); Theoretical Computer Science (Q1)|44,038|6.795|0.04908|403779303|1633962588
||||i-i|2015 IEEE International Congress on Big Data|The following topics are dealt with: data mining; social network; data privacy; learning; query processing; big data processing; big data quality; big data platform; big data semantics; health care; network management; distributed processing; social media; and image processing.|10.1109/BigDataCongress.2015.1|||||2015|[Title page i]||inproceedings|7207185||June||23797703|||||||||||||||||||||||||404353377|1226158248
||Trajectory manipulation, Air traffic control, Image based techniques, Information visualization||207-227||Objectives: The objective of the presented work is to present novel methods for big data exploration in the Air Traffic Control (ATC) domain. Data is formed by sets of airplane trajectories, or trails, which in turn records the positions of an aircraft in a given airspace at several time instants, and additional information such as flight height, speed, fuel consumption, and metadata (e.g. flight ID). Analyzing and understanding this time-dependent data poses several non-trivial challenges to information visualization. Materials and methods: To address this Big Data challenge, we present a set of novel methods to analyze aircraft trajectories with interactive image-based information visualization techniques.As a result, we address the scalability challenges in terms of data manipulation and open questions by presenting a set of related visual analysis methods that focus on decision-support in the ATC domain. All methods use image-based techniques, in order to outline the advantages of such techniques in our application context, and illustrated by means of use-cases from the ATC domain. Results: For each considered use-case, we outline the type of questions posed by domain experts, data involved in addressing these questions, and describe the specific image-based techniques we used to address these questions. Further, for each of the proposed techniques, we describe the visual representation and interaction mechanisms that have been used to address the above-mentioned goals. We illustrate these use-cases with real-life datasets from the ATC domain, and show how our techniques can help end-users in the ATC domain discover new insights, and solve problems, involving the presented datasets.|https://doi.org/10.1016/j.trc.2014.03.005|https://www.sciencedirect.com/science/article/pii/S0968090X14000710||||2014|Interactive image-based information visualization for aircraft trajectory analysis|C. Hurter and S. Conversy and D. Gianazza and A.C. Telea|article|HURTER2014207|||Transportation Research Part C: Emerging Technologies|0968090X||47||Special Issue: Emerging Technologies Special Issue of ICTIS 2013 – Guest Editors: Liping Fu and Ming Zhong and Special Issue: Visualization & Visual Analytics in Transportation – Guest Editors: Patricia S. Hu and Michael L. Pack|||20893|3,185|Q1|133|327|834|17795|8956|824|10,15|54,42|United Kingdom|Western Europe|1993-2020|Automotive Engineering (Q1); Civil and Structural Engineering (Q1); Computer Science Applications (Q1); Management Science and Operations Research (Q1); Transportation (Q1)||||406657183|307592163
||||176-181|||https://doi.org/10.1016/j.profnurs.2017.10.005|https://www.sciencedirect.com/science/article/pii/S8755722316303283||||2018|A plea to nurse educators: Incorporate big data use as a foundational skill for undergraduate and graduate nurses|Betty Rambur and Therese Fitzpatrick|article|RAMBUR2018176|||Journal of Professional Nursing|87557223|3|34|||||||||||||||||||||||415627680|983295838
ICBDC '19|Guangzhou, China|decision tree, data mining, currency rules, course selection information|6|247–252|Proceedings of the 4th International Conference on Big Data and Computing|The currency of data can ensure that data is not obsolete and outdated. As one of the important bases for evaluating data quality, it plays an important role in the availability of data. Data currency rules can effectively discriminate the temporal relationship between data sets. The decision tree can availably classify and predict the data, and can test the attribute values very well. In this paper, the currency rules are combined with the C4.5 algorithm in the decision tree, and the improved algorithm is applied to the college elective data in recent years. Through experiments, the algorithm used in this paper can extract the statute rules from the student elective database. According to the currency rules, the college teaching plan can be planned in advance and the curriculum resources can be allocated reasonably.|10.1145/3335484.3335541|https://doi.org/10.1145/3335484.3335541|New York, NY, USA|Association for Computing Machinery|9781450362788|2019|Data Mining of Students' Course Selection Based on Currency Rules and Decision Tree|Liang, Yu and Duan, Xuliang and Ding, Yuanjun and Kou, Xifeng and Huang, Jingcheng|inproceedings|10.1145/3335484.3335541|||||||||||||||||||||||||||||415911467|42
||data linkage, clinical registry, health services research, ophthalmic epidemiology, big data||443-465||Large population-based health administrative databases, clinical registries, and data linkage systems are a rapidly expanding resource for health research. Ophthalmic research has benefited from the use of these databases in expanding the breadth of knowledge in areas such as disease surveillance, disease etiology, health services utilization, and health outcomes. Furthermore, the quantity of data available for research has increased exponentially in recent times, particularly as e-health initiatives come online in health systems across the globe. We review some big data concepts, the databases and data linkage systems used in eye research—including their advantages and limitations, the types of studies previously undertaken, and the future direction for big data in eye research.|https://doi.org/10.1016/j.survophthal.2016.01.003|https://www.sciencedirect.com/science/article/pii/S0039625716000023||||2016|Big data and ophthalmic research|Antony Clark and Jonathon Q. Ng and Nigel Morlet and James B. Semmens|article|CLARK2016443|||Survey of Ophthalmology|00396257|4|61|||||20107|2,131|Q1|132|69|235|6991|1131|199|4,75|101,32|United States|Northern America|1956-1970, 1972-2020|Ophthalmology (Q1)|7,296|6.048|0.00487|422635506|449155725
Chandos Information Professional Series||Research data quality, Stakeholders, Trust, Intrinsic and extrinsic data quality, Semiotic representation, Time-related dimensions, Data retrievability, Data reuse, Data governance||49-75|Research Data Management and Data Literacies|This chapter acquaints the reader with the general and often changing nature of research on data quality. It is emphasized that research data quality is closely related to business data; however, the goals of scholarly research have become different, especially as the environments shaping the two are different. From among data quality’s attributes, trust receives particular attention. Technical and scientific quality, the relationship of data quality to data reuse, and other quality factors are also examined, including big data quality, intrinsic and extrinsic data quality, and the semiotic representation of quality attributes, as well as their time-related dimensions and retrievability. Although data reuse was addressed in an earlier chapter, its relationship to data quality is touched on in this chapter as well. Sharing the previously mentioned origin with data quality and being closely associated with it, data governance is also portrayed.|https://doi.org/10.1016/B978-0-12-824475-3.00004-7|https://www.sciencedirect.com/science/article/pii/B9780128244753000047||Chandos Publishing|978-0-12-824475-3|2022|Chapter 3 - Data quality, the essential “ingredient”|Tibor Koltay|incollection|KOLTAY202249||||||||||Tibor Koltay|||||||||||||||||||423091050|42
||||27-38||Big data promises to transform public decision-making for the better by making it more responsive to actual needs and policy effects. However, much recent work on big data in public decision-making assumes a rational view of decision-making, which has been much criticized in the public administration debate. In this paper, we apply this view, and a more political one, to the context of big data and offer a qualitative study. We question the impact of big data on decision-making, realizing that big data – including its new methods and functions – must inevitably encounter existing political and managerial institutions. By studying two illustrative cases of big data use processes, we explore how these two worlds meet. Specifically, we look at the interaction between data analysts and decision makers. In this we distinguish between a rational view and a political view, and between an information logic and a decision logic. We find that big data provides ample opportunities for both analysts and decision makers to do a better job, but this doesn't necessarily imply better decision-making, because big data also provides opportunities for actors to pursue their own interests. Big data enables both data analysts and decision makers to act as autonomous agents rather than as links in a functional chain. Therefore, big data's impact cannot be interpreted only in terms of its functional promise; it must also be acknowledged as a phenomenon set to impact our policymaking institutions, including their legitimacy.|https://doi.org/10.1016/j.giq.2018.10.011|https://www.sciencedirect.com/science/article/pii/S0740624X17304951||||2019|Rationality and politics of algorithms. Will the promise of big data survive the dynamics of public decision making?|H.G. {van der Voort} and A.J. Klievink and M. Arnaboldi and A.J. Meijer|article|VANDERVOORT201927|||Government Information Quarterly|0740624X|1|36|||||14735|2,121|Q1|103|76|205|5894|1946|193|8,39|77,55|United Kingdom|Western Europe|1984-2020|E-learning (Q1); Law (Q1); Library and Information Sciences (Q1); Sociology and Political Science (Q1)|5,379|7.279|0.00502|423224241|1582933551
https://doi.org/10.1016/j.ijchy.2020.100027|https://www.sciencedirect.com/science/article/pii/S2590086220300045||||2020|Uses and opportunities for machine learning in hypertension research|Dhammika Amaratunga and Javier Cabrera and Davit Sargsyan and John B. Kostis and Stavros Zinonos and William J. Kostis|article|AMARATUNGA2020100027|||International Journal of Cardiology Hypertension|25900862||5||||||||||||||||||||||||||||||425566389|42
||Information management, Data quality, Query language extensions, Data profiling, Decision support systems, Big data||8304-8326||This paper describes the design and implementation of the Data Quality Query System (DQ2S), a query processing framework and tool incorporating data quality profiling functionality in the processing of queries involving quality-aware query language extensions. DQ2S supports the combination of performance and quality-oriented query optimizations, and a query processing platform that enables advanced data profiling queries to be formulated based on well established query language constructs, often used to interact with relational database management systems. DQ2S encompasses a declarative query language and a data model that provides users with the capability to express constraints on the quality of query results as well as query quality-related information; a set of algebraic operators for manipulating data quality-related information, and optimization heuristics. The proposed query language and algebra represent seamless extensions to SQL and relational database engines, respectively. The constructs of the proposed data model are implemented at the user’s view level and are internally mapped into relational model constructs. The quality-aware extensions and features are extremely useful when users need to assess the quality of relational data sets and define quality constraints for acceptable data prior to using candidate data sources in decision support systems and conducting big data analytical tasks.|https://doi.org/10.1016/j.eswa.2015.06.050|https://www.sciencedirect.com/science/article/pii/S0957417415004522||||2015|DQ2S – A framework for data quality-aware information management|Sandra de F. {Mendes Sampaio} and Chao Dong and Pedro Sampaio|article|MENDESSAMPAIO20158304|||Expert Systems with Applications|09574174|21|42|||||24201|1,368|Q1|207|770|1945|42314|17345|1943|8,67|54,95|United Kingdom|Western Europe|1990-2021|Artificial Intelligence (Q1); Computer Science Applications (Q1); Engineering (miscellaneous) (Q1)|55,444|6.954|0.04053|431075338|1377770283
||Data integrity;Big Data;Data models;Dictionaries;Machine learning algorithms;Machine learning;Measurement;data quality;missing data;big data;functional dependancy;master data;machine learning||498-501|2018 IEEE 5th International Congress on Information Science and Technology (CiSt)|Improving data quality is not a recent field but in the context of big data this is a challenging area as there is a crucial need for data quality to, for example, increase the accuracy of big data analytics or avoid storing redundant data. Missing data is one of the major problem that faces the quality of data. There are several methods and approaches that have been used in relational databases to handle missing data most of which have been adapted to big data. This paper aims to provide an overview of some methods and approaches for handling missing data in big data contexts.|10.1109/CIST.2018.8596389|||||2018|A Study of Handling Missing Data Methods for Big Data|Ezzine, Imane and Benhlima, Laila|inproceedings|8596389||Oct||23271884|||||||||||||||||||||||||431413021|2086419430
||Data governance, Data-Centric architecture, Industry 4.0, Big data, IoT||103595||The fourth industrial revolution, or Industry 4.0, represents a new stage of evolution in the organization, management and control of the value chain throughout the product or service life cycle. This is mainly based on the digitalization of the industrial environment by means of the convergence of Information Technologies (IT) and operational Technologies (OT) through cyber-physical systems and the Industrial IoT (IIoT) and the use of data generated in real time for gaining insights and making decisions. Therefore data becomes a critical asset for Industry 4.0 and must be managed and governed like a strategic asset. We rely on Data Governance (DG) as a key instrument for carrying out this transformation. This paper presents the design of a specific governance framework for Industry 4.0. First, this contextualizes data governance for Industry 4.0 environments and identifies the requirements that this framework must address, which are conditioned by the specific features of Industry 4.0, among others, the intensive use of big data, the cloud and edge computing, the artificial intelligence and the current regulations. Next, we formally define a reference framework for the implementation of Data Governance Systems for Industry 4.0 using international standards and providing several examples of architecture building blocks.|https://doi.org/10.1016/j.csi.2021.103595|https://www.sciencedirect.com/science/article/pii/S0920548921000908||||2022|A reference framework for the implementation of data governance systems for industry 4.0|Marta Zorrilla and Juan Yebenes|article|ZORRILLA2022103595|||Computer Standards & Interfaces|09205489||81|||||24303|0,556|Q1|63|36|246|2302|1013|243|3,71|63,94|Netherlands|Western Europe|1985-2020|Law (Q1); Hardware and Architecture (Q2); Software (Q2)||||433594480|827980402
||C# languages;Data quality;Telecom operators;Generic data;Standards compliance testing||302-306|2016 16th International Symposium on Communications and Information Technologies (ISCIT)|The big data is bringing new opportunities to the world. Every traditional telecom operator is exploring new ways to increase revenues and profits from the explosive growth of data traffic, but few have demonstrated the data quality needed for data applications. Standards compliance testing on generic data of telecom operators is the key means to improve competitive quality of comprehensive telecom information services. With analysis of current contradiction between data supply and data demand of telecom operators, this paper defines the generic data of telecom operators and its standards compliance testing, presents the testing procedures and the testing applications, and points out a series of testing research directions.|10.1109/ISCIT.2016.7751640|||||2016|Standards compliance testing on generic data of telecom operators|Mao, Xu and Su, Fei|inproceedings|7751640||Sep.|||||||||||||||||||||||||||433807360|42
||Cleaning;Distribution networks;Big Data;Prediction algorithms;Clustering algorithms;Data models;Anomaly detection;Data cleaning;Outliers detection;missing data imputation;LOF;DBSCAN;Random Forest||1-10||"\"In order to improve the data quality, the big data cleaning method of distribution network was studied in this paper. First, the Local Outlier Factor (LOF) algorithm based on DBSCAN clustering was used to detect outliers. However, due to the difficulty in determining the LOF threshold, a method of dynamically calculating the threshold based on the transformer districts and time was proposed. Besides, the LOF algorithm combines the statistical distribution method to reduce the \"\"misjudgment rate\"\". Aiming at the diversity and complexity of data missing forms in power big data, this paper improved the Random Forest imputation algorithm, which can be applied to various forms of missing data, especially the blocked missing data and even some horizontal or vertical data completely missing. The data in this paper were from real data of 44 transformer districts of a certain 10kV line in distribution network. Experimental results showed that outlier detection was accurate and suitable for any shape and multidimensional power big data. The improved Random Forest imputation algorithm was suitable for all missing forms, with higher imputation accuracy and better model stability. By comparing the network loss prediction between the data using this data cleaning method and the data removing outliers and missing values, it was found that the accuracy of network loss prediction had been improved by nearly 4 percentage points using the data cleaning method mentioned in this paper. Additionally, as the proportion of bad data increased, the difference between the prediction accuracy of cleaned data and that of uncleaned data was greater.\""|10.17775/CSEEJPES.2020.04080|||||2020|A big data cleaning method based on improved CLOF and Random Forest for distribution network|Liu, Jie and Cao, Yijia and Li, Yong and Guo, Yixiu and Deng, Wei|article|9299499|||CSEE Journal of Power and Energy Systems|20960042|||||||21101017898|0,118|Q4|4|43|148|1634|27|148|0,18|38,00|United States|Northern America|2020|Electrical and Electronic Engineering (Q4); Electronic, Optical and Magnetic Materials (Q4); Energy (miscellaneous) (Q4)|1,205|3.938|0.0027|433984467|1925416150
ACM SE '14|Kennesaw, Georgia|normal mixture models, EM algorithm, outlier detection, data mining, mahalanobis distance, k-means clustering algorithm|4||Proceedings of the 2014 ACM Southeast Regional Conference|Mining outliers has become more and more important in recent years. It has wide applications in military surveillance for enemy activities, detection of potential terrorist attacks, credit card fraud detection, network intrusion, computer virus attack, clinical trials, severe weather prediction, athlete performance analysis, and many other data mining tasks. In today's big data age, multivariate data sets are very complex. Variables among different dimensions are usually correlated with different variations. Classical data mining methods with Euclidean distance measure are not working well for mining multivariate outliers. In this study, we propose a normal mixture model-based framework of multivariate outlier detection. We fit the model parameters through the robust EM algorithm. The K-means clustering algorithm is used to provide the initial inputs for the EM algorithm. The well-know Mahalanobis distance is used to determine the cutoff points for outlier detection via the chi-square distribution critical values. Implementation details of this framework are also discussed.|10.1145/2638404.2638526|https://doi.org/10.1145/2638404.2638526|New York, NY, USA|Association for Computing Machinery|9781450329231|2014|Mining Multivariate Outliers: A Mixture Model-Based Framework|Wang, Maximilian J. and Mao, Guifen and Chen, Haiquan|inproceedings|10.1145/2638404.2638526|51||||||||||||||||||||||||||||434295135|42
||Small and medium-sized enterprises, Data-driven PHM, Industrial data management, Data quality metrics, PHM implementation strategy||23-36||The fourth industrial revolution is derived from advances in digitization and prognostic and health management (PHM) disciplines to make plants smarter and more efficient. However, an adapted approach for data-driven PHM process implementation in small and medium-sized enterprises (SMEs) has not been yet discussed. This research gap is due to the specificities of SMEs and the lack of documentation. In this paper, we examine existing standards for implementing PHM in the industrial field and discuss the limitations within SMEs. Based on that, a novel strategy to implement a data-driven PHM approach in SMEs is proposed. Accordingly, the data management process and the impact of data quality are reviewed to address some critical data problems in SMEs (e.g., data volume and data accuracy). A first set of simulations was carried out to study the impact of the data volume and percentage of missing data on classification problems in PHM. A general model of the evolution of the results accuracy in function of data volume and missing data is then generated, and an economic data volume notion is proposed for data infrastructure resizing. The proposed strategy and the developed models are then applied to the Scoder enterprise, which is a French SME. The feedback on the first results of this application is reported and discussed.|https://doi.org/10.1016/j.jmsy.2020.04.002|https://www.sciencedirect.com/science/article/pii/S0278612520300467||||2020|Industrial data management strategy towards an SME-oriented PHM|N. Omri and Z. {Al Masry} and N. Mairot and S. Giampiccolo and N. Zerhouni|article|OMRI202023|||Journal of Manufacturing Systems|02786125||56|||||14966|2,310|Q1|70|155|294|8906|2949|288|10,88|57,46|Netherlands|Western Europe|1982-2020|Control and Systems Engineering (Q1); Hardware and Architecture (Q1); Industrial and Manufacturing Engineering (Q1); Software (Q1)|5,413|8.633|0.00561|436274017|619364890
||Companies;Big Data;Data visualization;Social networking (online);Decision making;Data mining;Data centers||121-149|Data Control: Major Challenge for the Digital Society|The exploitation of Big Data requires reconsidering the processes of data collection, processing and management. There is a need for governance to classify data and prioritize analytical priorities. The challenge of data value is to specify and determine which data are intelligently usable. Data governance combines a set of people, processes and technologies to ensure the quality and value of an organization's data. The technological part of data governance combines data quality, integration and management. All the fields that complement business intelligence, such as knowledge management, information protection, lobbying, can be grouped together in the overall concept of strategic intelligence. Strategic economic intelligence is a mode of governance whose purpose is the control and protection of strategic and relevant information for any economic actor. Economic intelligence can be seen as a new managerial practice at the service of the company's strategy, enabling it to improve its competitiveness.|10.1002/9781119779780.ch6|https://ieeexplore.ieee.org/document/9823132||Wiley|9781119779810|2020|From Knowledge to Strategic Business Intelligence|Monino, Jean-Louis|inbook|9823132|||||||||||||||||||||||||||||438161537|42
SCA '19|Casablanca, Morocco|smart city, quality of information (QoI), internet of things (IoT)|7||Proceedings of the 4th International Conference on Smart City Applications|Lately, the world attention is directed to transforming daily life to a smarter one, we cannot deny the smart city concept that became pervading. This concept will give every device the chance to communicate with other devices, it will simply create the smarter version of everything. However, data heterogeneity and quality changes are one of the best priorities and challenges that should be handled in this promising concept.In this paper we present a review about data categories circulating in a smart city depending on its required services. We also study the quality of information as one of both, major challenges and treasures in a smart city.|10.1145/3368756.3368965|https://doi.org/10.1145/3368756.3368965|New York, NY, USA|Association for Computing Machinery|9781450362894|2019|Study of Smart City Data: Categories and Quality Challenges|Rhazal, Oumaima El and Tomader, Mazri|inproceedings|10.1145/3368756.3368965|4||||||||||||||||||||||||||||438362959|42
dg.o '18|Delft, The Netherlands|social care, data analytics, Birmingham, service provision, local authority, spatio-temporal analysis|8||Proceedings of the 19th Annual International Conference on Digital Government Research: Governance in the Data Age|There is significant national interest in tackling issues surrounding the needs of vulnerable children and adults. At the same time, UK local authorities face severe financial challenges as a result of decreasing financial settlements and increasing demands from growing urban populations. This research employs state-of-the-art data analytics and visualisation techniques to analyse six years of local government social care data for the city of Birmingham, the UK's second most populated city. This analysis identifies: (i) service cost profiles over time; (ii) geographical dimensions to service demand and delivery; (iii) patterns in the provision of services, and (iv) the extent to which data value and data protection interact. The research accesses data held by the local authority to discover patterns and insights that may assist in the understanding of service demand, support decision making and resource management, whilst protecting and safeguarding its most vulnerable citizens. The use of data in this manner could also inform the approach a local authority has to its data, its capture and use, and the potential for supporting data-led management and service improvements.|10.1145/3209281.3209300|https://doi.org/10.1145/3209281.3209300|New York, NY, USA|Association for Computing Machinery|9781450365260|2018|Big Data Analytics in Social Care Provision: Spatial and Temporal Evidence from Birmingham|Chotvijit, Sarunkorn and Thiarai, Malkiat and Jarvis, Stephen|inproceedings|10.1145/3209281.3209300|5||||||||||||||||||||||||||||438442980|42
||Performance, Incremental, Knowledge base construction|25|81–105||Populating a database with information from unstructured sources--also known as knowledge base construction (KBC)--is a long-standing problem in industry and research that encompasses problems of extraction, cleaning, and integration. In this work, we describe DeepDive, a system that combines database and machine learning ideas to help develop KBC systems, and we present techniques to make the KBC process more efficient. We observe that the KBC process is iterative, and we develop techniques to incrementally produce inference results for KBC systems. We propose two methods for incremental inference, based, respectively, on sampling and variational techniques. We also study the trade-off space of these methods and develop a simple rule-based optimizer. DeepDive includes all of these contributions, and we evaluate DeepDive on five KBC systems, showing that it can speed up KBC inference tasks by up to two orders of magnitude with negligible impact on quality.|10.1007/s00778-016-0437-2|https://doi.org/10.1007/s00778-016-0437-2|Berlin, Heidelberg|Springer-Verlag||2017|Incremental Knowledge Base Construction Using DeepDive|Sa, Christopher and Ratner, Alex and R\'{e}, Christopher and Shin, Jaeho and Wang, Feiran and Wu, Sen and Zhang, Ce|article|10.1007/s00778-016-0437-2||feb|The VLDB Journal|10668888|1|26|February  2017||||13646|0,653|Q1|90|64|117|4824|544|113|4,46|75,38|United States|Northern America|1992-2020|Hardware and Architecture (Q1); Information Systems (Q1)|2,106|2.868|0.0023|438670896|924310569
|||12|1310–1321||Populating a database with unstructured information is a long-standing problem in industry and research that encompasses problems of extraction, cleaning, and integration. Recent names used for this problem include dealing with dark data and knowledge base construction (KBC). In this work, we describe DeepDive, a system that combines database and machine learning ideas to help develop KBC systems, and we present techniques to make the KBC process more efficient. We observe that the KBC process is iterative, and we develop techniques to incrementally produce inference results for KBC systems. We propose two methods for incremental inference, based respectively on sampling and variational techniques. We also study the tradeoff space of these methods and develop a simple rule-based optimizer. DeepDive includes all of these contributions, and we evaluate DeepDive on five KBC systems, showing that it can speed up KBC inference tasks by up to two orders of magnitude with negligible impact on quality.|10.14778/2809974.2809991|https://doi.org/10.14778/2809974.2809991||VLDB Endowment||2015|Incremental Knowledge Base Construction Using DeepDive|Shin, Jaeho and Wu, Sen and Wang, Feiran and De Sa, Christopher and Zhang, Ce and R\'{e}, Christopher|article|10.14778/2809974.2809991||jul|Proc. VLDB Endow.|21508097|11|8|July 2015||||21100199855|0,946|Q1|134|246|598|12401|3759|566|5,50|50,41|United States|Northern America|2008-2020|Computer Science (miscellaneous) (Q1)|7,198|2.047|0.00891|438670896|1216159931
||Big data analytics, Business value, Capability building view, Resource-based theory, Information technology source management, Health care industries||287-299||Although big data analytics have tremendous benefits for healthcare organizations, extant research has paid insufficient attention to the exploration of its business value. In order to bridge this knowledge gap, this study proposes a big data analytics-enabled business value model in which we use the resource-based theory (RBT) and capability building view to explain how big data analytics capabilities can be developed and what potential benefits can be obtained by these capabilities in the health care industries. Using this model, we investigate 109 case descriptions, covering 63 healthcare organizations to explore the causal relationships between the big data analytics capabilities and business value and the path-to-value chains for big data analytics success. Our findings provide new insights to healthcare practitioners on how to constitute big data analytics capabilities for business transformation and offer an empirical basis that can stimulate a more detailed investigation of big data analytics implementation.|https://doi.org/10.1016/j.jbusres.2016.08.002|https://www.sciencedirect.com/science/article/pii/S0148296316304891||||2017|Exploring the path to big data analytics success in healthcare|Yichuan Wang and Nick Hajli|article|WANG2017287|||Journal of Business Research|01482963||70|||||20550|2,049|Q1|195|878|1342|73221|11507|1315|7,38|83,40|United States|Northern America|1973-2021|Marketing (Q1)|46,935|7.550|0.03523|439003007|1502892296
||Healthcare, Security, Internet of Things, Artificial intelligence, Machine learning, Deep learning, 5G networks||107691||Healthcare applications demand systematic approaches to eradicate inevitable human errors to design a framework that systematically eliminates cyber-threats. The key focus of this paper is to provide a comprehensive survey on the use of modern enabling technologies, such as the Internet of Things (IoT), 5G networks, artificial intelligence (AI), and big data analytics, for providing secure and resilient healthcare solutions. A detailed taxonomy of existing technologies has been demonstrated for tackling various healthcare problems, along with their security-related issues in handling healthcare data. The application areas of each of the emerging technologies, along with their security aspects, are explained. Furthermore, an IoT-enabled smart pill bottle prototype is designed and illustrated as a case study for providing better understanding of the subject. Finally, various key research challenges are summarized with future research directions.|https://doi.org/10.1016/j.compeleceng.2022.107691|https://www.sciencedirect.com/science/article/pii/S0045790622000131||||2022|A holistic survey on the use of emerging technologies to provision secure healthcare solutions|Senthil Kumar Jagatheesaperumal and Preeti Mishra and Nour Moustafa and Rahul Chauhan|article|JAGATHEESAPERUMAL2022107691|||Computers and Electrical Engineering|00457906||99|||||18159|0,630|Q1|64|298|1040|7464|4626|979|4,79|25,05|United Kingdom|Western Europe|1973-1984, 1986-2020|Computer Science (miscellaneous) (Q1); Control and Systems Engineering (Q2); Electrical and Electronic Engineering (Q2)||||439877556|1285201041
||Myopia prevention and control, Artificial intelligent, National multicenter project||51-55||In recent years, the incidence of myopia has increased at an alarming rate among children and adolescents in China. The exploration of an effective prevention and control method for myopia is in urgent need. With the development of information technology in the past decade, artificial intelligence with the Internet of Things technology (AIoT) is characterized by strong computing power, advanced algorithm, continuous monitoring, and accurate prediction of long-term progression. Therefore, big data and artificial intelligence technology have the potential to be applied to data mining of myopia etiology and prediction of myopia occurrence and development. More recently, there has been a growing recognition that myopia study involving AIoT needs to undergo a rigorous evaluation to demonstrate robust results.|https://doi.org/10.1016/j.imed.2021.05.001|https://www.sciencedirect.com/science/article/pii/S2667102621000085||||2021|The national multi-center artificial intelligent myopia prevention and control project|Xun Wang and Yahan Yang and Yuxuan Wu and Wenbin Wei and Li Dong and Yang Li and Xingping Tan and Hankun Cao and Hong Zhang and Xiaodan Ma and Qin Jiang and Yunfan Zhou and Weihua Yang and Chaoyu Li and Yu Gu and Lin Ding and Yanli Qin and Qi Chen and Lili Li and Mingyue Lian and Jin Ma and Dongmei Cui and Yuanzhou Huang and Wenyan Liu and Xiao Yang and Shuiming Yu and Jingjing Chen and Dongni Wang and Zhenzhe Lin and Pisong Yan and Haotian Lin|article|WANG202151|||Intelligent Medicine|26671026|2|1|||||||||||||||||||||||440948609|714385722
CAIH2020|Taiyuan, China|visualization, Data journalism, information cocoons, data opening|5|127–131|Proceedings of the 2020 Conference on Artificial Intelligence and Healthcare|In the field of news, with the operation of data journalism, the traditional press is facing great innovation and shock in the production, circulation, distribution and consumption of information. As McLuhan said, the birth of new media has opened up new possibilities in this era. Data, as a medium of the new era, is creating a new way for people to understand the world.This paper mainly discusses that it is still facing the problem of low degree of data opening in the current development, and the negative impact of disclosing users' personal privacy and information cocoon room. In view of these problems, relevant departments need to further strengthen the policy of data opening, improve the legal system, and optimize the link mode of information content dissemination, so as to promote the better development of data journalism and make data benefit people truly.|10.1145/3433996.3434019|https://doi.org/10.1145/3433996.3434019|New York, NY, USA|Association for Computing Machinery|9781450388641|2020|Development Dilemma and Countermeasures of Data Journalism|Li, Ting and Zhang, Bo|inproceedings|10.1145/3433996.3434019|||||||||||||||||||||||||||||442358932|42
MK Series on Business Intelligence||Big Data, data warehousing, sentiments, social media, machine data||3-14|Data Warehousing in the Age of Big Data|Why this book? Why now? The goal of this book is to provide readers with a concise perspective into the biggest buzz in the industry—Big Data—and, more importantly, its impact on data processing, management, decision support, and data warehousing. At the time of this writing, there is a lot of interest to adopt a Big Data solution, but the profound confusion is what is the future of data warehousing and many investments that have been made over the years into building the decision support platform. This book addresses those areas of concern and provides readers an introduction to the next-generation of data management and data warehousing. This chapter provides you a concise and example driven introduction to what is Big Data, and how any organization needs to understand the value of Big Data.|https://doi.org/10.1016/B978-0-12-405891-0.00001-5|https://www.sciencedirect.com/science/article/pii/B9780124058910000015|Boston|Morgan Kaufmann|978-0-12-405891-0|2013|Chapter 1 - Introduction to Big Data|Krish Krishnan|incollection|KRISHNAN20133||||||||||Krish Krishnan|||||||||||||||||||445123406|42
||Arable crops, Yield gaps, Crop ecology, Crop coefficients (kc), The Netherlands||107828||Yield gaps and water productivity are key indicators to monitor the progress towards more sustainable and productive cropping systems. Individual farmers are collecting increasing amounts of data (‘big data’), which can help monitor the process of sustainable intensification at local level. In this study, we build upon such data to quantify the magnitude and identify the biophysical and management determinants of on-farm yield gaps and water productivity for the main arable crops cultivated in the Netherlands. The analysis focused on ware, seed and starch potatoes, sugar beet, spring onion, winter wheat and spring barley and covered the period 2015–2017. A crop modelling approach based on crop coefficients (kc) and daily weather data was used to estimate the potential yield (Yp), radiation intercepted and potential evapotranspiration (ETP) for each crop. Yield gaps were estimated to be ca. 10% of Yp for sugar beet, 25–30% of Yp for ware, seed and starch potato and spring barley, and 35–40% of Yp for spring onion and winter wheat. Variation in actual yields was associated with water availability in key periods of the growing season as well as with sowing and harvest dates. However, the R2 of the fitted regressions was rather low (20–49%). Current levels of crop water productivity ranged between 13 kg DM ha−1 mm−1 for spring barley, ca. 15 kg DM ha−1 mm−1 for seed potato, spring onion and winter wheat, 23 kg DM ha−1 mm−1 for ware potato and ca. 25 kg DM ha−1 mm−1 for starch potato and sugar beet. These values are about half of their potential, but increasing actual water productivity further is restricted by rainfall amount and distribution. However, doing so should not be prioritized over reducing environmental impacts of these intensive cropping systems in the short-term and may require large investments from farm to regional levels in the long-term. Although these findings are most relevant to similar cropping systems in NW Europe, the underlying methods are generic and can be used to benchmark crop performance in other cropping systems. Based on this work, we argue that ‘big data’ are currently most useful to describe cropping systems at regional scale and derive benchmarks of farm performance but not as much to predict and explain crop yield variability in time and space.|https://doi.org/10.1016/j.fcr.2020.107828|https://www.sciencedirect.com/science/article/pii/S0378429019318039||||2020|Can big data explain yield variability and water productivity in intensive cropping systems?|João Vasco Silva and Tomás R. Tenreiro and Léon Spätjens and Niels P.R. Anten and Martin K. {van Ittersum} and Pytrik Reidsma|article|SILVA2020107828|||Field Crops Research|03784290||255|||||||||||||||||||||||446073367|166248485
||Big data, Disruption, Responsible, Process industry, Economic efficiency, Economic geography||100105||Disruptive innovations are usually identified as ideas that are created ‘outside the box’. They are expected to fundamentally change existing business models and processes founded on technological applications. Disruptive innovations can be challenging to define. Information technology (IT) solutions focus on collecting, processing, and reporting different types of data. Commonly, is the solutions are expected (in cybernetics or self-regulating processes) to provide feedback to original processes and to steer them based on the data. To achieve continuous improvement with regard to environmental responsibility and profitability, new thinking and, in particular, accurate and reliable data are needed for decision-making. Very large data storages, known as big data, contain an increasing mass of different types of homogenous and non-homogenous information, as well as extensive time-series. New, innovative algorithms are required to reveal relevant information and opportunities hidden in these data storages. Global environmental challenges and zero-emission responsible production issues can only be solved using relevant and reliable continuous data as the basis. The final goal should be the creation of scalable environmental solutions based on disruptive innovations and accurate data. The aim of this paper is to determine the explicit steps for replacing silo-based reporting with company-wide, refined information, which enables decision-makers in all industries the chance to make responsible choices.|https://doi.org/10.1016/j.jii.2019.100105|https://www.sciencedirect.com/science/article/pii/S2452414X19300044||||2019|Industrial applications of big data in disruptive innovations supporting environmental reporting|Esa Hämäläinen and Tommi Inkinen|article|HAMALAINEN2019100105|||Journal of Industrial Information Integration|2452414X||16|||||21100787106|2,042|Q1|24|28|86|2152|1361|83|12,26|76,86|Netherlands|Western Europe|2016-2020|Industrial and Manufacturing Engineering (Q1); Information Systems and Management (Q1)|1,149|10.063|0.00148|446715668|121356201
||Cyber-physical systems (CPS), Wireless Sensor Networks(WSN), Data quality management, Data quality dimensions, Smart cities, Quality of observations||101951||Cyber-physical systems (CPSs) are integrated systems engineered to combine computational control algorithms and physical components such as sensors and actuators, effectively using an embedded communication core. Smart cities can be viewed as large-scale, heterogeneous CPSs that utilise technologies like the Internet of Things (IoT), surveillance, social media, and others to make informed decisions and drive the innovations of automation in urban areas. Such systems incorporate multiple layers and complex structure of hardware, software, analytical algorithms, business knowledge and communication networks, and operate under noisy and dynamic conditions. Thus, large-scale CPSs are vulnerable to enormous technical and operational challenges that may compromise the quality of data of their applications and accordingly reduce the quality of their services. This paper presents a systematic literature review to investigate data quality challenges in smart-cities large-scale CPSs and to identify the most common techniques used to address these challenges. This systematic literature review showed that significant work had been conducted to address data quality management challenges in smart cities, large-scale CPS applications. However, still, more is required to provide a practical, comprehensive data quality management solution to detect errors in sensor nodes’ measurements associated with the main data quality dimensions of accuracy, timeliness, completeness, and consistency. No systematic or generic approach was demonstrated for detecting sensor nodes and sensor node networks failures in large-scale CPS applications. Moreover, further research is required to address the challenges of ensuring the quality of the spatial and temporal contextual attributes of sensor nodes’ observations.|https://doi.org/10.1016/j.is.2021.101951|https://www.sciencedirect.com/science/article/pii/S0306437921001484||||2022|Data quality challenges in large-scale cyber-physical systems: A systematic review|Ahmed Abdulhasan Alwan and Mihaela Anca Ciupala and Allan J. Brimicombe and Seyed Ali Ghorashi and Andres Baravalle and Paolo Falcarin|article|ALWAN2022101951|||Information Systems|03064379||105|||||12305|0,547|Q2|85|100|271|4739|1027|255|3,39|47,39|United Kingdom|Western Europe|1975-2021|Hardware and Architecture (Q2); Information Systems (Q2); Software (Q2)|2,604|2.309|0.00331|448356034|303930735
DPH2019|Marseille, France|bioinformatics, precision medicine, genomics, clinical databases, big data|8|47–54|Proceedings of the 9th International Conference on Digital Public Health|The massive adoption of high-throughput genomics, deep sequencing technologies and big data technologies have made possible the era of precision medicine. However, the volume of data and its complexity remain important challenges for precision medicine research, hindering development in this field. The literature on precision medicine research describes a few platforms to support specific types of studies, but none of these offer researchers the level of customization required to meet their specific needs [1]. Methods: We propose to design and develop a platform able to import and integrate a very large volume of genetics, clinical, demographical and environmental data in a cloud computing infrastructure. In our previous publication, we presented an approach that can customize existing data models to fit any precision medicine research data requirement [1] and the requirement for future large-scale precision medicine platforms, in terms of data extensibility and the scalability of processing on demand. We also proposed a framework to meet the specific requirement of any precision medicine research [2]. In this paper, we describe how this new framework was implemented and trialed by the precision medicine researchers at the Centre Hospitalier Universitaire de l'Universit\'{e} de Montr\'{e}al (CHUM). Results: The data analysis simulations showed that the random forest algorithm presents better accuracy results. We obtained an F1-Score of 72% for random forest, 69% using linear regression and 62% using the neural network algorithm. Conclusion: The results suggest that the proposed precision medicine data analysis platform allows researchers to configure, prepare the analysis environment and customize the platform data model to their specific research in very optimal delays, at very low cost and with minimal technical skills.|10.1145/3357729.3357742|https://doi.org/10.1145/3357729.3357742|New York, NY, USA|Association for Computing Machinery|9781450372084|2019|A Large-Scale and Extensible Platform for Precision Medicine Research|Belghait, Fodil and April, Alain and Hamet, Pavel and Tremblay, Johanne and Desrosiers, Christian|inproceedings|10.1145/3357729.3357742|||||||||||||||||||||||||||||448506041|42
LocWeb '14|Shanghai, China|record linkage, large-scale data, address parsing|4|33–36|Proceedings of the 4th International Workshop on Location and the Web|Record linkage is the task of identifying which records in one or more data collections refer to the same entity, and address is one of the most commonly used fields in databases. Hence, segmentation of the raw addresses into a set of semantic fields is the primary step in this task. In this paper, we present a probabilistic address parsing system based on the Hidden Markov Model. We also introduce several novel approaches of synthetic training data generation to build robust models for noisy real-world addresses, obtaining 95.6% F-measure. Furthermore, we demonstrate the viability and efficiency of this system for large-scale data by scaling it up to parse billions of addresses.|10.1145/2663713.2664430|https://doi.org/10.1145/2663713.2664430|New York, NY, USA|Association for Computing Machinery|9781450314596|2014|HMM-Based Address Parsing with Massive Synthetic Training Data Generation|Li, Xiang and Kardes, Hakan and Wang, Xin and Sun, Ang|inproceedings|10.1145/2663713.2664430|||||||||||||||||||||||||||||449318630|42
SPBPU IDE '21|Saint - Petersburg, Russian Federation|Digitalization of the economy, Analytical procedures, Continuous online auditing, Risk of material misstatement, Financial indicators|6|402–407|Proceedings of the 3rd International Scientific Conference on Innovations in Digital Economy|In the time of digitalization of the economy, the audit of financial reporting is bound to shift from periodic inspections (annual, semi-annual, quarterly) to continuous monitoring of financial information. Continuous monitoring ensures that the control results of activities are produced simultaneously or within the shortest period possible after discovering the relevant events. The risk of possible misstatement of financial statements in the process of this monitoring is assessed using analytical procedures. The source data for them are the values of financial indicators of the audited entity selected by the auditor. Analytical procedures are aimed at obtaining audit evidence and carried out through a number of actions taken to find out, study, analyze and assess the correlations between the financial-economic and other performance indicators of the organization in order to discover non-standard phenomena as well as facts and the causes of these discrepancies. The main purpose of the article was to develop a system of financial indicators, the continuous monitoring of which would allow for assessing the risk of material misstatement in real time. The methodological basis of the research is determined by the following fundamental techniques and principles: analysis, synthesis, attributive and proportional analogies, abstractions, descriptive generalizations, formulation and confirmation of working analytical hypotheses, economic/mathematical and mathematical/statistical methods. The paper analyzes the financial indicators, suggested for these purposes by a number of authors, and highlights the lack of substantiation of the former. The relationships between possible financial indicators and the items of financial statements are studied and a set of indicators has been formed. Their growth rate, if monitored in the online auditing process, makes it possible to assess the risk of material misstatement of accounting information and react accordingly.|10.1145/3527049.3527050|https://doi.org/10.1145/3527049.3527050|New York, NY, USA|Association for Computing Machinery|9781450386944|2022|A System of Financial Indicators for Assessing the Risk of Material Misstatement of Accounting Information in the Context of a Continuous Online Audit|Kochinev, Yury and Antysheva, Elena and Alpysbayev, Kaisar|inproceedings|10.1145/3527049.3527050|||||||||||||||||||||||||||||450047391|42
CSAE 2020|Sanya, China|Opening and sharing of data resource, Big data, Scientific data management, Scientific data|6||Proceedings of the 4th International Conference on Computer Science and Application Engineering|Scientific data is an important strategic resource in the era of big data. Efficient management and wide circulation are the key ways to enhance the value of scientific data resources. With the transformation of the industrial society into the information society, the importance of scientific data management is also increasing all over the world, which continuously promotes the maturity of scientific data management and sharing. In this article, through comprehensive research of scientific data management ideas, policies, practices and results, the analysis summarizes the advanced experience of international scientific data management, for the similar problems and challenges existing in the research in China, puts forward the future a period of time the direction and suggestions on the development of scientific data management: 1. the specification of various kinds of degree of the standardization of scientific data resources; 2. To strengthen data mining capacity; 3. To strengthen the cultivation of talents in data science; 4. To strengthen international cooperation and enhance core competitiveness in the big data era.|10.1145/3424978.3425010|https://doi.org/10.1145/3424978.3425010|New York, NY, USA|Association for Computing Machinery|9781450377720|2020|Research on Scientific Data Management in Big Data Era|Man, Rui and Zhou, Guomin and Fan, Jingchao|inproceedings|10.1145/3424978.3425010|32||||||||||||||||||||||||||||450366455|42
|||8|3–10||"\"This position paper reviews the achievements and open challenges of movement analysis within Geographical Information Science. The paper argues that the simple problems of movement analysis have mostly been addressed to a sufficient level (\"\"the low hanging fruit\"\"), leaving the research community with the much more challenging problems for the years ahead (\"\"the high hanging fruit\"\"). Whereas the community has made good progress in structuring trajectory data (segmentation, similarity, clustering) and conceptualizing and detecting movement patterns, the much harder task of semantic annotation of structures and patterns remains difficult. The position paper summarizes both achievements and challenges with two sets assertions and calls for the establishment of a unifying theory of Computational Movement Analysis.\""|10.1145/2782759.2782762|https://doi.org/10.1145/2782759.2782762|New York, NY, USA|Association for Computing Machinery||2015|The Low Hanging Fruit is Gone: Achievements and Challenges of Computational Movement Analysis|Laube, Patrick|article|10.1145/2782759.2782762||may|SIGSPATIAL Special||1|7|March 2015||||||||||||||||||||||451695647|42
|||36|237–272|Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice|||https://doi.org/10.1145/3447404.3447418|New York, NY, USA|Association for Computing Machinery|9781450390293|2021|Brain–Computer Interfacing with Interactive Systems—Case Study 2|Vourvopoulos, A. and Niforatos, E. and Bermudez i Badia, S. and Liarokapis, Fotis|inbook|10.1145/3447404.3447418|||||||||1||||||||||||||||||||453365970|42
||CPQ, Semantic Technologies, Ontologies, Ontology Matching, Data Quality||307-312||"\"In recent years, due to a progressive complexity of handling and processing business data, proper big data management has become a challenge, especially for SMEs that have limited resources for investing in the requested business transformation process. As a solution, we suggest an ontology-based CPQ software approach, where we show how the implementation of semantic technologies and ontologies affects data integration processes. We also propose a method called \"\"ontology-based data matching\"\", which allows the semiautomatic generation of alignments used to formalize the coherence between ontologies. The proposed method will ensure consistency during integration, significantly improving the productivity of enterprises.\""|https://doi.org/10.1016/j.promfg.2020.11.051|https://www.sciencedirect.com/science/article/pii/S2351978920321958||||2020|Big Data Management Using Ontologies for CPQ Solutions|Alexander Binder and Eva-Maria Iwer and Werner Quint|article|BINDER2020307|||Procedia Manufacturing|23519789||52||System-Integrated Intelligence – Intelligent, Flexible and Connected Systems in Products and ProductionProceedings of the 5th International Conference on System-Integrated Intelligence (SysInt 2020), Bremen, Germany|||21100792109|0,504|Q2|43|1390|3628|27760|8346|3572|1,79|19,97|Netherlands|Western Europe|2015-2020|Artificial Intelligence (Q2); Industrial and Manufacturing Engineering (Q2)||||454606084|896540749
||grain big data;data cleaning;task merging;hadoop;mapReduce||225-233|2019 6th International Conference on Information Science and Control Engineering (ICISCE)|Data quality has exerted important influence over the application of grain big data, so data cleaning is a necessary and important work. In MapReduce frame, we can use parallel technique to execute data cleaning in high scalability mode, but due to the lack of effective design there are amounts of computing redundancy in the process of data cleaning, which results in lower performance. In this research, we found some tasks often are carried out multiple times on same input files, or require same operation results in the process of data cleaning. For this problem, we proposed a new optimization technique that is based on task merge. By merging simple or redundancy computations on same input files, the number of the loop computation in MapReduce can be reduced greatly. The experiment shows, by this means, the overall system runtime is significantly reduced, which proves that the process of data cleaning is optimized. In this paper, we optimized several modules of data cleaning such as entity identification, inconsistent data restoration, and missing value filling. Experimental results show that the proposed method in this paper can increase efficiency for grain big data cleaning.|10.1109/ICISCE48695.2019.00053|||||2019|Data Cleaning Optimization for Grain Big Data Processing using Task Merging|Ju, Xingang and Lian, Feiyu and Zhang, Yuan|inproceedings|9107804||Dec|||||||||||||||||||||||||||455111823|42
||text input, vibration intelligence, micro finger writing, wearable devices|25|||Wearable devices, such as smartwatches and head-mounted devices (HMD), demand new input devices for a natural, subtle, and easy-to-use way to input commands and text. In this paper, we propose and investigate ViFin, a new technique for input commands and text entry, which harness finger movement induced vibration to track continuous micro finger-level writing with a commodity smartwatch. Inspired by the recurrent neural aligner and transfer learning, ViFin recognizes continuous finger writing, works across different users, and achieves an accuracy of 90% and 91% for recognizing numbers and letters, respectively. We quantify our approach's accuracy through real-time system experiments in different arm positions, writing speeds, and smartwatch position displacements. Finally, a real-time writing system and two user studies on real-world tasks are implemented and assessed.|10.1145/3448119|https://doi.org/10.1145/3448119|New York, NY, USA|Association for Computing Machinery||2021|ViFin: Harness Passive Vibration to Continuous Micro Finger Writing with a Commodity Smartwatch|Chen, Wenqiang and Chen, Lin and Ma, Meiyi and Parizi, Farshid Salemi and Patel, Shwetak and Stankovic, John|article|10.1145/3448119|45|mar|Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.||1|5|March 2021||||||||||||||||||||||460197537|42
GECCO '20|Canc\'{u}n, Mexico||18|1206–1223|Proceedings of the 2020 Genetic and Evolutionary Computation Conference Companion||10.1145/3377929.3389894|https://doi.org/10.1145/3377929.3389894|New York, NY, USA|Association for Computing Machinery|9781450371278|2020|Addressing Ethical Challenges within Evolutionary Computation Applications: GECCO 2020 Tutorial|Torresen, Jim|inproceedings|10.1145/3377929.3389894|||||||||||||||||||||||||||||460537851|42
||Tourism forecasting, Social network analysis, Semantic analysis, Online community, Text mining, Big data||113075||Forecasting tourism demand has important implications for both policy makers and companies operating in the tourism industry. In this research, we applied methods and tools of social network and semantic analysis to study user-generated content retrieved from online communities which interacted on the TripAdvisor travel forum. We analyzed the forums of 7 major European capital cities, over a period of 10 years, collecting more than 2,660,000 posts, written by about 147,000 users. We present a new methodology of analysis of tourism-related big data and a set of variables which could be integrated into traditional forecasting models. We implemented Factor Augmented Autoregressive and Bridge models with social network and semantic variables which often led to a better forecasting performance than univariate models and models based on Google Trend data. Forum language complexity and the centralization of the communication network – i.e. the presence of eminent contributors – were the variables that contributed more to the forecasting of international airport arrivals.|https://doi.org/10.1016/j.dss.2019.113075|https://www.sciencedirect.com/science/article/pii/S0167923619301046||||2019|Using social network and semantic analysis to analyze online travel forums and forecast tourism demand|Andrea {Fronzetti Colladon} and Barbara Guardabascio and Rosy Innarella|article|FRONZETTICOLLADON2019113075|||Decision Support Systems|01679236||123|||||21933|1,564|Q1|151|115|342|7152|2672|338|7,04|62,19|Netherlands|Western Europe|1985-2020|Arts and Humanities (miscellaneous) (Q1); Developmental and Educational Psychology (Q1); Information Systems (Q1); Information Systems and Management (Q1); Management Information Systems (Q1)|13,580|5.795|0.0081|462172107|1234879127
||Financial database, data quality, stock market, data integration|23|||This article presents the technical aspects of designing and building a historical database of the Italian Stock Market. The database contains daily market data from 1973 to 2011 and is constructed by merging two main digital sources and several other hand-collected data sources. We analyzed and developed semiautomatic tools to deal with problems related to time-series matchings, quality of data, and numerical errors. We also developed a concatenation structure to allow the handling of company name changes, mergers, and spin-offs without artificially altering numerical series. At the same time, we maintained the transparency of the historical information on each individual company listed. Thanks to the overlapping of digital and hand-collected data, the completed database has a very high level of detail and accuracy. The dataset is particularly suited for any empirical research in financial economics and for more practically oriented numerical applications and forecasting simulations.|10.1145/2822898|https://doi.org/10.1145/2822898|New York, NY, USA|Association for Computing Machinery||2015|Design and Construction of a Historical Financial Database of the Italian Stock Market 1973--2011|Coletti, Paolo and Murgia, Maurizio|article|10.1145/2822898|16|oct|J. Data and Information Quality|19361955|4|6|October 2015||||||||||||||||||||||463481269|833754770
||Big data, Occupant behaviour, Energy modelling, Mobility, Urban data, Sensors, Machine learning, Energy in buildings, Energy in cities||106964||The proliferation of urban sensing, IoT, and big data in cities provides unprecedented opportunities for a deeper understanding of occupant behaviour and energy usage patterns at the urban scale. This enables data-driven building and energy models to capture the urban dynamics, specifically the intrinsic occupant and energy use behavioural profiles that are not usually considered in traditional models. Although there are related reviews, none investigated urban data for use in modelling occupant behaviour and energy use at multiple scales, from buildings to neighbourhood to city. This survey paper aims to fill this gap by providing a critical summary and analysis of the works reported in the literature. We present the different sources of occupant-centric urban data that are useful for data-driven modelling and categorise the range of applications and recent data-driven modelling techniques for urban behaviour and energy modelling, along with the traditional stochastic and simulation-based approaches. Finally, we present a set of recommendations for future directions in data-driven modelling of occupant behaviour and energy in buildings at the urban scale.|https://doi.org/10.1016/j.buildenv.2020.106964|https://www.sciencedirect.com/science/article/pii/S0360132320303231||||2020|Modelling urban-scale occupant behaviour, mobility, and energy in buildings: A survey|Flora D. Salim and Bing Dong and Mohamed Ouf and Qi Wang and Ilaria Pigliautile and Xuyuan Kang and Tianzhen Hong and Wenbo Wu and Yapan Liu and Shakila Khan Rumi and Mohammad Saiedur Rahaman and Jingjing An and Hengfang Deng and Wei Shao and Jakub Dziedzic and Fisayo Caleb Sangogboye and Mikkel Baun Kjærgaard and Meng Kong and Claudia Fabiani and Anna Laura Pisello and Da Yan|article|SALIM2020106964|||Building and Environment|03601323||183|||||26874|1,736|Q1|154|754|1692|43938|12000|1687|6,90|58,27|United Kingdom|Western Europe|1976-2020|Building and Construction (Q1); Civil and Structural Engineering (Q1); Environmental Engineering (Q1); Geography, Planning and Development (Q1)|38,699|6.456|0.02925|464155955|723885742
||Big data, Data storage, Spatio-temporal querying, Decision-making, Earth observation, Disaster management||1-14||The rapid increase in the number of Earth Observation (EO) systems generates a massive amount of heterogeneous data. It has raised big issues in collecting, preprocessing, storing, and the visualization these data. However, traditional techniques are facing serious challenges when dealing with big EO data dimensions (i.e., Volume, Veracity, Variety, and Velocity), especially in natural hazards management. Therefore, big data techniques and tools attract more attention. In this paper we propose a multidimensional model framework for Big EO data warehousing. This framework includes 3 parts: (1) Data collection and preprocessing, being responsible for collecting data and improving their quality; (2) Data loading and storage, performing the ingestion task which consists of transferring the data from external resources to the Big data platform for storage; and (3) Visualization and interpretation, aiming to provide spatio-temporal analysis. This framework could be useful for decision-makers in monitoring the effects of drought disasters and, consequently, planning the mitigation and remediation measures. Experiments are carried out on drought monitoring in China along the period 2000–2020. The input data include remote sensing data, biophysical data, and climatological data. The results reveal that the proposed framework has a higher retrieval speed and a greater elasticity with different kinds (i.e. spatial, temporal, or spatiotemporal) of requests compared to traditional frameworks, indicating its superiority.|https://doi.org/10.1016/j.future.2022.05.010|https://www.sciencedirect.com/science/article/pii/S0167739X22001753||||2022|Multidimensional architecture using a massive and heterogeneous data: Application to drought monitoring|Hanen Balti and Ali Ben Abbes and Nedra Mellouli and Imed Riadh Farah and Yanfang Sang and Myriam Lamolle|article|BALTI20221|||Future Generation Computer Systems|0167739X||136|||||12264|1,262|Q1|119|798|1935|36054|16877|1868|9,11|45,18|Netherlands|Western Europe|1984-2021|Computer Networks and Communications (Q1); Hardware and Architecture (Q1); Software (Q1)||||464568719|562237118
SIGSPATIAL '20|Seattle, WA, USA|Image Preprocessing, Deep Learning, Geospatial Data Gateway, Scientific Reproducibility, Geospatial Big Data, Remote Sensing|4|593–596|Proceedings of the 28th International Conference on Advances in Geographic Information Systems|Geospatial data providers have adopted a variety of science gateways as the primary method for accessing remote geospatial data. Early systems provided little more than a simple file transfer mechanism but over the past decade, advanced features were incorporated to allow users to retrieve data seamlessly without concern for native file formats, data resolution, or even spatial projections. However, the recent growth in Deep Learning models in the geospatial domains has exposed additional requirements for accessing geospatial repositories. In this paper we discussed the major data accessibility challenges faced by the Deep Learning community namely: (1) reproducibility of data preprocessing workflows, (2) optimizing data transfer between gateways and computational environments, and (3) minimizing local storage requirements using on-the-fly augmentation. In this paper, we present our vision of spatial data generators to act as middleware between geospatial data gateways and Deep Learning models. We propose advanced features for spatial data generators and describe how they could satisfy the data accessibility requirements of the geospatial Deep Learning community. Lastly, we argue that satisfying these data accessibility requirements will not only enhance the reproducibility of Deep Learning workflows and speed their development but will also improve the quality of training and prediction of operational Deep Learning models.|10.1145/3397536.3422232|https://doi.org/10.1145/3397536.3422232|New York, NY, USA|Association for Computing Machinery|9781450380195|2020|Leveraging Geospatial Data Gateways to Support the Operational Application of Deep Learning Models: Vision Paper|Soliman, Aiman and Terstriep, Jeffrey|inproceedings|10.1145/3397536.3422232|||||||||||||||||||||||||||||464759453|42
||Elevators;Inspection;Big Data;Safety;Market research;Testing;Monitoring;Elevator;inspection;big data;quality evaluation||238-242|2019 IEEE 9th International Conference on Electronics Information and Emergency Communication (ICEIEC)|Elevator inspection information has typical big data characteristics. This paper points out that the elevator inspection data introduces the method of elevator inspection big data analysis. Taking elevator inspection as an example, it lists several kinds of big data analysis methods for inspection data, including the risk points describing the basic information of the elevator, the scanning inspection process and the inspection quality. Based on frequency analysis of active factors, outlier test, quality assessment, correlation analysis. Using big data technology, it can make statistical analysis on the data obtained by elevator inspection, make the inspection situation more intuitive, help the management organization to understand the overall elevator quality and elevator inspection, and build an elevator inspection quality evaluation system to make the work more transparent and management more precise. Find more accurate questions, deeper supervision, and more scientific government decisions.|10.1109/ICEIEC.2019.8784484|||||2019|Construction of Elevator Inspection Quality Evaluation System Based on Big Data|Zhang, Xupeng and Liang, Du|inproceedings|8784484||July||2377844X|||||||||||||||||||||||||466714845|967917689
SIGMOD '16|San Francisco, California, USA|information extraction, focused crawling, massively parallel data analysis|13|759–771|Proceedings of the 2016 International Conference on Management of Data|"\"In many domains, a plethora of textual information is available on the web as news reports, blog posts, community portals, etc. Information extraction (IE) is the default technique to turn unstructured text into structured fact databases, but systematically applying IE techniques to web input requires highly complex systems, starting from focused crawlers over quality assurance methods to cope with the HTML input to long pipelines of natural language processing and IE algorithms. Although a number of tools for each of these steps exists, their seamless, flexible, and scalable combination into a web scale end-to-end text analytics system still is a true challenge. In this paper, we report our experiences from building such a system for comparing the \"\"web view\"\" on health related topics with that derived from a controlled scientific corpus, i.e., Medline. The system combines a focused crawler, applying shallow text analysis and classification to maintain focus, with a sophisticated text analytic engine inside the Big Data processing system Stratosphere. We describe a practical approach to seed generation which led us crawl a corpus of ~1 TB web pages highly enriched for the biomedical domain. Pages were run through a complex pipeline of best-of-breed tools for a multitude of necessary tasks, such as HTML repair, boilerplate detection, sentence detection, linguistic annotation, parsing, and eventually named entity recognition for several types of entities. Results are compared with those from running the same pipeline (without the web-related tasks) on a corpus of 24 million scientific abstracts and a third corpus made of ~250K scientific full texts. We evaluate scalability, quality, and robustness of the employed methods and tools. The focus of this paper is to provide a large, real-life use case to inspire future research into robust, easy-to-use, and scalable methods for domain-specific IE at web scale.\""|10.1145/2882903.2903736|https://doi.org/10.1145/2882903.2903736|New York, NY, USA|Association for Computing Machinery|9781450335317|2016|Potential and Pitfalls of Domain-Specific Information Extraction at Web Scale|"\"Rheinl\"\"{a}nder, Astrid and Lehmann, Mario and Kunkel, Anja and Meier, J\\\"\"{o}rg and Leser, Ulf\""|inproceedings|10.1145/2882903.2903736|||||||||||||||||||||||||||||469192381|42
||schema matching, data labeling, Deep learning|3|||This editorial summarizes the content of the Special Issue on Deep Learning for Data Quality of the Journal of Data and Information Quality (JDIQ).|10.1145/3513135|https://doi.org/10.1145/3513135|New York, NY, USA|Association for Computing Machinery||2022|Editorial: Special Issue on Deep Learning for Data Quality|Santoro, Donatello and Thirumuruganathan, Saravanan and Papotti, Paolo|article|10.1145/3513135|14|aug|J. Data and Information Quality|19361955|3|14|September 2022||||||||||||||||||||||471505967|833754770
||Knowledge graph, Big data, Data integration, Toxicology, Drug design, Chemical safety||4837-4849||Big Data pervades nearly all areas of life sciences, yet the analysis of large integrated data sets remains a major challenge. Moreover, the field of life sciences is highly fragmented and, consequently, so is its data, knowledge, and standards. This, in turn, makes integrated data analysis and knowledge gathering across sub-fields a demanding task. At the same time, the integration of various research angles and data types is crucial for modelling the complexity of organisms and biological processes in a holistic manner. This is especially valid in the context of drug development and chemical safety assessment where computational methods can provide solutions for the urgent need of fast, effective, and sustainable approaches. At the same time, such computational methods require the development of methodologies suitable for an integrated and data centred Big Data view. Here we discuss Knowledge Graphs (KG) as a solution to a data centred analysis approach for drug and chemical development and safety assessment. KGs are knowledge bases, data analysis engines, and knowledge discovery systems all in one, allowing them to be used from simple data retrieval, over meta-analysis to complex predictive and knowledge discovery systems. Therefore, KGs have immense potential to advance the data centred approach, the re-usability, and informativity of data. Furthermore, they can improve the power of analysis, and the complexity of modelled processes, all while providing knowledge in a natively human understandable network data model.|https://doi.org/10.1016/j.csbj.2022.08.061|https://www.sciencedirect.com/science/article/pii/S2001037022003956||||2022|The potential of a data centred approach & knowledge graph data representation in chemical safety and drug design|Alisa Pavel and Laura A. Saarimäki and Lena Möbus and Antonio Federico and Angela Serra and Dario Greco|article|PAVEL20224837|||Computational and Structural Biotechnology Journal|20010370||20|||||21100318415|1,908|Q1|45|357|255|28093|2022|255|7,89|78,69|Sweden|Western Europe|2012-2020|Biochemistry (Q1); Biophysics (Q1); Biotechnology (Q1); Computer Science Applications (Q1); Genetics (Q1); Structural Biology (Q2)|3,620|7.271|0.00677|471598271|175296720
GROUP '16|Sanibel Island, Florida, USA|data analytics, mythology, big data, business intelligence|10|325–334|Proceedings of the 19th International Conference on Supporting Group Work|"\"We present results from a case study of the use of business intelligence systems in a human services organization. We characterize four mythologies of business intelligence that informants experience as shared organizational values and are core to their trajectory towards a \"\"culture of data\"\": data-driven, predictive and proactive, shared accountability, and inquisitive. Yet, for each mythology, we also discuss the ways in which being actionable is impeded by a disconnect between the aggregate views of data that allows them to identify areas of focus for decision making and the desired \"\"drill down\"\" views of data that would allow them to understand how to act in a data-driven context. These findings contribute initial empirical evidence for the impact of business intelligence's epistemological biases on organizations and suggest implications for the design of technologies to better support data-driven decision making.\""|10.1145/2957276.2957283|https://doi.org/10.1145/2957276.2957283|New York, NY, USA|Association for Computing Machinery|9781450342766|2016|On Being Actionable: Mythologies of Business Intelligence and Disconnects in Drill Downs|Verma, Nitya and Voida, Amy|inproceedings|10.1145/2957276.2957283|||||||||||||||||||||||||||||473450531|42
||Big data, Internet of Things (IoT), Cyberspace, Routine Activity Theory, Systemic||1147-1155||The increased of cybercrime incidents taking place in the world is at its perilous magnitude causing losses in term of money and trust. Even though there are various cybersecurity solutions in place; the threat of cybercrime is still a hard problem. Exploration of cybercrime challenges, especially the preventions and detections of the cybercrime should be investigated by composing all the stakeholders and players of a cybercrime issue. In this paper; an exploration of several cybercrime stakeholders is done. It is argued that cybercrime is a systemic threat and cannot be tackled with cybersecurity and legal systems. The architectural model proposed is significant and should become one of the considered milestones in designing security control in tackling cybercrime globally.|https://doi.org/10.1016/j.procs.2019.11.227|https://www.sciencedirect.com/science/article/pii/S1877050919319386||||2019|A Systemic Cybercrime Stakeholders Architectural Model|Manmeet Mahinderjit Singh and Anizah Abu Bakar|article|SINGH20191147|||Procedia Computer Science|18770509||161||The Fifth Information Systems International Conference, 23-24 July 2019, Surabaya, Indonesia|||19700182801|0,334|-|76|1907|6483|38511|13722|6359|2,09|20,19|Netherlands|Western Europe|2010-2020|Computer Science (miscellaneous)||||475851074|2108686752
||Big data, Classification, Naive Bayes, Large-scale taxonomy, Lossless, Pruned||27-36||In a fast growing big data era, volume and varieties of data processed in Internet applications drastically increase. Real-world search engines commonly use text classifiers with thousands of classes to improve relevance or data quality. These large scale classification problems lead to severe runtime performance challenges, so practitioners often resort to fast approximation techniques. However, the increase in classification speed comes at a cost, as approximations are lossy, mis-assigning classes relative to the original reference classification algorithm. To address this problem, we introduce a Lossless Pruned Naive Bayes (LPNB) classification algorithm tailored to real-world, big data applications with thousands of classes. LPNB achieves significant speed-ups by drawing on Information Retrieval (IR) techniques for efficient posting list traversal and pruning. We show empirically that LPNB can classify text up to eleven times faster than standard Naive Bayes on a real-world data set with 7205 classes, with larger gains extrapolated for larger taxonomies. In practice, the achieved acceleration is significant as it can greatly cut required computation time. In addition, it is lossless: the output is identical to standard Naive Bayes, in contrast to extant techniques such as hierarchical classification. The acceleration does not rely on the taxonomy structure, and it can be used for both hierarchical and flat taxonomies.|https://doi.org/10.1016/j.bdr.2018.05.007|https://www.sciencedirect.com/science/article/pii/S2214579616301320||||2018|Lossless Pruned Naive Bayes for Big Data Classifications|Nanfei Sun and Bingjun Sun and Jian (Denny) Lin and Michael Yu-Chi Wu|article|SUN201827|||Big Data Research|22145796||14|||||21100356018|0,565|Q2|25|11|76|547|324|69|4,06|49,73|United States|Northern America|2014-2020|Computer Science Applications (Q2); Information Systems (Q2); Information Systems and Management (Q2); Management Information Systems (Q2)|586|3.578|0.00126|476195802|1627174784
KDD '18|London, United Kingdom|classification, deep neural networks, denoising, active learning|10|685–694|Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining|The great success of supervised learning has initiated a paradigm shift from building a deterministic software system to a probabilistic artificial intelligent system throughout the industry. The historical records in enterprise domains can potentially bootstrap the traditional business into the modern data-driven approach almost everywhere. The introduction of the Deep Neural Networks (DNNs) significantly reduces the efforts of feature engineering so that supervised learning becomes even more automated. The last bottleneck is to ensure the data quality, particularly the label quality, because the performance of supervised learning is bounded by the errors present in labels. In this paper, we present a new Active Deep Denoising (ADD) approach that first builds a DNN noise model, and then adopts an active learning algorithm to identify the optimal denoising function. We prove that under the low noise condition, we only need to query the oracle with log n examples where n is the total number in the data. We apply ADD on one enterprise application and show that it can effectively reduce 1/3 of the prediction error with only 0.1% of examples verified by the oracle.|10.1145/3219819.3219914|https://doi.org/10.1145/3219819.3219914|New York, NY, USA|Association for Computing Machinery|9781450355520|2018|Active Deep Learning to Tune Down the Noise in Labels|Samel, Karan and Miao, Xu|inproceedings|10.1145/3219819.3219914|||||||||||||||||||||||||||||477045850|42
||Servers;Satellites;Big data;Computer architecture;Algorithm design and analysis;Decision making;Feature extraction;Big Data;divide-and-conquer;machine ID;efficiency||1819-1824|2015 IEEE 12th Intl Conf on Ubiquitous Intelligence and Computing and 2015 IEEE 12th Intl Conf on Autonomic and Trusted Computing and 2015 IEEE 15th Intl Conf on Scalable Computing and Communications and Its Associated Workshops (UIC-ATC-ScalCom)|Machine-to-Machine (M2M) technology unremittingly motivates any time-place-objects connectivity of the devices in and around the world. Every day, a rapid growth of large M2M networks and digital storage technology, lead to a massive heterogeneous data depository, in which the M2M data are captured and warehoused in the diverse database frameworks as a magnitude of heterogeneous data sources. Hence, the M2M that handles Big Data might perform poorly or not according to the goals of their operator due to massive heterogeneous data sources may face various incompatibilities, such as data quality, processing and computational efficiency, analysis and feature extraction applications. Therefore, to address the aforementioned constraints, this paper presents a Big Data Analytical architecture based on Divide-and-Conquer approach. The designed system architecture exploits divide-and-conquer approach, where big data sets are first transformed into a several data blocks that can be quickly processed, then it classifies and reorganizes these data blocks from the same source. In addition, the data blocks are aggregated in a sequential manner based on a machine ID, and equally partitions the data using filtration and load balancing algorithms. The feasibility and efficiency of the proposed system architecture are implemented on Hadoop single node setup. The results show that the proposed system architecture efficiently extract various features (such as River) from the massive volume of satellite data.|10.1109/UIC-ATC-ScalCom-CBDCom-IoP.2015.330|||||2015|Big Data Analytical Architecture Using Divide-and-Conquer Approach in Machine-to-Machine Communication|Ahmad, Awais and Paul, Anand and Rathore, M. Mazhar and Rho, Seungmin|inproceedings|7518511||Aug|||||||||||||||||||||||||||481715872|42
||Data models;Adaptation models;Computational modeling;Analytical models;Image reconstruction;Data integrity;Adaptive systems;Big Scientific Simulation Data;Adaptive Temporal Data Selection and Reconstruction;Multi-branch Decoder Network||1-1||A key challenge in scientific simulation is that the simulation outputs often require intensive I/O and storage space to store the results for effective post hoc analysis. This paper focuses on a quality-aware adaptive temporal data selection and reconstruction problem where the goal is to adaptively select simulation data samples at certain key timesteps in situ and reconstruct the discarded samples with quality assurance during post hoc analysis. This problem is motivated by the limitation of current solutions that a significant amount of simulation data samples are either discarded or aggregated during the sampling process, leading to inaccuratemodeling of the simulated phenomena. Two unique challenges exist: 1) the sampling decisions have to be made in situ and adapted tothe dynamics of the complex scientific simulation data; 2) the reconstruction error must be strictly bounded to meet the application requirement. To address the above challenges, we developDeepSample, an error-controlled convolutional neural network framework, that jointly integrates a set of coherent multi-branch deep decoders to effectively reconstruct the simulation data with rigorous quality assurance. The results on two real-world scientific simulation applications show that DeepSample significantly outperforms other state-of-the-art methods on both sampling efficiency and reconstructed simulation data quality.|10.1109/TBDATA.2021.3092174|||||2021|A Multi-branch Decoder Network Approach toAdaptive Temporal Data Selection andReconstruction for Big Scientific Simulation Data|Zhang, Yang and Guo, Hanqi and Shang, Lanyu and Wang, Dong and Peterka, Tom|article|9464670|||IEEE Transactions on Big Data|23327790|||||||21101019393|0,959|Q1|6|71|9|998|37|8|4,11|14,06|United States|Northern America|2020|Information Systems (Q1); Information Systems and Management (Q1)|636|3.344|0.00134|482746817|1510992500
ICCAE 2018|Brisbane, Australia|social media, Trust, Twitter, data mining, machine learning, users' trustworthiness|5|190–194|Proceedings of the 2018 10th International Conference on Computer and Automation Engineering|In the light of the information revolution, and the propagation of big social data, the dissemination of misleading information is certainly difficult to control. This is due to the rapid and intensive flow of information through unconfirmed sources under the propaganda and tendentious rumors. This causes confusion, loss of trust between individuals and groups and even between governments and their citizens. This necessitates a consolidation of efforts to stop penetrating of false information through developing theoretical and practical methodologies aim to measure the credibility of users of these virtual platforms. This paper presents an approach to domain-based prediction to user's trustworthiness of Online Social Networks (OSNs). Through incorporating three machine learning algorithms, the experimental results verify the applicability of the proposed approach to classify and predict domain-based trustworthy users of OSNs.|10.1145/3192975.3193004|https://doi.org/10.1145/3192975.3193004|New York, NY, USA|Association for Computing Machinery|9781450364102|2018|Tree-Based Classification to Users' Trustworthiness in OSNs|Nabipourshiri, Rouzbeh and Abu-Salih, Bilal and Wongthongtham, Pornpit|inproceedings|10.1145/3192975.3193004|||||||||||||||||||||||||||||483819122|42
||Machine learning, Business value, Competitive advantage, Dynamic capabilities theory||232-243||Machine learning (ML) is expected to transform the business landscape in the near future completely. Hitherto, some successful ML case-stories have emerged. However, how organizations can derive business value (BV) from ML has not yet been substantiated. We assemble a conceptual model, grounded on the dynamic capabilities theory, to uncover key drivers of ML BV, in terms of financial and strategic performance. The proposed model was assessed by surveying 319 corporations. Our findings are that ML use, big data analytics maturity, platform maturity, top management support, and process complexity are, to some extent, drivers of ML BV. We also find that platform maturity has, to some degree, a moderator influence between ML use and ML BV, and between big data analytics maturity and ML BV. To the best of our knowledge, this is the first research to deliver such findings in the ML field.|https://doi.org/10.1016/j.jbusres.2020.05.053|https://www.sciencedirect.com/science/article/pii/S0148296320303581||||2020|Assessing the drivers of machine learning business value|Carolina Reis and Pedro Ruivo and Tiago Oliveira and Paulo Faroleiro|article|REIS2020232|||Journal of Business Research|01482963||117|||||20550|2,049|Q1|195|878|1342|73221|11507|1315|7,38|83,40|United States|Northern America|1973-2021|Marketing (Q1)|46,935|7.550|0.03523|484224197|1502892296
||Safety intelligence (SI), Safety big data, Safety 4.0, Safety management, Safety decision-making||189-199||In the age of big data, intelligence, and Industry 4.0, intelligence plays an increasingly significant role in management or, more specifically, decision making; thus, it becomes a popular topic and is recognised as an important discipline. Hence, safety intelligence (SI) as a new safety concept and term was proposed. SI aims to transform raw safety data and information into meaningful and actionable information for safety management; it is considered an essential perspective for safety management in the era of Safety 4.0 (computational safety science—a new paradigm for safety science in the age of big data, intelligence, and Industry 4.0). However, thus far, no existing research provides a framework that comprehensively describes SI and guides the implementation of SI practices in organisations. To address this research gap and to provide a framework for SI and its practice in the context of safety management, based on a systematic and comprehensive explanation on SI from different perspectives, this study attempts to propose a theoretical framework for SI from a safety management perspective and then presents an SI practice model aimed at supporting safety management in organisations.|https://doi.org/10.1016/j.psep.2020.10.008|https://www.sciencedirect.com/science/article/pii/S0957582020318000||||2021|Safety intelligence as an essential perspective for safety management in the era of Safety 4.0: From a theoretical to a practical framework|Bing Wang|article|WANG2021189|||Process Safety and Environmental Protection|09575820||148|||||||||||||||||||||||488654329|1631786940
https://doi.org/10.1039/d1np00061f|https://www.sciencedirect.com/science/article/pii/S0265056822008856||||2021|Benefiting from big data in natural products: importance of preserving foundational skills and prioritizing data quality|Nadja B. Cech and Marnix H. Medema and Jon Clardy|article|CECH20211947|||Natural Product Reports|02650568|11|38||||||||||||||||||||||||||||||489358078|42
||Analytical models;Decision making;Measurement uncertainty;Process control;Random access memory;Organizations;Reliability engineering;Data cubes;Metrics;Decision support||1-3|2021 Annual Reliability and Maintainability Symposium (RAMS)|"\"Summary & ConclusionsThis paper describes an approach to extracting reliability data from transaction data and performing analysis on it. Organizations typically collect significant amounts of data that could be used for reliability analysis but is not.Data science is a field that offers reliability engineers insights when faced with analyzing so-called \"\"big data.\"\" One subset of data science that can be helpful in this regard is the idea of using data cubes as the basis for analysis. Data cubes use many techniques, such as slicing, aggregation, drill-downs, and pivots [1]. These concepts are widely implemented, and most engineers use them, even if they do not explicitly name them. For example, drill-down would occur when only the data from a particular equipment model is examined; aggregation reverses this. Slicing occurs with all but two dimensions (defined below) are held constant. Pivots occur when data is \"\"rotated\"\" by changing the way rows and columns are selected and displayed.This paper will report on the results of using data cubes to support and drive the culture of reliability engineering:•Rapidly model large datasets to confirm or deny reliability and process measures•Drive data quality improvements•Build confidence in the way business rules are modeled•Develop metrics•Support decision making in the face of uncertaintyThe paper describes the work done and offers some recommendations for implementation.\""|10.1109/RAMS48097.2021.9605783|||||2021|Solving Problems with Rapid Data Discovery|Franklin, Paul|inproceedings|9605783||May||25770993|||||||||||||||||||||||||489792799|137887340
||Big Data;Quality of experience;Edge computing;Quality of service;Computational modeling;Training;Streaming media;Quality-of-Experience (QoE); high-dimensional big data management; deep learning; pervasive edge||222-233||In the age of big data, services in the pervasive edge environment are expected to offer end-users better Quality-of-Experience (QoE) than that in a normal edge environment. However, the combined impact of the storage, delivery, and sensors used in various types of edge devices in this environment is producing volumes of high-dimensional big data that are increasingly pervasive and redundant. Therefore, enhancing the QoE has become a major challenge in high-dimensional big data in the pervasive edge computing environment. In this paper, to achieve high QoE, we propose a QoE model for evaluating the qualities of services in the pervasive edge computing environment. The QoE is related to the accuracy of high-dimensional big data and the transmission rate of this accurate data. To realize high accuracy of high-dimensional big data and the transmission of accurate data through out the pervasive edge computing environment, in this study we focused on the following two aspects. First, we formulate the issue as a high-dimensional big data management problem and test different transmission rates to acquire the best QoE. Then, with respect to accuracy, we propose a Tensor-Fast Convolutional Neural Network (TF-CNN) algorithm based on deep learning, which is suitable for high-dimensional big data analysis in the pervasive edge computing environment. Our simulation results reveal that our proposed algorithm can achieve high QoE performance.|10.26599/BDMA.2018.9020020|||||2018|QoE-driven big data management in pervasive edge computing environment|Meng, Qianyu and Wang, Kun and He, Xiaoming and Guo, Minyi|article|8361574||Sep.|Big Data Mining and Analytics|20960654|3|1|||||||||||||||||||||||492245827|417390997
||Big Data, Big Data model, Big Data governance, Data management, Big Data governance framework, Big Data analytic||271-277||The recent explosion in ICT and digital data has led organizations, both private and public, to efficient decision-making. Nowadays organizations can store huge amounts of data, which can be accessible at any time. Big Data governance refers to the management of huge volumes of an organization’s data, exploiting it in the organization’s decision-making using different analytical tools. Big Data emergence provides great convenience, but it also brings challenges. Nevertheless, for Big Data governance, data has to be prepared in a timely manner, keeping in view the consistency and reliability of the data, and being able to trust its source and the meaningfulness of the result. Hence, a framework for Big Data governance would have many advantages. There are Big Data governance frameworks, which guide the management of Big Data. However, there are also limitations associated with these frameworks. Therefore, this study aims to explore the existing Big Data governance frameworks and their shortcomings, and propose a new framework. The proposed framework consists of eight components. As a framework validation, the proposed framework has been compared with the ISO 8000 data governance framework.|https://doi.org/10.1016/j.procs.2018.10.181|https://www.sciencedirect.com/science/article/pii/S1877050918318313||||2018|Exploring Big Data Governance Frameworks|Ali Al-Badi and Ali Tarhini and Asharul Islam Khan|article|ALBADI2018271|||Procedia Computer Science|18770509||141||The 9th International Conference on Emerging Ubiquitous Systems and Pervasive Networks (EUSPN-2018) / The 8th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare (ICTH-2018) / Affiliated Workshops|||19700182801|0,334|-|76|1907|6483|38511|13722|6359|2,09|20,19|Netherlands|Western Europe|2010-2020|Computer Science (miscellaneous)||||492691603|2108686752
||Big data, Evidence based governance, Knowledge discovery and data mining (KDD), Data science, Population informatics, Policy informatics, Academic government partnership, Administrative data||127-136||Numerous approaches are available for improving governance of the child welfare system, all of which require longitudinal data reporting on child welfare clients. A substantial amount of agency administrative information – big data – can be transformed into knowledge for policy and management actions through a rigorous information generation process. Important properties of the information generation process are that it must generate accurate, timely information while protecting the confidentiality of the clients. In addition, it must be extensible to serve an ever-changing policy and technology environment. Knowledge discovery and data mining (KDD), aka data science, is a method developed in the private sector to mine consumer data and can be used in public settings to support evidence based governance. KDD consists of a rigorous 5-step process that includes a Web-based end-user interface. The relationship between KDD and governance is a continuous feedback cycle that enables ongoing development of new information and knowledge as stakeholders identify emerging needs. In this paper, we synthesis the different frameworks for utilizing big data for public governance, introduce the KDD process, describe the nature of big data in child welfare, and then present an updated KDD architecture that can support these frameworks to utilize big data for governance. We also demonstrate the role KDD plays in child welfare management through 2 case studies. We conclude with a discussion on implications for agency–university partnerships and research-to-practice.|https://doi.org/10.1016/j.childyouth.2015.09.014|https://www.sciencedirect.com/science/article/pii/S0190740915300591||||2015|Using big data for evidence based governance in child welfare|Hye-Chung Kum and C. {Joy Stewart} and Roderick A. Rose and Dean F. Duncan|article|KUM2015127|||Children and Youth Services Review|01907409||58|||||24482|0,816|Q1|89|978|1349|58646|3628|1315|2,29|59,97|United Kingdom|Western Europe|1979-2020|Education (Q1); Social Work (Q1); Sociology and Political Science (Q1); Developmental and Educational Psychology (Q2)|13,303|2.393|0.01343|493663093|411345842
||Springs;Databases;Service-oriented architecture;Data integrity;Tools;Servers;Big Data;auto configuration;CRUD;java framework;service-oriented architecture;REST;spring boot||5328-5329|2018 IEEE International Conference on Big Data (Big Data)|Web application technologies are growing rapidly with continuous innovation and improvements. This paper focuses on the popular Spring Boot [1] java-based framework for building web and enterprise applications and how it provides the flexibility for service-oriented architecture (SOA). One challenge with any Spring-based applications is its level of complexity with configurations. Spring Boot makes it easy to create and deploy stand-alone, production-grade Spring applications with very little Spring configuration. Example, if we consider Spring Model-View-Controller (MVC) framework [2], we need to configure dispatcher servlet, web jars, a view resolver, and component scan among other things. To solve this, Spring Boot provides several Auto Configuration options to setup the application with any needed dependencies. Another challenge is to identify the framework dependencies and associated library versions required to develop a web application. Spring Boot offers simpler dependency management by using a comprehensive, but flexible, framework and the associated libraries in one single dependency, which provides all the Spring related technology that you need for starter projects as compared to CRUD web applications. This framework provides a range of additional features that are common across many projects such as embedded server, security, metrics, health checks, and externalized configuration. Web applications are generally packaged as war and deployed to a web server, but Spring Boot application can be packaged either as war or jar file, which allows to run the application without the need to install and/or configure on the application server. In this paper, we discuss how Atmospheric Radiation Measurement (ARM) Data Center (ADC) at Oak Ridge National Laboratory, is using Spring Boot to create a SOA based REST [4] service API, that bridges the gap between frontend user interfaces and backend database. Using this REST service API, ARM scientists are now able to submit reports via a user form or a command line interface, which captures the same data quality or other important information about ARM data.|10.1109/BigData.2018.8621924|||||2018|Spring Boot based REST API to Improve Data Quality Report Generation for Big Scientific Data: ARM Data Center Example|Guntupally, Kavya and Devarakonda, Ranjeet and Kehoe, Kenneth|inproceedings|8621924||Dec|||||||||||||||||||||||||||495560742|42
||Conferences;Big Data;Manufacturing processes;Manufacturing industries;Data analysis;Data integrity||3141-3146|2018 IEEE International Conference on Systems, Man, and Cybernetics (SMC)|In the field of manufacturing industry, it is difficult to make full use of the research results for production optimization and/or management due to the low quality of real workshop data. Typical quality problems of the real workshop data include: data conflict, missing recessive data, and false error identification. The conventional data analysis methods cannot handle most such issues because they fail to consider professional insights into and domain knowledge about the data. The real production data from an actual semiconductor manufacturing workshop are adopted as the objective data in this paper. A series of data process techniques with domain knowledge are proposed to solve those data quality problems according to specific flaws of the data respectively. The work in this paper has the potential to be further extended and applied to other big data applications beyond the manufacturing industry.|10.1109/SMC.2018.00532|||||2018|Real Manufacturing Oriented Data Process Techniques with Domain Knowledge|Kong, Weichang and Qiao, Fei and Wu, Qidi|inproceedings|8616529||Oct||25771655|||||||||||||||||||||||||497870277|1064315132
||Cleaning;Big data;Measurement;Business;Software;Instruments;Industries||1595-1602|2016 IEEE International Conference on Big Data (Big Data)|Data quality issues pose a significant barrier to operationalizing big data. They pertain to the meaning of the data, the consistency of that meaning, the human interpretation of results, and the contexts in which the results are used. Data quality issues arise after organizations have moved past clear-cut technical solutions to early bottlenecks in using data. Left unaddressed, such issues can and have led to high profile missteps, and raise doubts about the data-driven world view altogether. In this paper, we share real-world case studies of tackling data quality challenges across industry verticals. We present initial ideas on how to systematically address data quality issues via technology. The success of operationalizing big data will depend on the quality of data involved, and whether such data causes uncertainty and disruptions, or delivers genuine knowledge and value.|10.1109/BigData.2016.7840769|||||2016|Data quality: Experiences and lessons from operationalizing big data|Ganapathi, Archana and Chen, Yanpei|inproceedings|7840769||Dec|||||||||||||||||||||||||||500156897|42
EM-GIS'17|Redondo Beach, CA, USA|spatio-temporal visualization, emergency management, review|7||Proceedings of the 3rd ACM SIGSPATIAL Workshop on Emergency Management Using|Recent years, extreme events caused a great loss of human society. Emergency management is playing a more and more important role in handling disaster events. With the raising of data-intensive decision making, how to visualize large, multi-dimension data become an important challenge. Spatial temporal data visualization, a powerful tool, could transform data in to visual structure and make core information easily be captured by human. It could support spatial analysis, decision making and be used in all phase of emergency management. In this paper, we reviewed the general method of spatial temporal data visualization and the methods in data-intensive environment. Summarized the problems of each phase of emergency management and presented how spatial temporal visualization tools applied in each phase of emergency management. Finally, we conduct a short conclusion and outlook the future of spatial temporal visualization applied in data-driven emergency management environment.|10.1145/3152465.3152473|https://doi.org/10.1145/3152465.3152473|New York, NY, USA|Association for Computing Machinery|9781450354936|2017|Spatial Temporal Data Visualization In Emergency Management: A View from Data-Driven Decision|Wang, Deqiang and Guo, Danhuai and Zhang, Hui|inproceedings|10.1145/3152465.3152473|8||||||||||||||||||||||||||||501252256|42
AIAM 2019|Dublin, Ireland|artificial intelligence, Sports rehabilitation, rehabilitation training, big data, judicial administrative|5||Proceedings of the 2019 International Conference on Artificial Intelligence and Advanced Manufacturing|"\"Under the background of \"\"Wisdom Drug Rehabilitation\"\", we introduced \"\"Artificial Intelligence and Big Data\"\" into \"\"exercise rehabilitation\"\" work of drug addicts in judicial administrative system. It is a practical innovation of drug treatment in China. This article will elaborate this innovation of the construction and application of \"\"Exercise Rehabilitation\"\" intelligence platform system. This system will improve mental status, alleviate physical and psychological symptoms, ensure safety in places, lighten the burden of professional police officers, make rapid analysis, make accurate decisions and improve the integrity rate of the addict.\""|10.1145/3358331.3358336|https://doi.org/10.1145/3358331.3358336|New York, NY, USA|Association for Computing Machinery|9781450372022|2019|"\"Application of \"\"Artificial Intelligence and Big Data\"\" in Sports Rehabilitation for Chinese Judicial Administrative Drug Addicts\""|Jia, Dong-Ming and Yuan, Cun-Feng and Guo, Song and Jiang, Zu-Zhen and Xu, Ding and Wang, Da-An|inproceedings|10.1145/3358331.3358336|5||||||||||||||||||||||||||||502184342|42
DOLAP '12|Maui, Hawaii, USA|materialized view maintenance, data warehouse benchmarking, stream data warehousing|8|105–112|Proceedings of the Fifteenth International Workshop on Data Warehousing and OLAP|"\"Data management systems are facing two challenges driven by the requirements of emerging data-intensive applications: more data and less time to process the data. Data volumes continue to increase as new sources and data collecting mechanisms appear. At the same time, these sources tend to be highly dynamic and generate data in the form of a stream, which requires quick reaction to newly arrived data. Traditional data warehouses enable scalable data storage and analytics, including the ability to define nested levels of materialized views. However, views are typically refreshed during downtimes---e.g., every night---which does not meet the latency requirements of many applications. Stream data warehousing is a new data management technology that allows nearly-continuous view refresh as new data arrive, which enables seamless integration of real-time monitoring and business intelligence with long-term data mining. In this paper, we argue that a new benchmark is required for stream warehouses, which should focus on measuring the property that determines the utility of these systems, namely how well they can keep up with the incoming data and guarantee the \"\"freshness\"\" of materialized views.\""|10.1145/2390045.2390062|https://doi.org/10.1145/2390045.2390062|New York, NY, USA|Association for Computing Machinery|9781450317214|2012|Towards Benchmarking Stream Data Warehouses|"\"B\"\"{a}r, Arian and Golab, Lukasz\""|inproceedings|10.1145/2390045.2390062|||||||||||||||||||||||||||||503090407|42
||big data;data quality;subordination;fuzzy comprehensive evaluation;On-line monitoring||753-757|2019 6th International Conference on Information Science and Control Engineering (ICISCE)|This paper analyses the existing problems in on-line monitoring and data quality evaluation of substation equipment, and proposes a multi-dimensional fuzzy comprehensive evaluation method for on-line monitoring data quality of substation equipment. The evaluation index set of online monitoring data quality of substation equipment with 5 dimensions and 11 secondary indexes is established. The weight is determined by combining subjective and objective methods. The fuzzy transformation is completed based on membership function and a multi-dimensional fuzzy comprehensive evaluation model is established. Finally, the evaluation grade of online monitoring data of substation equipment is obtained. Finally, compared with other methods, the validity and accuracy of this method are verified.|10.1109/ICISCE48695.2019.00154|||||2019|Fuzzy Comprehensive Evaluation Method for On-line Monitoring Data Quality of Substation Equipment|Dehui, Fu and Feng, Wang and Shuai, Yuan and Guangzhen, Wang and Mingxin, Shao|inproceedings|9107851||Dec|||||||||||||||||||||||||||503216736|42
|||12|1481–1492||As massive data acquisition and storage becomes increasingly affordable, a wide variety of enterprises are employing statisticians to engage in sophisticated data analysis. In this paper we highlight the emerging practice of Magnetic, Agile, Deep (MAD) data analysis as a radical departure from traditional Enterprise Data Warehouses and Business Intelligence. We present our design philosophy, techniques and experience providing MAD analytics for one of the world's largest advertising networks at Fox Audience Network, using the Greenplum parallel database system. We describe database design methodologies that support the agile working style of analysts in these settings. We present dataparallel algorithms for sophisticated statistical techniques, with a focus on density methods. Finally, we reflect on database system features that enable agile design and flexible algorithm development using both SQL and MapReduce interfaces over a variety of storage mechanisms.|10.14778/1687553.1687576|https://doi.org/10.14778/1687553.1687576||VLDB Endowment||2009|MAD Skills: New Analysis Practices for Big Data|Cohen, Jeffrey and Dolan, Brian and Dunlap, Mark and Hellerstein, Joseph M. and Welton, Caleb|article|10.14778/1687553.1687576||aug|Proc. VLDB Endow.|21508097|2|2|August 2009||||21100199855|0,946|Q1|134|246|598|12401|3759|566|5,50|50,41|United States|Northern America|2008-2020|Computer Science (miscellaneous) (Q1)|7,198|2.047|0.00891|504238941|1216159931
|||12|7–18||Data quantity and data quality, like two sides of a coin, are equally important to data management. This paper provides an overview of recent advances in the study of data quality, from theory to practice. We also address challenges introduced by big data to data quality management.|10.1145/2854006.2854008|https://doi.org/10.1145/2854006.2854008|New York, NY, USA|Association for Computing Machinery||2015|Data Quality: From Theory to Practice|Fan, Wenfei|article|10.1145/2854006.2854008||dec|SIGMOD Rec.|01635808|3|44|September 2015||||13622|0,372|Q2|142|39|81|808|127|57|1,67|20,72|United States|Northern America|1969, 1973-1978, 1981-2020|Information Systems (Q2); Software (Q2)|1,819|0.775|7.5E-4|511880717|962972343
||Linked data, Irregular data, Schema discovery, Semantic web|36|675–710||More and more weakly structured, and irregular data sources are becoming available every day. The schema of these sources is useful for a number of tasks, such as query answering, exploration and summarization. However, although semantic web data might contain schema information, in many cases this is completely missing or partially defined. In this paper, we present a survey of the state of the art on schema information extraction approaches. We analyze and classify these approaches into three families: (1) approaches that exploit the implicit structure of the data, without assuming that some explicit statements on the schema are provided in the dataset; (2) approaches that use the explicit schema statements contained in the dataset to complement and enrich the schema, and (3) those that discover structural patterns contained in a dataset. We compare these studies in terms of their approach, advantages and limitations. Finally we discuss the problems that remain open.|10.1007/s00778-021-00717-x|https://doi.org/10.1007/s00778-021-00717-x|Berlin, Heidelberg|Springer-Verlag||2021|A Survey on Semantic Schema Discovery|Kellou-Menouer, Kenza and Kardoulakis, Nikolaos and Troullinou, Georgia and Kedad, Zoubida and Plexousakis, Dimitris and Kondylakis, Haridimos|article|10.1007/s00778-021-00717-x||nov|The VLDB Journal|10668888|4|31|Jul 2022||||13646|0,653|Q1|90|64|117|4824|544|113|4,46|75,38|United States|Northern America|1992-2020|Hardware and Architecture (Q1); Information Systems (Q1)|2,106|2.868|0.0023|513370731|924310569
EuroPLoP '20|Virtual Event, Germany||24||Proceedings of the European Conference on Pattern Languages of Programs 2020|Remote Application Programming Interfaces (APIs), as for instance offered in microservices architectures, are used in almost any distributed system today and are thus enablers for many digitalization efforts. It is hard to design such APIs so that they are easy and effective to use; maintaining their runtime qualities while preserving backward compatibility is equally challenging. Finding well suited granularities in terms of the architectural capabilities of endpoints and the read-write semantics of their operations are particularly important design concerns. Existing pattern languages have dealt with local APIs in object-oriented programming, with remote objects, with queue-based messaging and with service-oriented computing platforms. However, patterns or equivalent guidances for the architectural design of API endpoints, operations and their request and response message structures are still missing. In this paper, we extend our microservice API pattern language (MAP) and introduce endpoint role and operation responsibility patterns, namely Processing Resource, Computation Function, State Creation Operation, Retrieval Operation, and State Transition Operation. Known uses and examples of the patterns are drawn from public Web APIs, as well as application development and system integration projects the authors have been involved in.|10.1145/3424771.3424822|https://doi.org/10.1145/3424771.3424822|New York, NY, USA|Association for Computing Machinery|9781450377690|2020|Interface Responsibility Patterns: Processing Resources and Operation Responsibilities|"\"Zimmermann, Olaf and L\"\"{u}bke, Daniel and Zdun, Uwe and Pautasso, Cesare and Stocker, Mirko\""|inproceedings|10.1145/3424771.3424822|9||||||||||||||||||||||||||||514569769|42
||Application programming interfaces, Automation, Big data, Crowd-sourced, Digitization, Geolocation, Information and communication technology, Intelligent transport systems, Internet of things, Location-based services, Mobility as a service, Real time information systems, Timestamp, Vehicle telematics||665-670|International Encyclopedia of Transportation|Transport modes and big data considers the characteristics of “Big Data” as described by the 5 “Vs,” Volume, Velocity, Variety, Veracity, and Value. It reviews the many sources of big data within the transport sector by modal group, and the sources of big data from the Information and Communication Technology (ICT) sector that may be applied to the understanding of transport. It briefly considers the uses, advantages, and disadvantages of these data sources, and the challenges to analyzing and interpreting them. It concludes that Big Data is necessary to prepare for the uncertain future of transport, but recognizes the challenges to applying it accurately and effectively to transport problems and policies.|https://doi.org/10.1016/B978-0-08-102671-7.10601-3|https://www.sciencedirect.com/science/article/pii/B9780081026717106013|Oxford|Elsevier|978-0-08-102672-4|2021|Transport Modes and Big Data|Hannah D Budnitz and Emmanouil Tranos and Lee Chapman|incollection|BUDNITZ2021665||||||||||Roger Vickerman|||||||||||||||||||517241282|42
ICBDR 2020|Tokyo, Japan|Fuzzy theory, Visualization, AIS data, Ship encounter|7|94–100|2020 the 4th International Conference on Big Data Research (ICBDR'20)|As an important application of big data technology in the maritime field, big data driven visualization of ship encounter patterns helps to intuitively understand the risk situation in the water traffic. However traditional methods based on fixed thresholds do not consider the fuzziness of classification on ship encounter situations. We proposed a visual method to visualize the risk situations caused by the interaction between vessel traffic flows in more detail based on fuzzy theory and big data intelligence on large scale AIS data. A case study is conducted to verify the applicability based on the AIS data from Singapore Strait. Visualization results of density in grids show that the proposed method can effectively reflect the ship encounter patterns, which are consistent with the real situation and can show more valuable details for the safety assessment of water traffic.|10.1145/3445945.3445962|https://doi.org/10.1145/3445945.3445962|New York, NY, USA|Association for Computing Machinery|9781450387750|2021|A Visual Method for Ship Close Encounter Pattern Recognition Based on Fuzzy Theory and Big Data Intelligence|Zhao, Liangbin and Fu, Xiuju|inproceedings|10.1145/3445945.3445962|||||||||||||||||||||||||||||519414057|42
||Structure health monitoring, Underground space infrastructure, Machine learning, Spatio–temporal data||||Intelligent sensing, mechanism understanding, and the deterioration forecasting based on spatio–temporal big data not only promote the safety of the infrastructure but also indicate the basic theory and key technology for the infrastructure construction to turn to intelligentization. The advancement of underground space utilization has led to the development of three characteristics (deep, big, and clustered) that help shape a tridimensional urban layout. However, compared to buildings and bridges overground, the diseases and degradation that occur underground are more insidious and difficult to identify. Numerous challenges during the construction and service periods remain. To address this gap, this paper summarizes the existing methods and evaluates their strong points and weak points based on real-world space safety management. The key scientific issues, as well as solutions, are discussed in a unified intelligent monitoring system.|https://doi.org/10.1016/j.eng.2022.07.016|https://www.sciencedirect.com/science/article/pii/S209580992200635X||||2022|Intelligent Monitoring System Based on Spatio–Temporal Data for Underground Space Infrastructure|Bowen Du and Junchen Ye and Hehua Zhu and Leilei Sun and Yanliang Du|article|DU2022|||Engineering|20958099|||||||21100780794|1,376|Q1|45|186|372|9769|3152|324|6,73|52,52|United Kingdom|Western Europe|2015-2020|Chemical Engineering (miscellaneous) (Q1); Computer Science (miscellaneous) (Q1); Energy Engineering and Power Technology (Q1); Engineering (miscellaneous) (Q1); Environmental Engineering (Q1); Materials Science (miscellaneous) (Q1)|4,023|7.553|0.00715|520173611|298111411
||Visualization, selection bias, exploratory analysis, intelligent visual interfaces, visual analytics|23|||Large and high-dimensional real-world datasets are being gathered across a wide range of application disciplines to enable data-driven decision making. Interactive data visualization can play a critical role in allowing domain experts to select and analyze data from these large collections. However, there is a critical mismatch between the very large number of dimensions in complex real-world datasets and the much smaller number of dimensions that can be concurrently visualized using modern techniques. This gap in dimensionality can result in high levels of selection bias that go unnoticed by users. The bias can in turn threaten the very validity of any subsequent insights. This article describes Adaptive Contextualization (AC), a novel approach to interactive visual data selection that is specifically designed to combat the invisible introduction of selection bias. The AC approach (1) monitors and models a user’s visual data selection activity, (2) computes metrics over that model to quantify the amount of selection bias after each step, (3) visualizes the metric results, and (4) provides interactive tools that help users assess and avoid bias-related problems. This article expands on an earlier article presented at ACM IUI 2016 [16] by providing a more detailed review of the AC methodology and additional evaluation results.|10.1145/3009973|https://doi.org/10.1145/3009973|New York, NY, USA|Association for Computing Machinery||2017|Adaptive Contextualization Methods for Combating Selection Bias during High-Dimensional Visualization|Gotz, David and Sun, Shun and Cao, Nan and Kundu, Rita and Meyer, Anne-Marie|article|10.1145/3009973|17|nov|ACM Trans. Interact. Intell. Syst.|21606455|4|7|December 2017||||||||||||||||||||||520235684|668229522
||Big data companies, Big data technologies, Online news, Gartner magic quadrant||1-8||Big Data is dominating the landscape as data originated in many sources keeps piling up. Information Technology (IT) business companies are making tremendous efforts to keep the pace with this wave of innovative technologies. This study aims to identify how the different IT companies are aligned with emerging Big Data technologies. The approach consisted in analyzing 11,505 news published between 2013 and 2016 and aggregated through Google News. The companies were categorized according to their position in the 2017 Gartner Magic Quadrant for advanced analytics. A text mining and topic modeling procedure assisted in summarizing the main findings. Leaders dominated a large fraction of the published news. Challengers are making a significant effort in investing in predictive analytics, overlooking other technologies such as those related to data preparation and integration. The results helped to shed light on the emerging field of Big Data from a corporate perspective.|https://doi.org/10.1016/j.compind.2018.03.018|https://www.sciencedirect.com/science/article/pii/S016636151830040X||||2018|Unfolding the relations between companies and technologies under the Big Data umbrella|João Canito and Pedro Ramos and Sérgio Moro and Paulo Rita|article|CANITO20181|||Computers in Industry|01663615||99|||||19080|1,432|Q1|100|115|323|6926|3165|319|9,96|60,23|Netherlands|Western Europe|1979-2020|Computer Science (miscellaneous) (Q1); Engineering (miscellaneous) (Q1)|6,018|7.635|0.00584|520919518|605146181
||Vehicles;Vehicular ad hoc networks;Roads;Big data;Quality of service;Internet;Real-time systems||1-6|2016 IEEE International Conference on Communications (ICC)|With the explosive growth of Internet of Vehicles (IoV), it is undoubted that vehicular demands for real-time Internet access would get a surge in the near future. Therefore, it is foreseeable that the cars within the IoV will generate enormous data. On the one hand, the huge volume of data mean we could get much information (e.g., vehicle's condition and real-time traffic distribution) through the big data analysis. On the other hand, the huge volume of data will overload the cellular network since the cellular infrastructure still represents the dominant access methods for ubiquitous connections. The vehicular ad hoc network (VANET) offloading is a promising solution to alleviate the conflict between the limited capacity of cellular network and big data collection. In a vehicular heterogeneous network formed by cellular network and VANET, an efficient network selection is crucial to ensure vehicles' quality of service. To address this issue, we develop an intelligent network recommendation system supported by traffic big data analysis. Firstly, the traffic model for network recommendation is built through big data analysis. Secondly, vehicles are recommended to access an appropriate network by employing the analytic framework which takes traffic status, user preferences, service applications and network conditions into account. Furthermore an Android application is developed, which enables individual vehicle to access network automatically based on the access recommender. Finally, extensive simulation results show that our proposal can effectively select the optimum network for vehicles, and network resource is fully utilized at the same time.|10.1109/ICC.2016.7510775|||||2016|Traffic big data analysis supporting vehicular network access recommendation|Liu, Yunshu and Chen, Xuanyu and Chen, Cailian and Guan, Xingping|inproceedings|7510775||May||19381883|||||||||||||||||||||||||522924022|276554849
||Phasor measurement units;Data integrity;Big Data;Power systems;Machine learning;Training;Statistical analysis;Phasor measurement units;synchrophasor datasets;statistical analysis;data quality analysis;label quality;nonymized datasets;supervised machine learning;big data;wide area monitoring system||386-397||This manuscript presents a data quality analysis and holistic ‘machine learning-readiness’ evaluation of a representative set of large-scale, real-world phasor measurement unit (PMU) datasets provided under the United States Department of Energy-funded FOA 1861 research program. A major focus of this study is to understand the present-day suitability of large-scale, real-world synchrophasor datasets for application of commercially-available, off-the-shelf big data and supervised or semi-supervised machine learning (ML) tools and catalogue any major obstacles to their application. To this end, dataset quality is methodically examined through an interconnect-wide quantifications of basic bad data occurrences, a summary of several harder-to-detect data quality issues that can jeopardize successful application of machine learning, and an evaluation of the adequacy of event log labeling for supervised training of models used for online event classification. A global ‘six-point’ statistical analyses of several key dataset variables is demonstrated as a means by which to identify additional hard-to-detect data quality issues, also providing an example successful application of big data technology to extract insights regarding reasonable operational bounds of the US power system. Obstacles for application of commercial ML technologies are summarized, with a particular focus on supervised and semi-supervised ML. Lessons-learned are provided regarding challenges associated with present-day event labeling practices, large spatial scope of the dataset, and dataset anonymization. Finally, insight into efficacy of employed mitigation strategies are discussed, and recommendations for future work are made.|10.1109/OAJPE.2022.3197553|||||2022|Application of Big Data Analytics and Machine Learning to Large-Scale Synchrophasor Datasets: Evaluation of Dataset ‘Machine Learning-Readiness’|Hart, Philip and He, Lijun and Wang, Tianyi and Kumar, Vijay S. and Aggour, Kareem and Subramanian, Arun and Yan, Weizhong|article|9852477|||IEEE Open Access Journal of Power and Energy|26877910||9|||||||||||||||||||||||525349846|406178434
SIGIR '17|Shinjuku, Tokyo, Japan|experimentation, a/b testing|3|1395–1397|Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval|The Internet provides developers of connected software, including web sites, applications, and devices, an unprecedented opportunity to accelerate innovation by evaluating ideas quickly and accurately using controlled experiments, also known as A/B tests. From front-end user-interface changes to backend algorithms, from search engines (e.g., Google, Bing, Yahoo!) to retailers (e.g., Amazon, eBay, Etsy) to social networking services (e.g., Facebook, LinkedIn, Twitter) to travel services (e.g., Expedia, Airbnb, Booking.com) to many startups, online controlled experiments are now utilized to make data-driven decisions at a wide range of companies. While the theory of a controlled experiment is simple, and dates back to Sir Ronald A. Fisher's experiments at the Rothamsted Agricultural Experimental Station in England in the 1920s, the deployment and evaluation of online controlled experiments at scale (100's of concurrently running experiments) across variety of web sites, mobile apps, and desktop applications presents many pitfalls and new research challenges. In this tutorial we will give an introduction to A/B testing, share key lessons learned from scaling experimentation at Bing to thousands of experiments per year, present real examples, and outline promising directions for future work. The tutorial will go beyond applications of A/B testing in information retrieval and will also discuss on practical and research challenges arising in experimentation on web sites and mobile and desktop apps. Our goal in this tutorial is to teach attendees how to scale experimentation for their teams, products, and companies, leading to better data-driven decisions. We also want to inspire more academic research in the relatively new and rapidly evolving field of online controlled experimentation.|10.1145/3077136.3082060|https://doi.org/10.1145/3077136.3082060|New York, NY, USA|Association for Computing Machinery|9781450350228|2017|A/B Testing at Scale: Accelerating Software Innovation|Deng, Alex and Dmitriev, Pavel and Gupta, Somit and Kohavi, Ron and Raff, Paul and Vermeer, Lukas|inproceedings|10.1145/3077136.3082060|||||||||||||||||||||||||||||526822600|42
|||2|2048–2049||In the era of Big Data, data entries, even describing the same objects or events, can come from a variety of sources, where a data source can be a web page, a database or a person. Consequently, conflicts among sources become inevitable. To resolve the conflicts and achieve high quality data, truth discovery and crowdsourcing aggregation have been studied intensively. However, although these two topics have a lot in common, they are studied separately and are applied to different domains. To answer the need of a systematic introduction and comparison of the two topics, we present an organized picture on truth discovery and crowdsourcing aggregation in this tutorial. They are compared on both theory and application levels, and their related areas as well as open questions are discussed.|10.14778/2824032.2824136|https://doi.org/10.14778/2824032.2824136||VLDB Endowment||2015|Truth Discovery and Crowdsourcing Aggregation: A Unified Perspective|Gao, Jing and Li, Qi and Zhao, Bo and Fan, Wei and Han, Jiawei|article|10.14778/2824032.2824136||aug|Proc. VLDB Endow.|21508097|12|8|August 2015||||21100199855|0,946|Q1|134|246|598|12401|3759|566|5,50|50,41|United States|Northern America|2008-2020|Computer Science (miscellaneous) (Q1)|7,198|2.047|0.00891|527891669|1216159931
||Performance evaluation;Job shop scheduling;Computational modeling;Time series analysis;Big Data;Predictive models;Prediction algorithms;Multivariate time series;Anomaly detection;Hierarchical temporal memory;Industrial bigdata||759-763|2021 IEEE International Conference on Computer Science, Electronic Information Engineering and Intelligent Control Technology (CEI)|In recent years, industrial big data has attracted much attention as the key technical support of “Intelligent Manufacturing” and “Industrial Internet”. And as the dependence of intelligent manufacturing on digitalization continues to increase, data quality problems caused by device and system failures, harsh environment, improper scheduling and management, duplication or missing of data fields, etc., have more significant impacts on industrial processes. Therefore, the anomaly detection of industrial big data is particularly important. Among the methods onto time series data for anomaly detection, HTM(Hierarchical Temporal Memory) algorithm performs well in the unsupervised univariate time series data anomaly detection, but the capability of original HTM model for detecting multivariate time series anomaly data is insufficient. However, the multivariate data anomaly detection is common in industry and the performance requirements for data anomaly detection are relatively high. Thus, this paper proposes an improved HTM algorithm model - MSP-HTM(Multiple Spatial Poolers HTM) model. The MSP-HTM model respectively encode the value of different dimensions at the same time, and then put the result from encoder into spatial pooler respectively, finally the temporal memory layer merge result from spatial poolers, and predict future data. Experiments show that the MSP-HTM model can improve performance by processing the multivariate time series data in parallel and improve the effect of data anomaly detection.|10.1109/CEI52496.2021.9574505|||||2021|Multivariate Time Series Anomaly Detection On Improved HTM Model|Zeng, Hui and Zhao, Xiaoyong and Wang, Lei|inproceedings|9574505||Sep.|||||||||||||||||||||||||||528666650|42
||Space situational awareness, , Radio astronomy, Interactive workflow creation, High performance computing||100619||As the demand for software to support the processing and analysis of massive radio astronomy datasets increases in the era of the SKA, we demonstrate the interactive workflow building, data mining, processing, and visualisation capabilities of DUG Insight. We test the performance and flexibility of DUG Insight by processing almost 68,000 full sky radio images produced from the Engineering Development Array (EDA2) over the course of a three day period. The goal of the processing was to passively detect and identify known Resident Space Objects (RSOs: satellites and debris in orbit) and investigate how radio interferometry could be used to passively monitor aircraft traffic. These signals are observable due to both terrestrial FM radio signals reflected back to Earth and out-of-band transmission from RSOs. This surveillance of the low Earth orbit and airspace environment is useful as a contribution to space situational awareness and aircraft tracking technology. From the observations, we made 40 detections of 19 unique RSOs within a range of 1,500 km from the EDA2. This is a significant improvement on a previously published study of the same dataset and showcases the flexible features of DUG Insight that allow the processing of complex datasets at scale. Future enhancements of our DUG Insight workflow will aim to realise real-time acquisition, detect unknown RSOs, and continue to process data from SKA-relevant facilities.|https://doi.org/10.1016/j.ascom.2022.100619|https://www.sciencedirect.com/science/article/pii/S2213133722000452||||2022|DUG Insight: A software package for big-data analysis and visualisation, and its demonstration for passive radar space situational awareness using radio telescopes|D. Grigg and S.J. Tingay and M. Sokolowski and R.B. Wayth|article|GRIGG2022100619|||Astronomy and Computing|22131337||40|||||21100241218|0,692|Q2|31|46|133|2012|475|132|3,53|43,74|Netherlands|Western Europe|2013-2020|Astronomy and Astrophysics (Q2); Computer Science Applications (Q2); Space and Planetary Science (Q2)|796|1.927|0.0025|530703180|1131265093
||Big Data, Genomics, Computer science, Theory and methods||100253||The management of the exponential growth of data that Next Generation Sequencing techniques produce has become a challenge for researchers that are forced to delve into an ocean of complex data in order to extract new insights to unravel the secrets of human diseases. Initially, this can be faced as a Big Data-related problem, but the genomic data have particular and relevant challenges that make them different from other Big Data working domains. Genomic data are much more heterogeneous; they are spread in hundreds of repositories, represented in multiple formats, and have different levels of quality. In addition, getting meaningful conclusions from genomic data requires considering all of the relevant surrounding knowledge that is under continuous evolution. In this scenario, the precise identification of what makes Genome Data Management so different is essential in order to provide effective Big Data-based solutions. Genomic projects require dealing with the technological problems associated with data management, nomenclature standards, and quality issues that only robust Information Systems that use Big Data techniques can provide. The main contribution of this paper is to present a Big Data-driven approach for managing genomic data, that is adapted to the particularities of the domain and to show its applicability to improve genetic diagnoses, which is the core of the development of accurate Precision Medicine.|https://doi.org/10.1016/j.bdr.2021.100253|https://www.sciencedirect.com/science/article/pii/S2214579621000708||||2021|Enhancing Precision Medicine: A Big Data-Driven Approach for the Management of Genomic Data|Ana León and Óscar Pastor|article|LEON2021100253|||Big Data Research|22145796||26|||||21100356018|0,565|Q2|25|11|76|547|324|69|4,06|49,73|United States|Northern America|2014-2020|Computer Science Applications (Q2); Information Systems (Q2); Information Systems and Management (Q2); Management Information Systems (Q2)|586|3.578|0.00126|531116416|1627174784
||Energy innovation, Interindustry architectural innovation, Sustainable energy, Fuel mix, Grey TOPSIS, grey linear programming||381-393||Economic, social and environmental requirements make planning for a sustainable electricity generation mix a demanding endeavour. Technological innovation offers a range of renewable generation and energy management options which require fine tuning and accurate control to be successful, which calls for the use of large-scale, detailed datasets. In this paper, we focus on the UK and use Multi-Criteria Decision Making (MCDM) to evaluate electricity generation options against technical, environmental and social criteria. Data incompleteness and redundancy, usual in large-scale datasets, as well as expert opinion ambiguity are dealt with using a comprehensive grey TOPSIS model. We used evaluation scores to develop a multi-objective optimization model to maximize the technical, environmental and social utility of the electricity generation mix and to enable a larger role for innovative technologies. Demand uncertainty was handled with an interval range and we developed our problem with multi-objective grey linear programming (MOGLP). Solving the mathematical model provided us with the electricity generation mix for every 5 min of the period under study. Our results indicate that nuclear and renewable energy options, specifically wind, solar, and hydro, but not biomass energy, perform better against all criteria indicating that interindustry architectural innovation in the power generation mix is key to sustainable UK electricity production and supply.|https://doi.org/10.1016/j.techfore.2018.04.031|https://www.sciencedirect.com/science/article/pii/S0040162517315147||||2019|Sustainable resource allocation for power generation: The role of big data in enabling interindustry architectural innovation|Konstantinos J. Chalvatzis and Hanif Malekpoor and Nishikant Mishra and Fiona Lettice and Sonal Choudhary|article|CHALVATZIS2019381|||Technological Forecasting and Social Change|00401625||144|||||14704|2,226|Q1|117|448|1099|35581|10127|1061|9,01|79,42|United States|Northern America|1970-2020|Applied Psychology (Q1); Business and International Management (Q1); Management of Technology and Innovation (Q1)|21,116|8.593|0.02416|531422887|1949868303
ICSE '19|Montreal, Quebec, Canada|software analytics, DevOps, AIOps|2|4–5|Proceedings of the 41st International Conference on Software Engineering: Companion Proceedings|AIOps is about empowering software and service engineers (e.g., developers, program managers, support engineers, site reliability engineers) to efficiently and effectively build and operate online services and applications at scale with artificial intelligence (AI) and machine learning (ML) techniques. AIOps can help improve service quality and customer satisfaction, boost engineering productivity, and reduce operational cost. In this technical briefing, we first summarize the real-world challenges in building AIOps solutions based on our practice and experience in Microsoft. We then propose a roadmap of AIOps related research directions, and share a few successful AIOps solutions we have built for Microsoft service products.|10.1109/ICSE-Companion.2019.00023|https://doi.org/10.1109/ICSE-Companion.2019.00023||IEEE Press||2019|AIOps: Real-World Challenges and Research Innovations|Dang, Yingnong and Lin, Qingwei and Huang, Peng|inproceedings|10.1109/ICSE-Companion.2019.00023|||||||||||||||||||||||||||||533553740|42
ICAIIS 2021|Chongqing, China||5||2021 2nd International Conference on Artificial Intelligence and Information Systems|In the electric power spot market, input data quality is critical to an accurate and reliable clearing result. Missing and abnormal data will lead to the result that the clearing algorithm diverges or the results mismatch reality. This would seriously affect power system security and reduce the efficiency of the market. Therefore, to ensure a smooth convergence of the algorithm and reasonable results, this paper proposes a rule-based verification method for the input data in the power spot market. Data integrity and logical verification rules are established. During the verification process, information will be divided into different warning levels and displayed so that users can modify relevant data according to the actual situation.|10.1145/3469213.3470246|https://doi.org/10.1145/3469213.3470246|New York, NY, USA|Association for Computing Machinery|9781450390200|2021|Rule-Based Data Verification Method in Electricity Spot Market|Wu, Yang and Zou, Wentao and Liu, Shuangquan and Jiang, Yan and Shao, Qizhuan and Zhou, Han|inproceedings|10.1145/3469213.3470246|46||||||||||||||||||||||||||||534930201|42
||data provenance, accountability, Big data, privacy|8|||Providing a 360° view of a given data item especially for sensitive data is essential toward not only protecting the data and associated privacy but also assuring trust, compliance, and ethics of the systems that use or manage such data. With the advent of General Data Protection Regulation, California Data Privacy Law, and other such regulatory requirements, it is essential to support data transparency in all such dimensions. Moreover, data transparency should not violate privacy and security requirements. In this article, we put forward a vision for how data transparency would be achieved in a de-centralized fashion using blockchain technology.|10.1145/3312750|https://doi.org/10.1145/3312750|New York, NY, USA|Association for Computing Machinery||2019|Data Transparency with Blockchain and AI Ethics|Bertino, Elisa and Kundu, Ahish and Sura, Zehra|article|10.1145/3312750|16|aug|J. Data and Information Quality|19361955|4|11|December 2019||||||||||||||||||||||535520637|833754770
||Big data, Risk assessment, Risk analysis, Information security, Security standards||102155||Data is one of the most important assets for all types of companies, which have undoubtedly grown their quantity and the ways of exploiting them. Big Data appears in this context as a set of technologies that manage data to obtain information that supports decision-making. These systems were not conceived to be secure, resulting in significant risks that must be controlled. Security risks in Big Data must be analyzed and managed in an appropriate manner to protect the system and secure the information and the data being handled. This paper proposes a risk analysis approach for Big Data environments, which is based on a security analysis methodology called MARISMA (Methodology for the Analysis of Risks on Information System), supported by a technological environment in the cloud (eMARISMA tool) already used by numerous clients. Both MARISMA and eMARISMA are specifically designed to be easily adapted to particular contexts, such as Big Data. Our proposal, called MARISMA-BiDa, is based on the main related standards, such as ISO/IEC 27,000 and 31,000, or the NIST Big Data reference architecture or ENISA and CSA recommendations for Big Data.|https://doi.org/10.1016/j.cose.2020.102155|https://www.sciencedirect.com/science/article/pii/S0167404820304284||||2021|MARISMA-BiDa pattern: Integrated risk analysis for big data|David G. Rosado and Julio Moreno and Luis E. Sánchez and Antonio Santos-Olmo and Manuel A. Serrano and Eduardo Fernández-Medina|article|ROSADO2021102155|||Computers & Security|01674048||102|||||28898|0,861|Q1|92|321|559|17204|3843|550|6,75|53,60|United Kingdom|Western Europe|1982-2020|Computer Science (miscellaneous) (Q1); Law (Q1)||||535526219|1687137310
||Sensors;Quality control;Metadata;Meteorology;Interpolation;Quality assessment;Time series analysis;data quality;data stream processing;spatial-temporal data;quality control;interpolation||2636-2645|2016 IEEE International Conference on Big Data (Big Data)|There is a need for comprehensive solutions to address the challenges of spatio-temporal data quality assessment. Emphasis is often placed on the quality assessment of individual observations from sensors but not on the sensors themselves nor upon site metadata such as location and timestamps. The focus of this paper is on the development and evaluation of a representative and comprehensive, interpolation-based methodology for assessment of spatio-temporal data quality. We call our method the SMART method, short for Simple Mappings for the Approximation and Regression of Time series. When applied to a real-world, meteorological data set, we show that our method outperforms standard interpolators and we identify numerous problematic sites that otherwise would not have been flagged as bad. We further identify sites for which metadata is incorrect. We believe that there are many problems with real data sets like these and, in the absence of an approach like ours, these problems have largely gone unidentified. Our results bring into question the validity of provider-based quality control indicators. In addition to providing a comprehensive solution, our approach is novel for the simple but effective way that it accounts for spatial and temporal variation.|10.1109/BigData.2016.7840906|||||2016|The SMART approach to comprehensive quality assessment of site-based spatial-temporal data|Angryk, Rafal A. and Galarus, Douglas E.|inproceedings|7840906||Dec|||||||||||||||||||||||||||538109670|42
||Urban computing, location-based social networks, urban sensing, city dynamics, big data, urban societies, urban informatics|39|||Urban computing is an emerging area of investigation in which researchers study cities using digital data. Location-Based Social Networks (LBSNs) generate one specific type of digital data that offers unprecedented geographic and temporal resolutions. We discuss fundamental concepts of urban computing leveraging LBSN data and present a survey of recent urban computing studies that make use of LBSN data. We also point out the opportunities and challenges that those studies open.|10.1145/3301284|https://doi.org/10.1145/3301284|New York, NY, USA|Association for Computing Machinery||2019|Urban Computing Leveraging Location-Based Social Network Data: A Survey|Silva, Thiago H. and Viana, Aline Carneiro and Benevenuto, Fabr\'{\i}cio and Villas, Leandro and Salles, Juliana and Loureiro, Antonio and Quercia, Daniele|article|10.1145/3301284|17|feb|ACM Comput. Surv.|03600300|1|52|January 2020||||23038|2,079|Q1|163|112|365|15859|6978|365|19,16|141,60|United States|Northern America|1969-2020|Computer Science (miscellaneous) (Q1); Theoretical Computer Science (Q1)|10,790|10.282|0.01438|541037723|1517405264
||Big Data;Data integrity;Meteorology;Monitoring;Data mining;Organizations;Big Data;Big Data Quality;Data Quality;preprocessing;pre-processing||559-563|2019 International Conference on Machine Learning, Big Data, Cloud and Parallel Computing (COMITCon)|Big Data has become an imminent part of all industries and business sectors today. All organizations in any sector like energy, banking, retail, hardware, networking, etc all generate huge quantum of heterogenous data which if mined, processed and analyzed accurately can reveal immensely useful patterns for business heads to apply to generate and grow their businesses. Big Data helps in acquiring, processing and analyzing large amounts of heterogeneous data to derive valuable results. Quality of information is affected by size, speed and format in which data is generated. Hence, Quality of Big Data is of great relevance and importance. We propose addressing various aspects of the raw data to improve its quality in the pre-processing stage, as the raw data may not usable as-is. We are exploring process like Cleansing to fix as much data as feasible, Noise filters to remove bad data, as well sub-processes for Integration and Filtering along with Data Transformation/Normalization. We evaluate and profile the Big Data during acquisition stage, which is adapted to expectations to avoid cost overheads later while also improving and leading to accurate data analysis. Hence, it is imperative to improve Data quality even it is absorbed and utilized in an industry's Big Data system. In this paper, we propose a Pre-Processing Framework to address quality of data in a weather monitoring and forecasting application that also takes into account global warming parameters and raises alerts/notifications to warn users and scientists in advance.|10.1109/COMITCon.2019.8862267|||||2019|Big Data Quality Framework: Pre-Processing Data in Weather Monitoring Application|Juneja, Ashish and Das, Nripendra Narayan|inproceedings|8862267||Feb|||||||||||||||||||||||||||543231427|42
||Big Data, Industry 4.0, Big Data analytics, Big Data architecture, Bosch||750-760||People, devices, infrastructures and sensors can constantly communicate exchanging data and generating new data that trace many of these exchanges. This leads to vast volumes of data collected at ever increasing velocities and of different variety, a phenomenon currently known as Big Data. In particular, recent developments in Information and Communications Technologies are pushing the fourth industrial revolution, Industry 4.0, being data generated by several sources like machine controllers, sensors, manufacturing systems, among others. Joining volume, variety and velocity of data, with Industry 4.0, makes the opportunity to enhance sustainable innovation in the Factories of the Future. In this, the collection, integration, storage, processing and analysis of data is a key challenge, being Big Data systems needed to link all the entities and data needs of the factory. Thereby, this paper addresses this key challenge, proposing and implementing a Big Data Analytics architecture, using a multinational organisation (Bosch Car Multimedia – Braga) as a case study. In this work, all the data lifecycle, from collection to analysis, is handled, taking into consideration the different data processing speeds that can exist in the real environment of a factory (batch or stream).|https://doi.org/10.1016/j.ijinfomgt.2017.07.012|https://www.sciencedirect.com/science/article/pii/S0268401217306023||||2017|A Big Data system supporting Bosch Braga Industry 4.0 strategy|Maribel Yasmina Santos and Jorge {Oliveira e Sá} and Carina Andrade and Francisca {Vale Lima} and Eduarda Costa and Carlos Costa and Bruno Martinho and João Galvão|article|SANTOS2017750|||International Journal of Information Management|02684012|6|37|||||15631|2,770|Q1|114|224|382|20318|5853|373|16,16|90,71|United Kingdom|Western Europe|1970, 1986-2021|Artificial Intelligence (Q1); Computer Networks and Communications (Q1); Information Systems (Q1); Information Systems and Management (Q1); Library and Information Sciences (Q1); Management Information Systems (Q1); Marketing (Q1)|12,245|14.098|0.01167|545652517|747927863
ICIME 2017|Barcelona, Spain|data quality methodology, data quality dimensions, data quality problems, data quality management, Data quality|6|40–45|Proceedings of the 9th International Conference on Information Management and Engineering|Data quality (DQ) is an important issue for modern organizations, mainly for decision-making based on information, using solutions such as CRM, Business Analytics, and Big Data. In order to obtain quality data, it is necessary to implement methods, processes, and specific techniques that handle information as a product, with well established, controlled, and managed production processes. The literature provides several types of quality data management methodologies that treat structured data, and few treating semi- and non-structured data. Choosing the methodology to be adopted is one the major issues faced by organizations, when challenged to treat the data quality in a systematic manner. This paper makes a comparative analysis between TDQM -- Total Data Quality Management and TIQM -- Total Information Quality Management approaches, focusing on data quality problems in the context of a CRM -- Costumer Relationship Management application. Such analysis identifies the strengths and weaknesses of each methodology and suggests the most suitable for the CRM scenario.|10.1145/3149572.3149575|https://doi.org/10.1145/3149572.3149575|New York, NY, USA|Association for Computing Machinery|9781450353373|2017|Total Data Quality Management and Total Information Quality Management Applied to Costumer Relationship Management|Francisco, Maritza M. C. and Alves-Souza, Solange N. and Campos, Edit G. L. and De Souza, Luiz S.|inproceedings|10.1145/3149572.3149575|||||||||||||||||||||||||||||548848459|42
ICEGOV '18|Galway, Ireland|Transparency, Collaboration, Open government, E-Participation, Participation, Policy, E-government|8|191–198|Proceedings of the 11th International Conference on Theory and Practice of Electronic Governance|This paper describes why and how a conceptual framework for e-participation and open government has been developed and applied to six aspirant EU countries in the Western Balkans. It provides a rationale and background, and then examines the main academic and other relevant sources used. This is followed by an overview of the conceptual framework and a description of its main elements. Finally, the paper examines international data on e-participation covering the Western Balkan countries, uses this to examine the results of applying the conceptual framework in each country, and then provides conclusions and recommendations.|10.1145/3209415.3209459|https://doi.org/10.1145/3209415.3209459|New York, NY, USA|Association for Computing Machinery|9781450354219|2018|A Roadmap for E-Participation and Open Government: Empirical Evidence from the Western Balkans|Millard, Jeremy and Thomasen, Louise and Pastrovic, Goran and Cvetkovic, Bojan|inproceedings|10.1145/3209415.3209459|||||||||||||||||||||||||||||550467459|42
https://doi.org/10.1016/j.medine.2018.06.006|https://www.sciencedirect.com/science/article/pii/S217357271930013X||||2019|Big data and machine learning in critical care: Opportunities for collaborative research|A. {Núñez Reiz}|article|NUNEZREIZ201952|||Medicina Intensiva (English Edition)|21735727|1|43||||||||||||||||||||||||||||||551723239|42
||Edge analytics, Edge computing, Edge devices, Big data, Sensor, Artificial intelligence, Machine learning, Smart technology, Healthcare||||Edge technology aims to bring cloud resources (specifically, the computation, storage, and network) to the closed proximity of the edge devices, i.e., smart devices where the data are produced and consumed. Embedding computing and application in edge devices lead to emerging of two new concepts in edge technology: edge computing and edge analytics. Edge analytics uses some techniques or algorithms to analyse the data generated by the edge devices. With the emerging of edge analytics, the edge devices have become a complete set. Currently, edge analytics is unable to provide full support to the analytic techniques. The edge devices cannot execute advanced and sophisticated analytic algorithms following various constraints such as limited power supply, small memory size, limited resources, etc. This article aims to provide a detailed discussion on edge analytics. The key contributions of the paper are as follows-a clear explanation to distinguish between the three concepts of edge technology: edge devices, edge computing, and edge analytics, along with their issues. In addition, the article discusses the implementation of edge analytics to solve many problems and applications in various areas such as retail, agriculture, industry, and healthcare. Moreover, the research papers of the state-of-the-art edge analytics are rigorously reviewed in this article to explore the existing issues, emerging challenges, research opportunities and their directions, and applications.|https://doi.org/10.1016/j.dcan.2022.10.016|https://www.sciencedirect.com/science/article/pii/S2352864822002255||||2022|A review on edge analytics: Issues, challenges, opportunities, promises, future directions, and applications|Sabuzima Nayak and Ripon Patgiri and Lilapati Waikhom and Arif Ahmed|article|NAYAK2022|||Digital Communications and Networks|23528648|||||||21100823476|1,082|Q1|26|77|105|3226|881|96|8,81|41,90|China|Asiatic Region|2015-2020|Communication (Q1); Computer Networks and Communications (Q1); Hardware and Architecture (Q1)|823|6.797|0.00138|556311465|493706778
||Big data analytics, organisational readiness, organisational capabilities, frameworks, business analytics, thematic analysis||3-10||Despite some of the initial hype from marketers and consultants, the use of big data is now firmly established in many organisations worldwide. Big data analytics (BDA) is making use of huge volumes of data from a wide range of structured and unstructured sources. Surveys have however reported a number of barriers to organisational effectiveness with BDA. This research aims to determine what capabilities large organisations require to be ready for a successful BDA initiative. Drawing mainly on relevant results of two published research articles, key informed stakeholders from a large South African telecommunications company were interviewed on this topic. Thematic analysis identified the key themes and sub-themes relating to capabilities needed for the organization to be ready for effective BDA. These proved to be very similar to those given in the earlier research, although a new capability of legal compliance for data protection was now added.|https://doi.org/10.1016/j.procs.2019.12.147|https://www.sciencedirect.com/science/article/pii/S1877050919321866||||2019|Capabilities and Readiness for Big Data Analytics|Jenifer Pedro and Irwin Brown and Mike Hart|article|PEDRO20193|||Procedia Computer Science|18770509||164||CENTERIS 2019 - International Conference on ENTERprise Information Systems / ProjMAN 2019 - International Conference on Project MANagement / HCist 2019 - International Conference on Health and Social Care Information Systems and Technologies, CENTERIS/ProjMAN/HCist 2019|||19700182801|0,334|-|76|1907|6483|38511|13722|6359|2,09|20,19|Netherlands|Western Europe|2010-2020|Computer Science (miscellaneous)||||556602562|2108686752
||Quality management, process management, data quality management, data governance, costs of poor quality, big data, digital transformation, data privacy, data monetization||3-30|Meeting the Challenges of Data Quality Management|This chapter analyzes the role of data quality management in response to the rapid evolution of data in our world. It discusses the impact of poor-quality data on organizations, focusing on the costs and risks associated with poorly managed data. In many organizations, poor-quality data is tolerated to a degree that poor-quality products would not be. Data quality management reduces the costs and risks of poor-quality data and enables the benefits and opportunities of high-quality data, especially in an age of big data, digital transformation, and artificial intelligence.|https://doi.org/10.1016/B978-0-12-821737-5.00001-8|https://www.sciencedirect.com/science/article/pii/B9780128217375000018||Academic Press|978-0-12-821737-5|2022|Chapter 1 - The Importance of Data Quality Management|Laura Sebastian-Coleman|incollection|SEBASTIANCOLEMAN20223||||||||||Laura Sebastian-Coleman|||||||||||||||||||557509424|42
||Real-time analytics, Complex event processing, Streaming, Event processing, Advanced analytics, Data analysis, Machine learning, Finance, EventSwarm||39-61|Big Data|Real-time analytics is a special kind of Big Data analytics in which data elements are required to be processed and analyzed as they arrive in real time. It is important in situations where real-time processing and analysis can deliver important insights and yield business value. This chapter provides an overview of current processing and analytics platforms needed to support such analysis, as well as analytics techniques that can be applied in such environments. The chapter looks beyond traditional event processing system technology to consider a broader big data context that involves “data at rest” platforms and solutions. The chapter includes a case study showing the use of EventSwarm complex event processing engine for a class of analytics problems in finance. The chapter concludes with several research challenges, such as the need for new approaches and algorithms required to support real-time data filtering, data exploration, statistical data analysis, and machine learning.|https://doi.org/10.1016/B978-0-12-805394-2.00002-7|https://www.sciencedirect.com/science/article/pii/B9780128053942000027||Morgan Kaufmann|978-0-12-805394-2|2016|Chapter 2 - Real-Time Analytics|Z. Milosevic and W. Chen and A. Berry and F.A. Rabhi|incollection|MILOSEVIC201639||||||||||Rajkumar Buyya and Rodrigo N. Calheiros and Amir Vahid Dastjerdi|||||||||||||||||||557818947|42
CIKM '20|Virtual Event, Ireland|formal methods, big data analytics, processing mining, machine learning, deep learning, text mining, process calculi|2|3531–3532|Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management|DataMod 2020 aims to bring together practitioners and researchers from academia, industry and research institutions interested in the combined application of computational modelling methods with data-driven techniques from the areas of knowledge management, data mining and machine learning. Modelling methodologies of interest include automata, agents, Petri nets, process algebras and rewriting systems. Application domains include social systems, ecology, biology, medicine, smart cities, governance, security, education, software engineering, and any other field that deals with complex systems and large amounts of data. Papers can present research results in any of the themes of interest for the symposium as well as application experiences, tools and promising preliminary ideas. Papers dealing with synergistic approaches that integrate modelling and knowledge management/discovery or that exploit knowledge management/discovery to develop/syntesise system models are especially welcome.|10.1145/3340531.3414073|https://doi.org/10.1145/3340531.3414073|New York, NY, USA|Association for Computing Machinery|9781450368599|2020|"\"DataMod2020: 9th International Symposium \"\"From Data to Models and Back\"\"\""|Bowles, Juliana and Broccia, Giovanna and Nanni, Mirco|inproceedings|10.1145/3340531.3414073|||||||||||||||||||||||||||||558758885|42
||Industries;Machine learning algorithms;Data integrity;Clustering algorithms;Medical services;Big Data;Tools;Big Data;Data Quality;Data Inaccuracy;Data incompleteness;Machine Learning||58-66|2020 3rd International Conference on Emerging Trends in Electrical, Electronic and Communications Engineering (ELECOM)|Tackling Data Quality issues as part of Big Data can be challenging. For data cleansing activities, manual methods are not efficient due to the potentially very large amount of data.. This paper aims to qualitatively assess the possibilities for using machine learning in the process of detecting data incompleteness and inaccuracy, since these two data quality dimensions were found to be the most significant by a previous research study conducted by the authors. A review of existing literature concludes that there is no unique machine learning algorithm most suitable to deal with both incompleteness and inaccuracy of data. Various algorithms are selected from existing studies and applied against a representative big (healthcare) dataset. Following experiments, it was also discovered that the implementation of machine learning algorithms in this context encounters several challenges for Big Data quality activities. These challenges are related to the amount of data particualar machine learning algorithms can scale to and also to certain data type restrictions imposed by some machine learning algorithms. The study concludes that 1) data imputation works better with linear regression models, 2) clustering models are more efficient to detect outliers but fully automated systems may not be realistic in this context. Therefore, a certain level of human judgement is still needed.|10.1109/ELECOM49001.2020.9297009|||||2020|A Qualitative Assessment of Machine Learning Support for Detecting Data Completeness and Accuracy Issues to Improve Data Analytics in Big Data for the Healthcare Industry|Juddoo, Suraj and George, Carlisle|inproceedings|9297009||Nov|||||||||||||||||||||||||||558983246|42
||Software as a service;Checkpointing;Fault tolerance;Fault tolerant systems;Ontologies;Computer architecture||1131-1140|2015 IEEE 8th International Conference on Cloud Computing|Software-as-a-Service (SaaS) is a model of cloud computing in which software functions are delivered to the users as services. The past few years have witnessed its global flourishing. In the foreseeable future, SaaS applications will integrate with the Internet of Things, Mobile Computing, Big Data, Wireless Sensor Networks, and many other computing and communication technologies to deliver customizable intelligent services to a vast population. This will give rise to an era of what we call Big SaaS systems of unprecedented complexity and scale. They will have huge numbers of tenants/users interrelated in complex ways. The code will be complex too and require Big Data but provide great value to the customer. With these benefits come great societal risks, however, and there are other drawbacks and challenges. For example, it is difficult to ensure the quality of data and metadata obtained from crowd sourcing and to maintain the integrity of conceptual model. Big SaaS applications will also need to evolve continuously. This paper will discuss how to address these challenges at all stages of the software lifecycle.|10.1109/CLOUD.2015.167|||||2015|Big SaaS: The Next Step beyond Big Data|Zhu, Hong and Bayley, Ian and Younas, M. and Lightfoot, David and Yousef, Basel and Liu, Dongmei|inproceedings|7214177||June||21596190|||||||||||||||||||||||||559269801|919506426
||knowledge graph, Cyber threat intelligence, ontology, security|22|||For a strong, collective defense in the digital domain, we need to produce, consume, analyze, and share cyber threat intelligence. With an increasing amount of available information, we need automation to ensure adequate efficiency. We present the results from a questionnaire investigating the use of standards and standardization and how practitioners share and use cyber threat intelligence (CTI). We propose a strict data model for CTI that enables consumption of all relevant data, data validation, and analysis of consumed content. The main contribution of this article is insight into how CTI is shared and used by practitioners, and the strictness of the data model that enforces input of information and enables automation and deduction of new knowledge.|10.1145/3458027|https://doi.org/10.1145/3458027|New York, NY, USA|Association for Computing Machinery||2021|Investigating Sharing of Cyber Threat Intelligence and Proposing A New Data Model for Enabling Automation in Knowledge Representation and Exchange|Bromander, Siri and Swimmer, Morton and Muller, Lilly Pijnenburg and J\o{}sang, Audun and Eian, Martin and Skj\o{}tskift, Geir and Borg, Fredrik|article|10.1145/3458027|6|oct|Digital Threats|26921626|1|3|March 2022||||||||||||||||||||||561013842|1395644191
ISAIMS 2021|Beijing, China|deep learning, machine learning, type 1 diabetes, reinforcement learning, artificial intelligence, deep neural networks|10|351–360|Proceedings of the 2nd International Symposium on Artificial Intelligence for Medicine Sciences|Diabetes is a metabolic disease with the characteristic of hyperglycemia. The pathogenic principle is derived from the defect of insulin secretion or the impairment of biological effects, or both. We use machine learning models and deep learning models for forecasting future blood glucose levels in this paper, and study the efficiency of detecting hypoglycemia and hyperglycemia events. The data set used is in-silico data generated from the UVA/PADOVA type 1 diabetes simulator. We aim to compare support vector machines, random forests, linear regression, K-Nearest Neighbors regression (KNN), XGBoosted trees and other deep learning models in terms of Root Mean Squared Error (RMSE), and other several evaluation metrics to study their effectiveness in predicting future blood sugar, and the accuracy rate of predicting hypoglycemia and hyperglycemia events. In this work, we found a bidirectional long-short term memory (LSTM) model with the best prediction effect, which can predict the blood glucose level of simulated patients with leading accuracy within 30 minutes (RMSE = 7.55±0.19 [mg/dl], R2-SCORE=0.96). The hopeful results show that this method could have practical application value for self-management of blood glucose in patients with type 1 diabetes.|10.1145/3500931.3500993|https://doi.org/10.1145/3500931.3500993|New York, NY, USA|Association for Computing Machinery|9781450395588|2021|A Comparison of Machine Learning Algorithms in Blood Glucose Prediction for People with Type 1 Diabetes|Wang, Yiyang|inproceedings|10.1145/3500931.3500993|||||||||||||||||||||||||||||561035255|42
||||280-284|||https://doi.org/10.1016/j.mnl.2016.01.001|https://www.sciencedirect.com/science/article/pii/S1541461215300094||||2016|The Role of the Chief Nurse Executive in the Big Data Revolution|Jane Englebright and Barbara Caspers|article|ENGLEBRIGHT2016280|||Nurse Leader|15414612|4|14|||||28813|0,264|Q3|13|151|293|1667|106|248|0,39|11,04|United States|Northern America|2003-2020|Leadership and Management (Q3)||||564524344|390588809
SOCC '13|Santa Clara, California||2||Proceedings of the 4th Annual Symposium on Cloud Computing|To date, much research in data-intensive computing has focused on batch computation. Increasingly, however, it is necessary to derive knowledge from big data streams. As a motivating example, consider a content delivery network (CDN) such as Akamai [4], comprising thousands of servers in hundreds of globally distributed locations. Each of these servers produces a stream of log data, recording for example every user it serves, along with each video stream they access, when they play and pause streams, and more. Each server also records network- and system-level data such as TCP connection statistics. In aggregate, the servers produce billions of lines of log data from over a thousand locations daily.|10.1145/2523616.2525963|https://doi.org/10.1145/2523616.2525963|New York, NY, USA|Association for Computing Machinery|9781450324281|2013|Wide-Area Streaming Analytics: Distributing the Data Cube|Heintz, Benjamin and Chandra, Abhishek and Sitaraman, Ramesh K.|inproceedings|10.1145/2523616.2525963|55||||||||||||||||||||||||||||564744278|42
||Clouds;Remote sensing;Satellites;Earth;Image color analysis;Independent component analysis;Algorithm design and analysis;Independent component analysis (ICA);Landsat 8;Operational land imager (OLI) data;Quality assessment (QA) band;Thin clouds and their removal||921-924|2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)|Using independent component analysis (ICA) coupled with the quality assessment (QA) band of Landsat 8, an approach for thin cloud removal in Landsat 8 operational land imager (OLI) data was developed. After the ICA transformation of the visible, near infrared, short-wavelength and cirrus bands of OLI data, cloud component was identified by the mixing matrix. Then, a cloud mask derived from the analysis of the QA band was formed such that an image pixel with and without cloud cover was delineated. The cloud component and cloud mask were used to remove the thin clouds. Thin clouds disappeared visually within the OLI data. Using another cloud-free image acquired in the previous overflight as the reference, we assessed the accuracy level of the cloud removal. Before and after the cloud removal, the spatial correlation coefficients increased from 0.69 to 0.83 in band 1, 0.75 to 0.86 in band 2, 0.81 to 0.88 in band 3, 0.87 to 0.91 in band 4, and no change in bands 5, 6, and 7 for pixels identified with cloud cover.|10.1109/IGARSS.2015.7325916|||||2015|Thin cloud removal for Landsat 8 OLI data using independent component analysis|Shen, Yang and Wang, Yong and Lv, Haitao|inproceedings|7325916||July||21537003|||||||||||||||||||||||||568255285|1296020740
WIMS '14|Thessaloniki, Greece|LP-Based Hiding Approaches, Knowledge Hiding, Frequent Itemset Hiding, Privacy Preserving Data Mining|11||Proceedings of the 4th International Conference on Web Intelligence, Mining and Semantics (WIMS14)|The widespread use of the Internet caused the rapid growth of data on the Web. But as data on the Web grew larger in numbers, so did the perils due to the applications of data mining. Privacy preserving data mining (PPDM) is the field that investigates techniques to preserve the privacy of data and patterns. Knowledge Hiding, a subfield of PPDM, aims at preserving the sensitive patterns included in the data, which are going to be published. A wide variety of techniques fall under the umbrella of Knowledge Hiding, such as frequent pattern hiding, sequence hiding, classification rule hiding and so on.In this tutorial we create a taxonomy for the frequent itemset hiding techniques. We also provide as examples for each category representative works that appeared recently and fall into each one of these categories. Then, we focus on the detailed overview of a specific category, the so called linear programming-based techniques. Finally, we make a quantitative and qualitative comparison among some of the existing techniques that are classified into this category.|10.1145/2611040.2611044|https://doi.org/10.1145/2611040.2611044|New York, NY, USA|Association for Computing Machinery|9781450325387|2014|Knowledge Sanitization on the Web|Kagklis, Vasileios and Verykios, Vassilios S. and Tzimas, Giannis and Tsakalidis, Athanasios K.|inproceedings|10.1145/2611040.2611044|4||||||||||||||||||||||||||||569742930|42
||Big Data, Data Warehouse, Cluster, Education, IDEB||1031-1039||This study aims to understand the assessment of basic education in the perspective of the State Reviewer as a mechanism that generates information regarding the positivity and weaknesses of a school or an educational system to provide improvements. For this reason, a Data Warehouse was created and later some analysis of the indicators were performed through clustering.|https://doi.org/10.1016/j.procs.2015.07.061|https://www.sciencedirect.com/science/article/pii/S1877050915015367||||2015|Primary Education Evaluation in Brazil Using Big Data and Cluster Analysis|Thiago Graca Ramos and Jean Cristian Ferreira Machado and Bruna Principe Vieira Cordeiro|article|RAMOS20151031|||Procedia Computer Science|18770509||55||3rd International Conference on Information Technology and Quantitative Management, ITQM 2015|||19700182801|0,334|-|76|1907|6483|38511|13722|6359|2,09|20,19|Netherlands|Western Europe|2010-2020|Computer Science (miscellaneous)||||571826151|2108686752
|||3|70–72|||10.1145/2590989.2591002|https://doi.org/10.1145/2590989.2591002|New York, NY, USA|Association for Computing Machinery||2014|Report on the Second International Workshop on Energy Data Management (EnDM 2013)|Pedersen, Torben Bach and Lehner, Wolfgang|article|10.1145/2590989.2591002||feb|SIGMOD Rec.|01635808|4|42|December 2013||||13622|0,372|Q2|142|39|81|808|127|57|1,67|20,72|United States|Northern America|1969, 1973-1978, 1981-2020|Information Systems (Q2); Software (Q2)|1,819|0.775|7.5E-4|576084512|962972343
AMECSE '16|Cairo, Egypt|Ontology, Electronic Health Record Management, Query Language|6|40–45|Proceedings of the 2nd Africa and Middle East Conference on Software Engineering|Electronic health record (EHR) solutions are complex, spanning multiple specialties and domains of expertise. These systems need to handle clinical concepts, temporal data, documents, and financial transactions, which leads to a large code base that is tightly coupled with data models and inherently hard to maintain. These difficulties can greatly increase the cost of developing EHR systems, result in a high failure rate of implementation, and threaten investments in this sector. Moreover, due to the wide variance in the level of detail across different settings, data exchange is becoming a serious problem, further increasing the cost of development and maintenance.To overcome these issues, we adopt ontologies to model our proposed EHR solution, not only allowing code reuse; but also enabling later extension and customization. Adopting software factory techniques, we build tools to transform ontological models into deployment-ready code. This automatically provides handling of data persistence, access, and exchange. Business logic is expressed as ontology-based process flows and rules, ensuring data quality and supporting special needs. This logic is enforced transparently and can be modified on the fly. We optimized the user experience by facilitating fast data entry and retrieval.In this paper, we present the requirements of an effective EHR solution, explain the techniques we employed, describe the main modules of our proposed system, and discuss the technical decisions we made.|10.1145/2944165.2944172|https://doi.org/10.1145/2944165.2944172|New York, NY, USA|Association for Computing Machinery|9781450342933|2016|Building an Ontology-Based Electronic Health Record System|El-Atawy, Sameh S. and Khalefa, Mohamed E.|inproceedings|10.1145/2944165.2944172|||||||||||||||||||||||||||||580108167|42
SSTD '19|Vienna, Austria||4|222–225|Proceedings of the 16th International Symposium on Spatial and Temporal Databases|The research field of moving objects has been quite active in the past 20 years. The recording of position data becomes easy and huge amounts of mobile data are collected. Moving objects databases represent time-dependent objects and support queries with spatial and temporal constraints. In this paper we provide the vision of a multi-model and intelligent moving objects database. The goal is to enhance the data management of moving objects by providing extensive data models for different applications and fusing artificial intelligence techniques. Toward this goal, we propose how to develop corresponding modules and integrate them into the system to achieve the next-generation moving objects database.|10.1145/3340964.3340975|https://doi.org/10.1145/3340964.3340975|New York, NY, USA|Association for Computing Machinery|9781450362801|2019|Understanding Human Mobility: A Multi-Modal and Intelligent Moving Objects Database|"\"Xu, Jianqiu and Lu, Hua and G\"\"{u}ting, Ralf Hartmut\""|inproceedings|10.1145/3340964.3340975|||||||||||||||||||||||||||||582874634|42
||Temperature measurement;Temperature sensors;Rain;Smart phones;Humidity;Distributed devices;Measurement;Localized Rain||1405-1410|2014 IEEE International Instrumentation and Measurement Technology Conference (I2MTC) Proceedings|With rain-related hazards, it is difficult to forecast and prepare for it due to the decline in the availability and reliability of global daily weather reports. Therefore, we need to make use of available unconventional commercial weather instruments to provide supplementary information to existing systems. A more accurate and reliable weather forecast can then be made due to the prompt availability of information from ubiquitous devices. As smartphones become a widely used device, we propose a conceptual design of a multi-device ground weather observation network using smartphones and other sensors. In this paper, we first investigate the differences between smartphone-based sensors and other sensors to determine issues to address for big data on global weather information. In our experiments, we found that the data quality differs among devices in a very small area of 100 m grid.|10.1109/I2MTC.2014.6860977|||||2014|Surface weather observation via distributed devices|Fresco Zamora, Jane Louie and Sawada, Naoya and Sahara, Takemi and Kashihara, Shigeru and Taenaka, Yuzo and Yamaguchi, Suguru|inproceedings|6860977||May||10915281|||||||||||||||||||||||||583670306|1604477394
||Big Data;Organizations;Decision making;Standards organizations;Learning management systems;Market research;Big Data;Big Data Analytics;universities decision making;data quality||1-5|2018 International Conference on Advances in Big Data, Computing and Data Communication Systems (icABCD)|The technology Big Bang has seen organizations universities inclusive generating big volumes of data in various formats and at high speed than they used to do. Such data is referred to as Big Data. This voluminous data can be of great significance to organizations if better insights are drawn for management to improve decision making. However, to draw valued insight from Big Data, advanced forms of analytics need to be employed and such techniques are commonly known as Big Data analytics (BDA), This paper sough to report on analysis of factors influencing the leverage of BDA to improve performance in universities.|10.1109/ICABCD.2018.8465132|||||2018|Improve Decision Making Towards Universities Performance Through Big Data Analytics|Segooa, Mmatshuene Anna and Kalema, Billy Mathias|inproceedings|8465132||Aug|||||||||||||||||||||||||||584596243|42
||Type your keywords here, separated by semicolons||729-736||Today, the Big Data term has a multidimensional approach where five main characteristics stand out: volume, velocity, veracity, value and variety. It has changed from being an emerging theme to a growing research area. In this respect, this study analyses the literature on Big Data in the Economics, Econometrics and Finance field. To do that, 1.034 publications from 2015 to 2019 were evaluated using SciMAT as a bibliometric and network analysis software. SciMAT offers a complete approach of the field and evaluates the most cited and productive authors, countries and subject areas related to Big Data. Lastly, a science map is performed to understand the intellectual structure and the main research lines (themes).|https://doi.org/10.1016/j.procs.2019.12.044|https://www.sciencedirect.com/science/article/pii/S1877050919320551||||2019|The last five years of Big Data Research in Economics, Econometrics and Finance: Identification and conceptual analysis|José Ricardo López-Robles and Marisela Rodríguez-Salvador and Nadia Karina Gamboa-Rosales and Selene Ramirez-Rosales and Manuel Jesús Cobo|article|LOPEZROBLES2019729|||Procedia Computer Science|18770509||162||7th International Conference on Information Technology and Quantitative Management (ITQM 2019): Information technology and quantitative management based on Artificial Intelligence|||19700182801|0,334|-|76|1907|6483|38511|13722|6359|2,09|20,19|Netherlands|Western Europe|2010-2020|Computer Science (miscellaneous)||||587324215|2108686752
||Big Data;Data integrity;Data analysis;Software;Investment;Companies;Big data;business analytics;Hadoop;people;talent gap;implementation||58-66||We attempt to demonstrate the value of big data to enterprises by interweaving the perceptions, challenges, and opportunities of big data for businesses. While enterprises are aware of the value of big data to their businesses, there are challenges of exploiting big data in terms of data quality and usage. Executives might lack knowledge on how applications are related to one another in the big data ecosystem and the business benefits to reap. We summarize the results of a research study that explores emerging business perceptions of big data. We examine the current practice in 20 large enterprises, each having an annual revenue of more than USD 0.5 billion and present our findings related to executive perceptions. We provide insights on how firms can develop their big data expertise along various dimensions and identify critical ideas to be further investigated to better understand the issues that practitioners and researchers might be equally grappling with.|10.1109/EMR.2019.2900208|||||2019|Big Data Technology: Challenges, Prospects, and Realities|Singh, Nitin and Lai, Kee-Hung and Vejvar, Markus and Cheng, T. C. E.|article|8658160||Firstquarter|IEEE Engineering Management Review|19374178|1|47|||||||||||||||||||||||588977726|1983073660
||k-anonymity, quasi identifier attributes, big data, anonymization, privacy||52-59||Big data has become omnipresent and crucial for many application domains. Big data makes reference to the explosive quantity of data generated in today’s society that might contain personally identifiable information (PII). That’s why the challenge from the point of view of data privacy is one of the major hurdles for the application of big data. In that situation, several techniques were exposed in order to ensure privacy in big data including generalization, randomization and cryptographic techniques as well. It is well known that there exist two main types of attributes in the literature, quasi identifier and sensitive attributes. In this paper, we are going to focus on quasi identifier attributes. Over the years, k-anonymity has been treated with great interest as an anonymization technique ensuring privacy in big data when we are dealing with quasi identifier attributes. Despite the fact that many algorithms of k-anonymity have been proposed, most of them admit that the threshold k of k-anonymity has to be known before anonymizing the data set. Here, a novel way in applying k-anonymity for quasi identifier attributes is presented. It’s a new algorithm called “k-anonymity without prior value of the threshold k”. Our proposed algorithm was experimentally evaluated using a test table of quasi identifier attributes. Furthermore, we highlight all the steps of our proposed algorithm with detailed comments.|https://doi.org/10.1016/j.procs.2018.01.097|https://www.sciencedirect.com/science/article/pii/S187705091830108X||||2018|A new technique ensuring privacy in big data: K-anonymity without prior value of the threshold k|Zakariae {El Ouazzani} and Hanan {El Bakkali}|article|ELOUAZZANI201852|||Procedia Computer Science|18770509||127||PROCEEDINGS OF THE FIRST INTERNATIONAL CONFERENCE ON INTELLIGENT COMPUTING IN DATA SCIENCES, ICDS2017|||19700182801|0,334|-|76|1907|6483|38511|13722|6359|2,09|20,19|Netherlands|Western Europe|2010-2020|Computer Science (miscellaneous)||||592595611|2108686752
AIAM2020|Manchester, United Kingdom|C5.0 algorithm, Classification algorithm, Customer segmentation, Bank Direct Sales Project|5|150–154|Proceedings of the 2nd International Conference on Artificial Intelligence and Advanced Manufacture|At present, under the background that data mining technology is becoming more mature and widely used in various fields, and due to the advent of the customer-oriented era and increased competition from banks, data mining technology is being widely used in the field of banking and finance to determine the target customer group And promote bank sales. Therefore, based on the Bank Marketing data in the UCI Machine Learning Repository database, this article uses the C5.0 algorithm to classify customers on the clementine experimental platform, and proposes corresponding suggestions for bank marketing based on the classification results.This article first explores and understands the Bank Marketing data set, and describes the distribution of the customer background in the data set. The quality of the data set was further explored, and the outliers and outliers were corrected by replacing them with normal data that were closest to the outliers or extreme values.This paper further selects the optimal feature variable. First, use the Filter node to filter the unimportant variables of the classification, and further select one of the more relevant variables to reduce the redundancy of the variables. The final variables are: previous, age, duration, outcome, contact, housing, job, loan, marital, education.Secondly, this paper uses sampling nodes to perform undersampling to balance the data set. On this basis, the C5.0 algorithm is used to establish a classification model and optimize parameters, and finally obtain eight classification rules. Based on this, suggestions are provided for target group determination.Finally, this article introduces the remaining four classification algorithms: C&amp;T, QUEST, CHAID, Neural Networks, and compares the C5.0 algorithm with the four classification algorithms based on the balanced data set. It is concluded that several algorithms have certain differences and the overall prediction accuracy is good.This article combines data mining theory with practical problems of banking business, and establishes a bank target customer classification model based on C5.0 algorithm. The obtained classification rules can effectively help banks to divide customer groups and take targeted measures to improve marketing efficiency.|10.1145/3421766.3421800|https://doi.org/10.1145/3421766.3421800|New York, NY, USA|Association for Computing Machinery|9781450375535|2020|Research on Bank Marketing Behavior Based on Machine Learning|Wang, Deli|inproceedings|10.1145/3421766.3421800|||||||||||||||||||||||||||||592741226|42
||electronic health records, clinical terminology knowledge graph, clinical special disease case repository, evaluation of data quality, large scale cohort study||90-102||Regional healthcare platforms collect clinical data from hospitals in specific areas for the purpose of healthcare management. It is a common requirement to reuse the data for clinical research. However, we have to face challenges like the inconsistence of terminology in electronic health records (EHR) and the complexities in data quality and data formats in regional healthcare platform. In this paper, we propose methodology and process on constructing large scale cohorts which forms the basis of causality and comparative effectiveness relationship in epidemiology. We firstly constructed a Chinese terminology knowledge graph to deal with the diversity of vocabularies on regional platform. Secondly, we built special disease case repositories (i.e., heart failure repository) that utilize the graph to search the related patients and to normalize the data. Based on the requirements of the clinical research which aimed to explore the effectiveness of taking statin on 180-days readmission in patients with heart failure, we built a large-scale retrospective cohort with 29647 cases of heart failure patients from the heart failure repository. After the propensity score matching, the study group (n=6346) and the control group (n=6346) with parallel clinical characteristics were acquired. Logistic regression analysis showed that taking statins had a negative correlation with 180-days readmission in heart failure patients. This paper presents the workflow and application example of big data mining based on regional EHR data.|https://doi.org/10.24920/003579|https://www.sciencedirect.com/science/article/pii/S1001929419300318||||2019|Constructing Large Scale Cohort for Clinical Study on Heart Failure with Electronic Health Record in Regional Healthcare Platform: Challenges and Strategies in Data Reuse|Daowen Liu and Liqi Lei and Tong Ruan and Ping He|article|LIU201990|||Chinese Medical Sciences Journal|10019294|2|34|||||29227|0,215|Q4|21|51|128|1489|97|125|0,73|29,20|United Kingdom|Western Europe|1991-2020|Medicine (miscellaneous) (Q4)||||594695013|1072817640
|||2|104–ff||After decades of cybersecurity research, Elisa Bertino remains optimistic.|10.1145/3403976|https://doi.org/10.1145/3403976|New York, NY, USA|Association for Computing Machinery||2020|Seeing Light at the End of the Cybersecurity Tunnel|Hoffmann, Leah|article|10.1145/3403976||jul|Commun. ACM|00010782|8|63|August 2020||||||||||||||||||||||596124457|647144465
||Data-dependent world, data-driven, assets, technology, legal and regulatory tsunami, Internet of Things (IoT), big data, artificial intelligence (AI), machine learning (ML), The Leader’s Data Manifesto, data literacy, change, COVID-19||7-14|Executing Data Quality Projects (Second Edition)|Chapter 1 addresses topics in our world today, shows how they are dependent on data and information, why data quality is more relevant and critical now than ever before, and how the Ten Steps methodology will help. Topics include COVID-19, the legal and regulatory tsunami, big data, Internet of Things (IoT), 5G, artificial intelligence (AI) and machine learning (ML). Our data-dependent world is broader than, yet encompasses, being data-driven. Data and information are assets to be managed and are compared to how human and financial resources are managed. The Leader’s Data Manifesto is introduced as a starting point for conversations about the importance of managing data and information assets. While the Ten Steps methodology is meant for specific audiences, three recommendations were provided that anyone, in any organization, can do to help raise awareness of the importance of data quality: add data and information to the conversation, increase data literacy in the workplace, and include data (quality) management in learning institutions at all levels.|https://doi.org/10.1016/B978-0-12-818015-0.00021-9|https://www.sciencedirect.com/science/article/pii/B9780128180150000219||Academic Press|978-0-12-818015-0|2021|Chapter 1 - Data Quality and the Data-Dependent World|Danette McGilvray|incollection|MCGILVRAY20217|||||||||Second Edition|Danette McGilvray|||||||||||||||||||598170246|42
||Food geography, Healthy food access, Accessibility, Social inequalities, Transport mode, Multilevel regression||22-40||Urban studies attempt to identify the geographic areas with restricted access to healthy and affordable foods (defined as food deserts in the literature). While prior publications have reported the socioeconomic disparities in healthy food accessibility, little evidence has been released from developing countries, especially in China. This paper proposes a geo-big data approach to measuring transit-varying healthy food accessibility and applies it to identify the food deserts within Shenzhen, China. In particular, we develop a crawling tool to harvest the daily travel time from each community (8117) to each healthy food store (102) from the Baidu Map under four transport modes (walking, public transit, private car, and bicycle) during 17:30–20:30 in June 2016. Based on the travel time calculations, we develop four travel time indicators to measure the healthy food accessibility: the minimum, the maximum, the weighted average, and the standard deviation. Results show that the four accessibility indicators generate different estimations and the nearest service (minimum time) alone fails to reflect the multidimensional nature of healthy food accessibility. The communities within Shenzhen present quite different typology with respect to healthy food accessibility under different transport modes. Multilevel additive regression is further applied to examine the associations between healthy food accessibility and nested socioeconomic characteristics at two geographic levels (community and district). We discover that the associations are divergent with transport modes and with geographic levels. More specifically, significant social equalities in healthy food accessibility are identified via walking, public transit, and bicycle in Shenzhen. Based on the associations, we finally map the food deserts and propose corresponding planning strategies. The methods demonstrated in this study should offer deeper spatial insights into intra-urban foodscapes and provide more nuanced understanding of food deserts in urban settings of developing countries.|https://doi.org/10.1016/j.habitatint.2017.04.007|https://www.sciencedirect.com/science/article/pii/S0197397517300498||||2017|A geo-big data approach to intra-urban food deserts: Transit-varying accessibility, social inequalities, and implications for urban planning|Shiliang Su and Zekun Li and Mengya Xu and Zhongliang Cai and Min Weng|article|SU201722|||Habitat International|01973975||64|||||13760|1,542|Q1|78|117|365|8006|2114|361|5,46|68,43|United Kingdom|Western Europe|1970, 1976-2020|Nature and Landscape Conservation (Q1); Urban Studies (Q1)|8,872|5.369|0.00827|598544425|643693784
||Change of support, Sampling design, Data transformation, Prediction standard error||87-91||Following from Krivoruchko and Bivand (2009), we consider some general points related to challenges to the usefulness of big data in spatial statistical applications when data collection is compromised or one or more model assumptions are violated. We look further at the desirability of comparison of new methods intended to handle large spatial and spatio-temporal datasets.|https://doi.org/10.1016/j.spl.2018.02.012|https://www.sciencedirect.com/science/article/pii/S0167715218300579||||2018|Big data sampling and spatial analysis: “which of the two ladles, of fig-wood or gold, is appropriate to the soup and the pot?”|Roger Bivand and Konstantin Krivoruchko|article|BIVAND201887|||Statistics & Probability Letters|01677152||136||The role of Statistics in the era of big data|||14794|0,576|Q2|66|241|862|4066|1014|860|1,17|16,87|Netherlands|Western Europe|1982-2021|Statistics and Probability (Q2); Statistics, Probability and Uncertainty (Q2)||||603281108|1688735168
||wearables, human-in-the-loop method, interpretable modeling, segmented mixed-effects regression, smart health, explainability||||Wearables are an important source of big data as they provide real-time high-resolution data logs of health indicators of individuals. Higher-order associations between pairs of variables is common in wearables data. Representing higher-order association curves as piece-wise linear segments in a regression model makes them more interpretable. However, existing methods for identifying the change points for segmented modeling either overfit or have low external validity for wearables data containing repeated measures. Therefore, we propose a human-in-the-loop method for segmented modeling of higher-order pairwise associations between variables in wearables data. Our method uses the smooth function estimated by a generalized additive mixed model to allow the analyst to annotate change point estimates for a segmented mixed-effects model, and thereafter employs the Brent's constrained optimization procedure to fine-tuning the manually provided estimates. We validate our method using three real-world wearables datasets. Our method not only outperforms state-of-the-art modeling methods in terms of prediction performance but also provides more interpretable results. Our study contributes to health data science in terms of developing a new method for interpretable modeling of wearables data. Our analysis uncovers interesting insights on higher order associations for health researchers.|10.1145/3564276|https://doi.org/10.1145/3564276|New York, NY, USA|Association for Computing Machinery||2022|A Human-in-the-Loop Segmented Mixed-Effects Modeling Method For Analyzing Wearables Data|Srinivasan, Karthik and Currim, Faiz and Ram, Sudha|article|10.1145/3564276||sep|ACM Trans. Manage. Inf. Syst.|2158656X||||Just Accepted|||||||||||||||||||||603450016|2097885649
BiDEDE '21|Virtual Event, China|multimodal machine learning, function-as-a-service, serverless computing|6||Proceedings of the International Workshop on Big Data in Emergent Distributed Environments|The availability of large quantities of data has given an impulse for methods and techniques to extract unseen useful knowledge and process it in a fast and scalable manner. In order to extract the most complete possible knowledge from the continuous data stream, it is necessary to use the heterogeneous data sources and process information from multiple modalities. The systems that utilize multimodal data must take advantage of the up-to-date approaches for data storage, usage, cleaning and storage. Neural networks and machine learning approaches are widely used for data-heavy software where pattern extractions and predictions need to be conducted while serverless computing frameworks are being increasingly used for machine learning solutions to optimize cost and speed of such systems. This work presents a framework for processing data from multimodal sources where the task of feature and pattern extraction is performed on a serverless computing platform. The use cases for public safety solutions to increase situational awareness are described and compared with other implementation approaches.|10.1145/3460866.3461769|https://doi.org/10.1145/3460866.3461769|New York, NY, USA|Association for Computing Machinery|9781450384650|2021|Towards Situational Awareness with Multimodal Streaming Data Fusion: Serverless Computing Approach|Nesen, Alina and Bhargava, Bharat|inproceedings|10.1145/3460866.3461769|5||||||||||||||||||||||||||||603863085|42
SBSI 2015|Goiania, Goias, Brazil|Data Governance, System Information, Management Frameworks|6|267–272|Proceedings of the Annual Conference on Brazilian Symposium on Information Systems: Information Systems: A Computer Socio-Technical Perspective - Volume 1|Organizations are increasingly looking for data integrity and quality to assist in strategic making decision and value creation. In this context Data Governance (DG) provide processes and practices that assist in the management and maintenance data. There are many frameworks to implementation DG process and benefits they may provide, however there are few implementation reported in the literature. This study aims to identify the DG process and frameworks implemented in Brazilian organizations and compare the benefits in implementation with those proposed by literature. For this will be carried out case studies in Brazilian organizations that implemented or are implementing DG frameworks.|||Porto Alegre, BRA|Brazilian Computer Society||2015|Data Governance in Brazilian Organizations|Barata, Andre Montoia and Prado, Edmir Parada Vasques|inproceedings|10.5555/2814058.2814102|||||||||||||||||||||||||||||604603208|42
||Forestry;Support vector machines;Data models;Cleaning;Feature extraction;Training;Big Data;Big data;Data Analytics;missing data;data cleaning;Machine learning;Random Forest;Gini Index||208-213|2018 7th International Conference on Computer and Communication Engineering (ICCCE)|Accurate data is a key success factor influencing the performance of data analytics results, especially for the detection and prediction purpose. Nowadays, Big Data analytics (BDA) is used to analyze the sheer volume of data available in an organization. These data quality must be maintained in order to obtain correct alert and valuable insights from the rapidly changing data of high volume, velocity, variety, veracity, and value. This paper aim is to modify existing framework of big data analytics by improving an important step in pre-processing (i.e. Data Cleaning). Initially, feature selection based on Random Forest is used to extract effective features. Then, two classifier algorithms (i.e. Random Forest classifier and Linear SVM classifier) are applied to train using the dataset to classify data quality and to develop an intelligent model. In evaluation, our experimental results show a consistent accuracy of Random Forest and Linear Regression around 90%. Using this approach, we expect to provide a set of cleaned data for further processing. Besides, analysts can benefit from this system in data analytical process in cleaning stage and conclude that the data is cleaned. Finally, a comparison is presented between available functions which are used to handle missing values with the developed system.|10.1109/ICCCE.2018.8539254|||||2018|Modifying Cleaning Method in Big Data Analytics Process using Random Forest Classifier|Hossen, J. and Jesmeen H, M. Z. and Sayeed, Shohel|inproceedings|8539254||Sep.|||||||||||||||||||||||||||605610298|42
BuildSys '19|New York, NY, USA|smart thermostats, Buildings, gray box models, thermal characteristics|10|223–232|Proceedings of the 6th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation|The development of quantitative techniques for determining the amount of heat lost through the building envelope is essential for targeted retrofits. This type of evaluation is traditionally a resource intensive process that involves onsite appraisal and in-situ measurements. In order to build more efficient and scalable methods for retrofit analysis, new sources of data could be used. Smart thermostat data, for example, provide a valuable resource, however they often lack detailed information about the building characteristics and energy loads. This paper presents and compares three methods for assessing heating characteristics of households using a dataset that does not contain heating power. The three methods are based on: (1) balance point plots, (2) the extraction of indoor temperature decay curves, and (3) the classic differential equation for indoor temperature. These methods all take a gray box approach in which physics-based and machine learning models are combined. The dataset used for this study consists of over 4,000 houses in Ontario and New York. The three methods are applied to each building and the resulting data is analyzed to determine whether the results are statistically sound. It is found that there is a positive linear correlation between characteristics derived for each method, although there is uncertainty about absolute values. This result indicates that the methods can be used to ascertain relative values for the thermal characteristics of a building. The methods suggested in this paper may therefore be used to filter heating profiles to target potential retrofit measures or other stock-level decisions.|10.1145/3360322.3360836|https://doi.org/10.1145/3360322.3360836|New York, NY, USA|Association for Computing Machinery|9781450370059|2019|Comparing Gray Box Methods to Derive Building Properties from Smart Thermostat Data|"\"Baasch, Gaby and Wicikowski, Adam and Faure, Ga\"\"{e}lle and Evins, Ralph\""|inproceedings|10.1145/3360322.3360836|||||||||||||||||||||||||||||605672327|42
||||10-13||A group of disparate translational bioinformatics experts convened at the 6th Annual Precision Medicine Partnership Meeting, October 29–30, 2014 to discuss big data challenges and key strategic decisions needed to advance precision medicine, emerging solutions, and the anticipated path to success. This article reports the panel discussion.|https://doi.org/10.1016/j.atg.2015.02.002|https://www.sciencedirect.com/science/article/pii/S2212066115000046||||2015|Harnessing big data for precision medicine: A panel of experts elucidates the data challenges and proposes key strategic decisions points|Carol Isaacson Barash and Keith O. Elliston and W. {Andrew Faucett} and Jonathan Hirsch and Gauri Naik and Alice Rathjen and Grant Wood|article|BARASH201510|||Applied & Translational Genomics|22120661||4|||||||||||||||||||||||607724348|1989158935
||||260-271||Although large volumes of information are entered into our electronic health care records, radiation oncology information systems and treatment planning systems on a daily basis, the goal of extracting and using this big data has been slow to emerge. Development of strategies to meet this goal is aided by examining issues with a data farming instead of a data mining conceptualization. Using this model, a vision of key data elements, clinical process changes, technology issues and solutions, and role for professional societies is presented. With a better view of technology, process and standardization factors, definition and prioritization of efforts can be more effectively directed.|https://doi.org/10.1016/j.adro.2016.10.001|https://www.sciencedirect.com/science/article/pii/S2452109416300550||||2016|The big data effort in radiation oncology: Data mining or data farming?|Charles S. Mayo and Marc L. Kessler and Avraham Eisbruch and Grant Weyburne and Mary Feng and James A. Hayman and Shruti Jolly and Issam {El Naqa} and Jean M. Moran and Martha M. Matuszak and Carlos J. Anderson and Lynn P. Holevinski and Daniel L. McShan and Sue M. Merkel and Sherry L. Machnak and Theodore S. Lawrence and Randall K. {Ten Haken}|article|MAYO2016260|||Advances in Radiation Oncology|24521094|4|1|||||21100465104|0,989|Q1|19|211|271|5221|746|264|2,65|24,74|United States|Northern America|2016-2020|Radiology, Nuclear Medicine and Imaging (Q1); Oncology (Q2)||||609252324|755965515
||Big Data, Mobile Network Data, Passive GPS Data, Spatio-temporal behaviour, Tourist classification||100061||In this paper we aim to classify digital data sources for the measurement of tourist mobility, to establish a set of assessment indicators, and to compare two Big Data sources to gain empirical insights into how we can measure tourism with Big Data. For three holiday destinations in Germany, passive mobile data and passive global positioning systems (GPS) data are compared with reference data from the destinations for twelve weeks in the summer of 2019. Results show that mobile network data are on a plausible level compared to the local reference data and are able to predict the temporal pattern to a very high degree. GPS app-based data also perform well, but are less plausible and precise than mobile network data.|https://doi.org/10.1016/j.annale.2022.100061|https://www.sciencedirect.com/science/article/pii/S2666957922000295||||2022|Measuring tourism with big data? Empirical insights from comparing passive GPS data and passive mobile data|Dirk Schmücker and Julian Reif|article|SCHMUCKER2022100061|||Annals of Tourism Research Empirical Insights|26669579|2|3|||||||||||||||||||||||609474034|1094628814
||Responsible data science, Information Resilience, Effective information use, Data quality, Value creation|26|1059–1084||The appetite for effective use of information assets has been steadily rising in both public and private sector organisations. However, whether the information is used for social good or commercial gain, there is a growing recognition of the complex socio-technical challenges associated with balancing the diverse demands of regulatory compliance and data privacy, social expectations and ethical use, business process agility and value creation, and scarcity of data science talent. In this vision paper, we present a series of case studies that highlight these interconnected challenges, across a range of application areas. We use the insights from the case studies to introduce Information Resilience, as a scaffold within which the competing requirements of responsible and agile approaches to information use can be positioned. The aim of this paper is to develop and present a manifesto for Information Resilience that can serve as a reference for future research and development in relevant areas of responsible data management.|10.1007/s00778-021-00720-2|https://doi.org/10.1007/s00778-021-00720-2|Berlin, Heidelberg|Springer-Verlag||2022|Information Resilience: The Nexus of Responsible and Agile Approaches to Information Use|Sadiq, Shazia and Aryani, Amir and Demartini, Gianluca and Hua, Wen and Indulska, Marta and Burton-Jones, Andrew and Khosravi, Hassan and Benavides-Prado, Diana and Sellis, Timos and Someh, Ida and Vaithianathan, Rhema and Wang, Sen and Zhou, Xiaofang|article|10.1007/s00778-021-00720-2||jan|The VLDB Journal|10668888|5|31|Sep 2022||||13646|0,653|Q1|90|64|117|4824|544|113|4,46|75,38|United States|Northern America|1992-2020|Hardware and Architecture (Q1); Information Systems (Q1)|2,106|2.868|0.0023|612160357|924310569
|||2||||10.1145/3423321|https://doi.org/10.1145/3423321|New York, NY, USA|Association for Computing Machinery||2020|Editorial: Special Issue on Metadata Discovery for Assessing Data Quality|Polese, Giuseppe and Deufemia, Vincenzo and Song, Shaoxu|article|10.1145/3423321|17|oct|J. Data and Information Quality|19361955|4|12|December 2020||||||||||||||||||||||613415628|833754770
||||||"\"At the ACM Awards banquet in June 2017, during the 50th anniversary celebration of the A.M. Turing Award, ACM announced the launch of the ACM A.M. Turing Book Series, a sub-series of ACM Books, to celebrate the winners of the A.M. Turing Award, computing's highest honor, the \"\"Nobel Prize\"\" for computing. This series aims to highlight the accomplishments of awardees, explaining their major contributions of lasting importance in computing.\"\"Making Databases Work: The Pragmatic Wisdom of Michael Stonebraker,\"\" the first book in the series, celebrates Mike's contributions and impact. What accomplishments warranted computing's highest honor? How did Stonebraker do it? Who is Mike Stonebraker---researcher, professor, CTO, lecturer, innovative product developer, serial entrepreneur, and decades-long leader, and research evangelist for the database community. This book describes Mike's many contributions and evaluates them in light of the Turing Award.The book describes, in 36 chapters, the unique nature, significance, and impact of Mike's achievements in advancing modern database systems over more than 40 years. The stories involve technical concepts, projects, people, prototype systems, failures, lucky accidents, crazy risks, startups, products, venture capital, and lots of applications that drove Mike Stonebraker's achievements and career. Even if you have no interest in databases at all, you'll gain insights into the birth and evolution of Turing Award-worthy achievements from the perspectives of 39 remarkable computer scientists and professionals.Today, data is considered the world's most valuable resource (\"\"The Economist,\"\" May 6, 2017), whether it is in the tens of millions of databases used to manage the world's businesses and governments, in the billions of databases in our smartphones and watches, or residing elsewhere, as yet unmanaged, awaiting the elusive next generation of database systems. Every one of the millions or billions of databases includes features that are celebrated by the 2014 A.M. Turing Award and are described in this book.\""||||Association for Computing Machinery and Morgan &amp; Claypool|9781947487192|2018|Making Databases Work: The Pragmatic Wisdom of Michael Stonebraker||book|10.1145/3226595||||||22||||Brodie, Michael L.|||||||||||||||||||615426703|42
SBD'18|Houston, TX, USA|Truth detection, Data Management, Linked Data, RDF|6||Proceedings of the International Workshop on Semantic Big Data|We envision a responsible web environment, termed TrueWeb, where a user should be able to find out whether any sentence he or she encounters in the web is true or false. The user should be able to track the provenance of any sentence or paragraph in the web. The target of TrueWeb is to compose factual knowledge from Internet resources about any subject of interest and present the collected knowledge in chronological order and distribute facts spatially and temporally as well as assign some belief factor for each fact. Another important target of TrueWeb is to be able to identify whether a statement in the Internet is true or false. The aim is to create an Internet infrastructure that, for each piece of published information, will be able to identify the truthfulness (or the degree of truthfulness) of that piece of information.|10.1145/3208352.3208357|https://doi.org/10.1145/3208352.3208357|New York, NY, USA|Association for Computing Machinery|9781450357791|2018|TrueWeb: A Proposal for Scalable Semantically-Guided Data Management and Truth Finding in Heterogeneous Web Sources|Madkour, Amgad and Aref, Walid G. and Prabhakar, Sunil and Ali, Mohamed and Bykau, Siarhei|inproceedings|10.1145/3208352.3208357|5||||||||||||||||||||||||||||617175834|42
|||4|32–35||Creating a holistic view of patient data comes with many challenges but also brings many benefits for disease prediction, prevention, diagnosis, and treatment. Especially in the COVID-19 era, this is more important than ever before. The third International Workshop on Semantic Web Meets Health Data Management (SWH) was aimed at bringing together an interdisciplinary audience who was interested in the fields of Semantic Web, data management, and health informatics. The workshop goal was to discuss the challenges in healthcare data management and to propose new solutions for the next generation of data-driven healthcare systems. In this article, we summarize the outcomes of the workshop, and we present a number of key observations and research directions that emerged from presentations.|10.1145/3503780.3503792|https://doi.org/10.1145/3503780.3503792|New York, NY, USA|Association for Computing Machinery||2021|Report on the Third International Workshop on Semantic Web Meets Health Data Management (SWH 2020)|Kondylakis, Haridimos and Stefanidis, Kostas and Rao, Praveen|article|10.1145/3503780.3503792||dec|SIGMOD Rec.|01635808|3|50|September 2021||||13622|0,372|Q2|142|39|81|808|127|57|1,67|20,72|United States|Northern America|1969, 1973-1978, 1981-2020|Information Systems (Q2); Software (Q2)|1,819|0.775|7.5E-4|618291789|962972343
|Yokohama, Japan||||||||New York, NY, USA|Association for Computing Machinery|9781450380966|2021|CHI '21: Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems||proceedings|10.1145/3411764|||||||||||||||||||||||||||||618613151|42
||Data transparency, machine learning, data cleaning|9|||The data and Artificial Intelligence revolution has had a massive impact on enterprises, governments, and society alike. It is fueled by two key factors. First, data have become increasingly abundant and are often available openly. Enterprises have more data than they can process. Governments are spearheading open data initiatives by setting up data portals such as data.gov and releasing large amounts of data to the public. Second, AI engineering development is becoming increasingly democratized. Open source frameworks have enabled even an individual developer to engineer sophisticated AI systems. But with such ease of use comes the potential for irresponsible use of data.Ensuring that AI systems adhere to a set of ethical principles is one of the major problems of our age. We believe that data and model transparency has a key role to play in mitigating the deleterious effects of AI systems. In this article, we describe a framework to synthesize ideas from various domains such as data transparency, data quality, data governance among others to tackle this problem. Specifically, we advocate an approach based on automated annotations (of both data and the AI model), which has a number of appealing properties. The annotations could be used by enterprises to get visibility of potential issues, prepare data transparency reports, create and ensure policy compliance, and evaluate the readiness of data for diverse downstream AI applications. We propose a model architecture and enumerate its key components that could achieve these requirements. Finally, we describe a number of interesting challenges and opportunities.|10.1145/3460000|https://doi.org/10.1145/3460000|New York, NY, USA|Association for Computing Machinery||2021|Automated Annotations for AI Data and Model Transparency|Thirumuruganathan, Saravanan and Kunjir, Mayuresh and Ouzzani, Mourad and Chawla, Sanjay|article|10.1145/3460000|2|dec|J. Data and Information Quality|19361955|1|14|March 2022||||||||||||||||||||||619698982|833754770
|||12|864–875||Data deduplication refers to the process of identifying tuples in a relation that refer to the same real world entity. The complexity of the problem is inherently quadratic with respect to the number of tuples, since a similarity value must be computed for every pair of tuples. To avoid comparing tuple pairs that are obviously non-duplicates, blocking techniques are used to divide the tuples into blocks and only tuples within the same block are compared. However, even with the use of blocking, data deduplication remains a costly problem for large datasets. In this paper, we show how to further speed up data deduplication by leveraging parallelism in a shared-nothing computing environment. Our main contribution is a distribution strategy, called Dis-Dedup, that minimizes the maximum workload across all worker nodes and provides strong theoretical guarantees. We demonstrate the effectiveness of our proposed strategy by performing extensive experiments on both synthetic datasets with varying block size distributions, as well as real world datasets.|10.14778/2983200.2983203|https://doi.org/10.14778/2983200.2983203||VLDB Endowment||2016|Distributed Data Deduplication|Chu, Xu and Ilyas, Ihab F. and Koutris, Paraschos|article|10.14778/2983200.2983203||jul|Proc. VLDB Endow.|21508097|11|9|July 2016||||21100199855|0,946|Q1|134|246|598|12401|3759|566|5,50|50,41|United States|Northern America|2008-2020|Computer Science (miscellaneous) (Q1)|7,198|2.047|0.00891|620808415|1216159931
||Big Data, High performance computing, Education, Modeling, Geodynamics||841-855|Encyclopedia of Geology (Second Edition)|Since 2015 much has developed in geodynamical modeling because of the arrival of Big Data. We present in two parts an overview of numerical techniques but also a scan of the new opportunities in this age of Big Data and prepare the community for the coming decade, the roaring twenties, when Data Analytics will reign. We begin with a review of traditional numerical methods (Part I), followed by a survey of the current techniques used for data analytics and high-performance computing (HPC) (Part II). Our aim is to cover topics of machine learning, neural networks and deep learning, unsupervised learning as well as the role that HPC will play in the Big Data era, especially in hardware of various calibers. Finally, we will address the need for education of students and professionals, in particular, on the use of the emerging programming languages and the importance of scientific software communities.|https://doi.org/10.1016/B978-0-08-102908-4.00111-9|https://www.sciencedirect.com/science/article/pii/B9780081029084001119|Oxford|Academic Press|978-0-08-102909-1|2021|Fresh Outlook on Numerical Methods for Geodynamics. Part 2: Big Data, HPC, Education|Gabriele Morra and David A. Yuen and Henry M. Tufo and Matthew G. Knepley|incollection|MORRA2021841|||||||||Second Edition|David Alderton and Scott A. Elias|||||||||||||||||||621073221|42
||Medical treatment;Diseases;Prediction algorithms;Predictive models;Pediatrics;Monitoring;Big data analytics;artificial intelligence;asthma;telemonitoring;exacerbation prediction||1-5|2016 IEEE 7th Annual Ubiquitous Computing, Electronics & Mobile Communication Conference (UEMCON)|Advanced prediction of asthma exacerbations may significantly improve patient quality of life and reduce costs of urgent care delivery. Majority of current algorithms predict who is likely to experience asthma exacerbation rather than when it is about to occur. We used data from asthma home-based telemonitoring for advanced prediction of asthma exacerbation. The goal of this project was to develop an algorithm that predicts asthma exacerbation one day in advance based on previous 7-day window. CART was used for predictive modeling. Resulting algorithm had specificity 0.971, sensitivity of 0.647, and accuracy of 0.809. We concluded that machine learning has great potential for advanced prediction of chronic disease exacerbations based on home telemonitoring data.|10.1109/UEMCON.2016.7777890|||||2016|Using CART for advanced prediction of asthma attacks based on telemonitoring data|Finkelstein, Joseph and Jeong, In Cheol|inproceedings|7777890||Oct|||||||||||||||||||||||||||621827590|42
GEOAI '21|Beijing, China|data visualization, reinforcement learning, vehicle trajectory, privacy protection, transportation|4|43–46|Proceedings of the 4th ACM SIGSPATIAL International Workshop on AI for Geographic Knowledge Discovery|Trajectory data is among the most sensitive data and the society increasingly raises privacy concerns. In this demo paper, we present a privacy-preserving Vehicle Trajectory Simulation and Visualization (VTSV) web platform (demo video: https://youtu.be/NY5L4bu2kTU), which automatically generates navigation routes between given pairs of origins and destinations and employs a deep reinforcement learning model to simulate vehicle trajectories with customized driving behaviors such as normal driving, overspeed, aggressive acceleration, and aggressive turning. The simulated vehicle trajectory data contain high-sample-rate of attributes including GPS location, speed, acceleration, and steering angle, and such data are visualized in VTSV using streetscape.gl, an autonomous driving data visualization framework. Location privacy protection methods such as origin-destination geomasking and trajectory k-anonymity are integrated into the platform to support privacy-preserving trajectory data generation and publication. We design two application scenarios to demonstrate how VTSV performs location privacy protection and customize driving behavior, respectively. The demonstration shows that VTSV is able to mitigate data privacy, sparsity, and imbalance sampling issues, which offers new insights into driving trajectory simulation and GeoAI-powered privacy-preserving data publication.|10.1145/3486635.3491073|https://doi.org/10.1145/3486635.3491073|New York, NY, USA|Association for Computing Machinery|9781450391207|2021|VTSV: A Privacy-Preserving Vehicle Trajectory Simulation and Visualization Platform Using Deep Reinforcement Learning|Rao, Jinmeng and Gao, Song and Zhu, Xiaojin|inproceedings|10.1145/3486635.3491073|||||||||||||||||||||||||||||626835790|42
|||3|22–24||Evaluating the arguments for and against using digital data derived from security breaches.|10.1145/3368091|https://doi.org/10.1145/3368091|New York, NY, USA|Association for Computing Machinery||2019|Should Researchers Use Data from Security Breaches?|Douglas, David M.|article|10.1145/3368091||nov|Commun. ACM|00010782|12|62|December 2019||||||||||||||||||||||627391359|647144465
||Consumer interaction, Cutting-edge technologies, Artificial intelligence, Virtual reality and augmented reality, Robotics, Wearable technology, Big data analytics||106761||This article provides an overview of extant literature addressing consumer interaction with cutting-edge technologies. Six focal cutting-edge technologies are identified: artificial intelligence, augmented reality, virtual reality, wearable technology, robotics and big data analytics. Our analysis shows research on consumer interaction with cutting-edge technologies is at a nascent stage, and there are several gaps requiring attention. To further advance knowledge, our article offers avenues for future interdisciplinary research addressing implications of consumer interaction with cutting-edge technologies. More specifically, we propose six main areas for future research namely: rethinking consumer behaviour models, identifying behavioural differences among different generations of consumers, understanding how consumers interact with automated services, ethics, privacy and the blackbox, consumer security concerns and consumer interaction with new-age technologies during and after a major global crisis such as the COVID-19 pandemic.|https://doi.org/10.1016/j.chb.2021.106761|https://www.sciencedirect.com/science/article/pii/S0747563221000832||||2021|Consumer interaction with cutting-edge technologies: Implications for future research|Nisreen Ameen and Sameer Hosany and Ali Tarhini|article|AMEEN2021106761|||Computers in Human Behavior|07475632||120|||||19419|2,108|Q1|178|385|1597|26867|14501|1573|7,83|69,78|United Kingdom|Western Europe|1985-2021|Arts and Humanities (miscellaneous) (Q1); Human-Computer Interaction (Q1); Psychology (miscellaneous) (Q1)|45,035|6.829|0.05973|627453829|265090421
||Real-time, Big data processing, Anomaly detection and machine learning algorithms||289-307||The advent of connected devices and omnipresence of Internet have paved way for intruders to attack networks, which leads to cyber-attack, financial loss, information theft in healthcare, and cyber war. Hence, network security analytics has become an important area of concern and has gained intensive attention among researchers, off late, specifically in the domain of anomaly detection in network, which is considered crucial for network security. However, preliminary investigations have revealed that the existing approaches to detect anomalies in network are not effective enough, particularly to detect them in real time. The reason for the inefficacy of current approaches is mainly due the amassment of massive volumes of data though the connected devices. Therefore, it is crucial to propose a framework that effectively handles real time big data processing and detect anomalies in networks. In this regard, this paper attempts to address the issue of detecting anomalies in real time. Respectively, this paper has surveyed the state-of-the-art real-time big data processing technologies related to anomaly detection and the vital characteristics of associated machine learning algorithms. This paper begins with the explanation of essential contexts and taxonomy of real-time big data processing, anomalous detection, and machine learning algorithms, followed by the review of big data processing technologies. Finally, the identified research challenges of real-time big data processing in anomaly detection are discussed.|https://doi.org/10.1016/j.ijinfomgt.2018.08.006|https://www.sciencedirect.com/science/article/pii/S0268401218301658||||2019|Real-time big data processing for anomaly detection: A Survey|Riyaz Ahamed {Ariyaluran Habeeb} and Fariza Nasaruddin and Abdullah Gani and Ibrahim Abaker {Targio Hashem} and Ejaz Ahmed and Muhammad Imran|article|ARIYALURANHABEEB2019289|||International Journal of Information Management|02684012||45|||||15631|2,770|Q1|114|224|382|20318|5853|373|16,16|90,71|United Kingdom|Western Europe|1970, 1986-2021|Artificial Intelligence (Q1); Computer Networks and Communications (Q1); Information Systems (Q1); Information Systems and Management (Q1); Library and Information Sciences (Q1); Management Information Systems (Q1); Marketing (Q1)|12,245|14.098|0.01167|630276496|747927863
||big data analytics application, Big data, SLA, service layer, service level agreement, SLA metrics|40|||Recent years have witnessed the booming of big data analytical applications (BDAAs). This trend provides unrivaled opportunities to reveal the latent patterns and correlations embedded in the data, and thus productive decisions may be made. This was previously a grand challenge due to the notoriously high dimensionality and scale of big data, whereas the quality of service offered by providers is the first priority. As BDAAs are routinely deployed on Clouds with great complexities and uncertainties, it is a critical task to manage the service level agreements (SLAs) so that a high quality of service can then be guaranteed. This study performs a systematic literature review of the state of the art of SLA-specific management for Cloud-hosted BDAAs. The review surveys the challenges and contemporary approaches along this direction centering on SLA. A research taxonomy is proposed to formulate the results of the systematic literature review. A new conceptual SLA model is defined and a multi-dimensional categorization scheme is proposed on its basis to apply the SLA metrics for an in-depth understanding of managing SLAs and the motivation of trends for future research.|10.1145/3383464|https://doi.org/10.1145/3383464|New York, NY, USA|Association for Computing Machinery||2020|SLA Management for Big Data Analytical Applications in Clouds: A Taxonomy Study|Zeng, Xuezhi and Garg, Saurabh and Barika, Mutaz and Zomaya, Albert Y. and Wang, Lizhe and Villari, Massimo and Chen, Dan and Ranjan, Rajiv|article|10.1145/3383464|46|jun|ACM Comput. Surv.|03600300|3|53|May 2021||||23038|2,079|Q1|163|112|365|15859|6978|365|19,16|141,60|United States|Northern America|1969-2020|Computer Science (miscellaneous) (Q1); Theoretical Computer Science (Q1)|10,790|10.282|0.01438|630889278|1517405264
CIAT 2020|Guangzhou, China|insurance products, recommendation, Naive Bayes, dynamic estimation|6|219–224|Proceedings of the 2020 International Conference on Cyberspace Innovation of Advanced Technologies|Aiming at the dynamic estimation of insurance product recommendation, considering the particularity and complexity of purchasing insurance product and the uncertainty of influencing factors, a dynamic estimation model of insurance product recommendation based on Naive Bayes is proposed. The model combines customer insurance information with machine learning. The results show that the naive Bayesian classification algorithm can be compared with the decision tree and neural network classification algorithm, showing high accuracy and high speed.|10.1145/3444370.3444575|https://doi.org/10.1145/3444370.3444575|New York, NY, USA|Association for Computing Machinery|9781450387828|2021|Dynamic Estimation Model of Insurance Product Recommendation Based on Naive Bayesian Model|Zhang, Bo and Kong, Dehua|inproceedings|10.1145/3444370.3444575|||||||||||||||||||||||||||||631329283|42
||Big data analytics, Supply chain management, Big data application||319-330||This paper investigates big data analytics research and application in supply chain management between 2010 and 2016 and provides insights to industries. In recent years, the amount of data produced from end-to-end supply chain management practices has increased exponentially. Moreover, in current competitive environment supply chain professionals are struggling in handling the huge data. They are surveying new techniques to investigate how data are produced, captured, organized and analyzed to give valuable insights to industries. Big Data analytics is one of the best techniques which can help them in overcoming their problem. Realizing the promising benefits of big data analytics in the supply chain has motivated us to write a review on the importance/impact of big data analytics and its application in supply chain management. First, we discuss big data analytics individually, and then we discuss the role of big data analytics in supply chain management (supply chain analytics). Current research and application are also explored. Finally, we outline the insights to industries. Observations and insights from this paper could provide the guideline for academia and practitioners in implementing big data analytics in different aspects of supply chain management.|https://doi.org/10.1016/j.cie.2017.11.017|https://www.sciencedirect.com/science/article/pii/S0360835217305508||||2018|Big data analytics in supply chain management between 2010 and 2016: Insights to industries|Sunil Tiwari and H.M. Wee and Yosef Daryanto|article|TIWARI2018319|||Computers & Industrial Engineering|03608352||115|||||18164|1,315|Q1|128|641|1567|31833|9597|1557|5,97|49,66|United Kingdom|Western Europe|1976-2020|Computer Science (miscellaneous) (Q1); Engineering (miscellaneous) (Q1)||||632806581|1798521593
BDIoT'19|Rabat, Morocco|Blockchain, Edge Computing, Fog computing, Smart Contracts, SDN, Internet of Things, NFV|8||Proceedings of the 4th International Conference on Big Data and Internet of Things|In this paper, we propose a novel architecture that utilizes features of Blockchain, fog computing, and cloud computing to manage IoT data. Blockchain allows to have a distributed peer-to-peer network in which non-trusting participants can interact with each other without a trusted intermediary or third party. We evaluate how this mechanism works to face the challenges of IoT with respect to multiple accessibility to IoT devises. We consider a Blockchain architecture in presence of edge computing layer. With fog or fog computing, the sensitive data can be analyzed locally instead of sending it to the cloud for analysis. Edge nodes can also keep track and control of the IoT devices that collect, analyze and store data. We show that this control can be better executed when Software Defined Network (SDN) and Network Functions Virtualization (NFV) are integrated into our process for optimal resource management. In this paper, we present our system architecture with a detailed description of the different interactions. We remark that the integration of Blockchain, IoT, and edge computing when coupled with SDN and NFV-enabled cloud infrastructure can bring to more superior and efficient platform for accessing, managing, and processing the huge influx of IoT data.|10.1145/3372938.3372970|https://doi.org/10.1145/3372938.3372970|New York, NY, USA|Association for Computing Machinery|9781450372404|2020|Architecture to Manage Internet of Things Data Using Blockchain and Fog Computing|El Kafhali, Said and Chahir, Chorouk and Hanini, Mohamed and Salah, Khaled|inproceedings|10.1145/3372938.3372970|32||||||||||||||||||||||||||||634022563|42
||||c1-c1|2017 IEEE Third International Conference on Big Data Computing Service and Applications (BigDataService)|The following topics are dealt with: intelligent data mining; frameworks for Big Data processing; Big Data processing and mining; Big Data analysis; smart city Big Data; data analytics and visualization; Big Data applications; Big Data framework, technology and solutions; security services for smart cities; Big Data for security; Big Data quality assurance and validation; and quality assurance and validation for Big Data-based applications.|10.1109/BigDataService.2017.59|||||2017|[Front cover]||inproceedings|7944901||April|||||||||||||||||||||||||||634237382|42
||Transportation;Big data;Smart cities;analysis of big data;Yunnan province of China;regional economic development;geographic environment||869-872|2015 International Conference on Intelligent Transportation, Big Data and Smart City|This paper uses the analysis methods of data mining, data quality and management, semantic engine and prediction in big data to analyze the geographic environment of the Yunnan's economic development from its special location and geographic elements, beholding that the geographic elements lay basic material conditions for its economic development of Yunnan province. Geographic factors on one hand encourage the economic development and on the other hand affect together the economic development of Yunnan province.|10.1109/ICITBS.2015.220|||||2015|The Geographic Environment Analysis of Regional Economic Development of Yunnan Province of China Based on the Big Data Technology|Chenran, Xiong and Youde, Wu|inproceedings|7384166||Dec|||||||||||||||||||||||||||638202657|42
||Big data, Data analysis, Economic forecasting, Selection bias, Reporting lags, High-frequency data, Real-time forecasts, Leading indicator||481-492||The use of big data to help explain fluctuations in the broader economy and key business performance indicators is now so commonplace that in some instances it has even begun to rival more traditional measures. Big data sources can very often provide advantages when compared with these more traditional data sources, but with these advantages also come potential pitfalls. We lay out a checklist called SMALL that we have developed in order to help interested parties as they navigate the big data minefield. Based on a set of five questions, the SMALL checklist should help users of big data draw justifiable conclusions and avoid making mistakes in matters of interpretation. To demonstrate, we provide several case studies that demonstrate the subtle nuances of several of these new big data sets and show how the problems they face often closely relate to age-old concerns that more traditional data sources are also forced to tackle.|https://doi.org/10.1016/j.bushor.2021.06.004|https://www.sciencedirect.com/science/article/pii/S0007681321001178||||2022|The perils of working with big data, and a SMALL checklist you can use to recognize them|Scott A. Brave and R. Andrew Butters and Michael Fogarty|article|BRAVE2022481|||Business Horizons|00076813|4|65|||||20209|2,174|Q1|87|64|278|2475|2066|227|6,68|38,67|United Kingdom|Western Europe|1957-2020|Business and International Management (Q1); Marketing (Q1)|7,443|6.361|0.00571|639386116|847373672
||Big data, Spatiotemporal patterns, Resident tourists and non-resident tourists, Multiple city travel, Hotel check-in registers||100860||Previous research studied the spatiotemporal patterns in different visitor segments but lacks evidence of the segmentation of resident tourists and non-resident tourists in multi-city travel. To fill this gap, this study conducts a big data study using hotel check-in registers. The exploratory data analysis visualizes the spatiotemporal patterns and the differences between resident tourists and non-resident tourists. Then, the spatiotemporal patterns are measured by the length of stay and the number of visited cities. The regression shows that both the length of stay and the number of visited cities of non-resident tourists are higher than those of resident tourists. Moreover, non-resident tourists reduce their length of stay and their number of visited cities more than resident tourists on three-day holidays, while they increase their number of visited cities less than resident tourists on seven-day holidays. This study has significant implications for understanding spatiotemporal patterns and visitors' segmentations.|https://doi.org/10.1016/j.tmp.2021.100860|https://www.sciencedirect.com/science/article/pii/S2211973621000738||||2021|Comparing differences in the spatiotemporal patterns between resident tourists and non-resident tourists using hotel check-in registers|Yuquan Xu and Xiaobin Ran and Yuewen Liu and Wei Huang|article|XU2021100860|||Tourism Management Perspectives|22119736||39|||||21100202157|1,454|Q1|43|140|277|12102|1824|274|6,77|86,44|United States|Northern America|2012-2020|Tourism, Leisure and Hospitality Management (Q1)|3,902|6.586|0.00445|640094782|434694424
||Material science, Big data, Machine learning, Data analytics, Predictive Algorithms||1245-1249||The data volume is growing rapidly in material science. Every year data volume is getting double in many context of material science. The growing rate of data in material science is demanding for new computational infrastructures that can speed-up material discovery and deployment. In this survey, we are focusing on the challenges in material science due to growing data rate, and how Big Data technology can play a major role in research of material science. This survey includes various disciplines that can be used with Big Data to provide better analysis in the material science research.|https://doi.org/10.1016/j.matpr.2020.02.249|https://www.sciencedirect.com/science/article/pii/S2214785320310026||||2020|Big-data driven approaches in materials science: A survey|Manwendra K. Tripathi and Randhir Kumar and Rakesh Tripathi|article|TRIPATHI20201245|||Materials Today: Proceedings|22147853||26||10th International Conference of Materials Processing and Characterization|||21100370037|0,341|-|47|3996|10585|88521|14096|10409|1,24|22,15|United Kingdom|Western Europe|2005, 2014-2020|Materials Science (miscellaneous)||||642461969|400517803
AIDR '19|Pittsburgh, Pennsylvania|digital libraries, scholarly big data, CiteSeerX, search engines|4||Proceedings of the Conference on Artificial Intelligence for Data Discovery and Reuse|We overview CiteSeerX, the pioneer digital library search engine, that has been serving academic communities for more than 20 years (first released in 1998), from three perspectives. The system perspective summarizes its architecture evolution in three phases over the past 20 years. The data perspective describes how CiteSeerX has created searchable scholarly big datasets and made them freely available for multiple purposes. In order to be scalable and effective, AI technologies are employed in all essential modules. To effectively train these models, a sufficient amount of data has been labeled, which can then be reused for training future models. Finally, we discuss the future of CiteSeerX. Our ongoing work is to make CiteSeerX more sustainable. To this end, we are working to ingest all open access scholarly papers, estimated to be 30-40 million. Part of the plan is to discover dataset mentions and metadata in scholarly articles and make them more accessible via search interfaces. Users will have more opportunities to explore and trace datasets that can be reused and discover other datasets for new research projects. We summarize what was learned to make a similar system more sustainable and useful.|10.1145/3359115.3359119|https://doi.org/10.1145/3359115.3359119|New York, NY, USA|Association for Computing Machinery|9781450371841|2019|CiteSeerX: 20 Years of Service to Scholarly Big Data|Wu, Jian and Kim, Kunho and Giles, C. Lee|inproceedings|10.1145/3359115.3359119|1||||||||||||||||||||||||||||644607845|42
ICISCAE 2021|Dalian, China||5|1634–1638|2021 4th International Conference on Information Systems and Computer Aided Education|There are many big data analysis methods, and it is an effective method to build big data analysis model through machine learning. Big data is characterized by large data scale and long calculation cycle. In order to speed up the calculation speed and shorten the calculation cycle, distributed computing method is one of the effective methods to solve the above problems. With the wide application and rapid development of information technology, cloud computing as a new business computing model has attracted more and more attention. However, the security of cloud computing data storage model lacks reliability. Under the mainstream cloud computing and big data basic environment, building a better model from resource aggregation to analysis and mining, and modeling distributed big data analysis can provide high-reliability, high-security, high-efficiency analysis services for practical analysis and mining applications such as intelligence judgment, information deployment and control, stakeholder analysis and intelligent decision-making.|10.1145/3482632.3484007|https://doi.org/10.1145/3482632.3484007|New York, NY, USA|Association for Computing Machinery|9781450390255|2021|A Method of Constructing Distributed Big Data Analysis Model for Machine Learning Based on Cloud Computing|Li, Jicai and Liu, Dan|inproceedings|10.1145/3482632.3484007|||||||||||||||||||||||||||||646724511|42
||Big data, Big service, Big service composition, Quality of big services, Fuzzy RCA, Spark||102732||Over the last years, big data has emerged as a new paradigm for the processing and analysis of massive volumes of data. Big data processing has been combined with service and cloud computing, leading to a new class of services called “Big Services”. In this new model, services can be seen as an abstract layer that hides the complexity of the processed big data. To meet users' complex and heterogeneous needs in the era of big data, service reuse is a natural and efficient means that helps orchestrating available services' operations, to provide customer on-demand big services. However different from traditional Web service composition, composing big services refers to the reuse of, not only existing high-quality services, but also high-quality data sources, while taking into account their security constraints (e.g., data provenance, threat level and data leakage). Moreover, composing heterogeneous and large-scale data-centric services faces several challenges, apart from security risks, such as the big services' high execution time and the incompatibility between providers' policies across multiple domains and clouds. Aiming to solve the above issues, we propose a scalable approach for big service composition, which considers not only the quality of reused services (QoS), but also the quality of their consumed data sources (QoD). Since the correct representation of big services requirements is the first step towards an effective composition, we first propose a quality model for big services and we quantify the data breaches using L-Severity metrics. Then to facilitate processing and mining big services' related information during composition, we exploit the strong mathematical foundation of fuzzy Relational Concept Analysis (fuzzy RCA) to build the big services' repository as a lattice family. We also used fuzzy RCA to cluster services and data sources based on various criteria, including their quality levels, their domains, and the relationships between them. Finally, we define algorithms that parse the lattice family to select and compose high-quality and secure big services in a parallel fashion. The proposed method, which is implemented on top of Spark big data framework, is compared with two existing approaches, and experimental studies proved the effectiveness of our big service composition approach in terms of QoD-aware composition, scalability, and security breaches.|https://doi.org/10.1016/j.jnca.2020.102732|https://www.sciencedirect.com/science/article/pii/S108480452030206X||||2020|On the use of big data frameworks for big service composition|Mokhtar Sellami and Haithem Mezni and Mohand Said Hacid|article|SELLAMI2020102732|||Journal of Network and Computer Applications|10848045||166|||||||||||||||||||||||648403252|2040591560
||||104954|||https://doi.org/10.1016/j.cmpb.2019.06.013|https://www.sciencedirect.com/science/article/pii/S0169260719309174||||2019|Guest editorial: Special issue in biomedical data quality assessment methods|Carlos Sáez and Siaw-Teng Liaw and Eizen Kimura and Pascal Coorevits and Juan M Garcia-Gomez|article|SAEZ2019104954|||Computer Methods and Programs in Biomedicine|01692607||181||SI: Data Quality Assessment|||23604|0,924|Q1|102|538|789|23721|4743|753|6,27|44,09|Ireland|Western Europe|1985-2020|Computer Science Applications (Q1); Software (Q1); Health Informatics (Q2)|12,277|5.428|0.01119|650933511|47459595
||Big data, Cloud services, Healthcare, Privacy measures, Privacy security risk||247-263|Big Data Analytics for Healthcare|This chapter aims to discuss healthcare's development in China and the privacy and security risk factors in medical data under big data. First, the development status of China's healthcare sector is analyzed. The questionnaire is made to analyze the privacy and security risk factors of healthcare big data (HBD) and protection measures are proposed according to the data privacy and security risk factors in the context of cloud services in the literature. The results show that in recent years, the number of health institutions and medical personnel, the assets of medical institutions, the per capita hospitalization cost, and the insured population all increase annually. In 2017, the crude mortality rate of malignant tumor patients was the highest in China, and the mortality rate of rural patients was higher than that of urban patients. The questionnaire results reveal that the probability of data analysis, medical treatment process, disease diagnosis process, lack of protective measures, and imperfect access system are all greater than 0.8 when HBD is oriented to cloud services. Based on this, two levels of privacy protection measures are proposed: technology and management. It indicates that medical institutions need to emphasize data privacy protection and grasp using digital medical data to provide decision support for subsequent medical data analysis.|https://doi.org/10.1016/B978-0-323-91907-4.00020-0|https://www.sciencedirect.com/science/article/pii/B9780323919074000200||Academic Press|978-0-323-91907-4|2022|Chapter 19 - Privacy security risks of big data processing in healthcare|Zhihan Lv and Liang Qiao|incollection|LV2022247||||||||||Pantea Keikhosrokiani|||||||||||||||||||651067392|42
||Tools;Synchronization;Data visualization;Protocols;Distributed databases;Computers;Software||1158-1163|2017 IEEE Symposium on Computers and Communications (ISCC)|In this paper, we propose the China-US international data placement laboratory (iDPL) based on an inter-continental testbed for data placement research. iDPL is able to support various data placement research due to its scalability and flexibility in deploying the experiments in the real network environment. The core design of iDPL leverages reliable workflow management and lightweight I/O protocol to allow complex experiment setup and on-the-fly experiment deployment. It is also extensible to plugin different network profiling tools such as iperf. We expect the powerful measurement capability of iDPL promotes research study on the intelligent data placement policies which adapt to the uncertainty of the wide-area network and guarantee the quality of service (QoS) of the big data applications. As a case study, we setup a set of data placement experiments to measure the end-to-end network performance constantly among several sites between China and US using different data placement tools. The experiments have been running for more than one year, and its measurement data is public available (http://mickey.buaa.edu.cn:8080/). We believe the measurement data is valuable for both network and big data researchers to understand the performance disparity between the raw network and the actual data placement, which provides useful insights to design big data applications with performance awareness. We encourage more researchers to deploy their own data placement experiments on iDPL, expediting the research direction of intelligent data placement with real network environment.|10.1109/ISCC.2017.8024681|||||2017|iDPL: A scalable and flexible inter-continental testbed for data placement research and experiment|Guang Wei and Hailong Yang and Zhongzhi Luan and Depei Qian|inproceedings|8024681||July|||||||||||||||||||||||||||651699319|42
||Exposure assessment, Indoor PM, Ambient PM, In-home monitors, Shanghai||403-411||An accurate estimation of population exposure to particulate matter with an aerodynamic diameter <2.5 μm (PM2.5) is crucial to hazard assessment and epidemiology. This study integrated annual data from 1146 in-home air monitors, air quality monitoring network, public applications, and traffic smart cards to determine the pattern of PM2.5 concentrations and activities in different microenvironments (including outdoors, indoors, subways, buses, and cars). By combining massive amounts of signaling data from cell phones, this study applied a spatio-temporally weighted model to improve the estimation of PM2.5 exposure. Using Shanghai as a case study, the annual average indoor PM2.5 concentration was estimated to be 29.3 ± 27.1 μg/m3 (n = 365), with an average infiltration factor of 0.63. The spatio-temporally weighted PM2.5 exposure was estimated to be 32.1 ± 13.9 μg/m3 (n = 365), with indoor PM2.5 contributing the most (85.1%), followed by outdoor (7.6%), bus (3.7%), subway (3.1%), and car (0.5%). However, considering that outdoor PM2.5 makes a significant contribution to indoor PM2.5, outdoor PM2.5 was responsible for most of the exposure in Shanghai. A heatmap of PM2.5 exposure indicated that the inner-city exposure index was significantly higher than that of the outskirts city, which demonstrated that the importance of spatial differences in population exposure estimation.|https://doi.org/10.1016/j.envpol.2019.07.034|https://www.sciencedirect.com/science/article/pii/S026974911930795X||||2019|A spatio-temporally weighted hybrid model to improve estimates of personal PM2.5 exposure: Incorporating big data from multiple data sources|YuJie Ben and FuJun Ma and Hao Wang and Muhammad Azher Hassan and Romanenko Yevheniia and WenHong Fan and Yubiao Li and ZhaoMin Dong|article|BEN2019403|||Environmental Pollution|02697491||253|||||23916|2,136|Q1|227|2109|4204|129684|35383|4169|8,04|61,49|United Kingdom|Western Europe|1970-1980, 1986-2020|Health, Toxicology and Mutagenesis (Q1); Medicine (miscellaneous) (Q1); Pollution (Q1); Toxicology (Q1)|84,491|8.071|0.07982|651739361|354628351
||Big data analytics, Smart manufacturing, Servitization, Sustainable production, Conceptual framework, Product lifecycle||1343-1365||Smart manufacturing has received increased attention from academia and industry in recent years, as it provides competitive advantage for manufacturing companies making industry more efficient and sustainable. As one of the most important technologies for smart manufacturing, big data analytics can uncover hidden knowledge and other useful information like relations between lifecycle decisions and process parameters helping industrial leaders to make more-informed business decisions in complex management environments. However, according to the literature, big data analytics and smart manufacturing were individually researched in academia and industry. To provide theoretical foundations for the research community to further develop scientific insights in applying big data analytics to smart manufacturing, it is necessary to summarize the existing research progress and weakness. In this paper, through combining the key technologies of smart manufacturing and the idea of ubiquitous servitization in the whole lifecycle, the term of sustainable smart manufacturing was coined. A comprehensive overview of big data in smart manufacturing was conducted, and a conceptual framework was proposed from the perspective of product lifecycle. The proposed framework allows analyzing potential applications and key advantages, and the discussion of current challenges and future research directions provides valuable insights for academia and industry.|https://doi.org/10.1016/j.jclepro.2018.11.025|https://www.sciencedirect.com/science/article/pii/S0959652618334255||||2019|A comprehensive review of big data analytics throughout product lifecycle to support sustainable smart manufacturing: A framework, challenges and future research directions|Shan Ren and Yingfeng Zhang and Yang Liu and Tomohiko Sakao and Donald Huisingh and Cecilia M.V.B. Almeida|article|REN20191343|||Journal of Cleaner Production|09596526||210|||||19167|1,937|Q1|200|5126|10603|304498|103295|10577|9,56|59,40|United Kingdom|Western Europe|1993-2021|Environmental Science (miscellaneous) (Q1); Industrial and Manufacturing Engineering (Q1); Renewable Energy, Sustainability and the Environment (Q1); Strategy and Management (Q1)|170,352|9.297|0.18299|651852724|1121054297
||Big data;Compressed sensing;Wireless sensor networks;Wireless communication;Redundancy;Industrial plants;Green design||53-59||New-generation industries heavily rely on big data to improve their efficiency. Such big data are commonly collected by smart nodes and transmitted to the cloud via wireless. Due to the limited size of smart node, the shortage of energy is always a critical issue, and the wireless data transmission is extremely a big power consumer. Aiming to reduce the energy consumption in wireless, this article introduces a potential breach from data redundancy. If redundant data are no longer collected, a large amount of wireless transmissions can be cancelled and their energy saved. Motivated by this breach, this article proposes a compressive-sensing-based collection framework to minimize the amount of collection while guaranteeing data quality. This framework is verified by experiments and extensive real-trace-driven simulations.|10.1109/MCOM.2016.7588229|||||2016|Embracing big data with compressive sensing: a green approach in industrial wireless networks|Kong, Linghe and Zhang, Daqiang and He, Zongjian and Xiang, Qiao and Wan, Jiafu and Tao, Meixia|article|7588229||October|IEEE Communications Magazine|15581896|10|54|||||||||||||||||||||||652919447|160873938
||RPA, Financial shared service center, Cost management, Process optimization, Big data||115-119||"\"With the development of artificial intelligence technology, the widespread application of robot process automation (RPA) in the future financial field has become an inevitable trend. Through the review of the current situation of cost management of A Group’s financial shared service center, the article deeply expounds the problems that the current cross-system data cannot be automatically collected, the cost accounting is not timely, and the cost analysis report mode is too fixed. Based on Robot Process Automation (RPA), cost management process optimization and improvement were made on the cross-system data acquisition, \"\"Cloud Purchasing Platform\"\" construction, and comprehensive multi-dimensional cost analysis. It is expected to provide reference for the robot process automation application of the financial shared service center.\""|https://doi.org/10.1016/j.procs.2020.02.031|https://www.sciencedirect.com/science/article/pii/S1877050920301538||||2020|Research on Cost Management Optimization of Financial Sharing Center Based on RPA|Yu Lian Qiu and Guo Fang Xiao|article|QIU2020115|||Procedia Computer Science|18770509||166||Proceedings of the 3rd International Conference on Mechatronics and Intelligent Robotics (ICMIR-2019)|||19700182801|0,334|-|76|1907|6483|38511|13722|6359|2,09|20,19|Netherlands|Western Europe|2010-2020|Computer Science (miscellaneous)||||654854404|2108686752
||Data-driven probabilistic machine learning, Energy distribution, Discovery and design of energy materials, Big data analytics and smart grid, Strategic energy planning and smart manufacturing, Energy demand-side response||112128||The current trend indicates that energy demand and supply will eventually be controlled by autonomous software that optimizes decision-making and energy distribution operations. New state-of-the-art machine learning (ML) technologies are integral in optimizing decision-making in energy distribution networks and systems. This study was conducted on data-driven probabilistic ML techniques and their real-time applications to smart energy systems and networks to highlight the urgency of this area of research. This study focused on two key areas: i) the use of ML in core energy technologies and ii) the use cases of ML for energy distribution utilities. The core energy technologies include the use of ML in advanced energy materials, energy systems and storage devices, energy efficiency, smart energy material manufacturing in the smart grid paradigm, strategic energy planning, integration of renewable energy, and big data analytics in the smart grid environment. The investigated ML area in energy distribution systems includes energy consumption and price forecasting, the merit order of energy price forecasting, and the consumer lifetime value. Cybersecurity topics for power delivery and utilization, grid edge systems and distributed energy resources, power transmission, and distribution systems are also briefly studied. The primary goal of this work was to identify common issues useful in future studies on ML for smooth energy distribution operations. This study was concluded with many energy perspectives on significant opportunities and challenges. It is noted that if the smart ML automation is used in its targeting energy systems, the utility sector and energy industry could potentially save from $237 billion up to $813 billion.|https://doi.org/10.1016/j.rser.2022.112128|https://www.sciencedirect.com/science/article/pii/S1364032122000569||||2022|Data-driven probabilistic machine learning in sustainable smart energy/smart energy systems: Key developments, challenges, and future research opportunities in the context of smart grid paradigm|Tanveer Ahmad and Rafal Madonski and Dongdong Zhang and Chao Huang and Asad Mujeeb|article|AHMAD2022112128|||Renewable and Sustainable Energy Reviews|13640321||160|||||27567|3,522|Q1|295|643|3239|73956|54087|3206|16,30|115,02|United Kingdom|Western Europe|1997-2021|Renewable Energy, Sustainability and the Environment (Q1)||||656213784|1303297312
||Record linkage, Theoretical model, Data quality, Cloud computing||97-106||Record Linkage (RL) is the task of identifying duplicate entities in a dataset or multiple datasets. In the era of Big Data, this task has gained notorious attention due to the intrinsic quadratic complexity of the problem in relation to the size of the dataset. In practice, this task can be outsourced to a cloud service, and thus, a service customer may be interested in estimating the costs of a record linkage solution before executing it. Since the execution time of a record linkage solution depends on a combination of various algorithms, their respective parameter values and the employed cloud infrastructure, in practice it is hard to perform an a priori estimation of infrastructure costs for executing a record linkage task. Besides estimating customer costs, the estimation of record linkage costs is also important to evaluate whether (or not) the application of a set of RL parameter values will satisfy predefined time and budget restrictions. Aiming to tackle these challenges, we propose a theoretical model for estimating RL costs taking into account the main steps that may influence the execution time of the RL task. We also propose an algorithm, denoted as TBF, for evaluating the feasibility of RL parameter values, given a set of predefined customer restrictions. We evaluate the efficacy of the proposed model combined with regression techniques using record linkage results processed in real distributed environments. Based on the experimental results, we show that the employed regression technique has significant influence over the estimated record linkage costs. Moreover, we conclude that specific regression techniques are more suitable for estimating record linkage costs, depending on the evaluated scenario.|https://doi.org/10.1016/j.jpdc.2020.05.003|https://www.sciencedirect.com/science/article/pii/S0743731520302756||||2020|Estimating record linkage costs in distributed environments|Dimas Cassimiro Nascimento and Carlos Eduardo Santos Pires and Tiago Brasileiro Araujo and Demetrio Gomes Mestre|article|NASCIMENTO202097|||Journal of Parallel and Distributed Computing|07437315||143|||||25621|0,638|Q1|87|169|592|7166|2473|568|4,72|42,40|United States|Northern America|1984-2021|Computer Networks and Communications (Q1); Hardware and Architecture (Q1); Artificial Intelligence (Q2); Software (Q2); Theoretical Computer Science (Q2)|4,371|3.734|0.00477|657727375|1083163244
DG.O'21|Omaha, NE, USA||14|444–457|DG.O2021: The 22nd Annual International Conference on Digital Government Research|"\"This paper aims to identify key institutional factors that contribute to effective open data systems. Rapid advancement in new technologies such as machine learning, algorithms, IoT, and Cloud Computing has amplified the importance of national open data systems. The availability of relevant public data has become a crucial factor in creating sophisticated machine learning platforms or algorithms that will have a considerable impact on national competitiveness. Effective national open data strategies will matter in shaping an environment that will facilitate data production, dissemination, and utilization. Using multiple sources of data that measure the qualities of open data systems and various political, governmental, and legal attributes at the national level, we seek to identify key institutional factors that contribute to robust open data policies and outcomes. Our findings point to the importance of the existence of a national open data strategy and support (especially \"\"open by default\"\" strategy), pre-existing e-government capability, and countries operating under full democracy with its guarantees to civil liberties and political freedom. In addition, the nature of the open data matters as different managerial, political, and demographic conditions affected the quality of different open data systems. Policy implications of our findings are discussed.\""|10.1145/3463677.3463732|https://doi.org/10.1145/3463677.3463732|New York, NY, USA|Association for Computing Machinery|9781450384926|2021|What Matters in Maintaining Effective Open Government Data Systems? The Role of Government Managerial Capacity, and Political and Legal Environment|Ahn, Michael and Chu, Shengli|inproceedings|10.1145/3463677.3463732|||||||||||||||||||||||||||||661619258|42
MobiGIS '16|Burlingame, California|moving objects, multi-modal trajectory, big data, trajectory|8|19–26|Proceedings of the 5th ACM SIGSPATIAL International Workshop on Mobile Geographic Information Systems|Trajectory analysis is a central problem in the era of big data due to numerous interconnected mobile devices generating unprecedented amounts of spatio-temporal trajectories. Unfortunately, datasets of spatial trajectories are quite difficult to analyse because of the computational complexity of the various existing distance measures. A significant amount of work in comparing two trajectories stems from calculating temporal alignments of the involved spatial points. With this paper, we propose an alignment-free method of representing spatial trajectories using low-dimensional feature vectors by summarizing the combinatorics of shape-derived string sequences. Therefore, we propose to translate trajectories into strings describing the evolving shape of each trajectory, and then provide a sparse matrix representation of these strings using frequencies of adjacencies of characters (n-grams). The final feature vectors are constructed by approximating this matrix with low-dimensional column space using singular value decomposition. New trajectories can be projected into this geometry for comparison. We show that this construction leads to low-dimensional feature vectors with surprising expressive power. We illustrate the usefulness of this approach in various datasets.|10.1145/3004725.3004733|https://doi.org/10.1145/3004725.3004733|New York, NY, USA|Association for Computing Machinery|9781450345828|2016|A Low-Dimensional Feature Vector Representation for Alignment-Free Spatial Trajectory Analysis|Werner, Martin and Kiermeier, Marie|inproceedings|10.1145/3004725.3004733|||||||||||||||||||||||||||||662672064|42
||risk assessment;electric power;fuzzy comprehensive evaluation;Hadoop;index||88-91|2019 International Conference on Smart Grid and Electrical Automation (ICSGEA)|This paper studies the characteristics of big data of power, and aims at the data quality problems faced by power system. It puts forward an assessment method of power system data quality. Based on the characteristics of large power data, a series of indicators influencing the data are analyzed and hierarchically divided to determine the measurement standard of power production data during the process of risk management, namely, the risk index system. Then, the risk assessment model of power data is established by referring to the assessment model in other fields or the rules of deduction and induction in data mining. It can be used to evaluate the quality of power system data, and find a framework and solution suitable for large data quality assessment. Finally, the model is implemented on Hadoop platform, which proves that it takes into account the completeness of the index system, the objectivity of the assessment method and the rapidity of the calculation method.|10.1109/ICSGEA.2019.00028|||||2019|Risk Assessment Model and Experimental Analysis of Electric Power Production Based on Big Data|Zeyong, Wang and Yutian, Hong and Zhongzheng, Tong|inproceedings|8901326||Aug|||||||||||||||||||||||||||663232812|42
||Big data analytics, Technology adoption, Diffusion of innovations model, Cross-national differences, Retail industry||102827||Big data analytics (BDA) has emerged as a significant area of research for both researchers and practitioners in the retail industry, indicating the importance and influence of solving data-related problems in contemporary business organization. The present study utilised a quantitative-methods approach to investigate factors affecting retailers' adoption of BDA across three countries. A survey questionnaire was used to collect data from managers and decision-makers in the retail industry. Data of 2278 respondents were analysed through structural equation modelling. The findings revealed that security concerns, external support, top management support, and rational decision making culture have a greater effect on BDA adoption in developed countries UK than in UAE and Egypt. However, competition intensity and firm size have a greater effect on BDA adoption in UAE and Egypt than in UK. Finally, human variables (competence of information system's staff and staff's information system knowledge) have a greater effect on BDA adoption in Egypt than UK and UAE. The findings indicate that a “one-size-fits-all” approach is insufficient in capturing the heterogeneity of managers across countries. Implications for practice and theory were demonstrated.|https://doi.org/10.1016/j.jretconser.2021.102827|https://www.sciencedirect.com/science/article/pii/S0969698921003933||||2022|Cross-national differences in big data analytics adoption in the retail industry|Mayada Abd El-Aziz Youssef and Riyad Eid and Gomaa Agag|article|YOUSSEF2022102827|||Journal of Retailing and Consumer Services|09696989||64|||||22992|1,568|Q1|89|346|555|27784|4555|550|7,77|80,30|United Kingdom|Western Europe|1994-2021|Marketing (Q1)|10,506|7.135|0.00876|664265214|296517282
||Multi-energy power generation, Pollutant emission, Big data analysis, Renewable energy generation, Thermal power||128154||With the development of the integrated energy Internet, energy structure optimization and emission reduction have led to higher requirements for developing various energy sources to enable coordinated and sustainable development. However, data-mining methods are rarely used to study the coordination of multi-energy generation in published research results. In this study, from the perspective of power industry emissions, coordinated generation of various energy sources, and balance of power generation and consumption, a data-mining algorithm was used to analyze the development of thermal power, hydropower, wind power, waste heat, gas, and other power sources. The chi-square automatic interaction detection tree (CHAID), logistic regression, and two-step clustering methods were applied. The results show that: a) CO2 and SO2 emissions were mainly affected by thermal power generation, whereas NOx emissions were jointly affected by thermal power, garbage power, and gas-fired power, and the emissions of various pollutants increased with an increase in power consumption. The optimal power-generation scheme under minimum emission can be obtained. b) There was a strong correlation between thermal power generation and residential electricity consumption, and renewable energy (wind energy, photovoltaic, hydropower) exhibited the highest correlation with the electricity consumption of the tertiary industry, which indicates that renewable energy generation can be promoted by managing electricity consumption in the tertiary industry. c) When the electricity demand of all users was small, the proportion of renewable energy power generation increased; in contrast, the thermal power generation was larger. This indicates the importance of improving the sustainable and stable power supply of renewable energy. This study provides a data analysis model for the coordinated development of multiple energies, which will contribute to the decision-making basis for controlling power emissions, improving the utilization rate of renewable energy, and optimizing the energy structure.|https://doi.org/10.1016/j.jclepro.2021.128154|https://www.sciencedirect.com/science/article/pii/S0959652621023726||||2021|Research on big data analysis model of multi energy power generation considering pollutant emission—Empirical analysis from Shanxi Province|Dongfang Ren and Xiaopeng Guo and Cunbin Li|article|REN2021128154|||Journal of Cleaner Production|09596526||316|||||19167|1,937|Q1|200|5126|10603|304498|103295|10577|9,56|59,40|United Kingdom|Western Europe|1993-2021|Environmental Science (miscellaneous) (Q1); Industrial and Manufacturing Engineering (Q1); Renewable Energy, Sustainability and the Environment (Q1); Strategy and Management (Q1)|170,352|9.297|0.18299|665512254|1121054297
||Geocoding, Missing values imputation, High Performance Computing, Industrial spatial distribution, Urban spatial structure, Short text classification||9-23||Big, fine-grained enterprise registration data that includes time and location information enables us to quantitatively analyze, visualize, and understand the patterns of industries at multiple scales across time and space. However, data quality issues like incompleteness and ambiguity, hinder such analysis and application. These issues become more challenging when the volume of data is immense and constantly growing. High Performance Computing (HPC) frameworks can tackle big data computational issues, but few studies have systematically investigated imputation methods for enterprise registration data in this type of computing environment. In this paper, we propose a big data imputation workflow based on Apache Spark as well as a bare-metal computing cluster, to impute enterprise registration data. We integrated external data sources, employed Natural Language Processing (NLP), and compared several machine-learning methods to address incompleteness and ambiguity problems found in enterprise registration data. Experimental results illustrate the feasibility, efficiency, and scalability of the proposed HPC-based imputation framework, which also provides a reference for other big georeferenced text data processing. Using these imputation results, we visualize and briefly discuss the spatiotemporal distribution of industries in China, demonstrating the potential applications of such data when quality issues are resolved.|https://doi.org/10.1016/j.compenvurbsys.2018.01.010|https://www.sciencedirect.com/science/article/pii/S0198971517301916||||2018|Big enterprise registration data imputation: Supporting spatiotemporal analysis of industries in China|Fa Li and Zhipeng Gui and Huayi Wu and Jianya Gong and Yuan Wang and Siyu Tian and Jiawen Zhang|article|LI20189|||Computers, Environment and Urban Systems|01989715||70|||||23269|1,549|Q1|92|85|327|4842|2135|324|6,11|56,96|United Kingdom|Western Europe|1980-1986, 1988-2020|Ecological Modeling (Q1); Environmental Science (miscellaneous) (Q1); Geography, Planning and Development (Q1); Urban Studies (Q1)||||665792367|1571752529
||Qualidata guide, Data quality, Data format, Life Cycle Inventory, Weighting factors||160-171||The generation of reliable life cycle inventories is essential towards Life Cycle Assessment (LCA) development, and the use of literature inventories as data sources can serve as a driving force for emerging LCA databases. The aim of this paper was to propose a method to select and rank scientific publications to be used as possible data sources for supplying LCA databases with new datasets. A case study was designed to identify eligible datasets to compose the emergent Brazilian Life Cycle Inventory Database System – the “SICV Brasil” launched in 2016. The methodology used was based on an exploratory research composed of three steps: i) a bibliographic survey on the scientific productions of Life Cycle Inventories (LCI) in Brazil from 2000 to 2017; ii) a cross-check of LCI data and information based on the 40 selected requirements used in order to analyze the quality of LCI datasets in terms of mandatory, recommended and optional requirements; and iii) an analysis of the data quality requirements for those datasets with support of principles of Analytical Hierarchy Process (AHP) to elect possible datasets to be included in the SICV Brasil database. In total, 57 publications were analyzed and the results indicated that mandatory requirements had under 50% acceptance and only 10 requirements (less than 25%) were fully met. The best LCI dataset received 73 points (90%) with the scoring method, while 16 datasets were given less than 40 points (50%). Therefore, it is necessary to improve data quality of LCI datasets found in literature before using them to integrate LCA databases. In this regard, this study proposed a guide with short, medium, and long-term measures to mitigate this problem. The idea is to put an action plan into practice to gather more LCI datasets from literature which may be eligible for publication to SICV Brasil to improve this national database with more and relevant high-quality datasets.|https://doi.org/10.1016/j.spc.2020.09.021|https://www.sciencedirect.com/science/article/pii/S2352550920303286||||2021|Evaluating and ranking secondary data sources to be used in the Brazilian LCA database – “SICV Brasil”|Luri {Shirosaki Marçal de Souza} and Andréa Oliveira Nunes and Gabriela Giusti and Yovana M.B. Saavedra and Thiago Oliveira Rodrigues and Tiago E. {Nunes Braga} and Diogo A. {Lopes Silva}|article|SHIROSAKIMARCALDESOUZA2021160|||Sustainable Production and Consumption|23525509||26|||||21100416081|1,019|Q1|26|91|198|6098|1058|194|5,34|67,01|Netherlands|Western Europe|2015-2021|Environmental Engineering (Q1); Industrial and Manufacturing Engineering (Q1); Environmental Chemistry (Q2); Renewable Energy, Sustainability and the Environment (Q2)|1,403|5.032|0.00196|672130599|91202035
||Diffraction;Big Data;Machine learning;Morphology;Support vector machines;Three-dimensional displays;Noise measurement;machine learning;deep learning;big data;data quality;cross-validation||140-147|2017 IEEE International Conference on Services Computing (SCC)|Providing an easily accessible data service with high quality data is important for building big data applications. In this paper, we introduce a big data service for managing and accessing massive-scale biomedical image data. The service includes three major components: a NoSQL database for storing images and data analytics results, a client consisting of a group of query scripts for data access and management, and a data quality enhancement component for improving the performance of data analytics. Low-quality data can result in incorrect analytics results and may lead to no value even harmful conclusions. Therefore, it is important to provide an effective mechanism for ensuring data quality improvement in a big data service. We describe the implement ion of a deep learning classifier to automatically filter low quality data in datasets. To improve the effectiveness of data separation, the classifier is rigorously validated with synthetic data generated by a collection of scientific tools. Design of big data services with data quality improvement as an integral component, along with the best practices collected from this experimental study, will help researchers and practitioners to develop strategies for improving the quality of big data services, building big data applications, and designing machine learning classifiers.|10.1109/SCC.2017.25|||||2017|Building a Deep Learning Classifier for Enhancing a Biomedical Big Data Service|Ding, Junhua and Kang, Xiaojun and Hu, Xin-Hua and Gudivada, Venkat|inproceedings|8034978||June||24742473|||||||||||||||||||||||||677216055|549097892
||||1-2|||https://doi.org/10.1016/j.jpdc.2017.05.020|https://www.sciencedirect.com/science/article/pii/S0743731517301776||||2017|Special Issue on Scalable Computing Systems for Big Data Applications|Xian-He Sun and Marc Frincu and Charalampos Chelmis|article|SUN20171|||Journal of Parallel and Distributed Computing|07437315||108||Special Issue on Scalable Computing Systems for Big Data Applications|||25621|0,638|Q1|87|169|592|7166|2473|568|4,72|42,40|United States|Northern America|1984-2021|Computer Networks and Communications (Q1); Hardware and Architecture (Q1); Artificial Intelligence (Q2); Software (Q2); Theoretical Computer Science (Q2)|4,371|3.734|0.00477|681730722|1083163244
||RFID data warehousing;OLAP;Hive;Cloud computing||476-483|2019 IEEE International Conferences on Ubiquitous Computing & Communications (IUCC) and Data Science and Computational Intelligence (DSCI) and Smart Computing, Networking and Services (SmartCNS)|Radio Frequency Identification (RFID) technology is used in many applications for monitoring object movement. The use of RFID in supply chain management systems enables to track the movement of products from suppliers to warehouses, store backrooms, and eventually points of sale. The vast amount of data resulting from the proliferation of RFID readers and tags poses challenges for data management and analytics. RFID data warehousing can enhance data quality and consistency, and give great potential benefits for Online Analytical Processing (OLAP) applications. Traditional data warehouses are built primarily on relational database management systems. However, the size of RFID data being collected and analyzed in the industry for business intelligence is growing rapidly, making traditional warehousing solutions prohibitively expensive. Hive is an open-source data warehousing solution built on top of Hadoop which is a popular Big Data computing framework. This paper presents alternative RFID data warehouse designs which can handle a large amount of RFID data and support a variety of OLAP queries. The proposed approaches are implemented on Hive and evaluated for query performance in cloud computing environment.|10.1109/IUCC/DSCI/SmartCNS.2019.00105|||||2019|RFID Data Warehousing and OLAP with Hive|Yoo, Yeisol and Yoo, Jin Soung|inproceedings|8982563||Oct|||||||||||||||||||||||||||681973597|42
||Data lake, Metadata, Big data, Data platform||299-313||The rise of data platforms has enabled the collection and processing of huge volumes of data, but has opened to the risk of losing their control. Collecting proper metadata about raw data and transformations can significantly reduce this risk. In this paper we propose MOSES, a technology-agnostic, extensible, and customizable framework for metadata handling in big data platforms. The framework hinges on a metadata repository that stores information about the objects in the big data platform and the processes that transform them. MOSES provides a wide range of functionalities to different types of users of the platform. Differently from previous high-level proposals, MOSES is fully implemented and it was not conceived for a specific technology. Besides discussing the rationale and the features of MOSES, in this paper we describe its implementation and we test it on a real case study. The ultimate goal is to take a significant step forward towards proving that metadata handling in big data platforms is feasible and beneficial.|https://doi.org/10.1016/j.future.2021.06.031|https://www.sciencedirect.com/science/article/pii/S0167739X21002260||||2021|Making data platforms smarter with MOSES|Matteo Francia and Enrico Gallinucci and Matteo Golfarelli and Anna Giulia Leoni and Stefano Rizzi and Nicola Santolini|article|FRANCIA2021299|||Future Generation Computer Systems|0167739X||125|||||12264|1,262|Q1|119|798|1935|36054|16877|1868|9,11|45,18|Netherlands|Western Europe|1984-2021|Computer Networks and Communications (Q1); Hardware and Architecture (Q1); Software (Q1)||||684778008|562237118
|||11|64–74||Perspectives on the role and responsibility of the data-management research community in designing, developing, using, and overseeing automated decision systems.|10.1145/3488717|https://doi.org/10.1145/3488717|New York, NY, USA|Association for Computing Machinery||2022|Responsible Data Management|Stoyanovich, Julia and Abiteboul, Serge and Howe, Bill and Jagadish, H. V. and Schelter, Sebastian|article|10.1145/3488717||may|Commun. ACM|00010782|6|65|June 2022||||||||||||||||||||||684983732|647144465
WSC '14|Savannah, Georgia||7|943–949|Proceedings of the 2014 Winter Simulation Conference|Computerized decision making is becoming a reality with exponentially growing data and machine capabilities. Some decision making is extremely complex, historically reserved for governing bodies or market places where the collective human experience and intelligence come to play. Other decision making can be trusted to computers that are on a path now into the future through novel software development and technological improvements in data access. In all cases, we should think about this carefully first: what data are really important for our goals and what data should be ignored or not even stored? The answer to these questions involves human intelligence and understanding before the data-to-decision process begins.||||IEEE Press||2014|The Future of Computerized Decision Making|Elmegreen, Bruce G. and Sanchez, Susan M. and Szalay, Alexander S.|inproceedings|10.5555/2693848.2693973|||||||||||||||||||||||||||||685209391|42
||Trajectory;Wavelet transforms;Noise reduction;Artificial intelligence;Marine vehicles;Navigation;Empirical mode decomposition;wavelet transform;data denoising;automatic identification system;trajectory data||106-111|2019 IEEE 4th International Conference on Big Data Analytics (ICBDA)|The Automatic Identification System (AIS) has attracted increasing attention in recent years for its superior properties in ocean engineering and maritime management. The spatio-temporal vessel trajectory data is highly related to the received AIS data. However, the AIS raw data often suffer from undesirable noise during signal acquisition and analog-to-digital conversion. To improve AIS-based vessel trajectory data quality, we propose to develop a vessel trajectory smoothing method by combining empirical mode decomposition (EMD) with wavelet transform. In particular, EMD is introduced to decompose the original vessel trajectory data into several sub-trajectories. The EMD decomposition is able to assist in enhancing the robustness of trajectory smoothing. Wavelet transform is directly adopted to smooth the decomposed sub-trajectories. The final high-quality trajectory is obtained by combining the smoothed sub-trajectories in this work. The proposed method has the capacity of removing the unwanted noise while preserving the important trajectory features. Numerous experiments have illustrated the superior smoothing performance of the proposed combined method.|10.1109/ICBDA.2019.8713242|||||2019|Spatio-Temporal Vessel Trajectory Smoothing Using Empirical Mode Decomposition and Wavelet Transform|Li, Xinyi and Feng, Zikun and Li, Yan and Liu, Zhao and Liu, Ryan Wen|inproceedings|8713242||March|||||||||||||||||||||||||||687408477|42
||quality of information, Distributed machine learning|28|||In the era of big data, data are usually distributed across numerous connected computing and storage units (i.e., nodes or workers). Under such an environment, many machine learning problems can be reformulated as a consensus optimization problem, which consists of one objective and constraint terms splitting into N parts (each corresponds to a node). Such a problem can be solved efficiently in a distributed manner via Alternating Direction Method of Multipliers (ADMM). However, existing consensus optimization frameworks assume that every node has the same quality of information (QoI), i.e., the data from all the nodes are equally informative for the estimation of global model parameters. As a consequence, they may lead to inaccurate estimates in the presence of nodes with low QoI. To overcome this challenge, in this article, we propose a novel consensus optimization framework for distributed machine-learning that incorporates the crucial metric, QoI. Theoretically, we prove that the convergence rate of the proposed framework is linear to the number of iterations, but has a tighter upper bound compared with ADMM. Experimentally, we show that the proposed framework is more efficient and effective than existing ADMM-based solutions on both synthetic and real-world datasets due to its faster convergence rate and higher accuracy.|10.1145/3522591|https://doi.org/10.1145/3522591|New York, NY, USA|Association for Computing Machinery||2022|Toward Quality of Information Aware Distributed Machine Learning|Xiao, Houping and Wang, Shiyu|article|10.1145/3522591|109|jul|ACM Trans. Knowl. Discov. Data|15564681|6|16|December 2022||||5800173377|0,728|Q1|59|71|169|3729|816|168|4,54|52,52|United States|Northern America|2007-2020|Computer Science (miscellaneous) (Q1)|1,740|2.713|0.00224|687466554|1302859451
DOLAP '14|Shanghai, China|ETL, data generator, process quality|10|23–32|Proceedings of the 17th International Workshop on Data Warehousing and OLAP|Obtaining the right set of data for evaluating the fulfillment of different quality standards in the extract-transform-load (ETL) process design is rather challenging. First, the real data might be out of reach due to different privacy constraints, while providing a synthetic set of data is known as a labor-intensive task that needs to take various combinations of process parameters into account. Additionally, having a single dataset usually does not represent the evolution of data throughout the complete process lifespan, hence missing the plethora of possible test cases. To facilitate such demanding task, in this paper we propose an automatic data generator (i.e., Bijoux). Starting from a given ETL process model, Bijoux extracts the semantics of data transformations, analyzes the constraints they imply over data, and automatically generates testing datasets. At the same time, it considers different dataset and transformation characteristics (e.g., size, distribution, selectivity, etc.) in order to cover a variety of test scenarios. We report our experimental findings showing the effectiveness and scalability of our approach.|10.1145/2666158.2666183|https://doi.org/10.1145/2666158.2666183|New York, NY, USA|Association for Computing Machinery|9781450309998|2014|Bijoux: Data Generator for Evaluating ETL Process Quality|Nakuc\c{c}i, Emona and Theodorou, Vasileios and Jovanovic, Petar and Abell\'{o}, Alberto|inproceedings|10.1145/2666158.2666183|||||||||||||||||||||||||||||689003217|42
||Cloud computing;Computational modeling;Big data;Quality of service;Distributed processing;Conferences;Educational institutions||850-851|2014 IEEE International Parallel & Distributed Processing Symposium Workshops|These keynote speeches discuss the following: Market-oriented cloud computing and Big Data applications; and Low-latency distributed analytics in Naiad.|10.1109/IPDPSW.2014.234|||||2014|HPGC Keynotes|Buyya, Rajkumar and Murray, Derek|inproceedings|6969470||May|||||||||||||||||||||||||||689488994|42
||Measurement;Feeds;Computer architecture;Wireless sensor networks;Energy efficiency;Clouds;Data models;Data quality;cloud computing;energy model;applications to sensing;green networks||169-176|2016 International Conference on Distributed Computing in Sensor Systems (DCOSS)|"\"Current research on wireless sensor networks \"\"WSNs\"\" in the Internet of Things \"\"IoT\"\" has focused on performance, scalability and energy efficiency. Innovations in these areas have many challenges due to the increasing volume of smart device data streams in the internet of Everything \"\"IoE\"\". Data feeds from future IoE systems such as the internet of vehicles, smart homes and smart-cities will need real time consolidation. This merger of technologies will require innovative big data algorithms and architectures that authenticate the data streams. A primary concern is in dynamically quantifying the data quality \"\"DQ\"\" of the streams while constructing real-time metrics to assess the energy efficiency \"\"EE\"\" of these IoE devices. In order to define the relationship between sensor stream DQ and EE, we propose our multi-tiered cloud-service architecture TAU-FIVE. The technical contributions of our framework includes data quality and energy efficiency models based on 7 DQ attributes and multiple reprogrammable smart sensors that dynamically modify and regulate the DQ and EE of a WSN. Our research maintains that WSN's can balance sustainability with quality of service by creating real-time metrics that merge energy usage with data stream integrity. This equilibrium will impact energy awareness in the IoT as the multitude of batch device data streams are integrated with the variety of social and professional networks and evolve into the IoE.\""|10.1109/DCOSS.2016.42|||||2016|TAU-FIVE: A Multi-tiered Architecture for Data Quality and Energy-Sustainability in Sensor Networks|Lawson, Victor J. and Ramaswamy, Lakshmish|inproceedings|7536334||May||23252944|||||||||||||||||||||||||689658197|1898617345
||Schema augmentation, Schema complement, Data quality, SAP HANA||101495||The production of analytic datasets is a significant big data trend and has gone well beyond the scope of traditional IT-governed dataset development. Analytic datasets are now created by data scientists and data analysts using big data frameworks and agile data preparation tools. However, despite the profusion of available datasets, it remains quite difficult for a data analyst to start from a dataset at hand and customize it with additional attributes coming from other existing datasets. This article describes a model and algorithms that exploit automatically extracted and user-defined semantic relationships for extending analytic datasets with new atomic or aggregated attribute values. Our framework is implemented as a REST service in SAP HANA and includes a careful theoretical analysis and practical solutions for several complex data quality issues.|https://doi.org/10.1016/j.is.2020.101495|https://www.sciencedirect.com/science/article/pii/S0306437920300065||||2020|Discovering and merging related analytic datasets|Rutian Liu and Eric Simon and Bernd Amann and Stéphane Gançarski|article|LIU2020101495|||Information Systems|03064379||91|||||12305|0,547|Q2|85|100|271|4739|1027|255|3,39|47,39|United Kingdom|Western Europe|1975-2021|Hardware and Architecture (Q2); Information Systems (Q2); Software (Q2)|2,604|2.309|0.00331|691002464|303930735
UbiComp '15|Osaka, Japan|wearable sensors, smoking cessation, smoking detection, smartwatch, mobile health (mHealth)|12|999–1010|Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing|Recent researches have demonstrated the feasibility of detecting smoking from wearable sensors, but their performance on real-life smoking lapse detection is unknown. In this paper, we propose a new model and evaluate its performance on 61 newly abstinent smokers for detecting a first lapse. We use two wearable sensors --- breathing pattern from respiration and arm movements from 6-axis inertial sensors worn on wrists. In 10-fold cross-validation on 40 hours of training data from 6 daily smokers, our model achieves a recall rate of 96.9%, for a false positive rate of 1.1%. When our model is applied to 3 days of post-quit data from 32 lapsers, it correctly pinpoints the timing of first lapse in 28 participants. Only 2 false episodes are detected on 20 abstinent days of these participants. When tested on 84 abstinent days from 28 abstainers, the false episode per day is limited to 1/6.|10.1145/2750858.2806897|https://doi.org/10.1145/2750858.2806897|New York, NY, USA|Association for Computing Machinery|9781450335744|2015|PuffMarker: A Multi-Sensor Approach for Pinpointing the Timing of First Lapse in Smoking Cessation|Saleheen, Nazir and Ali, Amin Ahsan and Hossain, Syed Monowar and Sarker, Hillol and Chatterjee, Soujanya and Marlin, Benjamin and Ertin, Emre and al'Absi, Mustafa and Kumar, Santosh|inproceedings|10.1145/2750858.2806897|||||||||||||||||||||||||||||692106637|42
||Big data ethics, Business data usage, Corporate data collection, Government data usage, Technology ethics, US-China similarities, US-China differences||45-55||As society continues its rapid change to a digitized individual, corporate, and government environment it is prudent for researchers to investigate the zeitgeist of the global citizenry. The technological changes brought about by big data analytics are changing the way we gather and view data. This big data analytics sentiment research examines how Chinese and American respondents may view big data collection and analytics differently. The paper follows with an analysis of reported attitudes toward possible viewpoints from each country on various big data analytics topics ranging from individual to business and governmental foci. Hofstede's cultural dimensions are used to inform and frame our research hypotheses. Findings suggest that Chinese and American perspectives differ on individual data values, with the Chinese being more open to data collection and analytic techniques targeted toward individuals. Furthermore, support is found that US respondents have a more favorable view of businesses' use of data analytics. Finally, there is a strong difference in the attitudes toward governmental use of data, where US respondents do not favor governmental big data analytics usage and the Chinese respondents indicated a greater acceptance of governmental data usage. These findings are helpful in better understanding appropriate technological change and adoption from a societal perspective. Specifically, this research provides insights for corporate business and government entities suggesting how they might adjust their approach to big data collection and management in order to better support and sustain their organization's services and products.|https://doi.org/10.1016/j.techfore.2017.06.029|https://www.sciencedirect.com/science/article/pii/S0040162517308612||||2018|Big data analytics sentiment: US-China reaction to data collection by business and government|Ryan C. LaBrie and Gerhard H. Steinke and Xiangmin Li and Joseph A. Cazier|article|LABRIE201845|||Technological Forecasting and Social Change|00401625||130|||||14704|2,226|Q1|117|448|1099|35581|10127|1061|9,01|79,42|United States|Northern America|1970-2020|Applied Psychology (Q1); Business and International Management (Q1); Management of Technology and Innovation (Q1)|21,116|8.593|0.02416|693019194|1949868303
BDE '22|Beijing, China|Boundary reconstruction, Credit card fraud detection, Machine learning, Integrated learning|8|86–93|Proceedings of the 4th International Conference on Big Data Engineering|With the popularity of electronic payment, it not only brings great convenience, but also increases the risk of fraudulent transactions. At present, there are two problems in the identification of credit card fraud. The number of fraud and normal transactions is extremely unbalanced and the classification boundary is fuzzy. In order to solve these problems, this paper proposes an integrated classification framework for boundary reconstruction, which uses different machine learning algorithms as base learners to compare the original data with the data modeling after boundary reconstruction. The research shows that data boundary reconstruction can not only effectively alleviate the deviation caused by data imbalance to machine learning. It can also improve the data quality, so as to improve the accuracy of model classification; The integrated classification method can accurately identify credit card transactions, and the prediction effect of decision tree is the best. The proposed model is also applicable in other abnormal situations.|10.1145/3538950.3538962|https://doi.org/10.1145/3538950.3538962|New York, NY, USA|Association for Computing Machinery|9781450395632|2022|Credit Card Fraud Detection Using Boundary Reconstruction and Integrated Classification|Zhou, Wei and Xue, Xiaorui and Luo, Danxue|inproceedings|10.1145/3538950.3538962|||||||||||||||||||||||||||||694483830|42
||Sensors;Correlation;Air quality;Data models;Estimation;Data integrity;Task analysis;Active learning;location selection;kriging interpolation;regression tree||661-666|2019 IEEE SmartWorld, Ubiquitous Intelligence & Computing, Advanced & Trusted Computing, Scalable Computing & Communications, Cloud & Big Data Computing, Internet of People and Smart City Innovation (SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI)|In environmental monitoring applications, selecting appropriate locations to sense is important relating to data quality and Sensing cost. This paper addresses the challenge by collecting data from a subset of locations, then leveraging the spatial and cross-domain correlations to deduce data of other locations, thus can obtain acceptable data quality with lower sensing cost. Referring to active learning, the proposed framework is constructed by two types modules (i.e., estimators and selectors) and a cyclic process of estimating and selecting. Estimators based on kriging interpolation and regression tree are implemented, and their corresponding selectors are designed. We evaluate the effectiveness of the framework by taking air quality sensing as an example. Results show that to reach data quality of about 25% MAPE, the framework only needs 15% locations, while random selector needs 25% locations.|10.1109/SmartWorld-UIC-ATC-SCALCOM-IOP-SCI.2019.00149|||||2019|Selecting Sensing Location Leveraging Spatial and Cross-Domain Correlations|Chang, Huijuan and Yu, Zhiyong and Yu, Zhiwen and Guo, Bin|inproceedings|9060201||Aug|||||||||||||||||||||||||||695188075|42
||Big data collection, Geographic information, Grid database, Data mining||87-95||Energy crisis and climate change have become two seriously concerned issues universally. As a feasible solution, Global Energy Interconnection (GEI) has been highly praised and positively responded by the international community once proposed by China. From strategic conception to implementation, GEI development has entered a new phase of joint action now. Gathering and building a global grid database is a prerequisite for conducting research on GEI. Based on the requirement of global grid data management and application, combining with big data and geographic information technology, this paper studies the global grid data acquisition and analysis process, sorts out and designs the global grid database structure supporting GEI research, and builds a global grid database system.|https://doi.org/10.14171/j.2096-5117.gei.2018.01.011|https://www.sciencedirect.com/science/article/pii/S2096511718300112||||2018|Application and research of global grid database design based on geographic information|Xuming Liang|article|LIANG201887|||Global Energy Interconnection|20965117|1|1|||||||||||||||||||||||695809158|814104778
WSC '17|Las Vegas, Nevada||15||Proceedings of the 2017 Winter Simulation Conference|Open Science is the practice of making scientific research accessible to all. It promotes open access to the artefacts of research, the software, data, results and the scientific articles in which they appear, so that others can validate, use and collaborate. Open Science is also being mandated by many funding bodies. The concept of Open Science is new to many Modelling &amp; Simulation (M&amp;S) researchers. To introduce Open Science to our field, this paper unpacks Open Science to understand some of its approaches and benefits. Good practice in the reporting of simulation studies is discussed and the Strengthening the Reporting of Empirical Simulation Studies (STRESS) standardized checklist approach is presented. A case study shows how Digital Object Identifiers, Researcher Registries, Open Access Data Repositories and Scientific Gateways can support Open Science practices for M&amp;S research. The article concludes with a set of guidelines for adopting Open Science for M&amp;S.||||IEEE Press|9781538634271|2017|Open Science: Approaches and Benefits for Modeling &amp; Simulation|Taylor, Simon J. E. and Anagnostou, Anastasia and Fabiyi, Adedeji and Currie, Christine and Monks, Thomas and Barbera, Roberto and Becker, Bruce|inproceedings|10.5555/3242181.3242220|36||||||||||||||||||||||||||||697468476|42
||Energy Efficiency, Big Data, Characteristics of Big Data, Big Data analysis, Construction, Web of Science||544-549||Data generation has increased drastically over the past few years. Data management has also grown in importance because extracting the significant value out of a huge pile of raw data is of prime important thing to make different decisions. One of the important sectors nowadays is construction sector, especially building energy efficiency field. Collecting big amount of data, using different kinds of big data analysis can help to improve construction process from the energy efficiency perspective. This article reviews the understanding of Big Data, methods used for Big Data analysis and the main problems with Big Data in the field of energy.|https://doi.org/10.1016/j.proeng.2017.02.064|https://www.sciencedirect.com/science/article/pii/S1877705817305702||||2017|Big Data in Building Energy Efficiency: Understanding of Big Data and Main Challenges|Natalija Koseleva and Guoda Ropaite|article|KOSELEVA2017544|||Procedia Engineering|18777058||172||Modern Building Materials, Structures and Techniques|||18700156717|0,320|-|74|0|5873|0|9870|5804|1,88|0,00|Netherlands|Western Europe|2009-2019|Engineering (miscellaneous)||||698606536|506091674
||Urban areas;Correlation;Spatial resolution;Data analysis;Mathematical model;Public transportation;Standards;data quality;data profiling;urban data||563-572|2019 IEEE International Conference on Big Data (Big Data)|Increasingly, decisions are based on insights and conclusions derived from the results of data analysis. Thus, determining the validity of these results is of paramount importance. In this paper, we take a step towards helping users identify potential issues in spatio-temporal data and thus gain trust in the results they derived from these data. We focus on processes that are captured by relationships among datasets that serve as the data exhaust for different components of urban environments. In this scenario, debugging data involves two important challenges: the inherent complexity of spatio-temporal data, and the number of possible relationships. We propose a framework for profiling spatio-temporal relationships that automatically identifies data slices that present a significant deviation from what is expected, and thus, helps focus a user's attention on slices of the data that may have quality issues and/or that may affect the conclusions derived from the analysis' results. We describe the profiling methodology and how it derives relationships, identifies candidate deviations, assesses their statistical significance, and measures their magnitude. We also present a series of cases studies using real datasets from New York City which demonstrate the usefulness of spatio-temporal profiling to build trust on data analysis' results.|10.1109/BigData47090.2019.9006289|||||2019|Understanding Spatio-Temporal Urban Processes|Rocha, Lais M. A. and Bessa, Aline and Chirigati, Fernando and OFriel, Eugene and Moro, Mirella M. and Freire, Juliana|inproceedings|9006289||Dec|||||||||||||||||||||||||||703019903|42
||Data Quality Assessment, Advanced Planning, Scheduling, Bayesian Network, Enterprise Resource Planning||194-204||In today’s manufacturing industry, enterprise-resource-planning (ERP) systems reach their limit when planning and scheduling production subject to multiple objectives and constraints. Advanced planning and scheduling (APS) systems provide these capabilities and are an extension for ERP systems. However, when integrating an APS and ERP system, the ERP data frequently lacks quality, hindering the APS system from working as required. This paper introduces a data quality (DQ) assessment framework that employs a Bayesian Network (BN) to perform quick DQ assessments based on expert interviews and DQ measurements with actual ERP data. We explain the BN’s functionality, design, and validation and show how using the perceived DQ of experts and a semi-supervised learning algorithm improves the BN’s predictions over time. We discuss applying our framework in an APS system implementation project involving an APS system provider and a medium-sized manufacturer of hydraulic cylinders. Despite considering the DQ assessment framework in such a specific context, it is not restricted to a particular domain. We close by discussing the framework’s limits, particularly the BN as a DQ assessment methodology and future works to improve its performance.|https://doi.org/10.1016/j.procs.2022.01.218|https://www.sciencedirect.com/science/article/pii/S1877050922002277||||2022|An ERP Data Quality Assessment Framework for the Implementation of an APS system using Bayesian Networks|Jan-Phillip Herrmann and Sven Tackenberg and Elio Padoano and Jörg Hartlief and Jens Rautenstengel and Christine Loeser and Jörg Böhme|article|HERRMANN2022194|||Procedia Computer Science|18770509||200||3rd International Conference on Industry 4.0 and Smart Manufacturing|||19700182801|0,334|-|76|1907|6483|38511|13722|6359|2,09|20,19|Netherlands|Western Europe|2010-2020|Computer Science (miscellaneous)||||705980844|2108686752
CIKM '20|Virtual Event, Ireland|hardening, perturbation, xor, sliding window, random sampling|4|2185–2188|Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management|Privacy-preserving record linkage (PPRL) facilitates the matching of records that correspond to the same real-world entities across different databases while preserving the privacy of the individuals in these databases. A Bloom filter (BF) is a space efficient probabilistic data structure that is becoming popular in PPRL as an efficient privacy technique to encode sensitive information in records while still enabling approximate similarity computations between attribute values. However, BF encoding is susceptible to privacy attacks which can re-identify the values that are being encoded. In this paper we propose two novel techniques that can be applied on BF encoding to improve privacy against attacks. Our techniques use neighbouring bits in a BF to generate new bit values. An empirical study on large real databases shows that our techniques provide high security against privacy attacks, and achieve better similarity computation accuracy and linkage quality compared to other privacy improvements that can be applied on BF encoding.|10.1145/3340531.3412105|https://doi.org/10.1145/3340531.3412105|New York, NY, USA|Association for Computing Machinery|9781450368599|2020|Securing Bloom Filters for Privacy-Preserving Record Linkage|Ranbaduge, Thilina and Schnell, Rainer|inproceedings|10.1145/3340531.3412105|||||||||||||||||||||||||||||709047359|42
||Data centers;Cloud computing;Optimization;Organizations;Servers;Maintenance engineering;Data Center;energy saving;green computing;server;network devices;cloud storage||184-189|2018 2nd International Conference on I-SMAC (IoT in Social, Mobile, Analytics and Cloud) (I-SMAC)I-SMAC (IoT in Social, Mobile, Analytics and Cloud) (I-SMAC), 2018 2nd International Conference on|Considering current evolving technology and the way data are growing, IT consulting and outsourcing industry expected to be strategic partner for technology innovation in addition to support on-going business with reduced operational cost. Data Center is backbone for digital economy, big data, cloud, artificial intelligence, IoT or wearable technology. Data growth and on-demand data access changed the focus of data center as storage and disaster recovery to access data instantly from cloud without compromising security controls and data quality. These technology transformations create demand for latency. Every organization like Facebook, Equinix, Amazon, and Google are having their own data centers and expanding their business on cloud services. Data Center plays major critical on success of digital business. It is important to find possible options to optimize infrastructure and improve efficiency and productivity of Data Center. At the same time, we need to make sure that environment is up and running without compromising quality and security of data. This paper gives few solutions to get more from Data Center, reduce operational cost and optimize infrastructure utilization.|10.1109/I-SMAC.2018.8653702|||||2018|Strategy for Data Center Optimization : Improve Data Center capability to meet business opportunities|Suresh, T. and Murugan, A.|inproceedings|8653702||Aug|||||||||||||||||||||||||||710795914|42
CHI '18|Montreal QC, Canada|boundary objects, family research, social care, data security, big data, personal data, civic data, design games, dynamic consent, user-centered design, family, ubicomp, data privacy, ethnographic interviews, family design games, healthcare, data sharing|13|1–13|Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems|"\"Across social care, healthcare and public policy, enabled by the \"\"big data\"\" revolution (which has normalized large-scale data-based decision-making), there are moves to \"\"join up\"\" citizen databases to provide care workers with holistic views of families they support. In this context, questions of personal data privacy, security, access, control and (dis-)empowerment are critical considerations for system designers and policy makers alike. To explore the family perspective on this landscape of what we call Family Civic Data, we carried out ethnographic interviews with four North-East families. Our design-game-based interviews were effective for engaging both adults and children to talk about the impact of this dry, technical topic on their lives. Our findings, delivered in the form of design guidelines, show support for dynamic consent: families would feel most empowered if involved in an ongoing co-operative relationship with state welfare and civic authorities through shared interaction with their data.\""|10.1145/3173574.3173710|https://doi.org/10.1145/3173574.3173710|New York, NY, USA|Association for Computing Machinery|9781450356206|2018|Understanding the Family Perspective on the Storage, Sharing and Handling of Family Civic Data|Bowyer, Alex and Montague, Kyle and Wheater, Stuart and McGovern, Ruth and Lingam, Raghu and Balaam, Madeline|inproceedings|10.1145/3173574.3173710|||||||||||||||||||||||||||||712726825|42
|||9|949–957||Data quality assessment is an essential process of any data analysis process including machine learning. The process is time-consuming as it involves multiple independent data quality checks that are performed iteratively at scale on evolving data resulting from exploratory data analysis (EDA). Existing solutions that provide computational optimizations for data quality assessment often separate the data structure from its data quality which then requires efforts from users to explicitly maintain state-like information. They demand a certain level of distributed system knowledge to ensure high-level pipeline optimizations from data analysts who should instead be focusing on analyzing the data. We, therefore, propose data-quality-aware dataframes, a data quality management system embedded as part of a data analyst's familiar data structure, such as a Python dataframe. The framework automatically detects changes in datasets' metadata and exploits the context of each of the quality checks to provide efficient data quality assessment on ever-changing data. We demonstrate in our experiment that our approach can reduce the overall data quality evaluation runtime by 40-80% in both local and distributed setups with less than 10% increase in memory usage.|10.14778/3503585.3503602|https://doi.org/10.14778/3503585.3503602||VLDB Endowment||2022|DQDF: Data-Quality-Aware Dataframes|Sinthong, Phanwadee and Patel, Dhaval and Zhou, Nianjun and Shrivastava, Shrey and Iyengar, Arun and Bhamidipaty, Anuradha|article|10.14778/3503585.3503602||apr|Proc. VLDB Endow.|21508097|4|15|December 2021||||21100199855|0,946|Q1|134|246|598|12401|3759|566|5,50|50,41|United States|Northern America|2008-2020|Computer Science (miscellaneous) (Q1)|7,198|2.047|0.00891|717231632|1216159931
|||||Data Cleaning|Data quality is one of the most important problems in data management, since dirty data often leads to inaccurate data analytics results and incorrect business decisions. Poor data across businesses and the U.S. government are reported to cost trillions of dollars a year. Multiple surveys show that dirty data is the most common barrier faced by data scientists. Not surprisingly, developing effective and efficient data cleaning solutions is challenging and is rife with deep theoretical and engineering problems.This book is about data cleaning, which is used to refer to all kinds of tasks and activities to detect and repair errors in the data. Rather than focus on a particular data cleaning task, we give an overview of the endto- end data cleaning process, describing various error detection and repair methods, and attempt to anchor these proposals with multiple taxonomies and views. Specifically, we cover four of the most common and important data cleaning tasks, namely, outlier detection, data transformation, error repair (including imputing missing values), and data deduplication. Furthermore, due to the increasing popularity and applicability of machine learning techniques, we include a chapter that specifically explores how machine learning techniques are used for data cleaning, and how data cleaning is used to improve machine learning models.This book is intended to serve as a useful reference for researchers and practitioners who are interested in the area of data quality and data cleaning. It can also be used as a textbook for a graduate course. Although we aim at covering state-of-the-art algorithms and techniques, we recognize that data cleaning is still an active field of research and therefore provide future directions of research whenever appropriate.||https://doi.org/10.1145/3310205.3310212|New York, NY, USA|Association for Computing Machinery|9781450371520|2019|Rule-Based Data Cleaning||inbook|10.1145/3310205.3310212|||||||||||||||||||||||||||||718628680|42
ICCCM '21|Singapore, Singapore||6|31–36|Proceedings of the 9th International Conference on Computer and Communications Management|The purposes of this research were: 1) Analyze factors affecting the student retention of higher education students, 2) Develop intelligent consulting system models with intellectual technology for the student retention of higher education students, 3) Design intelligent consulting system architecture with intellectual technology for the student retention of higher education students, 4) Develop intelligent consulting systems with intellectual technology for the student retention of higher education students, and 5) Study the results of intelligent consultation systems with intellectual technology for the student retention of higher education students. An intelligent counseling system with intellectual technology for the student retention of higher education students is a system that can reduce students' mid-exit rates and increase student retention rates. The research has synthesized analysis of factors that affect Student retention applied to Cognitive technology, machine learning can provide accurate student retention forecasts. Counselors can know before students drop out.|10.1145/3479162.3479167|https://doi.org/10.1145/3479162.3479167|New York, NY, USA|Association for Computing Machinery|9781450390071|2021|System Framework of Intelligent Consulting Systems with Intellectual Technology|Suaprae, Phanintorn and Nilsook, Prachyanun and Wannapiroon, Panita|inproceedings|10.1145/3479162.3479167|||||||||||||||||||||||||||||723703871|42
PCI '16|Patras, Greece|convex optimization, signal processing techniques, big data, statistical learning tools, stochastic approximation|6||Proceedings of the 20th Pan-Hellenic Conference on Informatics|Big data science has been developed into a topic that attracts attention from industry, academia and governments. The main objective in Big Data science is to recognize and extract meaningful information from huge amounts of heterogeneous data and unstructured data (which constitute 95% of big data). Signal Processing (SP) techniques and related statistical learning (SL) tools such as Principal Component Analysis (PCA), R-PCA (Robust PCA), Compressive Sampling (CS), convex optimization (CO), stochastic approximation (SA), kernel based learning (KBL) tasks are used for robustness, compression and dimensionality reduction in Big Data arising challenges. This review paper introduces Big Data related SP techniques and presents applications of this emerging field.|10.1145/3003733.3003767|https://doi.org/10.1145/3003733.3003767|New York, NY, USA|Association for Computing Machinery|9781450347891|2016|Signal Processing Techniques Restructure The Big Data Era|Petrou, Charilaos and Paraskevas, Michael|inproceedings|10.1145/3003733.3003767|52||||||||||||||||||||||||||||723813690|42
JCDL '19|Champaign, Illinois|name authority, NSTL, multi-source, name disambiguation|2|398–399|Proceedings of the 18th Joint Conference on Digital Libraries|Name authority is a common issue in digital library. This paper mainly summarizes the practice of constructing name authority database based on multi-source data in NSTL. Firstly, we load, integrate different source data and convert them into unified structure. Then, we extract scientific entities and relationships from these data, according to metadata model. For different entities, we use different disambiguation rules and algorithms. As a result, we have constructed author name authority database, institution authority database, journal authority database, and fund authority database. Compared with Incites, taking six institutions name authority data as a sample, the result shows that the average accuracy can reach 86.8%.1|10.1109/JCDL.2019.00088|https://doi.org/10.1109/JCDL.2019.00088||IEEE Press|9781728115474|2020|Practice of Constructing Name Authority Database Based on Multi-Source Data Integration|Yu, Qianqian and Zhang, Jianyong and Qian, Li and Dong, Zhipeng and Huang, Yongwen and Jianhua, Liu|inproceedings|10.1109/JCDL.2019.00088|||||||||||||||||||||||||||||724961505|42
||Big data, Effective use, Factors, Framework||103146||Information systems (IS) research has explored “effective use” in a variety of contexts. However, it is yet to specifically consider it in the context of the unique characteristics of big data. Yet, organizations have a high appetite for big data, and there is growing evidence that investments in big data solutions do not always lead to the derivation of intended value. Accordingly, there is a need for rigorous academic guidance on what factors enable effective use of big data. With this paper, we aim to guide IS researchers such that the expansion of the body of knowledge on the effective use of big data can proceed in a structured and systematic manner and can subsequently lead to empirically driven guidance for organizations. Namely, with this paper, we cast a wide net to understand and consolidate from literature the potential factors that can influence the effective use of big data, so they may be further studied. To do so, we first conduct a systematic literature review. Our review identifies 41 factors, which we categorize into 7 themes, namely data quality; data privacy and security and governance; perceived organizational benefit; process management; people aspects; systems, tools, and technologies; and organizational aspects. To explore the existence of these themes in practice, we then analyze 45 published case studies that document insights into how specific companies use big data successfully. Finally, we propose a framework for the study of effective use of big data as a basis for future research. Our contributions aim to guide researchers in establishing the relevance and relationships within the identified themes and factors and are a step toward developing a deeper understanding of effective use of big data.|https://doi.org/10.1016/j.im.2019.02.001|https://www.sciencedirect.com/science/article/pii/S0378720617308649||||2020|Factors influencing effective use of big data: A research framework|Feliks P. Sejahtera Surbakti and Wei Wang and Marta Indulska and Shazia Sadiq|article|SURBAKTI2020103146|||Information & Management|03787206|1|57||Big data and business analytics: A research agenda for realizing business value|||12303|2,147|Q1|162|130|248|11385|2482|246|8,94|87,58|Netherlands|Western Europe|1977-2020|Information Systems (Q1); Information Systems and Management (Q1); Management Information Systems (Q1)||||726514160|1945939487
SAICSIT '16|Johannesburg, South Africa|IS Profession, IT Skills, Typology, Business Intelligence, Analytics|10||Proceedings of the Annual Conference of the South African Institute of Computer Scientists and Information Technologists|Business Intelligence (BI) is regarded by executives as a critical practice to adopt and invest in. The purpose of this research is to develop a categorized typology of skills required by BI professionals. A review of extant literature resulted in the identification of twenty three skills. The research aimed to validate these skills, and add additional skills to this typology based on the experiences of BI professionals within industry. These experiences were captured through interviews. Skills were then categorized by identifying commonalities across them. No additional skills were identified by the interviewed participants. A categorized typology of skills was developed which grouped the initial twenty three skills into seven higher order categories. The seven categories of skills were identified as: (1) Prepare data for subject matter expert (SME), analyst or other external party for further analysis; (2) Apply simulation modelling, statistical techniques and provide business insight; (3) Manage stakeholders and project and operational tasks; (4) Develop strategic long term BI roadmap that links to corporate strategy; (5) Understand business processes in order to effectively extract user requirements; (6) Design and code sustainable solutions; (7) Absorb and distribute knowledge.|10.1145/2987491.2987521|https://doi.org/10.1145/2987491.2987521|New York, NY, USA|Association for Computing Machinery|9781450348058|2016|A Descriptive Categorized Typology of Requisite Skills for Business Intelligence Professionals|de Jager, Tiaan and Brown, Irwin|inproceedings|10.1145/2987491.2987521|14||||||||||||||||||||||||||||727512250|42
|||4|55–58|||10.1145/2737817.2737831|https://doi.org/10.1145/2737817.2737831|New York, NY, USA|Association for Computing Machinery||2015|Report on the Seventh International Workshop on Business Intelligence for the Real Time Enterprise (BIRTE 2013)|Pedersen, Torben Bach and Castellanos, Malu and Dayal, Umesh|article|10.1145/2737817.2737831||feb|SIGMOD Rec.|01635808|4|43|December 2014||||13622|0,372|Q2|142|39|81|808|127|57|1,67|20,72|United States|Northern America|1969, 1973-1978, 1981-2020|Information Systems (Q2); Software (Q2)|1,819|0.775|7.5E-4|727804275|962972343
||Scholarly data, Big data, Cloud-based big data analytics, Big scholarly data, Big data platform, Scholarly applications||923-944||Recently, there has been a shifting focus of organizations and governments towards digitization of academic and technical documents, adding a new facet to the concept of digital libraries. The volume, variety and velocity of this generated data, satisfies the big data definition, as a result of which, this scholarly reserve is popularly referred to as big scholarly data. In order to facilitate data analytics for big scholarly data, architectures and services for the same need to be developed. The evolving nature of research problems has made them essentially interdisciplinary. As a result, there is a growing demand for scholarly applications like collaborator discovery, expert finding and research recommendation systems, in addition to several others. This research paper investigates the current trends and identifies the existing challenges in development of a big scholarly data platform, with specific focus on directions for future research and maps them to the different phases of the big data lifecycle.|https://doi.org/10.1016/j.ipm.2017.03.006|https://www.sciencedirect.com/science/article/pii/S0306457316305994||||2017|A survey on scholarly data: From big data perspective|Samiya Khan and Xiufeng Liu and Kashish A. Shakil and Mansaf Alam|article|KHAN2017923|||Information Processing & Management|03064573|4|53|||||||||||||||||||||||730853149|1769516999
||Big data;Q-factor;Sociology;Statistics;Security;Reliability;Quality assurance;risk assessment;risk management;information quality;data semantics;data scrubbing;decision quality;transparency||1-8|2016 European Intelligence and Security Informatics Conference (EISIC)|The quality of inferences drawn from data, big or small, is heavily dependent on the quality of the data and the quality of the processes applied to it. Big data analytics is emerging from laboratories and being applied to intelligence and security needs. To achieve confidence in the outcomes of these applications, a quality assurance framework is needed. This paper outlines the challenges, and draws attention to the consequences of misconceived and misapplied projects. It presents key aspects of the necessary risk assessment and risk management approaches, and suggests opportunities for research.|10.1109/EISIC.2016.010|||||2016|Quality Assurance for Security Applications of Big Data|Clarke, Roger|inproceedings|7870183||Aug|||||||||||||||||||||||||||733089436|42
||Operations Management, Manufacturing Systems 4.0, Profit per Hour, Advanced Process Control, Big Data Analytics, Agile Manufacturing||715-720||The rise of Industry 4.0 and in particular Big Data analytics of production parameters offers exciting new ways for optimization. The majority of factories in process industries currently aim for example, either for output maximization, yield increase, or cost reduction. The availability of real-time data and online processing capability with advanced algorithms enables a profit per hour operational management approach. Profit per hour as a target control metric allows running factories at the optimal available operating point taking all revenue and cost drivers into account. This paper describes the suitability of profit per hour as a target process control parameter for production in process industries. The authors explain how this management approach helps to make better operational decisions, trading off yield, energy, throughput, among other factors, and the resulting cumulative benefits. They also lay out how Big Data and advanced algorithms are the key enabler to this new approach, as well as a standardized methodology for implementation. With profit per hour an agile control approach is presented which aims to optimize the performance of industrial manufacturing systems in a world of ever increasing volatility.|https://doi.org/10.1016/j.procir.2017.03.094|https://www.sciencedirect.com/science/article/pii/S2212827117302408||||2017|Profit Per Hour as a Target Process Control Parameter for Manufacturing Systems Enabled by Big Data Analytics and Industry 4.0 Infrastructure|Markus Hammer and Ken Somers and Hugo Karre and Christian Ramsauer|article|HAMMER2017715|||Procedia CIRP|22128271||63||Manufacturing Systems 4.0 – Proceedings of the 50th CIRP Conference on Manufacturing Systems|||21100243809|0,683|-|65|1456|3191|30451|8225|3145|2,40|20,91|Netherlands|Western Europe|2012-2020|Control and Systems Engineering; Industrial and Manufacturing Engineering||||733669248|2127027836
||Heuristic algorithms;Data integrity;Convergence;Roads;Wireless communication;Vehicle dynamics;Servers||1-1||The deployment of future intelligent transportation systems is contingent upon seamless and reliable operation of connected and autonomous vehicles (CAVs). One key challenge in developing CAVs is the design of an autonomous controller that can accurately execute near real-time control decisions, such as a quick acceleration when merging to a highway and frequent speed changes in a stop-and-go traffic. However, the use of conventional feedback controllers or traditional learning-based controllers, solely trained by each CAV’s local data, cannot guarantee a robust controller performance over a wide range of road conditions and traffic dynamics. In this paper, a new federated learning (FL) framework enabled by large-scale wireless connectivity is proposed for designing the autonomous controller of CAVs. In this framework, the learning models used by the controllers are collaboratively trained among a group of CAVs. To capture the varying CAV participation in the FL training process and the diverse local data quality among CAVs, a novel dynamic federated proximal (DFP) algorithm is proposed that accounts for the mobility of CAVs, the wireless fading channels, as well as the unbalanced and non-independent and identically distributed data across CAVs. A rigorous convergence analysis is performed for the proposed algorithm to identify how fast the CAVs converge to using the optimal autonomous controller. In particular, the impacts of varying CAV participation in the FL process and diverse CAV data quality on the convergence of the proposed DFP algorithm are explicitly analyzed. Leveraging this analysis, an incentive mechanism based on contract theory is designed to improve the FL convergence speed. Simulation results using real vehicular data traces show that the proposed DFP-based controller can accurately track the target CAV speed over time and under different traffic scenarios. Moreover, the results show that the proposed DFP algorithm has a much faster convergence compared to popular FL algorithms such as federated averaging (FedAvg) and federated proximal (FedProx). The results also validate the feasibility of the contract-theoretic incentive mechanism and show that the proposed mechanism can improve the convergence speed of the DFP algorithm by 40% compared to the baselines.|10.1109/TWC.2022.3183996|||||2022|Federated Learning on the Road Autonomous Controller Design for Connected and Autonomous Vehicles|Zeng, Tengchan and Semiariy, Omid and Chen, Mingzhe and Saad, Walid and Bennis, Mehdi|article|9806308|||IEEE Transactions on Wireless Communications|15582248|||||||||||||||||||||||||735032937|1402738362
||Big data, Big data analytics, Organizational theory, Firms’ performance, Research agenda||112-129||The purpose of this study is to enrich the existing state-of-the-art literature on the impact of big data on business growth by examining how dozens of organizational theories can be applied to enhance the understanding of the effects of big data on organizational performance. While the majority of management disciplines have had research dedicated to the conceptual discussion of how to link a variety of organizational theories to empirically quantified research topics, the body of research into big data so far lacks an academic work capable of systematising the organizational theories supporting big data domain. The three main contributions of this work are: (a) it addresses the application of dozens of organizational theories to big data research; (b) it offers a research agenda on how to link organizational theories to empirical research in big data; and (c) it foresees promising linkages between organizational theories and the effects of big data on organizational performance, with the aim of contributing to further research in this field. This work concludes by presenting implications for researchers and managers, and by highlighting intrinsic limitations of the research.|https://doi.org/10.1016/j.ijinfomgt.2018.07.005|https://www.sciencedirect.com/science/article/pii/S026840121830553X||||2018|Management theory and big data literature: From a review to a research agenda|Paula {de Camargo Fiorini} and Bruno Michel {Roman Pais Seles} and Charbel Jose {Chiappetta Jabbour} and Enzo {Barberio Mariano} and Ana Beatriz Lopes {de Sousa Jabbour}|article|DECAMARGOFIORINI2018112|||International Journal of Information Management|02684012||43|||||15631|2,770|Q1|114|224|382|20318|5853|373|16,16|90,71|United Kingdom|Western Europe|1970, 1986-2021|Artificial Intelligence (Q1); Computer Networks and Communications (Q1); Information Systems (Q1); Information Systems and Management (Q1); Library and Information Sciences (Q1); Management Information Systems (Q1); Marketing (Q1)|12,245|14.098|0.01167|735472126|747927863
||Energy-efficient optimization, Discrete manufacturing system, Data preprocessing, Data mining||101336||Because of big data on energy consumption, there is a lack of research on the discrete manufacturing system. The discrete manufacturing system has plenty of multi-source and heterogeneous data; it was challenging to collect real-time data. Recently, low carbon and green manufacturing is a hot field; especially, it can save electrical energy. This paper proposes a significant energy consumption data of a data-driven analysis framework, which promoting the energy efficiency of discrete manufacturing plant, equipment, and workshop production process. Firstly, put forward the evaluation standards of energy efficiency for discrete manufacturing shops. Then make energy-consumption data preprocessing. Efficiency optimization of big data mining method is put forward based on grid computing function. Design the discrete manufacturing system energy-consumption parameter values, then summarizes prediction algorithms and models in order to predict the results and the trends. Finally, introduce the application of a mobile phone shell manufacturing shop to verify the proposed framework. Further research will focus on energy-consumption data mining processing.|https://doi.org/10.1016/j.seta.2021.101336|https://www.sciencedirect.com/science/article/pii/S2213138821003465||||2021|A framework of energy-consumption driven discrete manufacturing system|Tao Zhang and Weixi Ji and Yongtao Qiu|article|ZHANG2021101336|||Sustainable Energy Technologies and Assessments|22131388||47|||||21100239262|1,040|Q1|39|270|333|13311|1833|331|5,73|49,30|United Kingdom|Western Europe|2013-2020|Energy Engineering and Power Technology (Q1); Renewable Energy, Sustainability and the Environment (Q2)|3,234|5.353|0.00348|737088670|1822851290
ICISCAE 2021|Dalian, China||5|1649–1653|2021 4th International Conference on Information Systems and Computer Aided Education|Lightning, also known as lightning, is a strong catastrophic discharge phenomenon between clouds and between clouds and the ground in the process of atmospheric convection. There are two types: cloud flash (between clouds) and ground flash (between clouds and the earth). Lightning location system is a system that uses telemetry technology to monitor lightning activities in full-automatic, large-area, high-precision, continuous and real-time. By analyzing lightning location data collected for a long time, lightning accident points can be quickly located, the distribution of regional lightning activities can be counted, the development trend can be analyzed, and early warning can be carried out, which can provide reference for lightning protection research of ground buildings, thus reducing the harm to human activities. In this paper, the grid method is used to store and query lightning data based on the classical measurement model. Taking the lightning protection technology of transmission network as an example, the method and application of statistics and mining of lightning monitoring data in power grid are studied.|10.1145/3482632.3484010|https://doi.org/10.1145/3482632.3484010|New York, NY, USA|Association for Computing Machinery|9781450390255|2021|Statistics and Mining Analysis of Lightning Monitoring Data in Power Grid Based on Classical Metrology Model|Yang, Rui|inproceedings|10.1145/3482632.3484010|||||||||||||||||||||||||||||737945617|42
||Business process, Event log, Data quality assessment, Data quality improvement||101874||The efficiency and effectiveness of business processes are usually evaluated by Process Performance Indicators (PPIs), which are computed using process event logs. PPIs can be insightful only when they are measurable, i.e., reliable. This paper proposes to define PPI measurability on the basis of the quality of the data in the process logs. Then, based on this definition, a framework for PPI measurability assessment and improvement is presented. For the assessment, we propose novel definitions of PPI accuracy, completeness, consistency, timeliness and volume that contextualise the traditional definitions in the data quality literature to the case of process logs. For the improvement, we define a set of guidelines for improving the measurability of a PPI. These guidelines may concern improving existing event logs, for instance through data imputation, implementation or enhancement of the process monitoring systems, or updating the PPI definitions. A case study in a large-sized institution is discussed to show the feasibility and the practical value of the proposed framework.|https://doi.org/10.1016/j.is.2021.101874|https://www.sciencedirect.com/science/article/pii/S0306437921000995||||2022|Assessing and improving measurability of process performance indicators based on quality of logs|Cinzia Cappiello and Marco Comuzzi and Pierluigi Plebani and Matheus Fim|article|CAPPIELLO2022101874|||Information Systems|03064379||103|||||12305|0,547|Q2|85|100|271|4739|1027|255|3,39|47,39|United Kingdom|Western Europe|1975-2021|Hardware and Architecture (Q2); Information Systems (Q2); Software (Q2)|2,604|2.309|0.00331|741005446|303930735
||Big data, Co-innovation, Open innovation, Bibliometric analysis, Literature review||102347||This is the first systematic literature review concerning the interconnections between big data (BD) and co-innovation. It uses BD as a common perspective of analysis as well as a concept aggregating different research streams (open innovation, co-creation and collaborative innovation). The review is based on the results of a bibliographic coupling analysis performed with 51 peer-reviewed papers published before the end of 2019. Three thematic clusters were discovered, which respectively focused on BD as a knowledge creation enabler within co-innovation contexts, BD as a driver of co-innovation processes based on customer engagement, and the impact of BD on co-innovation within service ecosystems. The paper theoretically argues that the use of BD, in addition to enhancing intentional and direct collaborative innovation processes, allows the development of passive and unintentional co-innovation that can be implemented through indirect relationships between the collaborative actors. This study also makes eleven unique research propositions concerning further theoretical developments and managerial implementations in the field of BD-driven co-innovation.|https://doi.org/10.1016/j.ijinfomgt.2021.102347|https://www.sciencedirect.com/science/article/pii/S0268401221000402||||2021|Using big data for co-innovation processes: Mapping the field of data-driven innovation, proposing theoretical developments and providing a research agenda|Stefano Bresciani and Francesco Ciampi and Francesco Meli and Alberto Ferraris|article|BRESCIANI2021102347|||International Journal of Information Management|02684012||60|||||15631|2,770|Q1|114|224|382|20318|5853|373|16,16|90,71|United Kingdom|Western Europe|1970, 1986-2021|Artificial Intelligence (Q1); Computer Networks and Communications (Q1); Information Systems (Q1); Information Systems and Management (Q1); Library and Information Sciences (Q1); Management Information Systems (Q1); Marketing (Q1)|12,245|14.098|0.01167|745546766|747927863
CHI EA '18|Montreal QC, Canada|ai, visual analytics, data, hci, business intelligence|9|1–9|Extended Abstracts of the 2018 CHI Conference on Human Factors in Computing Systems|The Business Intelligence (BI) paradigm is challenged by emerging use cases such as news and social media analytics in which the source data are unstructured, the analysis metrics are unspecified, and the appropriate visual representations are unsupported by mainstream tools. This case study documents the work undertaken in Microsoft Research to enable these use cases in the Microsoft Power BI product. Our approach comprises: (a) back-end pipelines that use AI to infer navigable data structures from streams of unstructured text, media and metadata; and (b) front-end representations of these structures grounded in the Visual Analytics literature. Through our creation of multiple end-to-end data applications, we learned that representing the varying quality of inferred data structures was crucial for making the use and limitations of AI transparent to users. We conclude with reflections on BI in the age of AI, big data, and democratized access to data analytics.|10.1145/3170427.3174367|https://doi.org/10.1145/3170427.3174367|New York, NY, USA|Association for Computing Machinery|9781450356213|2018|Bringing AI to BI: Enabling Visual Analytics of Unstructured Data in a Modern Business Intelligence Platform|Edge, Darren and Larson, Jonathan and White, Christopher|inproceedings|10.1145/3170427.3174367|||||||||||||||||||||||||||||745662330|42
Intelligent Data-Centric Systems||Agriculture, Big data, Weka, J48, Talend, Crops, Analytics, Fertilizers, Prediction||409-436|AI, Edge and IoT-based Smart Agriculture|The exponential growth and ubiquity of both structured and unstructured data have led us into the big data era. Big data analytics is increasingly becoming a trending practice that many organizations are adopting to construct valuable information from big data. This field has substantially attracted academics, practitioners, and industries. But there are some challenges for big data processing and analytics that include integration of data, volume ofdata, the rate of transformation of data, and the veracity and validity of data. The history of griculture in India dates back to the Indus Valley civilization. Due to variations in climatic conditions, it has become challenging to achieve the desired results in crop yields. The use of technology in agriculture has increased in recent years and data analytics is one such trend that has penetrated the agriculture field. The main challenge in using big data in agriculture is identifying the effectiveness of big data analytics. Big data analysis can be processed and analyzed using parallel databases such as Talend or analytical paradigms like MapReduce on a Hadoop distributed file system. There are other mechanisms such as Weka and R, which are two of the most popular data analytical and statistical computing tools produced by the open source community, but there are certain challenges compared to the other techniques mentioned. In this chapter, the comparative studies of various mechanisms will be provided that will give an insight to process and analyze big data generated from farms and the grains obtained from it according to the seasons, the soil health, and the location. In addition, various case studies are shown for data processing in context with planting, agricultural growth, and smart farming. From the experimentation, the authors have shown which is the right fertilizer for a specific state and soil. In addition, the authors have worked on the analysis of crop production per state and per year.|https://doi.org/10.1016/B978-0-12-823694-9.00030-X|https://www.sciencedirect.com/science/article/pii/B978012823694900030X||Academic Press|978-0-12-823694-9|2022|Chapter 24 - Exploring performance and predictive analytics of agriculture data|Madhavi Vaidya and Shweta Katkar|incollection|VAIDYA2022409||||||||||Ajith Abraham and Sujata Dash and Joel J.P.C. Rodrigues and Biswaranjan Acharya and Subhendu Kumar Pani|||||||||||||||||||749212597|42
||data, data warehouse, big data, decision support systems, scalability, business intelligence||1-15|Building a Scalable Data Warehouse with Data Vault 2.0|This chapter introduces basic terminology of data warehousing, its applications, and the business context. It provides a brief description of its history and where it is heading. Basic data warehouse architectures that have been established in the industry are presented. Issues faced by data warehouse practitioners are explained, including topics such as big data, changing business requirements, performance issues, complexity, auditability, restart checkpoints, and fluctuation of team members.|https://doi.org/10.1016/B978-0-12-802510-9.00001-5|https://www.sciencedirect.com/science/article/pii/B9780128025109000015|Boston|Morgan Kaufmann|978-0-12-802510-9|2016|Chapter 1 - Introduction to Data Warehousing|Daniel Linstedt and Michael Olschimke|incollection|LINSTEDT20161||||||||||Daniel Linstedt and Michael Olschimke|||||||||||||||||||749449032|42
||Big data;Measurement;Feature extraction;Quality assessment;Social network services;Companies;Context;Big Data;data quality dimensions;data quality evaluation;Big data sampling||759-765|2016 Intl IEEE Conferences on Ubiquitous Intelligence & Computing, Advanced and Trusted Computing, Scalable Computing and Communications, Cloud and Big Data Computing, Internet of People, and Smart World Congress (UIC/ATC/ScalCom/CBDCom/IoP/SmartWorld)|Data is the most valuable asset companies are proud of. When its quality degrades, the consequences are unpredictable, can lead to complete wrong insights. In Big Data context, evaluating the data quality is challenging, must be done prior to any Big data analytics by providing some data quality confidence. Given the huge data size, its fast generation, it requires mechanisms, strategies to evaluate, assess data quality in a fast, efficient way. However, checking the Quality of Big Data is a very costly process if it is applied on the entire data. In this paper, we propose an efficient data quality evaluation scheme by applying sampling strategies on Big data sets. The Sampling will reduce the data size to a representative population samples for fast quality evaluation. The evaluation targeted some data quality dimensions like completeness, consistency. The experimentations have been conducted on Sleep disorder's data set by applying Big data bootstrap sampling techniques. The results showed that the mean quality score of samples is representative for the original data, illustrate the importance of sampling to reduce computing costs when Big data quality evaluation is concerned. We applied the Quality results generated as quality proposals on the original data to increase its quality.|10.1109/UIC-ATC-ScalCom-CBDCom-IoP-SmartWorld.2016.0122|||||2016|Big Data Quality: A Quality Dimensions Evaluation|Taleb, Ikbal and Kassabi, Hadeel T. El and Serhani, Mohamed Adel and Dssouli, Rachida and Bouhaddioui, Chafik|inproceedings|7816918||July|||||||||||||||||||||||||||750995240|42
SIGMOD '16|San Francisco, California, USA|data analytics, cross-platform execution, big data|4|2069–2072|Proceedings of the 2016 International Conference on Management of Data|Many emerging applications, from domains such as healthcare and oil &amp; gas, require several data processing systems for complex analytics. This demo paper showcases system, a framework that provides multi-platform task execution for such applications. It features a three-layer data processing abstraction and a new query optimization approach for multi-platform settings. We will demonstrate the strengths of system by using real-world scenarios from three different applications, namely, machine learning, data cleaning, and data fusion.|10.1145/2882903.2899414|https://doi.org/10.1145/2882903.2899414|New York, NY, USA|Association for Computing Machinery|9781450335317|2016|Rheem: Enabling Multi-Platform Task Execution|Agrawal, Divy and Ba, Lamine and Berti-Equille, Laure and Chawla, Sanjay and Elmagarmid, Ahmed and Hammady, Hossam and Idris, Yasser and Kaoudi, Zoi and Khayyat, Zuhair and Kruse, Sebastian and Ouzzani, Mourad and Papotti, Paolo and Quiane-Ruiz, Jorge-Arnulfo and Tang, Nan and Zaki, Mohammed J.|inproceedings|10.1145/2882903.2899414|||||||||||||||||||||||||||||751873050|42
||Big data nanoindentation, Cross-scale characterization, Elastomer, Microstructure, Oilwell cement, Young’s modulus||129190||Intensive big data nanoindentation (BDNi) characterization was performed to reveal the cross-scale mechanical properties of, and hence distinguish the different phases in, inorganic–organic hybrid oilwell cement-elastomer composites, hydrothermally cured at 160 °C and 20 MPa for 28 days. Totally-three emulsified and particulate elastomers, including styrene-butadiene latex (SBL) emulsion (6, 12, and 14 wt.%), polypropylene (PP) powder (12 wt.%), and nitrile rubber (NR) powder (6 wt.%), and a weighting agent, hematite (50 wt.%), were used as additives to finely adjust the mechanical properties and microstructure of the hybrid composites, which were respectively examined by the BDNi and mercury intrusion porosimetry and scanning electron microscopy. BDNi data were statistically deconvoluted by the Gaussian mixture modeling (GMM) to discern mechanically distinct phases and their Young’s moduli and hardness at the micro/nano scale and the bulk composites’ properties at the macro scale. Results show that the SBL emulsion can be more homogeneously dispersed into the cement matrix, due to its emulsified soft consistency and hydrophilicity, resulting in the formation of soft coatings on, and softer infills intermixed with, the cement hydration products (CHPs). In contrast, the two hydrophobic, inert, particulate elastomers, PP and NR powders, only act as isolated soft inclusions embedded in the hydrated cement matrix. The NR melts at high temperatures and permeates into the pores of the cement matrix, leading to the formation of complex intervened micromorphology and hence functions better than the PP. All elastomers can effectively reduce the composites’ Young’s moduli: with increasing the elastomer contents, while the modulus of a BDNi-identified major CHP phase decreases from 20.9 to 11.3 GPa, the bulk composites’ counterpart from 17.3 to 10.7 GPa. The BDNi enables the identification of multiple mechanically distinct phases in the hybrid composites and quantification of the property changes of these phases.|https://doi.org/10.1016/j.conbuildmat.2022.129190|https://www.sciencedirect.com/science/article/pii/S095006182202846X||||2022|Big data nanoindentation characterization of cross-scale mechanical properties of oilwell cement-elastomer composites|Yucheng Li and Yunhu Lu and Li Liu and Shengmin Luo and Li He and Yongfeng Deng and Guoping Zhang|article|LI2022129190|||Construction and Building Materials|09500618||354|||||24443|1,662|Q1|170|3583|7707|175521|52599|7705|6,50|48,99|United Kingdom|Western Europe|1987-2020|Building and Construction (Q1); Civil and Structural Engineering (Q1); Materials Science (miscellaneous) (Q1)|123,941|6.141|0.09849|753253129|85944854
||Data collection, randomized response, differential privacy|26|||We study the value of data privacy in a game-theoretic model of trading private data, where a data collector purchases private data from strategic data subjects (individuals) through an incentive mechanism. One primary goal of the data collector is to learn some desired information from the elicited data. Specifically, this information is modeled by an underlying state, and the private data of each individual represents his of her knowledge about the state. Departing from most of the existing work on privacy-aware surveys, our model does not assume the data collector to be trustworthy. Further, an individual takes full control of his or her own data privacy and reports only a privacy-preserving version of his or her data.In this article, the value of ϵ units of privacy is measured by the minimum payment among all nonnegative payment mechanisms, under which an individual’s best response at a Nash equilibrium is to report his or her data in an ϵ-locally differentially private manner. The higher ϵ is, the less private the reported data is. We derive lower and upper bounds on the value of privacy that are asymptotically tight as the number of data subjects becomes large. Specifically, the lower bound assures that it is impossible to use a lower payment to buy ϵ units of privacy, and the upper bound is given by an achievable payment mechanism that we design. Based on these fundamental limits, we further derive lower and upper bounds on the minimum total payment for the data collector to achieve a given accuracy target for learning the underlying state and show that the total payment of the designed mechanism is at most one individual’s payment away from the minimum.|10.1145/3232863|https://doi.org/10.1145/3232863|New York, NY, USA|Association for Computing Machinery||2018|The Value of Privacy: Strategic Data Subjects, Incentive Mechanisms, and Fundamental Limits|Wang, Weina and Ying, Lei and Zhang, Junshan|article|10.1145/3232863|8|aug|ACM Trans. Econ. Comput.|21678375|2|6|May 2018||||21100853526|0,519|Q2|18|24|55|902|196|51|4,33|37,58|United States|Northern America|2013-2020|Computational Mathematics (Q2); Computer Science (miscellaneous) (Q2); Economics and Econometrics (Q2); Marketing (Q2); Statistics and Probability (Q3)||||755153721|678292104
https://doi.org/10.1016/j.adro.2022.100925|https://www.sciencedirect.com/science/article/pii/S245210942200032X||||2022|Head and Neck Radiotherapy Patterns of Practice Variability Identified as a Challenge to Real-World Big Data: results from the Learning from Analysis of Multicentre Big Data Aggregation (LAMBDA) Consortium|Amanda Caissie and Michelle Mierzwa and Clifton David Fuller and Murali Rajaraman and Alex Lin and Andrew MacDonald and Richard Popple and Ying Xiao and Lisanne VanDijk and Peter Balter and Helen Fong and Heping Xu and Matthew Kovoor and Joonsang Lee and Arvind Rao and Mary Martel and Reid Thompson and Brandon Merz and John Yao and Charles Mayo|article|CAISSIE2022100925|||Advances in Radiation Oncology|24521094||||||||||||||||||||||||||||||||755965515|42
SIGIR '18|Ann Arbor, MI, USA|information retrieval, natural language processing, text mining, digital libraries, information extraction, bibliometrics, citation analysis|4|1415–1418|The 41st International ACM SIGIR Conference on Research &amp; Development in Information Retrieval|The large scale of scholarly publications poses a challenge for scholars in information seeking and sensemaking. Information retrieval~(IR), bibliometric and natural language processing (NLP) techniques could enhance scholarly search, retrieval and user experience but are not yet widely used. To this purpose, we propose the third iteration of the Joint Workshop on Bibliometric-enhanced Information Retrieval and Natural Language Processing for Digital Libraries (BIRNDL). The workshop is intended to stimulate IR, NLP researchers and Digital Library professionals to elaborate on new approaches in natural language processing, information retrieval, scientometrics, text mining and recommendation techniques that can advance the state-of-the-art in scholarly document understanding, analysis, and retrieval at scale. The BIRNDL workshop will incorporate multiple invited talks, paper sessions, a poster session and the 4th edition of the Computational Linguistics (CL) Scientific Summarization Shared Task.|10.1145/3209978.3210194|https://doi.org/10.1145/3209978.3210194|New York, NY, USA|Association for Computing Machinery|9781450356572|2018|Joint Workshop on Bibliometric-Enhanced Information Retrieval and Natural Language Processing for Digital Libraries (BIRNDL 2018)|Kumar Chandrasekaran, Muthu and Jaidka, Kokil and Mayr, Philipp|inproceedings|10.1145/3209978.3210194|||||||||||||||||||||||||||||756688607|42
PRAI '19|Wenzhou, China|Proximity data averaging, Chebyshev theory, Power monitoring, Data cleaning|5|7–11|Proceedings of the 2019 the International Conference on Pattern Recognition and Artificial Intelligence|Stable and high-quality electric energy is the main driving force for the development of social science, technology, and the national economic leap. The assessment and monitoring of electrical safety rely on the generation, collection and statistics of large amounts of data by the power system. For the possible problems and impurities in these data, this paper uses the 'local Chebyshev theorem' and the 'near data averaging method' for the attribute values. The error is cleaned, and the 'sorting neighbor algorithm' is used to clean the duplicate data, thereby improving the data quality and realizing the accuracy of the safety monitoring of the power grid of the smart grid.|10.1145/3357777.3357781|https://doi.org/10.1145/3357777.3357781|New York, NY, USA|Association for Computing Machinery|9781450372312|2019|Safety Monitoring of Power Industrial Control Terminals Based on Data Cleaning|Lv, Zhining and Hu, Ziheng and Ning, Baifeng and Li, Wei and Yan, Gangfeng and Ding, Lifu and Shi, Xiasheng and Guo, Ningxuan|inproceedings|10.1145/3357777.3357781|||||||||||||||||||||||||||||759272374|42
dg.o '17|Staten Island, NY, USA|open data organization, organizational capabilities, competitive strategies, Competitiveness in open data businesses, open data capabilities, competitive advantage|10|250–259|Proceedings of the 18th Annual International Conference on Digital Government Research|Open data-driven organizations compete in a complex and uncertain environment with growing global competition, changing and emerging demand and market, and increasing levels of analytical tools and technology. For these organizations to exploit open data for competitive advantage, they need to develop the requisite competitive capabilities. This article presents an open data competitive capability framework grounded in theory and practice of open data. Based on extant literature and insights from domain experts, we identify and describe four dimensions of competitive capabilities required for open data driven organizations. We argue that by implementing the proposed framework, organizations can increase their chances to favorably compete in their respective markets. We further argue that by understanding open government data as a strategic resource for enterprises, government as producers or suppliers of this resource become key partners to data-driven organizations.|10.1145/3085228.3085280|https://doi.org/10.1145/3085228.3085280|New York, NY, USA|Association for Computing Machinery|9781450353175|2017|Competitive Capability Framework for Open Government Data Organizations|Zeleti, Fatemeh Ahmadi and Ojo, Adegboyega|inproceedings|10.1145/3085228.3085280|||||||||||||||||||||||||||||761656552|42
||Big data workflows, Internet of Things, Domain-specific languages, Software containers||100440||Big Data processing, especially with the increasing proliferation of Internet of Things (IoT) technologies and convergence of IoT, edge and cloud computing technologies, involves handling massive and complex data sets on heterogeneous resources and incorporating different tools, frameworks, and processes to help organizations make sense of their data collected from various sources. This set of operations, referred to as Big Data workflows, requires taking advantage of Cloud infrastructures’ elasticity for scalability. In this article, we present the design and prototype implementation of a Big Data workflow approach based on the use of software container technologies, message-oriented middleware (MOM), and a domain-specific language (DSL) to enable highly scalable workflow execution and abstract workflow definition. We demonstrate our system in a use case and a set of experiments that show the practical applicability of the proposed approach for the specification and scalable execution of Big Data workflows. Furthermore, we compare our proposed approach’s scalability with that of Argo Workflows – one of the most prominent tools in the area of Big Data workflows – and provide a qualitative evaluation of the proposed DSL and overall approach with respect to the existing literature.|https://doi.org/10.1016/j.iot.2021.100440|https://www.sciencedirect.com/science/article/pii/S2542660521000834||||2021|Conceptualization and scalable execution of big data workflows using domain-specific languages and software containers|Nikolay Nikolov and Yared Dejene Dessalk and Akif Quddus Khan and Ahmet Soylu and Mihhail Matskin and Amir H. Payberah and Dumitru Roman|article|NIKOLOV2021100440|||Internet of Things|25426605||16|||||||||||||||||||||||763524356|2077936548
||Big data, Drug discovery, Clinical trials, Precision medicine, Biomarkers||17-23||Oncology is undergoing a data-driven metamorphosis. Armed with new and ever more efficient molecular and information technologies, we have entered an era where data is helping us spearhead the fight against cancer. This technology driven data explosion, often referred to as “big data”, is not only expediting biomedical discovery, but it is also rapidly transforming the practice of oncology into an information science. This evolution is critical, as results to-date have revealed the immense complexity and genetic heterogeneity of patients and their tumors, a sobering reminder of the challenge facing every patient and their oncologist. This can only be addressed through development of clinico-molecular data analytics that provide a deeper understanding of the mechanisms controlling the biological and clinical response to available therapeutic options. Beyond the exciting implications for improved patient care, such advancements in predictive and evidence-based analytics stand to profoundly affect the processes of cancer drug discovery and associated clinical trials.|https://doi.org/10.1016/j.ygyno.2016.02.022|https://www.sciencedirect.com/science/article/pii/S0090825816300464||||2016|Use of “big data” in drug discovery and clinical trials|Guillaume Taglang and David B. Jackson|article|TAGLANG201617|||Gynecologic Oncology|00908258|1|141|||||27467|2,105|Q1|162|480|1150|15477|5359|1087|4,41|32,24|United States|Northern America|1972-2020|Obstetrics and Gynecology (Q1); Oncology (Q1)|29,012|5.482|0.02767|766664036|586725785
||Special issues and sections;Artificial intelligence;Big Data;Quality of service;Automation;5G mobile communication||1291-1295||The papers in this special section examine artificial intelligence (AI)-driven developments in 5G mobile communications for industrial automation applications from a Big Data perspective. With the recent advances in information and communication technologies, industrial automation is expanding at a rapid pace. This transition is characterized by “Industry 4.0”, the fourth revolution in the field of manufacturing. Industry 4.0, also called as “Industrial Internet of Things (IIoT)” or “Smart Factories”, is a reflection of new industrial revolution that is not only interconnected, but also communicate, analyze, and use the information to create a more holistic and better connected ecosystem for the industries.|10.1109/TII.2019.2955963|||||2020|Guest Editorial Special Section on AI-Driven Developments in 5G-Envisioned Industrial Automation: Big Data Perspective|Garg, S. and Guizani, M. and Guo, S. and Verikoukis, C.|article|8913513||Feb|IEEE Transactions on Industrial Informatics|19410050|2|16|||||||||||||||||||||||767075589|75062013
||Big data application, Data management, Data-driven architecture, Governance, Machine learning, Master data, Metadata||157-174|Building Big Data Applications|Building the big data application is very interesting and can provide multiple users with multiple perspectives, data discovery to end state analytics and beyond is very much what everybody wants to achieve. Enterprises are ready to spend millions of dollars to get a share of your wallet, they want to be a part of your life and be present at every event that gets your attention. They want to leverage their partnerships and influence you, how do they make this all happen? The most successful companies will tell you their story is built on governance. The aspect of governance is very critical to the success of this journey whether internal or external. What governance are we talking about? How do we implement the same? This chapter will focus on those aspects.|https://doi.org/10.1016/B978-0-12-815746-6.00009-0|https://www.sciencedirect.com/science/article/pii/B9780128157466000090||Academic Press|978-0-12-815746-6|2020|9 - Governance|Krish Krishnan|incollection|KRISHNAN2020157||||||||||Krish Krishnan|||||||||||||||||||768450922|42
||Data integrity;Conferences;Supervised learning;Training data;Machine learning;Big Data;Data models;counterfactual explanations;data quality;data science;supervised learning||1102-1108|2021 IEEE International Conference on Big Data (Big Data)|Although data quality is of paramount importance in algorithmic decision–making, most existing methods for supervised classification use training data without ever questioning their fidelity. At the same time, counterfactual explanation approaches widely used for post–hoc explanation of algorithmic decisions may result in unrealistic recommendations when left unconstrained. This work highlights a significant research problem, and introduces a novel framework to improve supervised classification in the presence of untrustworthy data, while offering actionable suggestions when an undesirable decision has been made (e.g., loan application rejection). Evaluation results spanning datasets from different domains demonstrate the superiority of the proposed approach, and its comparative advantage as the percentage of mislabeled instances increases.|10.1109/BigData52589.2021.9671677|||||2021|Improving Algorithmic Decision–Making in the Presence of Untrustworthy Training Data|Qi, Wenting and Chelmis, Charalampos|inproceedings|9671677||Dec|||||||||||||||||||||||||||769281454|42
||Databases;Big Data;Measurement;Cleaning;Data mining||163-168|2017 International Conference on Mechatronics, Electronics and Automotive Engineering (ICMEAE)|Big Data environment is a computing area with a great growth. Today it is common that we hear about databases with huge volumes of information and also we hear about Data Mining and Business Intelligence projects related with these huge databases. However, in general, little attention has been given to the quality of the data. Here we propose and present innovative metrics and schema designed to perform a basic task related to the Data Quality issue, this is, the diagnostic. The preliminary results that we obtained when we apply our approaches to Big Data encourage us to continue this work.|10.1109/ICMEAE.2017.29|||||2017|Effective Data Quality Diagnostic Schema for Big Data|Mejía-Lavalle, Manuel and Meusel, Winfrid and Tavira, Jonathan Villanueva and Cruz, Mirian Calderón|inproceedings|8241335||Nov|||||||||||||||||||||||||||770764299|42
||Training;Analytical models;Data preprocessing;Linear regression;Predictive models;Feature extraction;Cleaning;second-hand car price prediction;weighted regression model;LightGBM;XGBoost;random forest||90-95|2022 7th International Conference on Big Data Analytics (ICBDA)|"\"With the development of motor vehicles, the circulation demand of motor vehicles in the form of \"\"second-hand cars\"\" in circulation links is increasing. As a special \"\"e-commerce commodity\"\", second-hand cars are more complicated than ordinary e-commerce commodities. As a result, it is difficult to estimate the price of second-hand cars, which is not only influenced by the basic configuration of the car, but also by the car conditions. At present, the state has not issued a standard to judge the value of second-hand car. To solve this problem, in this paper, first making feature engineering, which includes data preprocessing and feature screening. Data preprocessing includes data cleaning and data transformation, data cleaning includes removing outliers and filling missing values, and data transformation is used to unify data format to improve data quality. The feature screening includes correlation analysis and feature extraction based on LightMBG, and the screened features provide the basis for model building, training and prediction. Then, five regression models are constructed by using the feature attributes obtained by the feature engineering for training, and evaluated. Then, Random Forest and XGBoost are weighted and mixed to got a novel regression model, and the effect of the model is better than that of the five regression models. Finally, the novel regression model is used to predict the price of second-hand cars.\""|10.1109/ICBDA55095.2022.9760371|||||2022|Second-hand Car Price Prediction Based on a Mixed-Weighted Regression Model|Han, Shengqiang and Qu, Jianhua and Song, Jinyi and Liu, Zijing|inproceedings|9760371||March|||||||||||||||||||||||||||771865899|42
||Big data, data quality, outlier, Sustainable Development Goals||685-692||Recently, Big Data analytics has been one of the most emerging topics in the business field. Data is collected, processed and analyzed to gain useful insight for their organization. Big Data analytics has the potential to improve the quality of life and help to achieve Sustainable Development Goals (SDG). To ensure that SDG goals are achieved, we must utilize existing data to meet those targets and ensure accountability. However, data quality is often left out when dealing with data. Any types of errors presented in the dataset should be properly addressed to ensure the analysis provided is accurate and truthful. In this paper, we have addressed the concept of data quality diagnosis to identify the outlier presented in the dataset. The cause of the outlier is further discussed to identify potential improvements that can be done to the dataset. In addition, recommendations to improve the quality of data and data collection systems are provided.|https://doi.org/10.1016/j.procs.2021.12.189|https://www.sciencedirect.com/science/article/pii/S1877050921024133||||2022|Diagnostic analysis for outlier detection in big data analytics|Fakhitah Ridzuan and Wan Mohd Nazmee {Wan Zainon}|article|RIDZUAN2022685|||Procedia Computer Science|18770509||197||Sixth Information Systems International Conference (ISICO 2021)|||19700182801|0,334|-|76|1907|6483|38511|13722|6359|2,09|20,19|Netherlands|Western Europe|2010-2020|Computer Science (miscellaneous)||||776319788|2108686752
CSAE 2019|Sanya, China|Data analysis, Data management, Crop germplasm resources, Big data architecture|7||Proceedings of the 3rd International Conference on Computer Science and Application Engineering|Based on understanding the application of big data and the research status of crop germplasm resources, this paper proposes a system architecture that is suitable for crop germplasm resources big data. Among them, the overall architecture of germplasm resources is elaborated through six functional modules, including data source, data integration, data processing, data application, big data operation and maintenance platform, and data management and security. The logical functional architecture specification was formulated and the technical implementation and selection are defined. The technical implementation framework describes the technical implementation of germplasm resources big data, and jointly supports the construction and operation of germplasm resources big data. Finally, a verification system is established to verify the feasibility of the big data system framework for germplasm resources.|10.1145/3331453.3361308|https://doi.org/10.1145/3331453.3361308|New York, NY, USA|Association for Computing Machinery|9781450362948|2019|Construction and Implementation of Big Data Framework for Crop Germplasm Resources|Jing, Furong and Cao, Yongsheng and Fang, Wei and Chen, Yanqing|inproceedings|10.1145/3331453.3361308|27||||||||||||||||||||||||||||776741900|42
||Personal data, Privacy trust, Questionnaires, Interview, Big data||782-792||Privacy trust directly affects the personal willingness to share data and thus influences the quality and size of the data, thus affecting the development of big data technology and industry. As China is probably the largest personal data pool and vastest application market of big data, the situation of Chinese privacy trust plays a significant role. Based on the 17 most common data collection scenarios, the following aspects have been observed through 508 questionnaires and interviews of 20 samples. To start with, there is a severe privacy trust crisis in China, both in the field of enterprise services such as online shopping and social networks, etc. and in some public services like medical care and education, etc. Besides, there are also doubts about data collected by the government since individuals refuse to offer personal information or give false information as much as possible. Some people even buy two phone numbers, one is in use, while the other is not carried around or used by them, which is only bought to be offered to data collectors. Secondly, in terms of gender, females have lower trust in enterprises and social associations than males, especially in the fields of social networks and personal consumption. However, there is no obvious difference in fields of government and public services. Females possess stronger awareness but less skilled in precautions than males. Thirdly, people between the ages of 18 and 50 are more suspicious of data collected by enterprises, while age exerts little obvious influence on the credibility of data collected by the government, social associations and public services. Older people are less aware of precautions than people at other ages. In addition, from the perspective of education background, people with higher degrees possess stronger awareness of precautions and thus lower degree of trust. Therefore, it is suggested that more education on privacy consciousness should be given, and relative laws as well as regulations need improving. Besides, innovation in privacy protection technologies should be encouraged. What is more, we need to reinforce the management of the internet industry and strictly regulate personal data collection of the government.|https://doi.org/10.1016/j.clsr.2015.08.006|https://www.sciencedirect.com/science/article/pii/S0267364915001296||||2015|Privacy trust crisis of personal data in China in the era of Big Data: The survey and countermeasures|Zhong Wang and Qian Yu|article|WANG2015782|||Computer Law & Security Review|02673649|6|31|||||28888|0,815|Q1|38|66|242|1245|707|223|3,39|18,86|United Kingdom|Western Europe|1985-2020|Business, Management and Accounting (miscellaneous) (Q1); Computer Networks and Communications (Q1); Law (Q1)||||777014265|1769124433
||Fashion business operations, Supply chain centralization, Emerging markets, Sustainable operations, Social welfare||139-152||In emerging markets, there are data quality problems. In this paper, we establish theoretical models to explore how data quality problems affect sustainable fashion supply chain operations. We start with the decentralized supply chain and find that poor data quality lowers supply chain profit and social welfare. We consider the implementation of blockchain to help and identify the situation in which blockchain helps enhance social welfare but brings harm to supply chain profitability. We propose a government sponsor scheme as well as an environment taxation waiving scheme to help. We further extend the study to the centralized supply chain setting.|https://doi.org/10.1016/j.tre.2019.09.019|https://www.sciencedirect.com/science/article/pii/S1366554519311494||||2019|Data quality challenges for sustainable fashion supply chain operations in emerging markets: Roles of blockchain, government sponsors and environment taxes|Tsan-Ming Choi and Suyuan Luo|article|CHOI2019139|||Transportation Research Part E: Logistics and Transportation Review|13665545||131|||||20909|2,042|Q1|110|233|507|13126|4018|499|7,63|56,33|United Kingdom|Western Europe|1997-2020|Business and International Management (Q1); Civil and Structural Engineering (Q1); Management Science and Operations Research (Q1); Transportation (Q1)||||780242786|1729083653
WSC '21|Phoenix, Arizona||12||Proceedings of the Winter Simulation Conference|Gaining knowledge from a given data basis is a complex challenge. One of the frequently used methods in the context of a supply chain (SC) is knowledge discovery in databases (KDD). For a purposeful and successful knowledge discovery, valid and preprocessed input data are necessary. Besides preprocessing collected observational data, simulation can be used to generate a data basis as an input for the knowledge discovery process. The process of using a simulation model as a data generator is called data farming. This paper investigates the link between data farming and data mining. We developed a Farming-for-Mining-Framework, where we highlight requirements of knowledge discovery techniques and derive how the simulation model for data generation can be configured accordingly, e.g., to meet the required data accuracy. We suggest that this is a promising approach and is worth further research attention.||||IEEE Press||2022|A Farming-for-Mining-Framework to Gain Knowledge in Supply Chains|Hunker, Joachim and Wuttke, Alexander and Scheidler, Anne Antonia and Rabe, Markus|inproceedings|10.5555/3522802.3522903|130||||||||||||||||||||||||||||780455549|42
||Literature review, Big data, Big data analytics, Supply chain management, Research directions||254-264||The rapidly growing interest from both academics and practitioners in the application of big data analytics (BDA) in supply chain management (SCM) has urged the need for review of up-to-date research development in order to develop a new agenda. This review responds to the call by proposing a novel classification framework that provides a full picture of current literature on where and how BDA has been applied within the SCM context. The classification framework is structurally based on the content analysis method of Mayring (2008), addressing four research questions: (1) in what areas of SCM is BDA being applied? (2) At what level of analytics is BDA used in these SCM areas? (3) What types of BDA models are used in SCM? (4) What BDA techniques are employed to develop these models? The discussion tackling these four questions reveals a number of research gaps, which leads to future research directions.|https://doi.org/10.1016/j.cor.2017.07.004|https://www.sciencedirect.com/science/article/pii/S0305054817301685||||2018|Big data analytics in supply chain management: A state-of-the-art literature review|Truong Nguyen and Li ZHOU and Virginia Spiegler and Petros Ieromonachou and Yong Lin|article|NGUYEN2018254|||Computers & Operations Research|03050548||98|||||24355|1,506|Q1|152|210|755|9383|3837|746|4,85|44,68|United Kingdom|Western Europe|1974-2021|Computer Science (miscellaneous) (Q1); Management Science and Operations Research (Q1); Modeling and Simulation (Q1)||||782739050|1185516425
||big data technology, scientific research data, data analysis, decision||556-561||Scientific research data has an important strategic position for the development of enterprises and countries, and is an important basis for management to conduct strategic research and decision-making. Compared with the Internet industry, big data technology started late in the military enterprises, while military enterprises research data often has the characteristics of decentralization, low relevance, and diverse data types. It cannot fully utilize the advantages of data resources to enhance the core competitiveness of enterprises. To this end, this paper deeply explores the application methods of big data technology in military scientific research data management, and lays a foundation for the construction of scientific research big data platform.|https://doi.org/10.1016/j.procs.2019.01.221|https://www.sciencedirect.com/science/article/pii/S1877050919302406||||2019|Application of Big Data Technology in Scientific Research Data Management of Military Enterprises|Wang Kun and Liu Tong and Xie Xiaodan|article|KUN2019556|||Procedia Computer Science|18770509||147||2018 International Conference on Identification, Information and Knowledge in the Internet of Things|||19700182801|0,334|-|76|1907|6483|38511|13722|6359|2,09|20,19|Netherlands|Western Europe|2010-2020|Computer Science (miscellaneous)||||783568878|2108686752
||Data analytics, data science, data mining, clustering, classification, model building||31-67|Data Analytics for Intelligent Transportation Systems|This chapter provides a comprehensive and unified view of data analytics fundamentals. Four functional facets of data analytics—descriptive, diagnostic, predictive, and prescriptive—are described. The evolution of data analytics from SQL analytics, business analytics, visual analytics, big data analytics, to cognitive analytics is presented. Data science as the foundational discipline for the current generation of data analytics systems is explored in this chapter. Data lifecycle and data quality issues are outlined. Open source tools and resources for developing data analytics systems are listed. Future directions in data analytics are indicated. The chapter concludes by providing a summary. To reinforce and enhance the reader’s data analytics knowledge and tools, questions and exercise problems are provided at the end of the chapter.|https://doi.org/10.1016/B978-0-12-809715-1.00002-X|https://www.sciencedirect.com/science/article/pii/B978012809715100002X||Elsevier|978-0-12-809715-1|2017|Chapter 2 - Data Analytics: Fundamentals|Venkat N. Gudivada|incollection|GUDIVADA201731||||||||||Mashrur Chowdhury and Amy Apon and Kakan Dey|||||||||||||||||||783743477|42
||Laser radar;Three-dimensional displays;Solid modeling;System analysis and design;Laser modes;Laser beams;Autonomous vehicles;Evaluation indicator;lower limit;multi-beam light detection and ranging (LiDAR);perception assessment model;system design||1-15||Light detection and ranging (LiDAR) provides a 3-D understanding of environment and plays an important role in autonomous driving. To study the influence of 3-D data quality on the environment perception and provide a theoretical basis for optimizing system design, a multi-beam LiDAR perception assessment model has been established to reveal the relationship between data quality and multi-parameters, including system and motion parameters. A novel ground segmentation algorithm was proposed with a combination of the grid elevation and the neighbor relationship, which was used to validate how the data quality influences the results of environment perception. By the way of down-sampling based on the Karlsruhe Institute of Technology and Toyota Technological Institute (KITTI) dataset, the experimental results showed that the proposed ground segmentation with combination of grid-elevation and neighbor-relationship (GSCGN) method was superior than other general ground segmentation methods in terms of accuracy and efficiency. It should be noted that the mean vertical angular resolution (MVAR), laser repetition frequency, and beam numbers were the dominant influencing parameters on the point density and the accuracy of ground segmentation. Based on the experimental results, the lower limits of system parameters were determined as 16-beam and 4-kHz repetition frequency, with the acceptable recall of 92.2% for ground and 93.5% for object, the accuracy of 92.9% and the runtime of 0.036 s, which can not only provide a reliable environment perception effect, but also reducing the computational burden to satisfy the real-time autonomous driving. This study offers a meaningful investigation to guide LiDAR system design with balancing the contradiction between the optimized system design and the high-degree environment perception.|10.1109/TIM.2021.3094230|||||2021|Study of a Multi-Beam LiDAR Perception Assessment Model for Real-Time Autonomous Driving|Li, Xiaolu and Zhou, Yier and Hua, Baocheng|article|9475591|||IEEE Transactions on Instrumentation and Measurement|15579662||70|||||||||||||||||||||||785167088|845056910
||Enterprise architecture, Self-service data, Systems of insight, Data-driven security, Business-driven governance, Trust and confidence, Hybrid-cloud, Information supply chains||33-48|Software Architecture for Big Data and the Cloud|Big data and analytics, particularly when combined with the use of cloud-based deployments, can transform the operation of an organization – increasing innovation, improving time to value and decision-making. However, an organization only derives value from data and analytics when (1) the collection of big data is organized, systematic and automated and (2) the use of data and analytic insight is embedded in the organization's day-to-day operation. Often the ambition of a big data and analytics solution requires data to flow freely across an organization. This can be in direct conflict with the organization's political and process silos that exist to partition the work of the organization into manageable chunks of function and responsibility. Thus the architecture of a big data solution must accommodate the realities within the organization to ensure sufficient value is realized by all of the stakeholders that are needed to enable this data interchange. Through examples of architectures for big data and analytics solutions, we explain how the scope of a big data solution can affect its architecture and the additional components necessary when a big data solution needs to span multiple organization silos.|https://doi.org/10.1016/B978-0-12-805467-3.00003-X|https://www.sciencedirect.com/science/article/pii/B978012805467300003X|Boston|Morgan Kaufmann|978-0-12-805467-3|2017|Chapter 3 - Architecting to Deliver Value From a Big Data and Hybrid Cloud Architecture|Mandy Chessell and Dan Wolfson and Tim Vincent|incollection|CHESSELL201733||||||||||Ivan Mistrik and Rami Bahsoon and Nour Ali and Maritta Heisel and Bruce Maxim|||||||||||||||||||785395120|42
EGOSE '14|St. Petersburg, Russian Federation|Shared Environment, Open Government, Open Data, Linked Data, Data Standardization, Core Component|7|88–94|Proceedings of the 2014 Conference on Electronic Governance and Open Society: Challenges in Eurasia|Open government as a new system of public administration requires a qualitatively new level of information support for digital interactions of agencies, as well as between government and citizens, experts, and businesses. This article examines three categories of government counterparties, interactions with which are based on different principles: community of interest; subject areas; a loosely coupled environment. The public sector has now many information projects, and most of them deal with the data exchange, data delivery from and to the environment. The understanding of an environment is various for different projects. The categorization of projects depends on nature and the level of intellectual interaction with the external environment needed for the correct definition of project objectives, assessment of effectiveness. With the growing number of users and the transition to an open world, semantic principles are becoming more significant. Given the shift from systems integration to the semantic method, the role of subject matter expert is growing substantially.|10.1145/2729104.2729129|https://doi.org/10.1145/2729104.2729129|New York, NY, USA|Association for Computing Machinery|9781450334013|2014|Three Types of Data Exchange in the Open Government Information Projects|Lipuntsov, Yuri P.|inproceedings|10.1145/2729104.2729129|||||||||||||||||||||||||||||785470146|42
||Electrical engineering;Analytical models;Data integrity;Asia;Analytic hierarchy process;Big Data applications;Power grids;data asset value;electricity characteristics;energy characteristics;power grid enterprise;evaluation model||1462-1466|2022 7th Asia Conference on Power and Electrical Engineering (ACPEE)|Based on the energy characteristics, this paper constructs a data asset value evaluation model of power grid enterprises, in order to explores the operation and management of power system from a new perspective. Firstly, the influencing factors of data asset value of power grid enterprises are analyzed from the dimensions of data scale, data quality, data management, data application and data risk. Secondly, the evaluation index system of data asset value of power grid enterprises is analyzed based on the above perspectives, and the weighting evaluation is carried out by Delphi method and analytic hierarchy process. The model constructed in this paper provides a theoretical reference for the evaluation practice of data asset value of power grid enterprises in the future.|10.1109/ACPEE53904.2022.9783826|||||2022|A new perspective of power system operation and management -- Research on power/energy data asset value evaluation model for power grid enterprises|Bai, Junfeng and Li, Xiang and Wang, Weibin and Qu, Haini|inproceedings|9783826||April|||||||||||||||||||||||||||788657476|42
CHI EA '22|New Orleans, LA, USA|Data Work, Labor, Data-Driven, Datafication, Occupations|6||Extended Abstracts of the 2022 CHI Conference on Human Factors in Computing Systems|In the wake of the hype around big data, artificial intelligence, and “data-drivenness,” much attention has been paid to developing novel tools to capitalize upon the deluge of data being recorded and gathered automatically through IT systems. While much of this literature tends to overlook the data itself—sometimes even characterizing it as “data exhaust” that is readily available to be fed into algorithms, which will unlock the insights held within it—a growing body of literature has recently been directed at the (often intensive and skillful) work that goes into creating, collecting, managing, curating, analyzing, interpreting, and communicating data. These investigations detail the practices and processes involved in making data useful and meaningful so that aims of becoming ‘data-driven’ or ‘data-informed’ can become real. Further, In some cases, increased demands for data work have led to the formation of new occupations, whereas at other times data work has been added to the task portfolios of existing occupations and professions, occasionally affecting their core identity. Thus, the evolving forms of data work are requiring individual and organizational resources, new and re-tooled practices and tools, development of new competences and skills, and creation of new functions and roles. While differences exist across the global North and the global South experience of data work, such factors of data production remain paramount even as they exist largely for the benefit of the data-driven system [21, 32]. This one-day workshop will investigate existing and emerging tasks of data work. Further, participants will seek to understand data work as it impacts: individual data workers; occupations tasked with data work (existing and emerging); organizations (e.g. changing their skill-mix and infrastructuring to support data work); and teaching institutions (grappling with incorporation of data work into educational programs). Participants are required to submit a position paper or a case study drawn from their research to be reviewed and accepted by the organizing committee (submissions should be up to four pages in length). Upon acceptance, participants will read each other's paper, prepare to shortly present and respond to comments by two discussants and other participants. Subsequently, the workshop will focus on developing a set of core processes and tasks as well as an outline of a research agenda for a CHI-perspective on data work in the coming years.|10.1145/3491101.3503724|https://doi.org/10.1145/3491101.3503724|New York, NY, USA|Association for Computing Machinery|9781450391566|2022|Investigating Data Work Across Domains: New Perspectives on the Work of Creating Data|Pine, Kathleen and Bossen, Claus and Holten M\o{}ller, Naja and Miceli, Milagros and Lu, Alex Jiahong and Chen, Yunan and Horgan, Leah and Su, Zhaoyuan and Neff, Gina and Mazmanian, Melissa|inproceedings|10.1145/3491101.3503724|87||||||||||||||||||||||||||||792898996|42
||Big Data;Security;Data integrity;Data models;Organizations;Decision making;Reliability;Big Data Value Chain;Data Management;Data Quality;Data Security;Process Integration;Orchestration||1-8|2020 IEEE 2nd International Conference on Electronics, Control, Optimization and Computer Science (ICECOCS)|Big Data has grown significantly in recent years. This growth has led organizations to adopt Big Data Value Chains (BDVC) as the appropriate framework for unlocking the value to make suitable decisions. Despite its promising opportunities, Big Data raises new concerns such as data quality and security that could radically impact the effectiveness of the BDVC. These two essential aspects have become an urgent need for any Big Data project to provide meaningful datasets and reliable insights. In this contribution, we highlight the importance of considering data quality and security requirements. Then, we propose a coherent, unified framework that extends BDVC with security and quality aspects. Through quality and security reports, the model can self-evaluate and arrange tasks according to orchestration and monitoring process, allowing the BDVC to evolve at the organization pace and to align strategically with its objectives as well as to federate a sustainable ecosystem.|10.1109/ICECOCS50124.2020.9314391|||||2020|Big Data Value Chain: A Unified Approach for Integrated Data Quality and Security|Faroukhi, Abou Zakaria and El Alaoui, Imane and Gahi, Youssef and Amine, Aouatif|inproceedings|9314391||Dec|||||||||||||||||||||||||||794837449|42
COMPASS '20|Ecuador|public health, supply chain, Data collection, culture of data use, data use behavior, low-resource environment|11|65–75|Proceedings of the 3rd ACM SIGCAS Conference on Computing and Sustainable Societies|Public services, such as public health supply chains, in low- and middle-income countries can be characterized as low-resource environments, where both infrastructure and human capacity are limited. There is no strong culture of data recording or use, with ad hoc reporting practices, poor planning and lack of coordination. All these lead to poor supply chain performance, thereby restricting access to medicines, and eventually resulting in poorer health and mortality.We describe the ground-up design of Logistimo SCM, a supply chain management software, offered as a service, that has enabled a transformative change in public health supply chains, leading to improved performance. Our approach is rooted in bottom-up empowerment of the human value chain, based on the principle that higher self-efficacy amongst health workers and managers can lead to sustained changes in data recording and use behaviors. This is achieved through a service that optimizes data collection effort, maximizes supervisory bandwidth, promotes proactive and collaborative operations, and enables frictionless performance recognition. We describe the guiding principles of inclusive software service design and four mechanisms that enable the appropriate conditions for stimulating a behavior of data recording and use. We demonstrate their effectiveness in achieving good supply chain performance through case studies in India and Africa. The principles and methods discussed here are generic and can be applied to any low-resource environment.|10.1145/3378393.3402248|https://doi.org/10.1145/3378393.3402248|New York, NY, USA|Association for Computing Machinery|9781450371292|2020|Enabling Sustainable Behaviors of Data Recording and Use in Low-Resource Supply Chains|Ramanujapuram, Arun and Malemarpuram, Charan Kumar|inproceedings|10.1145/3378393.3402248|||||||||||||||||||||||||||||800980814|42
||industrial control systems (ICSs), Artificial intelligence (AI), Internet of Things (IoT), blockchain, Industry 4.0, digital twins (DTs), cyber-physical systems (CPSs)|34|||Industrial processes rely on sensory data for decision-making processes, risk assessment, and performance evaluation. Extracting actionable insights from the collected data calls for an infrastructure that can ensure the dissemination of trustworthy data. For the physical data to be trustworthy, it needs to be cross validated through multiple sensor sources with overlapping fields of view. Cross-validated data can then be stored on the blockchain, to maintain its integrity and trustworthiness. Once trustworthy data is recorded on the blockchain, product lifecycle events can be fed into data-driven systems for process monitoring, diagnostics, and optimized control. In this regard, digital twins (DTs) can be leveraged to draw intelligent conclusions from data by identifying the faults and recommending precautionary measures ahead of critical events. Empowering DTs with blockchain in industrial use cases targets key challenges of disparate data repositories, untrustworthy data dissemination, and the need for predictive maintenance. In this survey, while highlighting the key benefits of using blockchain-based DTs, we present a comprehensive review of the state-of-the-art research results for blockchain-based DTs. Based on the current research trends, we discuss a trustworthy blockchain-based DTs framework. We also highlight the role of artificial intelligence in blockchain-based DTs. Furthermore, we discuss the current and future research and deployment challenges of blockchain-supported DTs that require further investigation.|10.1145/3517189|https://doi.org/10.1145/3517189|New York, NY, USA|Association for Computing Machinery||2022|Blockchain-Based Digital Twins: Research Trends, Issues, and Future Challenges|Suhail, Sabah and Hussain, Rasheed and Jurdak, Raja and Oracevic, Alma and Salah, Khaled and Hong, Choong Seon and Matulevi\v{c}ius, Raimundas|article|10.1145/3517189|240|sep|ACM Comput. Surv.|03600300|11s|54|January 2022||||23038|2,079|Q1|163|112|365|15859|6978|365|19,16|141,60|United States|Northern America|1969-2020|Computer Science (miscellaneous) (Q1); Theoretical Computer Science (Q1)|10,790|10.282|0.01438|801072795|1517405264
RACS '22|Virtual Event, Japan|privacy-preserving, knowledge distillation, data analysis|6|15–20|Proceedings of the Conference on Research in Adaptive and Convergent Systems|"\"Over the past decade, there has been an increasing focus on data privacy, which is one of the most popular areas of research today. Due to legal restrictions, it is difficult to centralize data, so multi-party collaborative learning becomes difficult to achieve. In this paper, we propose a data collaborative analysis method for protecting data privacy: Knowledge Distillation-based Analysis (KDDA). The proposed method can effectively solve the above problems. The framework of the proposed method is based on \"\"teacher-student\"\" network. Specifically, each institution (e.g., hospital) used sensitive data to train local models, which we call teacher models. Then we introduce unlabeled public datasets. We design an aggregator that aggregates all teacher models to generate pseudo-labels for the public dataset. It is worth noting that to minimize the risk of data privacy exposure, we limit the number of generated pseudo-labels. Finally, we train the student model using semi-supervised learning on the public dataset with pseudo-labels. Since the student model is not directly trained on sensitive data, it will not lead to the leakage of data privacy, so it can be publicized. To evaluate the effectiveness of the proposed method, we conduct experiments on three popular image benchmark datasets: MNIST, SVHN, and CIFAR-10. Experimental results show that the proposed method is effective.\""|10.1145/3538641.3561482|https://doi.org/10.1145/3538641.3561482|New York, NY, USA|Association for Computing Machinery|9781450393980|2022|Knowledge Distillation-Based Privacy-Preserving Data Analysis|Huang, Dong and Ye, Xiucai and Sakurai, Tetsuya|inproceedings|10.1145/3538641.3561482|||||||||||||||||||||||||||||801162927|42
HT '21|Virtual Event, USA|political history, information aggregation, linked data, hypertext, australian history|6|245–250|Proceedings of the 32nd ACM Conference on Hypertext and Social Media|In this paper we report on a complex and complete archive of historical primary sources that map the political landscape of the anglophone world in the mid-to late 1800s. The ruthless pragmatism applied to the construction of the initial Humanities dataset resulted in an analogue equivalent of a hypertext system, which has already resulted in published academic books and articles. Here, we describe the processes of a current project, which consists of the translation of this analogue information aggregation system into a graph database using Linked Data and semantic Web technologies.|10.1145/3465336.3475107|https://doi.org/10.1145/3465336.3475107|New York, NY, USA|Association for Computing Machinery|9781450385510|2021|Reductio Ad Absurdum?: From Analogue Hypertext to Digital Humanities|Nurmikko-Fuller, Terhi and Pickering, Paul|inproceedings|10.1145/3465336.3475107|||||||||||||||||||||||||||||802288556|42
dg.o '18|Delft, The Netherlands|e-government, assessment, evaluation, e-governance|10||Proceedings of the 19th Annual International Conference on Digital Government Research: Governance in the Data Age|This paper presents an exploratory study aimed at identifying, exploring, and analyzing current EGOV assessment initiatives. We do so based on data obtained from a desktop research and from a worldwide questionnaire directed to the 193 countries that are part of the list used by the Statistics Division of the United Nations Department of Economic and Social Affairs (UNDESA). The study analyses 12 EGOV assessment initiatives: a) seven of them are international/regional EGOV assessment initiatives performed by the United Nations (UN), European Union (EU), Waseda-IAC, Organisation for Economic Co-operation and Development (OECD), World Bank (WB), WWW Foundation, and Open Knowledge Network (OKN); b) five of them are country-level EGOV assessment initiatives performed by Norway, Germany, India, Saudi Arabia, and the United Arab Emirates. Further, the study provides general results obtained from a questionnaire with participation from 18 countries: Afghanistan, Angola, Brazil, Cabo Verde, Denmark, Estonia, Finland, Germany, Ghana, Ireland, Latvia, the Netherlands, Norway, Oman, Pakistan, the Philippines, Portugal, and Slovenia. The findings show that there is no shortage of interest in assessing EGOV initiatives. However, the supply side of EGOV initiatives is the dominant perspective being assessed, particularly by regional and international organizations. While there is an increasing interest in assessing the users' perspective (demand side) by individual countries, such attempts still seem to be at an early stage. Additionally, the actual use and impact of various EGOV services and activities are rarely well identified and measured. This study represents a stepping stone for developing instruments for assessing EGOV initiatives in future works. For the current stage, the study presents several general suggestions to be considered during the assessment process.|10.1145/3209281.3209309|https://doi.org/10.1145/3209281.3209309|New York, NY, USA|Association for Computing Machinery|9781450365260|2018|Analyzing E-Governance Assessment Initiatives: An Exploratory Study|Alarabiat, Ayman and Soares, Delfina and Ferreira, Luis and de S\'{a}-Soares, Filipe|inproceedings|10.1145/3209281.3209309|30||||||||||||||||||||||||||||803273605|42
||Data quality, Data quality assessment, Data quality metric, Data consistency||95-106||We present a probability-based metric for semantic consistency using a set of uncertain rules. As opposed to existing metrics for semantic consistency, our metric allows to consider rules that are expected to be fulfilled with specific probabilities. The resulting metric values represent the probability that the assessed dataset is free of internal contradictions with regard to the uncertain rules and thus have a clear interpretation. The theoretical basis for determining the metric values are statistical tests and the concept of the p-value, allowing the interpretation of the metric value as a probability. We demonstrate the practical applicability and effectiveness of the metric in a real-world setting by analyzing a customer dataset of an insurance company. Here, the metric was applied to identify semantic consistency problems in the data and to support decision-making, for instance, when offering individual products to customers.|https://doi.org/10.1016/j.dss.2018.03.011|https://www.sciencedirect.com/science/article/pii/S0167923618300599||||2018|Assessing data quality – A probability-based metric for semantic consistency|Bernd Heinrich and Mathias Klier and Alexander Schiller and Gerit Wagner|article|HEINRICH201895|||Decision Support Systems|01679236||110|||||21933|1,564|Q1|151|115|342|7152|2672|338|7,04|62,19|Netherlands|Western Europe|1985-2020|Arts and Humanities (miscellaneous) (Q1); Developmental and Educational Psychology (Q1); Information Systems (Q1); Information Systems and Management (Q1); Management Information Systems (Q1)|13,580|5.795|0.0081|804216494|1234879127
||Big data, Data warehouse, Google Earth, mHealth, Spatial data, Volunteered geographic information||15-29||During the last 30years it has become commonplace for epidemiological studies to collect locational attributes of disease data. Although this advancement was driven largely by the introduction of handheld global positioning systems (GPS), and more recently, smartphones and tablets with built-in GPS, the collection of georeferenced disease data has moved beyond the use of handheld GPS devices and there now exist numerous sources of crowdsourced georeferenced disease data such as that available from georeferencing of Google search queries or Twitter messages. In addition, cartography has moved beyond the realm of professionals to crowdsourced mapping projects that play a crucial role in disease control and surveillance of outbreaks such as the 2014 West Africa Ebola epidemic. This paper provides a comprehensive review of a range of innovative sources of spatial animal and human health data including data warehouses, mHealth, Google Earth, volunteered geographic information and mining of internet-based big data sources such as Google and Twitter. We discuss the advantages, limitations and applications of each, and highlight studies where they have been used effectively.|https://doi.org/10.1016/j.sste.2015.04.003|https://www.sciencedirect.com/science/article/pii/S1877584515000179||||2015|Sources of spatial animal and human health data: Casting the net wide to deal more effectively with increasingly complex disease problems|Kim B. Stevens and Dirk U. Pfeiffer|article|STEVENS201515|||Spatial and Spatio-temporal Epidemiology|18775845||13|||||19700167025|0,726|Q1|26|40|90|1567|169|88|1,63|39,18|United Kingdom|Western Europe|2009-2020|Geography, Planning and Development (Q1); Health, Toxicology and Mutagenesis (Q2); Infectious Diseases (Q2); Epidemiology (Q3)||||806633853|714062951
IDEAS '21|Montreal, QC, Canada|Veracity, Big data|9|157–165|Proceedings of the 25th International Database Engineering &amp; Applications Symposium|Big data systems are becoming mainstream for big data management either for batch processing or real-time processing. In order to extract insights from data, quality issues are very important to address, particularly. A veracity assessment model is consequently needed. In this paper, we propose a model which ties quality of datasets and quality of query resultsets. We particularly examine quality issues raised by a given dataset, order attributes along their fitness for use and correlate veracity metrics to business queries. We validate our work using the open dataset NYC taxi’ trips.|10.1145/3472163.3472195|https://doi.org/10.1145/3472163.3472195|New York, NY, USA|Association for Computing Machinery|9781450389914|2021|Customized Eager-Lazy Data Cleansing for Satisfactory Big Data Veracity|Sahri, Soror and Moussa, Rim|inproceedings|10.1145/3472163.3472195|||||||||||||||||||||||||||||807744468|42
WiPSCE '15|London, United Kingdom|Big Data, CS Education, Data Management, Secondary Schools, Educational Reconstruction|4|88–91|Proceedings of the Workshop in Primary and Secondary Computing Education|This paper describes the application of the research framework educational reconstruction for investigating the field data management under a CS education perspective. Like the many other innovations in CS, Big Data and the field data management have strong influences on students' daily lives. In contrast, school does not yet sufficiently prepare students to handle the arising challenges. In this paper we will describe how we apply an educational reconstruction approach to prepare the teaching of essential data management competencies. We will summarize the main goals and principles of educational reconstruction and discuss the application of the framework to the topic data management, as well as first outcomes. Just as educational reconstruction is suitable for finding the essential aspects for teaching data management and for designing classes/courses on this topic, it also seems promising for the curricular development of other CS innovations as well.|10.1145/2818314.2818330|https://doi.org/10.1145/2818314.2818330|New York, NY, USA|Association for Computing Machinery|9781450337533|2015|Bringing the Innovations in Data Management to CS Education: An Educational Reconstruction Approach|Grillenberger, Andreas and Romeike, Ralf|inproceedings|10.1145/2818314.2818330|||||||||||||||||||||||||||||808334515|42
|||14|176–189||Incentive mechanism design and quality control are two key challenges in data crowdsourcing, because of the need for recruitment of crowd users and their limited capabilities. Without considering users’ social influences, existing mechanisms often result in low efficiency in terms of the platform’s cost. In this paper, we exploit social influences among users as incentives to motivate users’ participation, in order to reduce the cost of recruiting users. Based on social influences, we design incentive mechanisms with the goal of achieving high quality of crowdsourced data and low cost of incentivizing users’ participation. Specifically, we consider three scenarios. In the full information scenario, we design task assignment and user recruitment mechanisms to optimize the data quality while reducing the incentive cost. In the partial information scenario, users’ qualities and costs are unknown. We exploit the correlation between tasks to overcome the information asymmetry, for both cases of opportunistic crowdsourcing and participatory crowdsourcing. Further, in the dynamic social influence scenario, we investigate the dynamics of users’ social influences and design extra rewards for users to make full use of the social influence and achieve maximum cost saving. We evaluate the incentive mechanisms using numerical results, which demonstrate their effectiveness.|10.1109/TNET.2021.3105427|https://doi.org/10.1109/TNET.2021.3105427||IEEE Press||2022|Quality-Aware Incentive Mechanisms Under Social Influences in Data Crowdsourcing|Shi, Zhiguo and Yang, Guang and Gong, Xiaowen and He, Shibo and Chen, Jiming|article|10.1109/TNET.2021.3105427||feb|IEEE/ACM Trans. Netw.|10636692|1|30|Feb. 2022||||27237|1,022|Q1|174|187|653|6994|3573|652|5,40|37,40|United States|Northern America|1993-2020|Computer Networks and Communications (Q1); Computer Science Applications (Q1); Electrical and Electronic Engineering (Q1); Software (Q1)||||809138344|1453185914
SIGMOD '18|Houston, TX, USA|numeric errors, incremental validation, graph dependencies|13|381–393|Proceedings of the 2018 International Conference on Management of Data|Numeric inconsistencies are common in real-life knowledge bases and social networks. To catch such errors, we propose to extend graph functional dependencies with linear arithmetic expressions and comparison predicates, referred to as NGDs. We study fundamental problems for NGDs. We show that their satisfiability, implication and validation problems are Σ 2 p-complete, ¶II2 p-complete and coNP-complete, respectively. However, if we allow non-linear arithmetic expressions, even of degree at most 2, the satisfiability and implication problems become undecidable. In other words, NGDs strike a balance between expressivity and complexity.To make practical use of NGDs, we develop an incremental algorithm IncDect to detect errors in a graph G using NGDs, in response to updates Δ G to G. We show that the incremental validation problem is coNP-complete. Nonetheless, algorithm IncDect is localizable, i.e., its cost is determined by small neighbors of nodes in Δ G instead of the entire G. Moreover, we parallelize IncDect such that it guarantees to reduce running time with the increase of processors. Using real-life and synthetic graphs, we experimentally verify the scalability and efficiency of the algorithms.|10.1145/3183713.3183753|https://doi.org/10.1145/3183713.3183753|New York, NY, USA|Association for Computing Machinery|9781450347037|2018|Catching Numeric Inconsistencies in Graphs|Fan, Wenfei and Liu, Xueli and Lu, Ping and Tian, Chao|inproceedings|10.1145/3183713.3183753|||||||||||||||||||||||||||||809144427|42
||crowdsourcing, human computation, machine learning, Label quality|3|||Organizations that develop and use technologies around information retrieval, machine learning, recommender systems, and natural language processing depend on labels for engineering and experimentation. These labels, usually gathered via human computation, are used in machine-learned models for prediction and evaluation purposes. In such scenarios, collecting high-quality labels is a very important part of the overall process. We elaborate on these challenges and discuss research directions.|10.1145/2724721|https://doi.org/10.1145/2724721|New York, NY, USA|Association for Computing Machinery||2015|Challenges with Label Quality for Supervised Learning|Alonso, Omar|article|10.1145/2724721|2|mar|J. Data and Information Quality|19361955|1|6|March 2015||||||||||||||||||||||810067363|833754770
||Data science, Exploratory analysis, Internet of Things, Modelling, Multi-criteria decision analysis, Spatial analysis, Visualisation||213-220||Concurrent with global economic development in the last 50 years, the opportunities for the spread of existing diseases and emergence of new infectious pathogens, have increased substantially. The activities associated with the enormously intensified global connectivity have resulted in large amounts of data being generated, which in turn provides opportunities for generating knowledge that will allow more effective management of animal and human health risks. This so-called Big Data has, more recently, been accompanied by the Internet of Things which highlights the increasing presence of a wide range of sensors, interconnected via the Internet. Analysis of this data needs to exploit its complexity, accommodate variation in data quality and should take advantage of its spatial and temporal dimensions, where available. Apart from the development of hardware technologies and networking/communication infrastructure, it is necessary to develop appropriate data management tools that make this data accessible for analysis. This includes relational databases, geographical information systems and most recently, cloud-based data storage such as Hadoop distributed file systems. While the development in analytical methodologies has not quite caught up with the data deluge, important advances have been made in a number of areas, including spatial and temporal data analysis where the spectrum of analytical methods ranges from visualisation and exploratory analysis, to modelling. While there used to be a primary focus on statistical science in terms of methodological development for data analysis, the newly emerged discipline of data science is a reflection of the challenges presented by the need to integrate diverse data sources and exploit them using novel data- and knowledge-driven modelling methods while simultaneously recognising the value of quantitative as well as qualitative analytical approaches. Machine learning regression methods, which are more robust and can handle large datasets faster than classical regression approaches, are now also used to analyse spatial and spatio-temporal data. Multi-criteria decision analysis methods have gained greater acceptance, due in part, to the need to increasingly combine data from diverse sources including published scientific information and expert opinion in an attempt to fill important knowledge gaps. The opportunities for more effective prevention, detection and control of animal health threats arising from these developments are immense, but not without risks given the different types, and much higher frequency, of biases associated with these data.|https://doi.org/10.1016/j.prevetmed.2015.05.012|https://www.sciencedirect.com/science/article/pii/S0167587715002111||||2015|Spatial and temporal epidemiological analysis in the Big Data era|Dirk U. Pfeiffer and Kim B. Stevens|article|PFEIFFER2015213|||Preventive Veterinary Medicine|01675877|1|122|||||18838|0,816|Q1|95|299|618|12546|1722|609|2,50|41,96|Netherlands|Western Europe|1982-2020|Animal Science and Zoology (Q1); Food Animals (Q1)|9,469|2.670|0.00645|812069938|1643202011
||Artificial intelligence;Ethics;Data privacy;Biological system modeling;Training;Pervasive computing;Robustness;Artificial Intelligence;Pervasive Computing;Ethics;Data Fusion;Transparency;Privacy;Fairness;Accountability;Federated Learning||1-6|2020 IEEE International Conference on Pervasive Computing and Communications Workshops (PerCom Workshops)|The era of pervasive computing has resulted in countless devices that continuously monitor users and their environment, generating an abundance of user behavioural data. Such data may support improving the quality of service, but may also lead to adverse usages such as surveillance and advertisement. In parallel, Artificial Intelligence (AI) systems are being applied to sensitive fields such as healthcare, justice, or human resources, raising multiple concerns on the trustworthiness of such systems. Trust in AI systems is thus intrinsically linked to ethics, including the ethics of algorithms, the ethics of data, or the ethics of practice. In this paper, we formalise the requirements of trustworthy AI systems through an ethics perspective. We specifically focus on the aspects that can be integrated into the design and development of AI systems. After discussing the state of research and the remaining challenges, we show how a concrete use-case in smart cities can benefit from these methods.|10.1109/PerComWorkshops48775.2020.9156127|||||2020|Trustworthy AI in the Age of Pervasive Computing and Big Data|Kumar, Abhishek and Braud, Tristan and Tarkoma, Sasu and Hui, Pan|inproceedings|9156127||March|||||||||||||||||||||||||||813876656|42
ICSEB '18|Zhuhai, China|Data analysis, Clinical data, Visualization|5|49–53|Proceedings of the 2018 2nd International Conference on Software and E-Business|With the development of information technology, information systems have been widely used in medical institutions, and more and more clinical research data has been digitized, which provides the possibility to carry out clinical research with data as the source. However, the complexity and multi-dimensionality of clinical data make medical scientists' progress slow, and comprehensive use of various big data technologies is needed to help improve the efficiency of clinical research. Visualization technology can display data in an intuitive and easy-to-read way, helping medical researchers understand data, while parallel computing can greatly improve computing efficiency. Therefore, this paper explores the application strategies of data analysis technology and visualization technology in the management and analysis of clinical research data, and builds a set of clinical research data management analysis system, which combines various technologies to help effectively promote medical clinical.|10.1145/3301761.3301767|https://doi.org/10.1145/3301761.3301767|New York, NY, USA|Association for Computing Machinery|9781450361279|2018|Adopting Data Analysis and Visualization Technology to Construct Clinical Research Data Management and Analysis System|Tang, Haijing and Zhou, Yangdong and Yang, Xu and Gao, Keyan and Zheng, Wenhao and Zhao, Jinfeng|inproceedings|10.1145/3301761.3301767|||||||||||||||||||||||||||||816761809|42
SIGMOD '16|San Francisco, California, USA|data lake, data quality, data integration|4|2097–2100|Proceedings of the 2016 International Conference on Management of Data|As the challenge of our time, Big Data still has many research hassles, especially the variety of data. The high diversity of data sources often results in information silos, a collection of non-integrated data management systems with heterogeneous schemas, query languages, and APIs. Data Lake systems have been proposed as a solution to this problem, by providing a schema-less repository for raw data with a common access interface. However, just dumping all data into a data lake without any metadata management, would only lead to a 'data swamp'. To avoid this, we propose Constance, a Data Lake system with sophisticated metadata management over raw data extracted from heterogeneous data sources. Constance discovers, extracts, and summarizes the structural metadata from the data sources, and annotates data and metadata with semantic information to avoid ambiguities. With embedded query rewriting engines supporting structured data and semi-structured data, Constance provides users a unified interface for query processing and data exploration. During the demo, we will walk through each functional component of Constance. Constance will be applied to two real-life use cases in order to show attendees the importance and usefulness of our generic and extensible data lake system.|10.1145/2882903.2899389|https://doi.org/10.1145/2882903.2899389|New York, NY, USA|Association for Computing Machinery|9781450335317|2016|Constance: An Intelligent Data Lake System|Hai, Rihan and Geisler, Sandra and Quix, Christoph|inproceedings|10.1145/2882903.2899389|||||||||||||||||||||||||||||819558463|42
||data integrity, data misuse, big data, professional responsibility|8|60–67||"\"Like most significant changes in technology, Cloud Computing and Big Data along with their associated analytic techniques are claimed to provide us with new insights unattainable by any previous knowledge techniques. It is believed that the quantity of virtual data now available requires new knowledge production strategies. Although they have yielded significant results, there are problems with advocated processes and resulting facts. The primary process treats \"\"pattern recognition\"\" as a final result rather than using \"\"pattern recognition\"\" to lead to yet to be tested testable hypotheses. In data analytics, the discovery of a pattern is treated as knowledge rather than going further to understand the possible causes of those patterns. When this is used as the primary approach to knowledge acquisition unjustified inferences are made - \"\"fact generation\"\". These pseudo-facts are used to generate new pseudo-facts as those initial inferences are fed back into analytic engines as established facts. The approach of generating \"\"facts from data analytics\"\" is introducing highly risky scenarios where \"\"fiction becomes fact\"\" very quickly. These \"\"facts\"\" are then given elevated epistemic status and get used in decision making. This, misleading approach is inconsistent with the moral duty of computing professionals embodied in their Codes of Ethics. There are some ways to mitigate the problems generated by this single path approach to knowledge generation.\""|10.1145/2874239.2874248|https://doi.org/10.1145/2874239.2874248|New York, NY, USA|Association for Computing Machinery||2016|The Creation of Facts in the Cloud: A Fiction in the Making|Gotterbarn, Don|article|10.1145/2874239.2874248||jan|SIGCAS Comput. Soc.|00952737|3|45|September 2015||||||||||||||||||||||820047289|1394136391
SIGMOD '17|Chicago, Illinois, USA|data stitching, data discovery, join path discovery, polystore queries, data cleaning, data integration|4|1639–1642|Proceedings of the 2017 ACM International Conference on Management of Data|"\"Finding relevant data for a specific task from the numerous data sources available in any organization is a daunting task. This is not only because of the number of possible data sources where the data of interest resides, but also due to the data being scattered all over the enterprise and being typically dirty and inconsistent. In practice, data scientists are routinely reporting that the majority (more than 80%) of their effort is spent finding, cleaning, integrating, and accessing data of interest to a task at hand. We propose to demonstrate DATA CIVILIZER to ease the pain faced in analyzing data \"\"in the wild\"\". DATA CIVILIZER is an end-to-end big data management system with components for data discovery, data integration and stitching, data cleaning, and querying data from a large variety of storage engines, running in large enterprises.\""|10.1145/3035918.3058740|https://doi.org/10.1145/3035918.3058740|New York, NY, USA|Association for Computing Machinery|9781450341974|2017|A Demo of the Data Civilizer System|Castro Fernandez, Raul and Deng, Dong and Mansour, Essam and Qahtan, Abdulhakim A. and Tao, Wenbo and Abedjan, Ziawasch and Elmagarmid, Ahmed and Ilyas, Ihab F. and Madden, Samuel and Ouzzani, Mourad and Stonebraker, Michael and Tang, Nan|inproceedings|10.1145/3035918.3058740|||||||||||||||||||||||||||||820212989|42
||Data fusion, Sensor fusion, Smart city, Big data, Internet of things, Multi-perspectives classification||357-374||The advancement of various research sectors such as Internet of Things (IoT), Machine Learning, Data Mining, Big Data, and Communication Technology has shed some light in transforming an urban city integrating the aforementioned techniques to a commonly known term - Smart City. With the emergence of smart city, plethora of data sources have been made available for wide variety of applications. The common technique for handling multiple data sources is data fusion, where it improves data output quality or extracts knowledge from the raw data. In order to cater evergrowing highly complicated applications, studies in smart city have to utilize data from various sources and evaluate their performance based on multiple aspects. To this end, we introduce a multi-perspectives classification of the data fusion to evaluate the smart city applications. Moreover, we applied the proposed multi-perspectives classification to evaluate selected applications in each domain of the smart city. We conclude the paper by discussing potential future direction and challenges of data fusion integration.|https://doi.org/10.1016/j.inffus.2019.05.004|https://www.sciencedirect.com/science/article/pii/S1566253519300326||||2019|A survey of data fusion in smart city applications|Billy Pik Lik Lau and Sumudu Hasala Marakkalage and Yuren Zhou and Naveed Ul Hassan and Chau Yuen and Meng Zhang and U-Xuan Tan|article|LAU2019357|||Information Fusion|15662535||52|||||||||||||||||||||||820271358|1192976563
ISAIMS 2020|Beijing, China|Multimodal, Parkinson's disease (PD), Intelligent analysis system, Quantitative evaluation and diagnostic data set|5|166–170|Proceedings of the 2020 International Symposium on Artificial Intelligence in Medical Sciences|The GYENNO PD CIS is an AI-based multimodal data management and intelligent analysis system for Parkinson's disease (PD). The main purpose is to solve the problems in traditional diagnosis of PD such as lack of objective evaluation data, lack of reproducible diagnosis system, and lack of closed-loop treatment tracking, and then to construct a multimodal data management and intelligent analysis platform for PD, which can achieve the goals - standardization of data, objectification of evaluation, standardization of diagnosis, individualization of treatment, continuousness of management. It also helps Parkinson's experts in patient management, clinical data management, analysis and data mining, and supports multi-center projects, and finally lets patients benefit a lot from innovative technology.|10.1145/3429889.3429921|https://doi.org/10.1145/3429889.3429921|New York, NY, USA|Association for Computing Machinery|9781450388603|2020|AI-Based Multimodal Data Management and Intelligent Analysis System for Parkinson's Disease: GYENNO PD CIS|Ren, Kang and Liu, Fan and Zhuang, Haimei and Ling, Yun|inproceedings|10.1145/3429889.3429921|||||||||||||||||||||||||||||820419778|42
KDD '21|Virtual Event, Singapore|empirical Bayes, randomization, post-selection inference, regression, A/B testing, machine learning, bias correction, online metrics, winner's curse, big data|10|2743–2752|Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining|When interpreting A/B tests, we typically focus only on the statistically significant results and take them by face value. This practice, termed post-selection inference in the statistical literature, may negatively affect both point estimation and uncertainty quantification, and therefore hinder trustworthy decision making in A/B testing. To address this issue, in this paper we explore two seemingly unrelated paths, one based on supervised machine learning and the other on empirical Bayes, and propose post-selection inferential approaches that combine the strengths of both. Through large-scale simulated and empirical examples, we demonstrate that our proposed methodologies stand out among other existing ones in both reducing post-selection biases and improving confidence interval coverage rates, and discuss how they can be conveniently adjusted to real-life scenarios.|10.1145/3447548.3467129|https://doi.org/10.1145/3447548.3467129|New York, NY, USA|Association for Computing Machinery|9781450383325|2021|On Post-Selection Inference in A/B Testing|Deng, Alex and Li, Yicheng and Lu, Jiannan and Ramamurthy, Vivek|inproceedings|10.1145/3447548.3467129|||||||||||||||||||||||||||||822101183|42
||Medical services;Market research;Indexes;Data integrity;Informatics;Histograms;Software;popularity index;healthcare dataset;data quality;quantified measurement;regression||247-254|2018 IEEE International Conference on Healthcare Informatics (ICHI)|Data are critical in this age of big data and machine learning. Due to their inherent complexity, health-related data are unique in that the datasets are usually acquired for specific purposes and with special designs. As more and more healthcare datasets become available, of which many are public, choosing a quality dataset that is suitable for specific research inquiries is becoming a challenging question for health informatics researchers, especially the learners of this field. On the other hand, from the data provider's perspective, it is important to identify features of datasets that make some datasets more valuable than others so as to improve the design and acquisition of future datasets. To address these questions, we need to develop formal mechanisms to measure the goodness of datasets according to certain criteria. In this study, we propose one way of measuring the value of healthcare datasets that is based on how often the datasets are used and reported by researchers, which we call the Publication-based Popularity Index (PPI). In this article, we describe the design of the PPI and discuss its properties. We demonstrate the utility of the PPI by ranking 14 representative healthcare datasets. We believe that the PPI can enable an overall ranking of all healthcare datasets and thus provide an important dimension to sort search results for dataset integration systems as well as a starting point for identifying and examining the design of the most valuable healthcare datasets so that features of these datasets can inform future designs.|10.1109/ICHI.2018.00035|||||2018|A Publication-Based Popularity Index (PPI) for Healthcare Dataset Ranking|Shi, Jingyi and Zheng, Mingna and Yao, Lixia and Ge, Yaorong|inproceedings|8419368||June||25752634|||||||||||||||||||||||||823288624|29854004
||Big data Security, Big data in healthcare, Big data lifecycle, Security threat model||294-301||Big data is a concept that aimed at collecting, storing, processing and transforming large amount of data into value using new combination of strategies and technologies. Big data is characterized by data that have a large volume, massive velocity, numerous variety, useful value, and veracity. Big Data Analytics offers tremendous insights to different organizations especially in healthcare. Currently, Big healthcare data has the highest potential for improving patient outcomes, gaining valuable insights, predicting outbreaks of epidemics, avoiding preventable diseases and effectively minimizing the cost of healthcare delivery. However, the dynamic nature of health data presents various conceptual, technical, legal and ethical challenges associated with the data processing and analysis activities. The big data security and privacy concepts are some of the most pertinent issues and have become increasingly significant associated with big healthcare data in the modern world. In this paper, we give an overview of big data characteristics and challenges in healthcare and present big healthcare data lifecycle integrated with security threats and attacks to provide encompass policies and mechanisms that aim at solving the various security challenges in each step of big data lifecycle. The focus is also placed on the description of the recently proposed techniques related to authentication, encryption, anonymization, access control, and privacy. We finally propose an approach to secure threat model for big healthcare data lifecycle as a main contribution of this paper.|https://doi.org/10.1016/j.procs.2018.10.199|https://www.sciencedirect.com/science/article/pii/S1877050918318520||||2018|Security model for Big Healthcare Data Lifecycle|Hayat Khaloufi and Karim Abouelmehdi and Abderrahim Beni-hssane and Mostafa Saadi|article|KHALOUFI2018294|||Procedia Computer Science|18770509||141||The 9th International Conference on Emerging Ubiquitous Systems and Pervasive Networks (EUSPN-2018) / The 8th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare (ICTH-2018) / Affiliated Workshops|||19700182801|0,334|-|76|1907|6483|38511|13722|6359|2,09|20,19|Netherlands|Western Europe|2010-2020|Computer Science (miscellaneous)||||825068869|2108686752
||Remote sensing;Inspection;Sampling methods;Sociology;Big Data;Quality assessment;multiple stratified;spatial sampling;quality assessment;remote sensing products;big data||1-4|2015 7th Workshop on Hyperspectral Image and Signal Processing: Evolution in Remote Sensing (WHISPERS)|The number and volume of remote sensing data and its derived products, which are regarded as typical “big data”, have grown exponentially. How to assess the quality of these big remote sensing products become a challenge. As an importance technique, spatial sampling is regarded to be necessary for the quality assessment of remote sensing derived products. This paper proposes an approach of multiple stratified spatial sampling for assessing the remote sensing products, with the aim of resolving the issue of the quality inspection of big remote sensing products. The proposed method improves the sampling accuracy without increasing the sampling size, and the whole procedure is repeatable and easily adopted for the quality inspection of remote sensing derived products.|10.1109/WHISPERS.2015.8075416|||||2015|Multiple stratified sampling strategy for assessing the big remote sensing products|Xie, Huan and Tong, Xiaohua and Meng, Wen and Wang, Fang and Xu, Xiong|inproceedings|8075416||June||21586276|||||||21000195601|0,174|-|16|0|251|0|126|246|0,60|0,00|United States|Northern America|2011, 2012, 2017, 2019|Computer Vision and Pattern Recognition; Signal Processing||||825426358|1668017566
||Electronic Health Records, EHRs, Analytic tools, Big Data, Health Practitioners||60-68||Recently there has been an increasing adoption of electronic health records (EHRs) in different countries. Thanks to these systems, multiple health bodies can now store, manage and process their data effectively. However, the existence of such powerful and meticulous entities raise new challenges and issues for health practitioners. In fact, while the main objective of EHRs is to gain actionable big data insights from the health workflow, very few physicians exploit widely analytic tools, this is mainly due to the fact of having to deal with multiple systems and steps, which completely discourage them from engaging more and more. In this paper, we shed light and explore precisely the proper adaptation of analytical tools to EHRs in order to upgrade their use by health practitioners. For that, we present a case study of the implementation process of an EHR based OpenEHR and investigate health analytics adoption in each step of the methodology.|https://doi.org/10.1016/j.procs.2018.01.098|https://www.sciencedirect.com/science/article/pii/S1877050918301091||||2018|Improving the Use of Big Data Analytics within Electronic Health Records: A Case Study based OpenEHR|Fadoua Khennou and Youness Idrissi Khamlichi and Nour El Houda Chaoui|article|KHENNOU201860|||Procedia Computer Science|18770509||127||PROCEEDINGS OF THE FIRST INTERNATIONAL CONFERENCE ON INTELLIGENT COMPUTING IN DATA SCIENCES, ICDS2017|||19700182801|0,334|-|76|1907|6483|38511|13722|6359|2,09|20,19|Netherlands|Western Europe|2010-2020|Computer Science (miscellaneous)||||826067923|2108686752
||Big data analytics, Cloud, Health care domain, Internet of Things (IoT), Sensors||1-23|Handbook of Data Science Approaches for Biomedical Engineering|Data analytics play an active role in medical applications to extract relevant information from heaps of data samples. Internet of things (IoT) technology has slowly captured the market and is entering the health care sector too. With the help of big data analytics, various IoT-based devices can auto-monitor the health conditions of patients and can send the status to concerned physicians and family members. Thus, the integration of big data analytics with IoT technology forms a favorable combination in the health care domain. In this chapter, we discuss the two latest trends that include big data analytics and IoT with respect to its relevance in medical fields. We also analyze a health care monitoring system, which is an IoT-based model integrated with big data analytics. The system integrates patient specific information over the cloud. In the more developed model, the implementation was made to monitor the health status of the patients. The developed model was found to be faster and thus it can be easily implemented into a real time patient health monitoring and status management system. Medical experts can take advantage of this system model, thereby providing appropriate information to appropriate patient and doctors at appropriate time.|https://doi.org/10.1016/B978-0-12-818318-2.00001-5|https://www.sciencedirect.com/science/article/pii/B9780128183182000015||Academic Press|978-0-12-818318-2|2020|Chapter 1 - Analysis of the role and scope of big data analytics with IoT in health care domain|Sushruta Mishra and Brojo Kishore Mishra and Hrudaya Kumar Tripathy and Arijit Dutta|incollection|MISHRA20201||||||||||Valentina Emilia Balas and Vijender Kumar Solanki and Raghvendra Kumar and Manju Khari|||||||||||||||||||827007025|42
||IT business value, Big data analytics (BDA), Delphi method, Mixed methodology, Competitive advantage||160-173||Although big data analytics (BDA) is considered the next “frontier” in data science by creating potential business opportunities, the way to extract those opportunities is unclear. This paper aims to understand the antecedents of BDA value at a firm level. The authors performed a study using a mixed methodology approach. First, by carrying out a Delphi study to explore and rank the antecedents affecting the creation of BDA value. Based on the Delphi results, we propose an empirically validated model supported by a survey conducted on 175 European firms to explain the antecedents of BDA sustained value. The results show that the proposed model explains 62% of BDA sustained value at the firm level, where the most critical contributor is BDA use. We provide directions for managers to support their decisions on BDA strategy definition and refinement. For academics, we extend BDA value literature and outline some potential research opportunities.|https://doi.org/10.1016/j.jbusres.2018.12.072|https://www.sciencedirect.com/science/article/pii/S0148296318306908||||2019|Unlocking the drivers of big data analytics value in firms|Nadine Côrte-Real and Pedro Ruivo and Tiago Oliveira and Aleš Popovič|article|CORTEREAL2019160|||Journal of Business Research|01482963||97|||||20550|2,049|Q1|195|878|1342|73221|11507|1315|7,38|83,40|United States|Northern America|1973-2021|Marketing (Q1)|46,935|7.550|0.03523|827102245|1502892296
JCDL '20|Virtual Event, China|data integration, big data, knowledge system, data curation, data model, space traffic management, graph database|8|235–242|Proceedings of the ACM/IEEE Joint Conference on Digital Libraries in 2020|Scientific data publications may include interactive data applications designed by scientists to explore a scientific problem. Defined as knowledge systems, their development is complex when data are aggregated from multiple sources over time. Multimodal data are created, encoded, and maintained differently, and even when reporting about identical phenomena, fields and their values may be inconsistent across datasets. To assure the validity and accuracy of the application, the data has to abide by curation requirements similar to those ruling digital libraries. We present a novel, inquiry-driven curation approach aimed to optimize multimodal datasets curation and maximize data reuse by domain researchers. We demonstrate the method through the ASTRIAGraph project, in which multiple data sources about near earth space objects are aggregated into a central knowledge system. The process involves multidisciplinary collaboration, resulting in the design of a data model as the backbone for both data curation and scientific inquiry. We demonstrate a) how data provenance information is needed to assess the uncertainty of the results of scientific inquiries involving multiple data sources, and b) that continuous curation of integrated datasets is facilitated when undertaken as integral to the research project. The approach provides flexibility to support expansion of scientific inquiries and data in the knowledge system, and allows for transparent and explainable results.|10.1145/3383583.3398539|https://doi.org/10.1145/3383583.3398539|New York, NY, USA|Association for Computing Machinery|9781450375856|2020|Modeling Data Curation to Scientific Inquiry: A Case Study for Multimodal Data Integration|Esteva, Maria and Xu, Weijia and Simone, Nevan and Gupta, Amit and Jah, Moriba|inproceedings|10.1145/3383583.3398539|||||||||||||||||||||||||||||830013214|42
EuroPLoP '14|Irsee, Germany|scope identification, cognitive systems, social and ethical impact, enterprise integration pattern, pattern language, requirements engineering|9||Proceedings of the 19th European Conference on Pattern Languages of Programs|This paper discusses the influence of recent advances in cognitive computing systems on enterprise software architecture and design/development. Specifically, building on key features and capabilities of cognitive computing systems, we propose a new schema of enterprise application integration patterns in the tradition of the design pattern literature. Our schema has three groups of patterns addressing essential scoping, security and service integration issues related to cognitive components in enterprise architecture. While some patterns are modifications or refinements of known Enterprise Application Integration patterns, some of them are new and require dedicated consideration by enterprise architects and software designers.|10.1145/2721956.2721968|https://doi.org/10.1145/2721956.2721968|New York, NY, USA|Association for Computing Machinery|9781450334167|2014|Towards a Pattern Language for Cognitive Systems Integration|Leibold, Christian F. and Spies, Marcus|inproceedings|10.1145/2721956.2721968|17||||||||||||||||||||||||||||830101353|42
||Sensors;Task analysis;Resource management;Real-time systems;Big Data;Dynamic scheduling;Air quality;Social Sensing;Task Allocation;Online Learning||115-116|2018 14th International Conference on Distributed Computing in Sensor Systems (DCOSS)|"\"Social sensing has emerged as a new sensing paradigm where human sensors collectively report measurements about the physical world. This paper focuses on the cost-sensitive task allocation problem in social sensing where the goal is to effectively allocate sensing tasks to the human sensors to meet the desirable data quality requirement of the applications while minimizing the sensing cost. While recent progress has been made to tackle the cost-sensitive task allocation problem, an important challenge has not been well addressed, namely \"\"real time task allocation\"\", the task allocation schemes need to respond quickly to the potential large dynamics of the measured variables in social sensing. To address this challenge, this paper presents a Cost-Sensitive Task Allocation (CSTA) scheme inspired by techniques from online learning. The preliminary results show that our new scheme significantly outperforms the-state-of-the-art baselines.\""|10.1109/DCOSS.2018.00024|||||2018|Poster: On Cost-Sensitive Task Allocation in Social Sensing: An Online Learning Approach|Zhang, Yang and Wang, Dong|inproceedings|8510969||June||23252944|||||||||||||||||||||||||830799537|1898617345
||Metabolomics, Metabonomics, Metabolic phenotyping, Big data, Cloud computing, High-performance computing, Software tools, Databases, PhenoMeNal, Ethical, Legal, Social implications, ELSI||329-367|The Handbook of Metabolic Phenotyping|Metabolic phenotyping is entering the era of Big Data, leading to new opportunities and challenges. Cloud computing has been proposed as a novel paradigm, but as yet is not widely understood or used. In this chapter we introduce the concepts of Big Data and cloud computing, and discuss how they might change the landscape of metabolic phenotyping and analysis. We highlight some of the reasons for the increase in data size and explain advantages and disadvantages of large-scale computing in this context. We illustrate the area with a survey of software tools and databases currently available, and describe the newly developed cloud infrastructure “PhenoMeNal,” which will enable widespread use of these approaches. We conclude the chapter with a discussion of the important ethical, legal, and social implications (ELSI) of large-scale computing in this rapidly developing field.|https://doi.org/10.1016/B978-0-12-812293-8.00011-6|https://www.sciencedirect.com/science/article/pii/B9780128122938000116||Elsevier|978-0-12-812293-8|2019|Chapter 11 - Big Data and Databases for Metabolic Phenotyping|Timothy M.D. Ebbels and Jake T.M. Pearce and Noureddin Sadawi and Jianliang Gao and Robert C. Glen|incollection|EBBELS2019329||||||||||John C. Lindon and Jeremy K. Nicholson and Elaine Holmes|||||||||||||||||||830869424|42
||Big data analysis, Ecological environment, Water demand, Prediction||101196||The existing prediction model of eco-environmental water demand has the problem of large prediction error. In order to solve the above problems, the prediction model of eco-environmental water demand is constructed based on big data analysis. In order to reduce the prediction error of the ecological environment water demand prediction model, the framework of the ecological environment water demand prediction model is built. On this basis, the principal component analysis method is used to select the auxiliary variables of the model. Based on the selected auxiliary variables, the minimum monthly average flow method is used to analyze the basic water demand of the ecological environment, the leakage water demand and the water surface evaporation ecological environment water demand, so as to analyze based on the results, the water demand of ecological environment is predicted by big data analysis technology, and the prediction of water demand of ecological environment is realized. The experimental results show that compared with the existing ecological environment water demand prediction model, the prediction error of the model is within 19.3, which fully shows that the constructed ecological environment water demand prediction model has better prediction effect and can provide a certain reference value for the actual use of water resources.|https://doi.org/10.1016/j.eti.2020.101196|https://www.sciencedirect.com/science/article/pii/S2352186420314966||||2021|Prediction model of ecological environmental water demand based on big data analysis|Lihong Zhao|article|ZHAO2021101196|||Environmental Technology & Innovation|23521864||21|||||21100385961|0,866|Q1|28|409|310|23508|1661|310|5,51|57,48|Netherlands|Western Europe|2014-2020|Environmental Science (miscellaneous) (Q1); Plant Science (Q1); Soil Science (Q1)||||832369654|39526125
||Big data, Data science, Data quality, Decision quality, Regulation||467-476||The vague but vogue notion of ‘big data’ is enjoying a prolonged honeymoon. Well-funded, ambitious projects are reaching fruition, and inferences are being drawn from inadequate data processed by inadequately understood and often inappropriate data analytic techniques. As decisions are made and actions taken on the basis of those inferences, harm will arise to external stakeholders, and, over time, to internal stakeholders as well. A set of Guidelines is presented, whose purpose is to intercept ill-advised uses of data and analytical tools, prevent harm to important values, and assist organisations to extract the achievable benefits from data, rather than dreaming dangerous dreams.|https://doi.org/10.1016/j.clsr.2017.11.002|https://www.sciencedirect.com/science/article/pii/S0267364917303643||||2018|Guidelines for the responsible application of data analytics|Roger Clarke|article|CLARKE2018467|||Computer Law & Security Review|02673649|3|34|||||28888|0,815|Q1|38|66|242|1245|707|223|3,39|18,86|United Kingdom|Western Europe|1985-2020|Business, Management and Accounting (miscellaneous) (Q1); Computer Networks and Communications (Q1); Law (Q1)||||836206038|1769124433
||EIM Strategy, EIM Governance, EIM Components, Big Data and EIM, Challenges for EIM||23-38|Total Information Risk Management|This chapter gives an introduction to concept of enterprise information management, investigates the influence of Big Data on EIM, and discusses today's key challenges and pressures for EIM.|https://doi.org/10.1016/B978-0-12-405547-6.00002-X|https://www.sciencedirect.com/science/article/pii/B978012405547600002X|Boston|Morgan Kaufmann|978-0-12-405547-6|2014|Chapter 2 - Enterprise Information Management|Alexander Borek and Ajith K. Parlikad and Jela Webb and Philip Woodall|incollection|BOREK201423||||||||||Alexander Borek and Ajith K. Parlikad and Jela Webb and Philip Woodall|||||||||||||||||||837252185|42
||Big Data;Reliability;Cloud computing;Social networking (online);Sensors;Videos;Data integrity;Big Data;Big Data Analytics;Big Data Quality;Data Reliability;IOT;Hadoop;HDFS;Cloud Computing||150-156|2018 International Conference on Advances in Computing, Communication Control and Networking (ICACCCN)|In this digital era, here the sum of data is generate with store has prolonged inside a less period of time. The data in this era generated high speed leads to many challenges. The size is primarily and periodically, just the measurement that bounces in the big data position. In this survey, we have attempt to give a broad explanation of big data that captures its other unique and important features. We have discussed 4V's model. Also, the latest technologies uses in big data, like Hadoop, HDFS, MapReduce and different type of methods used by big data. Finally various benefits of using big data analysis and include some feature of cloud computing.|10.1109/ICACCCN.2018.8748630|||||2018|Big Data and Internet of Things: A Survey|Saha, Ajitesh Kumar and Kumar, Ashwini and Tyagi, Vishu and Das, Sanjoy|inproceedings|8748630||Oct|||||||||||||||||||||||||||837469136|42
https://doi.org/10.1016/j.vhri.2021.05.002|https://www.sciencedirect.com/science/article/pii/S2212109921000765||||2022|Real-World Data for Healthcare Research in China: Call for Actions|Jipan Xie and Eric Q. Wu and Shan Wang and Tao Cheng and Zhou Zhou and Jia Zhong and Larry Liu|article|XIE202272|||Value in Health Regional Issues|22121099||27||||||||||||||||||||||||||||||837541825|42
ICPE '15|Austin, Texas, USA|benchmarking, big data, internet of things, performance evaluation|12|133–144|Proceedings of the 6th ACM/SPEC International Conference on Performance Engineering|"\"The commoditization of sensors and communication networks is enabling vast quantities of data to be generated by and collected from cyber-physical systems. This ``Internet-of-Things\"\" (IoT) makes possible new business opportunities, from usage-based insurance to proactive equipment maintenance. While many technology vendors now offer ``Big Data\"\" solutions, a challenge for potential customers is understanding quantitatively how these solutions will work for IoT use cases. This paper describes a benchmark toolkit called IoTAbench for IoT Big Data scenarios. This toolset facilitates repeatable testing that can be easily extended to multiple IoT use cases, including a user's specific needs, interests or dataset. We demonstrate the benchmark via a smart metering use case involving an eight-node cluster running the HP Vertica analytics platform. The use case involves generating, loading, repairing and analyzing synthetic meter readings. The intent of IoTAbench is to provide the means to perform ``apples-to-apples\"\" comparisons between different sensor data and analytics platforms. We illustrate the capabilities of IoTAbench via a large experimental study, where we store 22.8 trillion smart meter readings totaling 727 TB of data in our eight-node cluster.\""|10.1145/2668930.2688055|https://doi.org/10.1145/2668930.2688055|New York, NY, USA|Association for Computing Machinery|9781450332484|2015|IoTAbench: An Internet of Things Analytics Benchmark|Arlitt, Martin and Marwah, Manish and Bellala, Gowtham and Shah, Amip and Healey, Jeff and Vandiver, Ben|inproceedings|10.1145/2668930.2688055|||||||||||||||||||||||||||||839163509|42
||Growth hacking, Digital transformation, Lean startup, Digital marketing, Big data||799-818||As companies become increasingly digital, growth hacking emerged as a new way of scaling businesses. While the term is fashionable in business, many executives remain confused about the concept. Even if firms have an idea of what growth hacking is, they may still be puzzled as to how to do it, creating a strategy-execution gap. Our article assists firms by bridging the growth hacking strategy-execution gap. First, we provide a growth hacking framework and deconstruct its building blocks: marketing, data analysis, coding, and the lean startup philosophy. We then present a taxonomy of 34 growth hacking patterns along the customer lifecycle of acquisition, activation, revenue, retention, and referral; categorize them on the two dimensions of resource intensity and time lag; and provide an example of how to apply the taxonomy in the case of a fitness application. Finally, we discuss seven opportunities and challenges of growth hacking that firms should keep in mind.|https://doi.org/10.1016/j.bushor.2019.09.001|https://www.sciencedirect.com/science/article/pii/S0007681319301247||||2019|What the hack? A growth hacking taxonomy and practical applications for firms|René Bohnsack and Meike Malena Liesner|article|BOHNSACK2019799|||Business Horizons|00076813|6|62||Digital Transformation & Disruption|||20209|2,174|Q1|87|64|278|2475|2066|227|6,68|38,67|United Kingdom|Western Europe|1957-2020|Business and International Management (Q1); Marketing (Q1)|7,443|6.361|0.00571|840599087|847373672
SIGMOD '16|San Francisco, California, USA|data quality, RDF, data lakes|4|2089–2092|Proceedings of the 2016 International Conference on Management of Data|"\"With the increasing incentive of enterprises to ingest as much data as they can in what is commonly referred to as \"\"data lakes\"\", and with the recent development of multiple technologies to support this \"\"load-first\"\" paradigm, the new environment presents serious data management challenges. Among them, the assessment of data quality and cleaning large volumes of heterogeneous data sources become essential tasks in unveiling the value of big data. The coveted use of unstructured and semi-structured data in large volumes makes current data cleaning tools (primarily designed for relational data) not directly adoptable.We present CLAMS, a system to discover and enforce expressive integrity constraints from large amounts of lake data with very limited schema information (e.g., represented as RDF triples). This demonstration shows how CLAMS is able to discover the constraints and the schemas they are defined on simultaneously. CLAMS also introduces a scale-out solution to efficiently detect errors in the raw data. CLAMS interacts with human experts to both validate the discovered constraints and to suggest data repairs.CLAMS has been deployed in a real large-scale enterprise data lake and was experimented with a real data set of 1.2 billion triples. It has been able to spot multiple obscure data inconsistencies and errors early in the data processing stack, providing huge value to the enterprise.\""|10.1145/2882903.2899391|https://doi.org/10.1145/2882903.2899391|New York, NY, USA|Association for Computing Machinery|9781450335317|2016|CLAMS: Bringing Quality to Data Lakes|Farid, Mina and Roatis, Alexandra and Ilyas, Ihab F. and Hoffmann, Hella-Franziska and Chu, Xu|inproceedings|10.1145/2882903.2899391|||||||||||||||||||||||||||||842357262|42
AIIPCC '19|Sanya, China|mobile crowdsensing, incentive mechanism, fairness competition, sensor network|6||Proceedings of the International Conference on Artificial Intelligence, Information Processing and Cloud Computing|While sensor networks have been pervasively deployed in the real world, more and more mobile crowdsensing (MCS) applications have come into realization to collaboratively detect events and collect data. This paper aims to design a novel incentive mechanism to achieve good services for mobile crowdsensing applications. Responding to insufficient participants, we propose a novel Experience-Based incentive mechanism using Reverse Auction (EBRA). Additionally, it can also guarantee fair competition while maximizing the total profit of the service platform. Through strictly proving, our proposed EBRA incentive mechanism satisfies four properties: computational efficiency, individual rationality, profitability, and truthfulness. The extensive simulations show that the proposed EBRA method has a better performance over 20% than other benchmark mechanisms.|10.1145/3371425.3371459|https://doi.org/10.1145/3371425.3371459|New York, NY, USA|Association for Computing Machinery|9781450376334|2019|A Novel Experience-Based Incentive Mechanism for Mobile Crowdsensing System|Tan, Wenan and Jiang, Zihui|inproceedings|10.1145/3371425.3371459|70||||||||||||||||||||||||||||843349751|42
||Transport mode recognition, Mobile sensed big data, Spatial awareness, Geographic information systems, Smart city, Support vector machines, Context mining, Urban data||38-52||Knowledge about what transport mode people use is important information of any mobility or travel behaviour research. With ubiquitous presence of smartphones, and its sensing possibilities, new opportunities to infer transport mode from movement data are appearing. In this paper we investigate the role of spatial context of human movements in inferring transport mode from mobile sensed data. For this we use data collected from more than 8000 participants over a period of four months, in combination with freely available geographical information. We develop a support vectors machines-based model to infer five transport modes and achieve success rate of 94%. The developed model is applicable across different mobile sensed data, as it is independent on the integration of additional sensors in the device itself. Furthermore, suggested approach is robust, as it strongly relies on pre-processed data, which makes it applicable for big data implementations in (smart) cities and other data-driven mobility platforms.|https://doi.org/10.1016/j.compenvurbsys.2017.07.004|https://www.sciencedirect.com/science/article/pii/S0198971516304367||||2017|Spatial context mining approach for transport mode recognition from mobile sensed big data|Ivana Semanjski and Sidharta Gautama and Rein Ahas and Frank Witlox|article|SEMANJSKI201738|||Computers, Environment and Urban Systems|01989715||66|||||23269|1,549|Q1|92|85|327|4842|2135|324|6,11|56,96|United Kingdom|Western Europe|1980-1986, 1988-2020|Ecological Modeling (Q1); Environmental Science (miscellaneous) (Q1); Geography, Planning and Development (Q1); Urban Studies (Q1)||||852013888|1571752529
||Big Data;Interviews;Market research;Organizations;Surges;Software;Big Data;organizations;information;Audit;technology||1-6|2017 12th Iberian Conference on Information Systems and Technologies (CISTI)|Big Data is one of the great trends in the short and medium term in organizations. There is a growing concern in provid solutions to address this trend, to methodical analysis of data and to do better decisions. The amount of data becomes less relevant when there is efficiency in Analytics. Internal auditors need to be in compliance with technology evolution. This research main objective is to understand how are internal auditors perceiving Big Data & Analytics' and which are the opportunities and difficulties pointed to address that challenge. To achieve this main goal, semi-structured interviews were conducted focused on internal auditors group. Those interviews intend to analyze and classify respondents' contributions in order to provide more insights for the present research. As a result, the main opportunities listed were greater information security and greater efficiency in data processing. Pointed obstacles were data quality, security and users' training.|10.23919/CISTI.2017.7976069|||||2017|Big data & analytics: An approach using audit experts' interviews|Vieira, Vanessa and Pedrosa, Isabel and Soares, Bruno Horta|inproceedings|7976069||June|||||||||||||||||||||||||||852378344|42
||data management technology, databases, big data, financial analytics, IT management||127-177|A Primer in Financial Data Management|In this chapter we look at the technology and tooling available for data management. We start with a discussion on the different components of an IT infrastructure and how to describe them. We provide a short taxonomy of tools from analytics and data distribution to data governance and end-user tools. This is followed by a discussion on data models and storage models, from traditional relational databases to different challenger technologies including NoSQL databases. We discuss data quality management and curation processes and data storage at different scales from data marts and warehouses to data lakes. We then discuss data analytics and big data technologies and what some of the use cases and implications for financial services firms are. After discussing privacy and security aspects, and the promising application areas of blockchain technology in master data, we discuss cloud storage models and what the cloud trend means for banks and asset managers. We end with a discussion on IT sourcing options, IT management, and IT maturity models before concluding with a look ahead.|https://doi.org/10.1016/B978-0-12-809776-2.00005-3|https://www.sciencedirect.com/science/article/pii/B9780128097762000053||Academic Press|978-0-12-809776-2|2017|Chapter 5 - Data Management Tools and Techniques|Martijn Groot|incollection|GROOT2017127||||||||||Martijn Groot|||||||||||||||||||852474889|42
||time series, Outlier explanation, outlier repairs, data profiling|17|||IoT data with timestamps are often found with outliers, such as GPS trajectories or sensor readings. While existing systems mostly focus on detecting temporal outliers without explanations and repairs, a decision maker may be more interested in the cause of the outlier appearance such that subsequent actions would be taken, e.g., cleaning unreliable readings or repairing broken devices or adopting a strategy for data repairs. Such outlier detection, explanation, and repairs are expected to be performed in either offline (batch) or online modes (over streaming IoT data with timestamps). In this work, we present TsClean, a new prototype system for detecting and repairing outliers with explanations over IoT data. The framework defines uniform profiles to explain the outliers detected by various algorithms, including the outliers with variant time intervals, and take approaches to repair outliers. Both batch and streaming processing are supported in a uniform framework. In particular, by varying the block size, it provides a tradeoff between computing the accurate results and approximating with efficient incremental computation. In this article, we present several case studies of applying TsClean in industry, e.g., how this framework works in detecting and repairing outliers over excavator water temperature data, and how to get reasonable explanations and repairs for the detected outliers in tracking excavators.|10.1145/3436239|https://doi.org/10.1145/3436239|New York, NY, USA|Association for Computing Machinery||2021|EXPERIENCE: Algorithms and Case Study for Explaining Repairs with Uniform Profiles over IoT Data|Liu, Zhicheng and Zhang, Yang and Huang, Ruihong and Chen, Zhiwei and Song, Shaoxu and Wang, Jianmin|article|10.1145/3436239|18|apr|J. Data and Information Quality|19361955|3|13|September 2021||||||||||||||||||||||852866084|833754770
|||4|26–29||Cyber-physical systems (CPS) have been developed in many industrial sectors and application domains in which the quality requirements of data acquired are a common factor. Data quality in CPS can deteriorate because of several factors such as sensor faults and failures due to operating in harsh and uncertain environments. How can software engineering and artificial intelligence (AI) help manage and tame data quality issues in CPS? This is the question we aimed to investigate in the SEA4DQ workshop. Emerging trends in software engineering need to take data quality management seriously as CPS are increasingly datacentric in their approach to acquiring and processing data along the edge-fog-cloud continuum. This workshop provided researchers and practitioners a forum for exchanging ideas, experiences, understanding of the problems, visions for the future, and promising solutions to the problems in data quality in CPS. Examples of topics include software/hardware architectures and frameworks for data quality management in CPS; software engineering and AI to detect anomalies in CPS data or to repair erroneous CPS data. SEA4DQ 2021, which took place on August 24th, 2021 was a satellite event of the ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC / FSE) 2021. The workshop attracted 35 international participants and was exciting with a great keynote, six excellent presentations, and concluded on a high note with a panel discussion. SEA4DQ was motivated by the common research interests from the EU projects for Zero-Defects Manufacturing such as InterQ and Dat4.Zero.|10.1145/3502771.3502781|https://doi.org/10.1145/3502771.3502781|New York, NY, USA|Association for Computing Machinery||2022|Software Engineering and AI for Data Quality in Cyber- Physical Systems - SEA4DQ'21 Workshop Report|Nguyen, Phu H. and Sen, Sagar and Jourdan, Nicolas and Cassoli, Beatriz and Myrseth, Per and Armendia, Mikel and Myklebust, Odd|article|10.1145/3502771.3502781||jan|SIGSOFT Softw. Eng. Notes|01635948|1|47|January 2022||||||||||||||||||||||860150166|1286430388
||Data science, Dashboards, E-government, Open government, Open data, Big data, Smart City, Design principles, Transparency, Accountability, Trust, Policy-making, Decision-making||101284||Dashboards visualize a consolidated set data for a certain purpose which enables users to see what is happening and to initiate actions. Dashboards can be used by governments to support their decision-making and policy processes or to communicate and interact with the public. The objective of this paper is to understand and to support the design of dashboards for creating transparency and accountability. Two smart city cases are investigated showing that dashboards can improve transparency and accountability, however, realizing these benefits was cumbersome and encountered various risks and challenges. Challenges include insufficient data quality, lack of understanding of data, poor analysis, wrong interpretation, confusion about the outcomes, and imposing a pre-defined view. These challenges can easily result in misconceptions, wrong decision-making, creating a blurred picture resulting in less transparency and accountability, and ultimately in even less trust in the government. Principles guiding the design of dashboards are presented. Dashboards need to be complemented by mechanisms supporting citizens' engagement, data interpretation, governance and institutional arrangements.|https://doi.org/10.1016/j.giq.2018.01.006|https://www.sciencedirect.com/science/article/pii/S0740624X18300303||||2020|Data science empowering the public: Data-driven dashboards for transparent and accountable decision-making in smart cities|Ricardo Matheus and Marijn Janssen and Devender Maheshwari|article|MATHEUS2020101284|||Government Information Quarterly|0740624X|3|37|||||14735|2,121|Q1|103|76|205|5894|1946|193|8,39|77,55|United Kingdom|Western Europe|1984-2020|E-learning (Q1); Law (Q1); Library and Information Sciences (Q1); Sociology and Political Science (Q1)|5,379|7.279|0.00502|861000734|1582933551
||||||Science is built on observations. If our observational data is bad, we are building a house on sand. Some of our data banks have quality measurements and maintenance, such as the National Climate Data Center and the National Center for Biotechnology Information; but others do not, and we do not even know which scientific data services have quality metrics or what they are.Data quality is an assertion about data properties, typically assumed within a context defined by a collection that holds the data. The assertion is made by the creator of the data. The collection context includes both metadata that describe provenance and representation information, and procedures that are able to parse and manipulate the data. However data quality from the perspective of users is defined based on the data properties that are required for use within their scientific research. The user believes data is of high quality when assertions about compliance can be shown to their research requirements.Digital data can accumulate rich contextual and derivative data as it is collected, analyzed, used, and reused, and planning for the management of this history requires new kinds of tools, techniques, standards, workflows, and attitudes. As science and industry recognize the need for digital curation, scientists and information professionals recognize that access and use of data depend on trust in the accuracy and veracity of data. In all data sets trust and reuse depend on accessible context and metadata that make explicit provenance, precision, and other traces of the datum and data life cycle. Poor data quality can be worse than missing data because it can waste resources and lead to faulty ideas and solutions, or at minimum challenges trust in the results and implications drawn from the data. Improvement in data quality can thus have significant benefits.|||USA|National Science Foundation||2012|Curating for Quality: Ensuring Data Quality to Enable New Science|Marchionini, Gary and Lee, Christopher A. and Bowden, Heather and Lesk, Michael|techreport|10.5555/2582001|||||||||||||||||||||||||||||863187782|42
||Malware;Big Data;Machine learning;Mobile applications;Genomics;Bioinformatics;Aerospace electronics;data profiling;data quality;big data;malware detection;mobile malware;machine learning;classification;Android;feature engineering||117-121|2018 International Congress on Big Data, Deep Learning and Fighting Cyber Terrorism (IBIGDELFT)|As the volume, variety, velocity aspects of big data are increasing, the other aspects such as veracity, value, variability, and venue could not be interpreted easily by data owners or researchers. The aspects are also unclear if the data is to be used in machine learning studies such as classification or clustering. This study proposes four techniques with fourteen criteria to systematically profile the datasets collected from different resources to distinguish from one another and see their strong and weak aspects. The proposed approach is demonstrated in five Android mobile malware datasets in the literature and in security industry namely Android Malware Genome Project, Drebin, Android Malware Dataset, Android Botnet, and Virus Total 2018. The results have shown that the proposed profiling methods reveal remarkable insight about the datasets comparatively and directs researchers to achieve big but more visible, qualitative, and internalized datasets.|10.1109/IBIGDELFT.2018.8625275|||||2018|New Techniques in Profiling Big Datasets for Machine Learning with a Concise Review of Android Mobile Malware Datasets|Canbek, Gürol and Sagiroglu, Seref and Taskaya Temizel, Tugba|inproceedings|8625275||Dec|||||||||||||||||||||||||||868658213|42
||||vii-ix|Meeting the Challenges of Data Quality Management||https://doi.org/10.1016/B978-0-12-821737-5.00016-X|https://www.sciencedirect.com/science/article/pii/B978012821737500016X||Academic Press|978-0-12-821737-5|2022|In praise of Meeting the Challenges of Data Quality Management||incollection|2022vii||||||||||Laura Sebastian-Coleman|||||||||||||||||||870122713|42
||Database systems;Data models;Organizations;Markov processes;Big data;Data quality;data timeliness;enterprise resource planning (ERP);Markov decision process;update policy||287-300||In order to maximize the value of an organization's data assets, it is important to keep data in its databases up-to-date. In the era of big data, however, constantly changing data sources make it a challenging task to assure data timeliness in enterprise systems. For instance, due to the high frequency of purchase transactions, purchase data stored in an enterprise resource planning system can easily become outdated, affecting the accuracy of inventory data and the quality of inventory replenishment decisions. Despite the importance of data timeliness, updating a database as soon as new data arrives is typically not optimal because of high update cost. Therefore, a critical problem in this context is to determine the optimal update policy for database systems. In this study, we develop a Markov decision process model, solved via dynamic programming, to derive the optimal update policy that minimizes the sum of data staleness cost and update cost. Based on real-world enterprise data, we conduct experiments to evaluate the performance of the proposed update policy in relation to benchmark policies analyzed in the prior literature. The experimental results show that the proposed update policy outperforms fixed interval update policies and can lead to significant cost savings.|10.1109/TEM.2017.2648516|||||2017|A Markov-Based Update Policy for Constantly Changing Database Systems|Zong, Wei and Wu, Feng and Jiang, Zhengrui|article|7835169||Aug|IEEE Transactions on Engineering Management|15580040|3|64|||||||||||||||||||||||871145434|2003339926
MEDES|Biarritz, France|data governance, data warehouses, data lab, digital transformation, data lakes, data reservoirs, data laboratory, internet of things|7|174–180|Proceedings of the 8th International Conference on Management of Digital EcoSystems|Data warehouses and data marts have long been considered as the unique solution for providing end-users with decisional information. More recently, data lakes have been proposed in order to govern data swamps. However, no formal definition has been proposed in the literature. Existing works are not complete and miss important parts of the topic. In particular, they do not focus on the influence of the data gravity, the infrastructure role of those solutions and of course are proposing divergent definitions and positioning regarding the usage and the interaction with existing decision support system.In this paper, we propose a novel definition of data lakes, together with a comparison with other over several criteria as the way to populate them, how to use, what is the Data Lake end user profile. We claim that data lakes are complementary components in decisional information systems and we discuss their position and interactions regarding the other components by proposing an interaction model.|10.1145/3012071.3012077|https://doi.org/10.1145/3012071.3012077|New York, NY, USA|Association for Computing Machinery|9781450342674|2016|The next Information Architecture Evolution: The Data Lake Wave|Madera, Cedrine and Laurent, Anne|inproceedings|10.1145/3012071.3012077|||||||||||||||||||||||||||||872361627|42
Data4U '14|Hangzhou, China|diverse data sources, data integration, Big data|4|25–28|"\"Proceedings of the First International Workshop on Bringing the Value of \"\"Big Data\"\" to Users (Data4U 2014)\""|"\"In an era where Big Data can greatly impact a broad population, many novel opportunities arise, chief among them the ability to integrate data from diverse sources and \"\"wrangle\"\" it to extract novel insights. Conceived as a tool that can help both expert and non-expert users better understand public data, MATTERS was collaboratively developed by the Massachusetts High Tech Council, WPI and other institutions as an analytic platform offering dynamic modeling capabilities. MATTERS is an integrative data source on high fidelity cost and talent competitiveness metrics. Its goal is to extract, integrate and model rich economic, financial, educational and technological information from renowned heterogeneous web data sources ranging from The US Census Bureau, The Bureau of Labor Statistics to the Institute of Education Sciences, all known to be critical factors influencing economic competitiveness of states. This demonstration of MATTERS illustrates how we tackle challenges of data acquisition, cleaning, integration and wrangling into appropriate representations, visualization and story-telling with data in the context of state competitiveness in the high-tech sector.\""|10.1145/2658840.2658845|https://doi.org/10.1145/2658840.2658845|New York, NY, USA|Association for Computing Machinery|9781450331869|2014|Taming Big Data: Integrating Diverse Public Data Sources for Economic Competitiveness Analytics|Neamtu, Rodica and Ahsan, Ramoza and Stokes, Jeff and Hoxha, Armend and Bao, Jialiang and Gvozdenovic, Stefan and Meyer, Ted and Patel, Nilesh and Rangan, Raghu and Wang, Yumou and Zhang, Dongyun and Rundensteiner, Elke A.|inproceedings|10.1145/2658840.2658845|||||||||||||||||||||||||||||876722070|42
||Spatial databases, Spatial access methods, Query optimization, Big Data, System architecture||159-171|Knowledge Discovery in Big Data from Astronomy and Earth Observation|In spite of their development in different communities, either astro-informatics or geo-informatics, data management and analytics of astronomical and geospatial data share the same characteristics, and raise the same challenges when it comes to access, query, or analysis of the spatial features over Big Data. The very first challenge is to deal with the data volume, which is tremendous in many geo and astro datasets. In this chapter, we highlight their main specificity and outline the main steps of query processing in big geospatial and astronomical data servers. Through the review of the state of the art, we show the advance in the topic of Big Data management in both contexts of geospatial and sky surveying, while highlighting their similarity. This progress notwithstanding, several issues remain to deal with the variety (such as multidimensional arrays) of the data.|https://doi.org/10.1016/B978-0-12-819154-5.00018-7|https://www.sciencedirect.com/science/article/pii/B9780128191545000187||Elsevier|978-0-12-819154-5|2020|Chapter 8 - Query Processing and Access Methods for Big Astro and Geo Databases|Karine Zeitouni and Mariem Brahem and Laurent Yeh and Atanas Hristov|incollection|ZEITOUNI2020159||||||||||Petr Škoda and Fathalrahman Adam|||||||||||||||||||876808102|42
||Big Data, granular computing, granularity, databases, rough set theory, database keyword search (DBKWS)||1331-1339||From very beginning, research and practice of database management systems (DBMSs) have been cantered on handling granulation and granularities at various levels, thus sharing common interests with granular computing (GrC). Although DBMS and GrC have different focuses, the advent of Big Data has brought these two research areas closer to each other, because Big Data requires integrated study of data storage and analysis. In this paper, we explore this issue. Starting with an examination of granularities from a database perspective, we discuss new challenges of Big Data. We then turn to data management issues related to GrC. As an example of possible cross-fertilization of these two fields, we examine the recent development of database keyword search (DBKWS). Even research in DBKWS is largely independent to GrC, DBKWS has to handle various issues related to granularity handling. In particular, aggregation of DBKWS results is closely related to studies in granularities and granulation, which echoes L. Zadeh's famous formula: Granulation = Summarization. We present our proposed approach, termed as extended keyword search, which illustrates that an integrated study of data management and data mining/analysis is not restricted to GrC or rough set theory|https://doi.org/10.1016/j.procs.2015.07.117|https://www.sciencedirect.com/science/article/pii/S1877050915015926||||2015|Towards Integrated Study of Data Management and Data Mining|Zhengxin Chen|article|CHEN20151331|||Procedia Computer Science|18770509||55||3rd International Conference on Information Technology and Quantitative Management, ITQM 2015|||19700182801|0,334|-|76|1907|6483|38511|13722|6359|2,09|20,19|Netherlands|Western Europe|2010-2020|Computer Science (miscellaneous)||||877067418|2108686752
||Streams of big data, Process efficiency, Product effectiveness, Competitive advantage||103451||Firms can achieve a competitive advantage by leveraging real-time Digital Data Streams (DDSs). The ability to profit from DDSs is emerging as a critical competency for firms and a novel area for Information Technology (IT) investments. We examine the relationship between DDS readiness and competitive advantage by studying the mediation effect of product effectiveness and process efficiency. The research model is tested with data obtained from 302 companies, and the results confirm the existence of the mediation effects. Interestingly, we confirm that competitive advantage is more significantly impacted by IT investments affecting product effectiveness than those affecting process efficiency.|https://doi.org/10.1016/j.im.2021.103451|https://www.sciencedirect.com/science/article/pii/S0378720621000252||||2021|Streams of digital data and competitive advantage: The mediation effects of process efficiency and product effectiveness|Elisabetta Raguseo and Federico Pigni and Claudio Vitari|article|RAGUSEO2021103451|||Information & Management|03787206|4|58|||||12303|2,147|Q1|162|130|248|11385|2482|246|8,94|87,58|Netherlands|Western Europe|1977-2020|Information Systems (Q1); Information Systems and Management (Q1); Management Information Systems (Q1)||||879026073|1945939487
||Data integration;Standards;Task analysis;Databases;Google;Data mining;Terminology;Record normalization;data quality;data fusion;web data integration;deep web||769-782||Data consolidation is a challenging issue in data integration. The usefulness of data increases when it is linked and fused with other data from numerous (Web) sources. The promise of Big Data hinges upon addressing several big data integration challenges, such as record linkage at scale, real-time data fusion, and integrating Deep Web. Although much work has been conducted on these problems, there is limited work on creating a uniform, standard record from a group of records corresponding to the same real-world entity. We refer to this task as record normalization. Such a record representation, coined normalized record, is important for both front-end and back-end applications. In this paper, we formalize the record normalization problem, present in-depth analysis of normalization granularity levels (e.g., record, field, and value-component) and of normalization forms (e.g., typical versus complete). We propose a comprehensive framework for computing the normalized record. The proposed framework includes a suit of record normalization methods, from naive ones, which use only the information gathered from records themselves, to complex strategies, which globally mine a group of duplicate records before selecting a value for an attribute of a normalized record. We conducted extensive empirical studies with all the proposed methods. We indicate the weaknesses and strengths of each of them and recommend the ones to be used in practice.|10.1109/TKDE.2018.2844176|||||2019|Normalization of Duplicate Records from Multiple Sources|Dong, Yongquan and Dragut, Eduard C. and Meng, Weiyi|article|8372637||April|IEEE Transactions on Knowledge and Data Engineering|15582191|4|31|||||||||||||||||||||||883455022|1598944404
||||164-172||Nowadays, social network data of ever increasing size is gathered, stored and analyzed by researchers from a range of disciplines. This data is often automatically gathered from API’s, websites or existing databases. As a result, the quality of this data is typically not manually validated, and the resulting social networks may be based on false, biased or incomplete data. In this paper, we investigate the effect of data quality issues on the analysis of large networks. We focus on the global board interlock network, in which nodes represent firms across the globe, and edges model social ties between firms – shared board members holding a position at both firms. First, we demonstrate how we can automatically assess the completeness of a large dataset of 160 million firms, in which data is missing not at random. Second, we present a novel method to increase the accuracy of the entries in our data. By comparing the expected and empirical characteristics of the resulting network topology, we develop a technique that automatically prunes and merges duplicate nodes and edges. Third, we use a case study of the board interlock network of Sweden to show how poor quality data results in distorted network topologies, incorrect community division, biased centrality values and abnormal influence spread under a well-known diffusion model. Finally, we demonstrate how the proposed data quality assessment methods help restore the network structure, ultimately allowing us to derive meaningful and correct results from the analysis of the network.|https://doi.org/10.1016/j.is.2017.10.005|https://www.sciencedirect.com/science/article/pii/S0306437917302272||||2018|The effects of data quality on the analysis of corporate board interlock networks|Javier Garcia-Bernardo and Frank W. Takes|article|GARCIABERNARDO2018164|||Information Systems|03064379||78|||||12305|0,547|Q2|85|100|271|4739|1027|255|3,39|47,39|United Kingdom|Western Europe|1975-2021|Hardware and Architecture (Q2); Information Systems (Q2); Software (Q2)|2,604|2.309|0.00331|884239266|303930735
||Data Warehousing, ETL, Parallel and Distributed Processing, Big Data, MapReduce||114-136||Among the so-called “4Vs” (volume, velocity, variety, and veracity) that characterize the complexity of Big Data, this paper focuses on the issue of “Volume” in order to ensure good performance for Extracting-Transforming-Loading (ETL) processes. In this study, we propose a new fine-grained parallelization/distribution approach for populating the Data Warehouse (DW). Unlike prior approaches that distribute the ETL only at coarse-grained level of processing, our approach provides different ways of parallelization/distribution both at process, functionality and elementary functions levels. In our approach, an ETL process is described in terms of its core functionalities which can run on a cluster of computers according to the MapReduce (MR) paradigm. The novel approach allows thereby the distribution of the ETL process at three levels: the “process” level for coarse-grained distribution and the “functionality” and “elementary functions” levels for fine-grained distribution. Our performance analysis reveals that employing 25 to 38 parallel tasks enables the novel approach to speed up the ETL process by up to 33% with the improvement rate being linear.|https://doi.org/10.1016/j.datak.2017.08.003|https://www.sciencedirect.com/science/article/pii/S0169023X16300611||||2017|A Fine‐Grained Distribution Approach for ETL Processes in Big Data Environments|Mahfoud Bala and Omar Boussaid and Zaia Alimazighi|article|BALA2017114|||Data & Knowledge Engineering|0169023X||111|||||24437|0,480|Q2|87|37|155|1512|440|152|2,55|40,86|Netherlands|Western Europe|1985, 1987-2020|Information Systems and Management (Q2)||||888882172|1516868485
||Big Data, Data deluge, Decision making, Data analysis, Data-intensive applications, Computational social science||747-765||The era of Big Data has arrived along with large volume, complex and growing data generated by many distinct sources. Nowadays, nearly every aspect of the modern society is impacted by Big Data, involving medical, health care, business, management and government. It has been receiving growing attention of researches from many disciplines including natural sciences, life sciences, engineering and even art & humanities. It also leads to new research paradigms and ways of thinking on the path of development. Lots of developed and under-developing tools improve our ability to make more felicitous decisions than what we have made ever before. This paper presents an overview on Big Data including four issues, namely: (i) concepts, characteristics and processing paradigms of Big Data; (ii) the state-of-the-art techniques for decision making in Big Data; (iii) felicitous decision making applications of Big Data in social science; and (iv) the current challenges of Big Data as well as possible future directions.|https://doi.org/10.1016/j.ins.2016.07.007|https://www.sciencedirect.com/science/article/pii/S0020025516304868||||2016|Towards felicitous decision making: An overview on challenges and trends of Big Data|Hai Wang and Zeshui Xu and Hamido Fujita and Shousheng Liu|article|WANG2016747|||Information Sciences|00200255||367-368|||||15134|1,524|Q1|184|928|2184|40007|17554|2168|7,89|43,11|United States|Northern America|1968-2021|Artificial Intelligence (Q1); Computer Science Applications (Q1); Control and Systems Engineering (Q1); Information Systems and Management (Q1); Software (Q1); Theoretical Computer Science (Q1)|44,038|6.795|0.04908|892013357|1633962588
dg.o '17|Staten Island, NY, USA|Decision-making, Evaluation, Collaborative Governance, Data Governance|2|564–565|Proceedings of the 18th Annual International Conference on Digital Government Research|This paper presents the preliminary framework proposed by the authors for drivers of Smart Governance. The research question of this study is: What are the drivers for Smart Governance to achieve evidence-based policy-making? The framework suggests that in order to create a smart governance model, data governance and collaborative governance are the main drivers. These pillars are supported by legal framework, normative factors, principles and values, methods, data assets or human resources, and IT infrastructure. These aspects will guide a real time evaluation process in all levels of the policy cycle, towards to the implementation of evidence-based policies.|10.1145/3085228.3085255|https://doi.org/10.1145/3085228.3085255|New York, NY, USA|Association for Computing Machinery|9781450353175|2017|Drivers of Smart Governance: Towards to Evidence-Based Policy-Making|Parycek, P. and Pereira, G. Viale|inproceedings|10.1145/3085228.3085255|||||||||||||||||||||||||||||892023106|42
||COVID-19 review, AU methods for COVID-19, Machine learning for COVID-19||21-66|Novel AI and Data Science Advancements for Sustainability in the Era of COVID-19|This book chapter presents the review of COVID-19 and its status, as well as the scientific Analysis big data analytics with the help of machine learning. We provide in-depth literature review, and provide a summary of the current AI and machine learning methods, which have become increasingly important to provide accurate analyses. Various conceptual diagrams have been used to illustrate how different technologies can contribute to effective analyses for COVID-19. We demonstrate our work from both theoretical contributions and practical implementations.|https://doi.org/10.1016/B978-0-323-90054-6.00007-6|https://www.sciencedirect.com/science/article/pii/B9780323900546000076||Academic Press|978-0-323-90054-6|2022|Chapter 2 - Investigation of COVID-19 and scientific analysis big data analytics with the help of machine learning|Victor Chang and Mohamed Aleem Ali and Alamgir Hossain|incollection|CHANG202221||||||||||Victor Chang and Mohamed Abdel-Basset and Muthu Ramachandran and Nicolas G. Green and Gary Wills|||||||||||||||||||897954404|42
||Big data, IoT data management, Disclosure risk, HIPAA, Patient privacy, Re-identification risk, Smart city||326-334||Smart City and IoT improves the performance of health, transportation, energy and reduce the consumption of resources. Among the smart city services, Big Data analytics is one of the imperative technologies that have a vast perspective to reach sustainability, enhanced resilience, effective quality of life and quick management of resources. This paper focuses on the privacy of big data in the context of smart health to support smart cities. Furthermore, the trade-off between the data privacy and utility in big data analytics is the foremost concern for the stakeholders of a smart city. The majority of smart city application databases focus on preserving the privacy of individuals with different disease data. In this paper, we propose a trust-based hybrid data privacy approach named as “MIDR-Angelization” to assure privacy and utility in big data analytics when sharing same disease data of patients in IoT industry. Above all, this study suggests that privacy-preserving policies and practices to share disease and health information of patients having the same disease should consider detailed disease information to enhance data utility. An extensive experimental study performed on a real-world dataset to measure instance disclosure risk which shows that the proposed scheme outperforms its counterpart in terms of data utility and privacy.|https://doi.org/10.1016/j.scs.2018.04.014|https://www.sciencedirect.com/science/article/pii/S2210670717314646||||2018|Privacy preserving data by conceptualizing smart cities using MIDR-Angelization|Adeel Anjum and Tahir Ahmed and Abid Khan and Naveed Ahmad and Mansoor Ahmad and Muhammad Asif and Alavalapati Goutham Reddy and Tanzila Saba and Nayma Farooq|article|ANJUM2018326|||Sustainable Cities and Society|22106707||40|||||19700194105|1,645|Q1|61|705|1286|43818|10974|1284|8,53|62,15|Netherlands|Western Europe|2011-2020|Civil and Structural Engineering (Q1); Geography, Planning and Development (Q1); Renewable Energy, Sustainability and the Environment (Q1); Transportation (Q1)|14,373|7.587|0.01684|898403167|1912866754
DEBS '20|Montreal, Quebec, Canada|cluster-sending, permissioned blockchains, sharding, geo-scale, resilient transaction processing, byzantine learning, consensus|4|218–221|Proceedings of the 14th ACM International Conference on Distributed and Event-Based Systems|Since the introduction of Bitcoin---the first wide-spread application driven by blockchains---the interest of the public and private sector in blockchains has skyrocketed. At the core of this interest are the ways in which blockchains can be used to improve data management, e.g., by enabling federated data management via decentralization, resilience against failure and malicious actors via replication and consensus, and strong data provenance via a secured immutable ledger.In practice, high-performance blockchains for data management are usually built in permissioned environments in which the participants are vetted and can be identified. In this setting, blockchains are typically powered by Byzantine fault-tolerant consensus protocols. These consensus protocols are used to provide full replication among all honest blockchain participants by enforcing an unique order of processing incoming requests among the participants.In this tutorial, we take an in-depth look at Byzantine fault-tolerant consensus. First, we take a look at the theory behind replicated computing and consensus. Then, we delve into how common consensus protocols operate. Finally, we take a look at current developments and briefly look at our vision moving forward.|10.1145/3401025.3404099|https://doi.org/10.1145/3401025.3404099|New York, NY, USA|Association for Computing Machinery|9781450380287|2020|Blockchain Consensus Unraveled: Virtues and Limitations|Gupta, Suyash and Hellings, Jelle and Rahnama, Sajjad and Sadoghi, Mohammad|inproceedings|10.1145/3401025.3404099|||||||||||||||||||||||||||||898810253|42
||5G mobile communication;Batteries;Wireless communication;Big Data;Machine learning;Optimization;big data;machine learning;deep reinforcement learning;radio resource management;UAV;energy efficiency;XAI||1-7|ICC 2020 - 2020 IEEE International Conference on Communications (ICC)|UAV enabled terrestrial wireless networks enables targeted user-centric service provisioning to en-richen both deep urban coverage and target various rural challenge areas. However, UAVs have to balance the energy consumption of flight with the benefits of wireless capacity delivery via a high dimensional optimisation problem. Classic reinforcement learning (RL) cannot meet this challenge and here, we propose to use deep reinforcement learning (DRL) to optimise both aggregate and minimum service provisioning. In order to achieve a trusted autonomy, the DRL agents have to be able to explain its actions for transparent human-machine interrogation. We design a Double Dueling Deep Q-learning Neural Network (DDDQN) with Prioritised Experience Replay (PER) and fixed Q-targets to achieve stable performance and avoid over-fitting, offering performance gains over naive DQN algorithms. We then use a big data driven case study and found that UAVs battery size determines the nature of its autonomous mission, ranging from an efficient exploiter of one hotspot (100% reward gain) to a stochastic explorer of many hotspots (60-150% reward gain). Using a variety of telecom and social media data, we infer driving Quality-of-Experience (QoE) and Quality-of-Service (QoS) metrics that are in contention with UAV power and communication constraints. Our greener UAVs (30-40% energy saved) address both quantitative QoS and qualitative QoE issues. Partial interpretability in the reinforcement learning is achieved using data features extracted in the hidden layers, offering an initial step for explainable AI (XAI) connecting machine intelligence with human expertise.|10.1109/ICC40277.2020.9149151|||||2020|Partially Explainable Big Data Driven Deep Reinforcement Learning for Green 5G UAV|Guo, Weisi|inproceedings|9149151||June||19381883|||||||||||||||||||||||||908660890|276554849
Methods in Chemical Process Safety||Data-driven models, Process safety, Data preparation, Data cleaning, Statistical models, Artificial intelligence models, Process monitoring, Fault detection and diagnosis, Fault prognosis, Video monitoring||61-99|Methods to Assess and Manage Process Safety in Digitalized Process System|Process safety is playing an important role with the rapid development of industry. With the advent of the Big Data era, various and massive data from the Internet of Things can be used for process safety. In this chapter, we aim to provide the reader with a comprehensive understanding of rapidly growing data-driven process safety approaches in the chemical industry. Data-driven approaches primarily use past process data without a complex mechanism model of chemical properties or processes; hence, they have advantages in practical industrial applications. In this chapter, first, we describe the importance of data in process safety. Then, we briefly introduce the ideas and methods of data pre-processing. We follow this with a discussion on statistical-based and artificial intelligence-based data-driven approaches. Then, we elaborate on the application of data-driven methods in the field of chemical process safety. Finally, we provide a summary and outlook for advancing data-driven methods.|https://doi.org/10.1016/bs.mcps.2022.04.002|https://www.sciencedirect.com/science/article/pii/S2468651422000022||Elsevier||2022|Chapter Three - Data-driven approaches: Use of digitized operational data in process safety|Yiming Bai and Shuaiyu Xiang and Zeheng Zhao and Borui Yang and Jinsong Zhao|incollection|BAI202261||||24686514||6||||Faisal Khan and Hans Pasman and Ming Yang|||||||||||||||||||912749833|1900868363
ACM ICEA '21|Jinan, China|CNN, Attention Network, Ocean Data, Online Learning, Time Series Prediction|6|170–175|Proceedings of the 2021 ACM International Conference on Intelligent Computing and Its Emerging Applications|Despite been extensively explored, current techniques in sequential data modeling and prediction are generally designed for solving regression tasks in a batch learning setting, making them not only computationally inefficient but also poorly scalable in real-world applications, especially for real-time intelligent ocean data quality control (QC), where the data arrives sequentially and the QC should be conducted in real time. This paper investigates the online learning for ocean data streams by resolving two main challenges: (i) how to develop a deep learning model to capture the complex ocean data distribution that could evolve dynamically, namely tackling the 'concept drift' problem for non-stationary time series; (ii) how to develop a deep learning model that can dynamically adapt its structure from shallow to deep with the inflow of the data to overcome under-fitting problem, namely tackling the 'model selection' problem. To tackle these challenges, we propose one Evolutive Convolutional Neural Network (ECNN) that dynamically re-weighting the sub-structure of the model from data streams in a sequential or online learning fashion, by which the capacity scalability and sustainability are introduced into the model. The experiments on real ocean observation data verify the effectiveness of our model. As far as we know, it is the first work that introduce online deep learning techniques into ocean data prediction research.|10.1145/3491396.3506519|https://doi.org/10.1145/3491396.3506519|New York, NY, USA|Association for Computing Machinery|9781450391603|2022|ECNN: One Online Deep Learning Model for Streaming Ocean Data Prediction|Li, Xiang and Zhang, Zhaoqian and Zhao, Zhigang and Wu, Lu and Huo, Jidong and Zhang, Jian and Wang, Yinglong|inproceedings|10.1145/3491396.3506519|||||||||||||||||||||||||||||913032365|42
||cyber-physical-systems, dependable time series analytics, IoT-based smart grid, sensor-network-regularization-based matrix factorization|18|||With the emergence of cyber-physical systems (CPS), we are now at the brink of next computing revolution. The Smart Grid (SG) built on top of IoT (Internet of Things) is one of the foundations of this CPS revolution, which involves a large number of smart objects connected by networks. The volume of time series of SG equipment is tremendous and the raw time series are very likely to contain missing values because of undependable network transferring. The problem of storing a tremendous volume of raw time series thereby providing a solid support for precise time series analytics now becomes tricky. In this article, we propose a dependable time series analytics (DTSA) framework for IoT-based SG. Our proposed DTSA framework is capable of providing a dependable data transforming from CPS to the target database with an extraction engine to preliminary refining raw data and further cleansing the data with a correction engine built on top of a sensor-network-regularization-based matrix factorization method. The experimental results reveal that our proposed DTSA framework is capable of effectively increasing the dependability of raw time series transforming between CPS and the target database system through the online lightweight extraction engine and the offline correction engine. Our proposed DTSA framework would be useful for other industrial big data practices.|10.1145/3145623|https://doi.org/10.1145/3145623|New York, NY, USA|Association for Computing Machinery||2018|A Dependable Time Series Analytic Framework for Cyber-Physical Systems of IoT-Based Smart Grid|Wang, Chang and Zhu, Yongxin and Shi, Weiwei and Chang, Victor and Vijayakumar, P. and Liu, Bin and Mao, Yishu and Wang, Jiabao and Fan, Yiping|article|10.1145/3145623|7|aug|ACM Trans. Cyber-Phys. Syst.|2378962X|1|3|January 2019||||||||||||||||||||||916988501|1535435511
||Facebook;Roads;Big Data;Electronic mail;LinkedIn;Measurement;Social network;graph sampler;data quality analysis;optimization||90-104||Because Online Social Networks (OSNs) have become increasingly important in the last decade, they have motivated a great deal of research on Social Network Analysis (SNA). Currently, SNA algorithms are evaluated on real datasets obtained from large-scale OSNs, which are usually sampled by Breadth-First-Search (BFS), Random Walk (RW), or some variations of the latter. However, none of the released datasets provides any statistical guarantees on the difference between the sampled datasets and the ground truth. Moreover, all existing sampling algorithms only focus on sampling a single OSN, but each OSN is actually a sampling of a complete social network. Hence, even if the whole dataset from a single OSN is sampled, the results may still be skewed and may not fully reflect the properties of the complete social network. To address the above issues, we have made the first attempt to explore the joint sampling of multiple OSNs and propose an approach called Quality-guaranteed Multi-network Sampler (QMSampler) that can jointly sample multiple OSNs. QMSampler provides a statistical guarantee on the difference between the sampled real dataset and the ground truth (the perfect integration of all OSNs). Our experimental results demonstrate that the proposed approach generates a much smaller bias than any existing method. QMSampler has also been released as a free download.|10.1109/TBDATA.2017.2715847|||||2018|QMSampler: Joint Sampling of Multiple Networks with Quality Guarantee|Shuai, Hong-Han and Yang, De-Nian and Shen, Chih-Ya and Yu, Philip S. and Chen, Ming-Syan|article|7953577||March|IEEE Transactions on Big Data|23327790|1|4|||||21101019393|0,959|Q1|6|71|9|998|37|8|4,11|14,06|United States|Northern America|2020|Information Systems (Q1); Information Systems and Management (Q1)|636|3.344|0.00134|921488840|1510992500
||revenue management, big data, Airline|35|||From the start, the airline industry has remarkably connected countries all over the world through rapid long-distance transportation, helping people overcome geographic barriers. Consequently, this has ushered in substantial economic growth, both nationally and internationally. The airline industry produces vast amounts of data, capturing a diverse set of information about their operations, including data related to passengers, freight, flights, and much more. Analyzing air travel data can advance the understanding of airline market dynamics, allowing companies to provide customized, efficient, and safe transportation services. Due to big data challenges in such a complex environment, the benefits of drawing insights from the air travel data in the airline industry have not yet been fully explored. This article aims to survey various components and corresponding proposed data analysis methodologies that have been identified as essential to the inner workings of the airline industry. We introduce existing data sources commonly used in the papers surveyed and summarize their availability. Finally, we discuss several potential research directions to better harness airline data in the future. We anticipate this study to be used as a comprehensive reference for both members of the airline industry and academic scholars with an interest in airline research.|10.1145/3469028|https://doi.org/10.1145/3469028|New York, NY, USA|Association for Computing Machinery||2021|Data Analytics for Air Travel Data: A Survey and New Perspectives|Tian, Haiman and Presa-Reyes, Maria and Tao, Yudong and Wang, Tianyi and Pouyanfar, Samira and Miguel, Alonso and Luis, Steven and Shyu, Mei-Ling and Chen, Shu-Ching and Iyengar, Sundaraja Sitharama|article|10.1145/3469028|167|oct|ACM Comput. Surv.|03600300|8|54|November 2022||||23038|2,079|Q1|163|112|365|15859|6978|365|19,16|141,60|United States|Northern America|1969-2020|Computer Science (miscellaneous) (Q1); Theoretical Computer Science (Q1)|10,790|10.282|0.01438|921514053|1517405264
||Roads;Crowdsourcing;Sensors;Estimation;Data models;Data integrity;Three-dimensional displays;Mobile crowdsourcing;road crack detection;image processing;sensors||944-951|2019 IEEE SmartWorld, Ubiquitous Intelligence & Computing, Advanced & Trusted Computing, Scalable Computing & Communications, Cloud & Big Data Computing, Internet of People and Smart City Innovation (SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI)|As a common road surface distress, cracks pose a serious threat to road infrastructure and traffic safety in cities today. Consequently, road crack detection is considered as an essential step for effective road maintenance and road structure sustainability. However, due to the high cost incurred by dedicated devices and professional operators, it is impossible for existing systems to achieve universal spatiotemporal coverage across citywide road networks. To fill this gap, in this paper, we present the CrackSense, a mobile crowdsourcing based system to detect urban road crack and estimate its damage degree. Specifically, for the heterogeneous crack data, we put forward a crowdsourcing data quality evaluation and selection mechanism. And then, by utilizing the multi-source sensing data aggregation, we propose tow algorithms, namely RCTR and RCDE, to recognize road crack types, i.e., horizontal crack, vertical crack, and net crack, and estimate the crack damage degree, respectively. We implement the system and develop a smartphone APP for mobile users. By conducting intensive experiments and field study, the results demonstrate the accuracy and effectiveness of our proposed approaches.|10.1109/SmartWorld-UIC-ATC-SCALCOM-IOP-SCI.2019.00188|||||2019|CrackSense: A CrowdSourcing Based Urban Road Crack Detection System|Wang, Liang and Yang, Congying and Yu, Zhiwen and Liu, Yimeng and Wang, Zhu and Guo, Bin|inproceedings|9060131||Aug|||||||||||||||||||||||||||925854622|42
PDC '20|Manizales, Colombia|Artificial Intelligence, Electronic Health Record data, precision medicine, Participatory Design, Primary- and Secondary Use data|4|26–29|Proceedings of the 16th Participatory Design Conference 2020 - Participation(s) Otherwise - Volume 2|In its promise to contribute to considerable cost savings and improved patient care through efficient analysis of the tremendous amount of data stored in electronic health records (EHR), there is currently a strong push for the proliferation of artificial intelligence (AI) in health-care. We identify, through a study of AI being used to predict patient no-show’s, that for the AI to gain full potential there lies a need to balance the introduction of AI with a proper focus on the patients and the clinicians’ interests. We call for a Participatory Design (PD) approach to understand and reconfigure the socio-technical setup in health-care, especially where AI is being used on EHR data that are manually being submitted by health-care personnel.|10.1145/3384772.3385138|https://doi.org/10.1145/3384772.3385138|New York, NY, USA|Association for Computing Machinery|9781450376068|2020|PD and The Challenge of AI in Health-Care|H. Gyldenkaerne, Christopher and From, Gustav and M\o{}nsted, Troels and Simonsen, Jesper|inproceedings|10.1145/3384772.3385138|||||||||||||||||||||||||||||926193427|42
|||13|1669–1681||"Pointwise order dependencies (PODs) are dependencies that specify ordering semantics on attributes of tuples. POD discovery refers to the process of identifying the set Σ of valid and minimal PODs on a given data set D. In practice D is typically large and keeps changing, and it is prohibitively expensive to compute Σ from scratch every time. In this paper, we make a first effort to study the incremental POD discovery problem, aiming at computing changes ΔΣ to Σ such that Σ ⊕ ΔΣ is the set of valid and minimal PODs on D with a set ΔD of tuple insertion updates. (1) We first propose a novel indexing technique for inputs Σ and D. We give algorithms to build and choose indexes for Σ and D, and to update indexes in response to ΔD. We show that POD violations w.r.t. Σ incurred by ΔD can be efficiently identified by leveraging the proposed indexes, with a cost dependent on log(|D|). (2) We then present an effective algorithm for computing ΔΣ, based on Σ and identified violations caused by ΔD. The PODs in Σ that become invalid on D + ΔD are efficiently detected with the proposed indexes, and further new valid PODs on D + ΔD are identified by refining those invalid PODs in Σ on D + ΔD. (3) Finally, using both real-life and synthetic datasets, we experimentally show that our approach outperforms the batch approach that computes from scratch, up to orders of magnitude."|10.14778/3401960.3401965|https://doi.org/10.14778/3401960.3401965||VLDB Endowment||2021|Fast Incremental Discovery of Pointwise Order Dependencies|Tan, Zijing and Ran, Ai and Ma, Shuai and Qin, Sheng|article|10.14778/3401960.3401965||mar|Proc. VLDB Endow.|21508097|10|13|June 2020||||21100199855|0,946|Q1|134|246|598|12401|3759|566|5,50|50,41|United States|Northern America|2008-2020|Computer Science (miscellaneous) (Q1)|7,198|2.047|0.00891|927114212|1216159931
||Linked Open Data, Ontology, Semantic Technologies, News Production, News Consumption, Knowledge Graphs, News, News Distribution, Journalism, Semantic Web, Literature Review, Linked Data||||ICT platforms for news production, distribution, and consumption must exploit the ever-growing availability of digital data. These data originate from different sources and in different formats; they arrive at different velocities and in different volumes. Semantic knowledge graphs (KGs) is an established technique for integrating such heterogeneous information. It is therefore well-aligned with the needs of news producers and distributors, and it is likely to become increasingly important for the news industry. This paper reviews the research on using semantic knowledge graphs for production, distribution, and consumption of news. The purpose is to present an overview of the field; to investigate what it means; and to suggest opportunities and needs for further research and development.|10.1145/3543508|https://doi.org/10.1145/3543508|New York, NY, USA|Association for Computing Machinery||2022|Semantic Knowledge Graphs for the News: A Review|Opdahl, Andreas L. and Al-Moslmi, Tareq and Dang-Nguyen, Duc-Tien and Gallofr\'{e} Oca\~{n}a, Marc and Tessem, Bj\o{}rnar and Veres, Csaba|article|10.1145/3543508||jun|ACM Comput. Surv.|03600300||||Just Accepted|||23038|2,079|Q1|163|112|365|15859|6978|365|19,16|141,60|United States|Northern America|1969-2020|Computer Science (miscellaneous) (Q1); Theoretical Computer Science (Q1)|10,790|10.282|0.01438|929043024|1517405264
||law, security research ethics|7|4–10||"\"With research using data available online, researcher conduct is not fully prescribed or proscribed by formal ethical codes of conduct or law because of ill-fitting \"\"expectations signals\"\" -- indicators of legal and ethical risk. This article describes where these ordering forces breakdown in the context of online research and suggests how to identify and respond to these grey areas by applying common legal and ethical tenets that run across evolving models. It is intended to advance the collective dialogue work-in-progress toward a path that revisits and harmonizes more appropriate ethical and legal signals for research using online data between and among researchers, oversight entities, policymakers and society.\""|10.1145/2738210.2738211|https://doi.org/10.1145/2738210.2738211|New York, NY, USA|Association for Computing Machinery||2015|How to Throw the Race to the Bottom: Revisiting Signals for Ethical and Legal Research Using Online Data|Kenneally, Erin|article|10.1145/2738210.2738211||feb|SIGCAS Comput. Soc.|00952737|1|45|February 2015||||||||||||||||||||||934374152|1394136391
|||5|50–54||New user interfaces can transform how we work with big data, and raise exciting research problems that span human-computer interaction, machine learning, and distributed systems.|10.1145/2331042.2331058|https://doi.org/10.1145/2331042.2331058|New York, NY, USA|Association for Computing Machinery||2012|Interactive Analysis of Big Data|Heer, Jeffrey and Kandel, Sean|article|10.1145/2331042.2331058||sep|XRDS|15284972|1|19|Fall 2012||||||||||||||||||||||936675600|613180419
||Ontologies, Data engineering, Software engineering, Alignment, Integration||476-484||In this paper we present a collection of ontologies specifically designed to model the information exchange needs of combined software and data engineering. Effective, collaborative integration of software and big data engineering for Web-scale systems, is now a crucial technical and economic challenge. This requires new combined data and software engineering processes and tools. Our proposed models have been deployed to enable: tool-chain integration, such as the exchange of data quality reports; cross-domain communication, such as interlinked data and software unit testing; mediation of the system design process through the capture of design intents and as a source of context for model-driven software engineering processes. These ontologies are deployed in web-scale, data-intensive, system development environments in both the commercial and academic domains. We exemplify the usage of the suite on case-studies emerging from two complex collaborative software and data engineering scenarios: one from the legal sector and the other from the Social sciences and Humanities domain.|https://doi.org/10.1016/j.jss.2018.12.017|https://www.sciencedirect.com/science/article/pii/S0164121218302772||||2019|Towards a knowledge driven framework for bridging the gap between software and data engineering|Monika Solanki and Bojan Božić and Christian Dirschl and Rob Brennan|article|SOLANKI2019476|||Journal of Systems and Software|01641212||149|||||19309|0,642|Q1|109|183|619|11845|3058|590|4,94|64,73|United States|Northern America|1979, 1981-2021|Hardware and Architecture (Q1); Information Systems (Q2); Software (Q2)|6,579|2.829|0.00727|938198565|1111852116
||Transport policy, Track and Trace, Mobile phone data, Mobility profile, Big Data||102672||High quality, reliable data and robust models are central to the development and appraisal of transportation planning and policy. Although conventional data may offer good ‘content’, it is widely observed that it lacks context i.e. who and why people are travelling. Transportation modelling has developed within these boundaries, with implications for the planning, design and management of transportation systems and policy-making. This paper establishes the potential of passively collected GPS-based “Track & Trace” (T&T) datasets of individual mobility profiles towards enhancing transportation modelling and policy-making. T&T is a type of New and Emerging Data Form (NEDF), lying within the broader ‘Big Data’ paradigm, and is typically collected using mobile phone sensors and related technologies. These capture highly grained mobility content and can be linked to the phone owner/user behavioural choices and other individual context. Our meta-analysis of existing literature related to spatio-temporal mobile phone data demonstrates that NEDF’s, and in particular T&T data, have had little mention to date within an applied transportation planning and policy context. We thus establish there is an opportunity for policy-makers, transportation modellers, researchers and a wide range of stakeholders to collaborate in developing new analytic approaches, revise existing models and build the skills and related capacity needed to lever greatest value from the data, as well as to adopt new business models that could revolutionise citizen participation in policy-making. This is of particular importance due to the growing awareness in many countries for a need to develop and monitor efficient cross-sectoral policies to deliver sustainable communities.|https://doi.org/10.1016/j.trc.2020.102672|https://www.sciencedirect.com/science/article/pii/S0968090X20305878||||2020|New and emerging data forms in transportation planning and policy: Opportunities and challenges for “Track and Trace” data|Gillian Harrison and Susan M. Grant-Muller and Frances C. Hodgson|article|HARRISON2020102672|||Transportation Research Part C: Emerging Technologies|0968090X||117|||||20893|3,185|Q1|133|327|834|17795|8956|824|10,15|54,42|United Kingdom|Western Europe|1993-2020|Automotive Engineering (Q1); Civil and Structural Engineering (Q1); Computer Science Applications (Q1); Management Science and Operations Research (Q1); Transportation (Q1)||||944519165|307592163
||Data analysis;Manufacturing;Sensors;Encryption;NIST;Synchronization;big data;challenge problems;data analytics;data quality;requirements;smart manufacturing||68-75|2015 IEEE International Conference on Big Data (Big Data)|Data analytics is increasingly becoming recognized as a valuable set of tools and techniques for improving performance in the manufacturing enterprise. However, data analytics requires data and a lack of useful and usable data has become an impediment to research in data analytics. In this paper, we describe issues that would help aid data availability including data quality, reliability, efficiency, and formats specific to data analytics in manufacturing. To encourage data availability, we present recommendations and requirements to guide future data contributions. We also describe the need for data for challenge problems in data analytics. A better understanding of these needs, recommendations, and requirements may improve the ability of researchers and other practitioners to improve research and more rapidly deploy data analytics in manufacturing.|10.1109/BigData.2015.7363743|||||2015|Considerations and recommendations for data availability for data analytics for manufacturing|Libes, Don and Shin, Seungjun and Woo, Jungyub|inproceedings|7363743||Oct|||||||||||||||||||||||||||944967658|42
UbiComp '13 Adjunct|Zurich, Switzerland|heterogeneity, representativeness, data collection, mobile sensing|12|1087–1098|Proceedings of the 2013 ACM Conference on Pervasive and Ubiquitous Computing Adjunct Publication|Gathering representative data using mobile sensing to answer research questions is becoming increasingly popular, driven by growing ubiquity and sensing capabilities of mobile devices. However, there are pitfalls along this path, which introduce heterogeneity in the gathered data, and which are rooted in the diversity of the involved device platforms, hardware, software versions and participants. Thus, we, as a research community, need to establish good practices and methodologies for addressing this issue in order to help ensure that, e.g., scientific results and policy changes based on collective, mobile sensed data are valid. In this paper, we aim to inform researchers and developers about mobile sensing data heterogeneity and ways to combat it. We do so via distilling a vocabulary of underlying causes, and via describing their effects on mobile sensing---building on experiences from three projects within citizen science, crowd awareness and trajectory tracking.|10.1145/2494091.2499576|https://doi.org/10.1145/2494091.2499576|New York, NY, USA|Association for Computing Machinery|9781450322157|2013|On Heterogeneity in Mobile Sensing Applications Aiming at Representative Data Collection|"\"Blunck, Henrik and Bouvin, Niels Olof and Franke, Tobias and Gr\\o{}nb\\ae{}k, Kaj and Kjaergaard, Mikkel B. and Lukowicz, Paul and W\"\"{u}stenberg, Markus\""|inproceedings|10.1145/2494091.2499576|||||||||||||||||||||||||||||945757272|42
||Big data, cost-effectiveness, next generation sequencing||1048-1053||Next-generation sequencing (NGS) is considered to be a prominent example of “big data” because of the quantity and complexity of data it produces and because it presents an opportunity to use powerful information sources that could reduce clinical and health economic uncertainty at a patient level. One obstacle to translating NGS into routine health care has been a lack of clinical trials evaluating NGS technologies, which could be used to populate cost-effectiveness analyses (CEAs). A key question is whether big data can be used to partially support CEAs of NGS. This question has been brought into sharp focus with the creation of large national sequencing initiatives. In this article we summarize the main methodological and practical challenges of using big data as an input into CEAs of NGS. Our focus is on the challenges of using large observational datasets and cohort studies and linking these data to the genomic information obtained from NGS, as is being pursued in the conduct of large genomic sequencing initiatives. We propose potential solutions to these key challenges. We conclude that the use of genomic big data to support and inform CEAs of NGS technologies holds great promise. Nevertheless, health economists face substantial challenges when using these data and must be cognizant of them before big data can be confidently used to produce evidence on the cost-effectiveness of NGS.|https://doi.org/10.1016/j.jval.2018.06.016|https://www.sciencedirect.com/science/article/pii/S1098301518322654||||2018|Using “Big Data” in the Cost-Effectiveness Analysis of Next-Generation Sequencing Technologies: Challenges and Potential Solutions|Sarah Wordsworth and Brett Doble and Katherine Payne and James Buchanan and Deborah A. Marshall and Christopher McCabe and Dean A. Regier|article|WORDSWORTH20181048|||Value in Health|10983015|9|21|||||22377|1,859|Q1|103|211|572|8179|2291|532|3,67|38,76|United Kingdom|Western Europe|1998-2020|Health Policy (Q1); Medicine (miscellaneous) (Q1); Public Health, Environmental and Occupational Health (Q1)|12,642|5.725|0.01786|948388370|748206584
CHI '17|Denver, Colorado, USA|temporal visualization, similarity, visual analytics, temporal event analytics, decision making|47|5498–5544|Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems|People often seek examples of similar individuals to guide their own life choices. For example, students making academic plans refer to friends; patients refer to acquaintances with similar conditions, physicians mention past cases seen in their practice. How would they want to search for similar people in databases? We discuss the challenge of finding similar people to guide life choices and report on a need analysis based on 13 interviews. Our PeerFinder prototype enables users to find records that are similar to a seed record, using both record attributes and temporal events found in the records. A user study with 18 participants and four experts shows that users are more engaged and more confident about the value of the results to provide useful evidence to guide life choices when provided with more control over the search process and more context for the results, even at the cost of added complexity.|10.1145/3025453.3025777|https://doi.org/10.1145/3025453.3025777|New York, NY, USA|Association for Computing Machinery|9781450346559|2017|Finding Similar People to Guide Life Choices: Challenge, Design, and Evaluation|Du, Fan and Plaisant, Catherine and Spring, Neil and Shneiderman, Ben|inproceedings|10.1145/3025453.3025777|||||||||||||||||||||||||||||948733701|42
||Cleaning;Data privacy;Pricing;Data models;Semantics;Osteoarthritis;Maintenance engineering;data quality;data cleaning;data privacy||1023-1030|2018 IEEE International Conference on Big Data (Big Data)|Data cleaning consumes up to 80% of the data analysis pipeline. This is a significant overhead for organizations where data cleaning is still a manually driven process requiring domain expertise. Recent advances have fueled a new computing paradigm called Database-as-a-Service, where data management tasks are outsourced to large service providers. We propose a new Data Cleaning-as-a-Service model that allows a client to interact with a data cleaning provider who hosts curated, and sensitive data. We present PACAS: a Privacy-Aware data Cleaning-As-a-Service framework that facilitates communication between the client and the service provider via a data pricing scheme where clients issue queries, and the service provider returns clean answers for a price while protecting her data. We propose a practical privacy model in such interactive settings called (X,Y,L)-anonymity that extends existing data publishing techniques to consider the data semantics while protecting sensitive values. Our evaluation over real data shows that PACAS effectively safeguards semantically related sensitive values, and provides improved accuracy over existing privacy-aware cleaning techniques.|10.1109/BigData.2018.8622249|||||2018|PACAS: Privacy-Aware, Data Cleaning-as-a-Service|Huang, Yu and Milani, Mostafa and Chiang, Fei|inproceedings|8622249||Dec|||||||||||||||||||||||||||949229816|42
||multiple workflows scheduling, Scientific workflows, multi-tenant platforms|39|||Workflows are an application model that enables the automated execution of multiple interdependent and interconnected tasks. They are widely used by the scientific community to manage the distributed execution and dataflow of complex simulations and experiments. As the popularity of scientific workflows continue to rise, and their computational requirements continue to increase, the emergence and adoption of multi-tenant computing platforms that offer the execution of these workflows as a service becomes widespread. This article discusses the scheduling and resource provisioning problems particular to this type of platform. It presents a detailed taxonomy and a comprehensive survey of the current literature and identifies future directions to foster research in the field of multiple workflow scheduling in multi-tenant distributed computing systems.|10.1145/3368036|https://doi.org/10.1145/3368036|New York, NY, USA|Association for Computing Machinery||2020|Multiple Workflows Scheduling in Multi-Tenant Distributed Systems: A Taxonomy and Future Directions|Hilman, Muhammad H. and Rodriguez, Maria A. and Buyya, Rajkumar|article|10.1145/3368036|10|feb|ACM Comput. Surv.|03600300|1|53|January 2021||||23038|2,079|Q1|163|112|365|15859|6978|365|19,16|141,60|United States|Northern America|1969-2020|Computer Science (miscellaneous) (Q1); Theoretical Computer Science (Q1)|10,790|10.282|0.01438|950821143|1517405264
||Online food delivery, Service quality, Big data analytic, OFD service quality scale||102938||The main purpose of this study is based on qualitative and quantitative research procedures, and integrates the key service factors for the online food delivery (OFD) industry extracted by Internet Big Data Analytics (IBDA) to construct a OFD service quality scale (OFD-SERV). This study takes OFD customers in Taipei City as the objects. The results show that 20 key service factors for the OFD industry are extracted through IBDA. The OFD-SERV scale contains six dimensions including reliability, maintenance of meal quality and hygiene, assurance, security, system operation and traceability, a total of 28 items. The results from the structural equation modeling showed that the reliability, assurance and system operation have a positive impact on customer satisfaction. Finally, the findings provide knowledge and inspiration for the current OFD, and enable OFD operators and future researchers to more accurately identify the deficiency of service quality.|https://doi.org/10.1016/j.ijhm.2021.102938|https://www.sciencedirect.com/science/article/pii/S0278431921000815||||2021|Construction of a service quality scale for the online food delivery industry|Ching-Chan Cheng and Ya-Yuan Chang and Cheng-Ta Chen|article|CHENG2021102938|||International Journal of Hospitality Management|02784319||95|||||28686|2,321|Q1|122|321|509|25314|4796|490|8,98|78,86|United Kingdom|Western Europe|1982-2020|Strategy and Management (Q1); Tourism, Leisure and Hospitality Management (Q1)|17,219|9.237|0.01134|951597786|1845514859
||Monitoring and evaluation, Health information systems, Developing countries||101994||Data to inform and improve health care systems in low- and middle-income countries (LMICs) has been facilitated by the development of monitoring and evaluation (M&E) systems. The drivers of change in M&E systems over the last 50 years have included a series of health concerns that have animated global donors (e.g., family planning, vaccination campaigns, and HIV/AIDS); the data requirements of donors; improved national economies enabling LMICs to invest more in M&E systems; and rapid advances in digital technologies. Progress has included the training and expansion of an M&E workforce, the creation of systems for data collection and use, and processes for assessing and ensuring data quality. Controversies have included the development of disease-specific systems that do not coordinate with each other, and a growing burden on health care deliverers to collect data for a proliferating number of health and process indicators. Digital technologies offer the promise of real time data and quick adaptation but also raise ethical and privacy concerns. The desire for speed can cast large-scale evaluations, considered by some to be the gold standard, in an unfavorable light as slow and expensive. Accordingly, there is a growing demand for speedy evaluations that rely on routine health information systems and privately collected “big data” from electronic health records and social media.|https://doi.org/10.1016/j.evalprogplan.2021.101994|https://www.sciencedirect.com/science/article/pii/S0149718921000896||||2021|Advances in monitoring and evaluation in low- and middle-income countries|James C. Thomas and Kathy Doherty and Stephanie Watson-Grant and Manish Kumar|article|THOMAS2021101994|||Evaluation and Program Planning|01497189||89|||||26602|0,555|Q2|62|107|353|5140|753|350|1,93|48,04|United Kingdom|Western Europe|1978-2020|Business and International Management (Q2); Geography, Planning and Development (Q2); Public Health, Environmental and Occupational Health (Q2); Social Psychology (Q2); Strategy and Management (Q2)|3,211|1.849|0.0031|955777464|1681316537
LAK '12|Vancouver, British Columbia, Canada|learning analytics, collaboration, ethics, practice, theory, research, data integration|5|4–8|Proceedings of the 2nd International Conference on Learning Analytics and Knowledge|"\"Learning analytics are rapidly being implemented in different educational settings, often without the guidance of a research base. Vendors incorporate analytics practices, models, and algorithms from datamining, business intelligence, and the emerging \"\"big data\"\" fields. Researchers, in contrast, have built up a substantial base of techniques for analyzing discourse, social networks, sentiments, predictive models, and in semantic content (i.e., \"\"intelligent\"\" curriculum). In spite of the currently limited knowledge exchange and dialogue between researchers, vendors, and practitioners, existing learning analytics implementations indicate significant potential for generating novel insight into learning and vital educational practices. This paper presents an integrated and holistic vision for advancing learning analytics as a research discipline and a domain of practices. Potential areas of collaboration and overlap are presented with the intent of increasing the impact of analytics on teaching, learning, and the education system.\""|10.1145/2330601.2330605|https://doi.org/10.1145/2330601.2330605|New York, NY, USA|Association for Computing Machinery|9781450311113|2012|Learning Analytics: Envisioning a Research Discipline and a Domain of Practice|Siemens, George|inproceedings|10.1145/2330601.2330605|||||||||||||||||||||||||||||962871672|42
||Big data analytics, Big data modeling, Big data toolkits, Domain-specific visual languages, Multidisciplinary teams, End-user tools||100964||We present BiDaML 2.0, an integrated suite of visual languages and supporting tool to help multidisciplinary teams with the design of big data analytics solutions. BiDaML tool support provides a platform for efficiently producing BiDaML diagrams and facilitating their design, creation, report and code generation. We evaluated BiDaML using two types of evaluations, a theoretical analysis using the “physics of notations”, and an empirical study with 1) a group of 12 target end-users and 2) five individual end-users. Participants mostly agreed that BiDaML was straightforward to understand/learn, and prefer BiDaML for supporting complex data analytics solution modeling than other modeling languages.|https://doi.org/10.1016/j.cola.2020.100964|https://www.sciencedirect.com/science/article/pii/S2590118420300241||||2020|An end-to-end model-based approach to support big data analytics development|Hourieh Khalajzadeh and Andrew J. Simmons and Mohamed Abdelrazek and John Grundy and John Hosking and Qiang He|article|KHALAJZADEH2020100964|||Journal of Computer Languages|25901184||58|||||21100904890|0,254|Q3|6|36|49|1889|104|47|2,12|52,47|United Kingdom|Western Europe|2019-2020|Computer Networks and Communications (Q3); Human-Computer Interaction (Q3); Software (Q3)|78|1.271|1.0E-4|963223688|188986129
||Big data, EHR, Healthcare, Omics, Precision medicine||63-72|Big Data Analytics for Healthcare|Precision medicine is a medical model that recommends custom-tailored products, techniques, treatments, and decisions for a subgroup of patients having the same biological basis of diseases. Due to the huge size and complexity of omics data and dataset of patient features, they cannot be analyzed directly by doctors. Big data is a term used for complex or large datasets that cannot be accurately processed or stored by traditional management tools. Omics and electronic health record (EHR) data are essential big biomedical data having a strong association with precision medicine. In this chapter, we review the importance of analyzing EHR and omics data in precision medicine. Big data analytics has been applied to healthcare in biomarker discovery and disease subtyping, drug repurposing, and integrating omics data into EHR. This will provide the most appropriate and efficient treatment to every patient on the basis of their subtyping data.|https://doi.org/10.1016/B978-0-323-91907-4.00005-4|https://www.sciencedirect.com/science/article/pii/B9780323919074000054||Academic Press|978-0-323-91907-4|2022|Chapter 6 - Big data analytics in precision medicine|Saurabh Biswas and Yasha Hasija|incollection|BISWAS202263||||||||||Pantea Keikhosrokiani|||||||||||||||||||964116553|42
||Big Data;Inspection;Data integration;Semantics;Hidden Markov models;Thesauri;Support vector machines;Big data of quality inspection;data fusion;named entity recognition;entity resolution;data conflict resolution||95-102|2018 5th IEEE International Conference on Cyber Security and Cloud Computing (CSCloud)/2018 4th IEEE International Conference on Edge Computing and Scalable Cloud (EdgeCom)|Quality big data comes from a wide range, the problem is how to eliminate the structure barriers between various types of data from different sources, to achieve the effective integration for isolated or fragmented data, and then to mine the information, knowledge and wisdom according to the actual needs. It is urgent for the quality inspection departments to solve the problem of making a decision the first time and take preventive measures. This paper introduced the research status of named entity recognition, solid unified detection, data conflict resolution, data fusion method from the characteristics of quality inspection data. The existence problems are analyzed and the research direction is looking forward of the research on data fusion in this paper.|10.1109/CSCloud/EdgeCom.2018.00025|||||2018|Review on Big Data Fusion Methods of Quality Inspection for Consumer Goods|Jiang, Wei and Ning, Xiuli and Xu, Yingcheng|inproceedings|8421858||June|||||||||||||||||||||||||||964385842|42
RICAI 2020|Shanghai, China|Distributed Framework, Deep Learning, Artificial Intelligence, Intelligent Web Crawler|5|229–233|Proceedings of the 2020 2nd International Conference on Robotics, Intelligent Control and Artificial Intelligence|With the rapid development of the Internet, webpages' content has become the central platform for people to publish and retrieve information. Recently, web crawlers could quickly and accurately find the information users need from the massive network information resources. There have been many different types of web crawlers in the literature, developed for data retrieval. However, most of the existing web crawlers have significant limitations. For example, they focus on the effective overall architecture instead of paying attention to the actual data's complexity. Moreover, the advertising links in the news and the public platform's promotional content have become ubiquitous noise. The existing web crawler collection strategy lacks sufficient identification of advertising information. The degree of automation to detect advertisements is low, so it isn't easy to form a complete and deployable large-scale distributed data crawling system. Therefore, the research and improvement of distributed web crawlers that intelligently distinguish advertisements is a work of practical significance. The distributed intelligent web crawler system designed and implemented in this paper solves low manual crawler efficiency and poor data quality. The crawler system can effectively identify and eliminate advertising information and significantly improve the automatically extracted data in the distributed crawler system from the experimental results.|10.1145/3438872.3439085|https://doi.org/10.1145/3438872.3439085|New York, NY, USA|Association for Computing Machinery|9781450388306|2020|Intelligent Distributed Web Crawler Based on Attention Mechanism|Wu, Yi and Song, Yan and Yang, Hongshan|inproceedings|10.1145/3438872.3439085|||||||||||||||||||||||||||||965007766|42
|||7|29–35||As geo-sensor webs have not grown as quickly as expected, new, alternative data sources have to be found for near real-time analysis in areas like emergency management, environmental monitoring, public health, or urban planning. This paper assesses the ability of human sensors, i.e., user-generated observations in a wide range of social networks, the mobile phone network, or micro-blogs, to complement geo-sensor networks. We clearly delineate the concepts of People as Sensors, Collective Sensing and Citizen Science. Furthermore, we point out current challenges in fusing data from technical and human sensors, and sketch future research areas in this field.|10.1145/2826686.2826692|https://doi.org/10.1145/2826686.2826692|New York, NY, USA|Association for Computing Machinery||2015|Fusing Human and Technical Sensor Data: Concepts and Challenges|Resch, Bernd and Blaschke, Thomas|article|10.1145/2826686.2826692||sep|SIGSPATIAL Special||2|7|July 2015||||||||||||||||||||||966544694|42
https://doi.org/10.1016/j.zefq.2020.11.002|https://www.sciencedirect.com/science/article/pii/S1865921720301744||||2020|Evaluation eines Zukunftsszenarios zur Nutzung von Big-Data-Anwendungen für die Verbesserung der Versorgung von Menschen mit seltenen Erkrankungen|Brita Sedlmayr and Andreas Knapp and Michéle Kümmel and Franziska Bathelt and Martin Sedlmayr|article|SEDLMAYR202081|||Zeitschrift für Evidenz, Fortbildung und Qualität im Gesundheitswesen|18659217||158-159||||||||||||||||||||||||||||||972041223|42
||Cloud Computing, Big Data Analytics, Cloud Analytics, Security, Privacy, Business Intelligence, MapReduce, AaaS, CLaaS||1112-1122||Cloud computing and big data analytics are, without a doubt, two of the most important technologies to enter the mainstream IT industry in recent years. Surprisingly, the two technologies are coming together to deliver powerful results and benefits for businesses. Cloud computing is already changing the way IT services are provided by so called cloud companies and how businesses and users interact with IT resources. Big Data is a data analysis methodology enables by recent advances in information and communications technology. However, big data analysis requires a huge amount of computing resources making adoption costs of big data technology is not affordable for many small to medium enterprises. In this paper, we outline the the benefits and challenges involved in deploying big data analytics through cloud computing. We argue that cloud computing can support the storage and computing requirements of big data analytics. We discuss how the consolidation of these two dominant technologies can enhance the process of big data mining enabling businesses to improve decision-making processes. We also highlight the issues and risks that should be addressed when using a so called CLaaS, cloud-based service model.|https://doi.org/10.1016/j.procs.2017.08.138|https://www.sciencedirect.com/science/article/pii/S1877050917314953||||2017|Challenges and Benefits of Deploying Big Data Analytics in the Cloud for Business Intelligence|Bala M. Balachandran and Shivika Prasad|article|BALACHANDRAN20171112|||Procedia Computer Science|18770509||112||Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 21st International Conference, KES-20176-8 September 2017, Marseille, France|||19700182801|0,334|-|76|1907|6483|38511|13722|6359|2,09|20,19|Netherlands|Western Europe|2010-2020|Computer Science (miscellaneous)||||976800034|2108686752
|||11|2512–2522|Proceedings of the 2016 Winter Simulation Conference|With the feature size shrinkage in advanced technology nodes, the modeling of process variations has become more critical for troubleshooting and yield enhancement. Misalignment among equipment tools or chambers in process stages is a major source of process variations. Because a process flow contains hundreds of stages during semiconductor fabrication, tool/chamber misalignment may more significantly affect the variation of transistor parameters in a wafer acceptance test. This study proposes a big data analytic framework that simultaneously considers the mean difference between tools and wafer-to-wafer variation and identifies possible root causes for yield enhancement. An empirical study was conducted to demonstrate the effectiveness of proposed approach and obtained promising results.||||IEEE Press|9781509044849|2016|Big Data Analytics for Modeling WAT Parameter Variation Induced by Process Tool in Semiconductor Manufacturing and Empirical Study|Chien, Chen-Fu and Chen, Ying-Jen and Wu, Jei-Zheng|inbook|10.5555/3042094.3042407|||||||||||||||||||||||||||||977707037|42
ICIAI '18|Shanghai, China|mining, Hadoop, combination, tech big data|5|59–63|Proceedings of the 2nd International Conference on Innovation in Artificial Intelligence|With the advent of the Internet + era in the field of Tech big data, the big data of Tech big data has a large amount of data and various characteristics. It is an important means to carry out research on the big data of Tech big data to realize the combination and mining of efficient multi-source foreign technology data. However, at present, the big data of Tech big data are divided into disciplines and different formats, which are difficult to realize the intersection of effective scientific and technological information and realize data sharing. This paper puts forward a kind of big data combined with Tech big data and mining technology based on the Hadoop framework.It includes a unified collection and preprocessing method of big data of Tech big data and the design of storage and management platform for data sources. It is based on Map/Reduce Tech big data parallelization computing model and system.Its correlation with important scientific data mining services.The framework has good practicability and expansibility.|10.1145/3194206.3194229|https://doi.org/10.1145/3194206.3194229|New York, NY, USA|Association for Computing Machinery|9781450363457|2018|Based on Hadoop's Tech Big Data Combination and Mining Technology Framework|Zhichao, Xu and Jiandong, Zhao and Huan, Huang|inproceedings|10.1145/3194206.3194229|||||||||||||||||||||||||||||979420284|42
||Satellites;Data communication;Real-time systems;Databases;Browsers;Big Data;Big Data;Satellite Data;Data Latency;Data Quality;NoSQL;MongoDB||384-387|2017 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)|A wide array of time-sensitive satellite data is required in the research and development activities for natural hazard assessment, storms and weather prediction, hurricane tracking, disaster and emergency response, and so on. Identifying and analyzing the latencies of large volumes of real-time and near real-time satellite data is very useful and helpful for detecting transmission issues, managing IT resources, and configuring and optimizing data management systems. This paper introduces how to monitor and collect important timestamps of data transmissions, organize them in a NoSQL database, and explore data latency via a user-friendly dashboard. Taking Sentinel series satellite data as an example, data transmission issues are illustrated and investigated further. Latency analysis and explorations help data providers and managers improve data transmission and enhance data management.|10.1109/IGARSS.2017.8126976|||||2017|Latency analysis of large volume satellite data transmissions|Han, Weiguo and Jochum, Matthew|inproceedings|8126976||July||21537003|||||||||||||||||||||||||981344548|1296020740
ARES '19|Canterbury, CA, United Kingdom|Cooperative and collaborative cybersecurity, quality parameters, cyber threat intelligence source evaluation, cyber threat information sharing, trust indicators|10||Proceedings of the 14th International Conference on Availability, Reliability and Security|Threat intelligence sharing has become a cornerstone of cooperative and collaborative cybersecurity. Sources providing such data have become more widespread in recent years, ranging from public entities (driven by legislatorial changes) to commercial companies and open communities that provide threat intelligence in order to help organisations and individuals to better understand and assess the cyber threat landscape putting their systems at risk. Tool support to automatically process this information is emerging concurrently. It has been observed that the quality of information received by the sources varies significantly and that in order to assess the quality of a threat intelligence source it is not sufficient to only consider qualitative indications of the source itself, but it is necessary to monitor the data provided by the source continuously to be able to draw conclusions about the quality of information provided by a source. In this paper, we propose a methodology for evaluating cyber threat information sources based on quantitative parameters. The methodology aims to facilitate trust establishment to threat intelligence sources, based on a weighted evaluation method that allows each entity to adapt it to its own needs and priorities. The approach facilitates automated tools utilising threat intelligence, since information to be considered can be prioritised based on which source is trusted the most at the time the intelligence arrives.|10.1145/3339252.3342112|https://doi.org/10.1145/3339252.3342112|New York, NY, USA|Association for Computing Machinery|9781450371643|2019|A Quantitative Evaluation of Trust in the Quality of Cyber Threat Intelligence Sources|Schaberreiter, Thomas and Kupfersberger, Veronika and Rantos, Konstantinos and Spyros, Arnolnt and Papanikolaou, Alexandros and Ilioudis, Christos and Quirchmayr, Gerald|inproceedings|10.1145/3339252.3342112|83||||||||||||||||||||||||||||982654867|42
|||11|88–98||BI technologies are essential to running today's businesses and this technology is going through sea changes.|10.1145/1978542.1978562|https://doi.org/10.1145/1978542.1978562|New York, NY, USA|Association for Computing Machinery||2011|An Overview of Business Intelligence Technology|Chaudhuri, Surajit and Dayal, Umeshwar and Narasayya, Vivek|article|10.1145/1978542.1978562||aug|Commun. ACM|00010782|8|54|August 2011||||||||||||||||||||||982745996|647144465
||Power IOT system, Hybrid model, Heterogeneous data consistency, Machine learning combination method||172-179||The data of the power Internet of Things (IOT) system is transferred from the IaaS layer to the SaaS layer. The general data preprocessing method mainly solves the problem of big data anomalies and missing at the PaaS layer, but it still lacks the ability to judge the high error data that meets the timing characteristics, making it difficult to deal with heterogeneous power inconsistent issues. This paper shows this phenomenon and its physical mechanism, showing the difficulty of building a quantitative model forward. A data-driven method is needed to form a hybrid model to correct the data. The research object is the electricity meter data on both sides of a commercial building transformer, which comes from different power IOT systems. The low-voltage side was revised based on the high-voltage side. Compared with the correction method based on purely using neural networks, the combined method, Linear Regression (LS) + Differential Evolution (DE) + Extreme Learning Machine (ELM), further reduces the deviation from approximately 4% to 1%.|https://doi.org/10.1016/j.isatra.2021.01.056|https://www.sciencedirect.com/science/article/pii/S0019057821000665||||2021|Data consistency method of heterogeneous power IOT based on hybrid model|Haoyu Jiang and Kai Chen and Quanbo Ge and Jinqiang Xu and Yingying Fu and Chunxi Li|article|JIANG2021172|||ISA Transactions|00190578||117|||||29805|1,147|Q1|79|559|892|22562|5669|884|6,08|40,36|United States|Northern America|1968-2020|Applied Mathematics (Q1); Computer Science Applications (Q1); Control and Systems Engineering (Q1); Electrical and Electronic Engineering (Q1); Instrumentation (Q1)|10,483|5.468|0.01283|983078782|669506487
||Error detection, bidirectional encoder, transformers, data augmentation|29|||Error detection is a crucial preliminary phase in any data analytics pipeline. Existing error detection techniques typically target specific types of errors. Moreover, most of these detection models either require user-defined rules or ample hand-labeled training examples. Therefore, in this article, we present TabReformer, a model that learns bidirectional encoder representations for tabular data. The proposed model consists of two main phases. In the first phase, TabReformer follows encoder architecture with multiple self-attention layers to model the dependencies between cells and capture tuple-level representations. Also, the model utilizes a Gaussian Error Linear Unit activation function with the Masked Data Model objective to achieve deeper probabilistic understanding. In the second phase, the model parameters are fine-tuned for the task of erroneous data detection. The model applies a data augmentation module to generate more erroneous examples to represent the minority class. The experimental evaluation considers a wide range of databases with different types of errors and distributions. The empirical results show that our solution can enhance the recall values by 32.95% on average compared with state-of-the-art techniques while reducing the manual effort by up to 48.86%.|10.1145/3447541|https://doi.org/10.1145/3447541|New York, NY, USA|Association for Computing Machinery||2021|TabReformer: Unsupervised Representation Learning for Erroneous Data Detection|Nashaat, Mona and Ghosh, Aindrila and Miller, James and Quader, Shaikh|article|10.1145/3447541|18|may|ACM/IMS Trans. Data Sci.|26911922|3|2|August 2021||||||||||||||||||||||984117388|961429963
ICEGOV '18|Galway, Ireland|data mining, Big data, policy Modelling, impact assessment, evidence based policy making, behavioural patterns, dynamic simulation|9|575–583|Proceedings of the 11th International Conference on Theory and Practice of Electronic Governance|Governments and policy makers are striving to respond to contemporary socio-economic challenges, however, often neglecting the human factor and the multidimensionality of policy implications. In this chapter, a framework for evidence based policy making is proposed, which integrates the usage of open big data coming from a multiplicity of sources with policy simulations. It encompasses the application of dynamic modelling methodologies and data mining techniques to extract knowledge from two types of data. On the one hand, objective data such as governmental and statistical data, are used to capture the interlinked policy domains and their underlying casual mechanisms. On the other hand, behavioural patterns and citizens' opinions are extracted from Web 2.0 sources, social media posts, polls and statistical surveys. To combine this multimodal information, our approach suggests a modelling methodology that bases on big data acquisition and processing for the identification of significant factors and counterintuitive interrelations between them, which can be applied in any policy domain. Then, to allow the practical application of the framework an ICT architecture is designed, with the aim to overcome challenges related with big data management and processing. Finally, validation of the approach for driving policy design and implementation in the future in diverse policy domains, is suggested.|10.1145/3209415.3209427|https://doi.org/10.1145/3209415.3209427|New York, NY, USA|Association for Computing Machinery|9781450354219|2018|A Framework for Evidence Based Policy Making Combining Big Data, Dynamic Modelling and Machine Intelligence|Androutsopoulou, Aggeliki and Charalabidis, Yannis|inproceedings|10.1145/3209415.3209427|||||||||||||||||||||||||||||984649288|42
||Engineer-to-order manufacturing, Operations management, Data analytics, Industrial data, Data mining, Big data||209-214||Manufacturing data offers big potential for improving management of manufacturing operations. The paper addresses an approach to data analytics in engineer-to-order (ETO) manufacturing systems where the product quality and due-date reliability play a key role in management decisionmaking. The objective of the research is to investigate manufacturing data which are collected by a manufacturing execution system (MES) during operations in an ETO enterprise and to develop tools for supporting scheduling of operations. The developed tools can be used for simulation of production and forecasting of potential resource overloads.|https://doi.org/10.1016/j.procir.2018.03.098|https://www.sciencedirect.com/science/article/pii/S2212827118302531||||2018|Big data analytics for operations management in engineer-to-order manufacturing|Dominik Kozjek and Rok Vrabič and Borut Rihtaršič and Peter Butala|article|KOZJEK2018209|||Procedia CIRP|22128271||72||51st CIRP Conference on Manufacturing Systems|||21100243809|0,683|-|65|1456|3191|30451|8225|3145|2,40|20,91|Netherlands|Western Europe|2012-2020|Control and Systems Engineering; Industrial and Manufacturing Engineering||||985120782|2127027836
||Drilling, Machine learning, Rate of penetration, Drilling data quality improvement, Recurrent neural networks||109904||In recent years, machine learning has been adopted in the Oil and Gas industry as a promising technology for solutions to the most demanding problems like downhole parameters estimations and incidents detection. A big amount of available data makes this technology an attractive option for solving a wide variety of drilling problems, as well as a reliable candidate for performing big-data analysis and interpretation. Nevertheless, this approach may cause, in some cases, that petroleum engineering concepts are disregarded in favor of more data-intensive approaches. This study aims to evaluate the impact of drilling data measurement correction on data-driven model performance. In our study, besides using the standard data processing technologies, like gap filling, outlier removal, noise reduction etc., the physics-based drilling models are also implemented for data quality improvement and data correction in consideration of the measurement physics, rarely mentioned in most of publications. In our case study, recurrent neural networks (RNN) that are able to capture temporal natures of a signal are employed for the rate of penetration (ROP) estimation with an adjustable predictive window. The results show that the RNN model produces the best results when using the drilling data recovered through analytical methods. Moreover, the comprehensive data-driven model evaluation and engineering interpretation are conducted to facilitate better understanding of the data-driven models and their applications.|https://doi.org/10.1016/j.petrol.2021.109904|https://www.sciencedirect.com/science/article/pii/S0920410521015217||||2022|Downhole data correction for data-driven rate of penetration prediction modeling|Mauro A. Encinas and Andrzej T. Tunkiel and Dan Sui|article|ENCINAS2022109904|||Journal of Petroleum Science and Engineering|09204105||210|||||17013|0,975|Q1|111|1174|2764|59146|13310|2752|4,78|50,38|Netherlands|Western Europe|1987-2021|Fuel Technology (Q1); Geotechnical Engineering and Engineering Geology (Q1)|25,616|4.346|0.02651|987547568|2008704552
SIGSPATIAL'13|Orlando, Florida|point of interest, volunteered geographic information, POI, location-based services, similarity, conflation|4|440–443|Proceedings of the 21st ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems|To a large degree, the attraction of Big Data lies in the variety of its heterogeneous multi-thematic and multi-dimensional data sources and not merely its volume. To fully exploit this variety, however, requires conflation. This is a two step process. First, one has to establish identity relations between information entities across the different data sources; and second, attribute values have to be merged according to certain procedures which avoid logical contradictions. The first step, also called matching, can be thought of as a weighted combination of common attributes according to some similarity measures. In this work, we propose such a matching based on multiple attributes of Points of Interests (POI) from the Location-based Social Network Foursquare and the Yelp local directory service. While both contain overlapping attributes that can be use for matching, they have specific strengths and weaknesses which makes their conflation desirable. We present a weighted multi-attribute matching strategy and evaluate its performance. Our strategy can automatically match 97% of randomly selected Yelp POI to their corresponding Foursquare entities.|10.1145/2525314.2525455|https://doi.org/10.1145/2525314.2525455|New York, NY, USA|Association for Computing Machinery|9781450325219|2013|Weighted Multi-Attribute Matching of User-Generated Points of Interest|McKenzie, Grant and Janowicz, Krzysztof and Adams, Benjamin|inproceedings|10.1145/2525314.2525455|||||||||||||||||||||||||||||988652440|42
||Bayesian networks, Calibration, Data integration, Social media, Information quality (InfoQ), Resampling techniques||76-90||In recent years, the growing availability of huge amounts of information, generated in every sector at high speed and in a wide variety of forms and formats, is unprecedented. The ability to harness big data is an opportunity to obtain more accurate analyses and to improve decision-making in industry, government and many other organizations. However, handling big data may be challenging and proper data integration is a key dimension in achieving high information quality. In this paper, we propose a novel approach to data integration that calibrates online generated big data with interview based customer survey data. A common issue of customer surveys is that responses are often overly positive, making it difficult to identify areas of weaknesses in organizations. On the other hand, online reviews are often overly negative, hampering an accurate evaluation of areas of excellence. The proposed methodology calibrates the levels of unbalanced responses in different data sources via resampling and performs data integration using Bayesian Networks to propagate the new re-balanced information. In this paper we show, with a case study example, how the novel data integration approach allows businesses and organizations to get a bias corrected appraisal of the level of satisfaction of their customers. The application is based on the integration of online data of review blogs and customer satisfaction surveys from the San Francisco airport. We illustrate how this integration enhances the information quality of the data analytic work in four of InfoQ dimensions, namely, Data Structure, Data Integration, Temporal Relevance and Chronology of Data and Goal.|https://doi.org/10.1016/j.eswa.2017.12.044|https://www.sciencedirect.com/science/article/pii/S0957417417308667||||2018|Social media big data integration: A new approach based on calibration|Luciana {Dalla Valle} and Ron Kenett|article|DALLAVALLE201876|||Expert Systems with Applications|09574174||111||Big Data Analytics for Business Intelligence|||24201|1,368|Q1|207|770|1945|42314|17345|1943|8,67|54,95|United Kingdom|Western Europe|1990-2021|Artificial Intelligence (Q1); Computer Science Applications (Q1); Engineering (miscellaneous) (Q1)|55,444|6.954|0.04053|988667930|1377770283
||||29-29|2015 Conference on Technologies and Applications of Artificial Intelligence (TAAI)|Spatial trajectory data is widely available today. Over a sustained period of time, trajectory data has been collected from numerous GPS devices, smartphones, sensors and social media applications. How do we manage them? What values can a business derive from them, and how? Due to their very large volumes, the nature of streaming itself, highly variable levels of data quality, as well as many possible links with other types of data, making sense of spatial trajectory data is one of the crucial areas for big data analytics. In this talk, we will introduce this increasingly important research area in the context of new applications, new problems and new opportunities. We will discuss recent advances in trajectory data management and trajectory mining, from their foundations to high performance processing with modern computing infrastructures.|10.1109/TAAI.2015.7407050|||||2015|Keynote speech V deriving values from spatial trajectories|Xiaofang Zhou|inproceedings|7407050||Nov||23766824|||||||||||||||||||||||||989402351|483586015
WSC '14|Savannah, Georgia||10|1897–1906|Proceedings of the 2014 Winter Simulation Conference|Setting up simulation scenarios in the field of Supply Chains (SCs) is a big challenge because complex input data must be specified and careful input data management as well as precise model design are necessary. SC simulation needs a large amount of input data -- especially in times of big data, in which the data is often approximated by statistical distributions from real world observations. This paper deals with the question how the model itself and its input can be effectively complemented. This takes into account the commonly known fact, that the accuracy of a model output depends on the model input. Therefore an approach for using techniques of Knowledge Discovery in Databases is introduced to derive logical relations from the data. We discuss how Knowledge Discovery would be applied, as a preprocessing step for simulation scenario setups, in order to provide benefits for the level of accuracy in simulation models.||||IEEE Press||2014|An Approach for Increasing the Level of Accuracy in Supply Chain Simulation by Using Patterns on Input Data|Rabe, Markus and Scheidler, Anne Antonia|inproceedings|10.5555/2693848.2694087|||||||||||||||||||||||||||||991955754|42
||Strategic plan, business requirements, technology adoption, massive scalability, data reuse, data repurposing, oversight, governance, mainstreaming technology, enterprise integration||29-37|Big Data Analytics|This chapter expands on the previous one by looking at some key issues that often plague new technology adoption and show that the key issues are not new ones, and that there is likely to be organizational knowledge that can help in fleshing out a reasonable strategic plan. We look at the typical hype cycle, and how its flaws can be mitigated by instituting good practices for defining expectations and continuing to measure performance. We help define the acceptability criteria for evaluating the result of a big data pilot that can be used to make a go/no-go decision. We then pose some thoughts about preparing the organization for massive scalability, data reuse, and the need for oversight and governance. The objective is to provide a pathway for mainstreaming big data into the technology infrastructure that is integrated with the existing investment.|https://doi.org/10.1016/B978-0-12-417319-4.00004-1|https://www.sciencedirect.com/science/article/pii/B9780124173194000041|Boston|Morgan Kaufmann|978-0-12-417319-4|2013|Chapter 4 - Developing a Strategy for Integrating Big Data Analytics into the Enterprise|David Loshin|incollection|LOSHIN201329||||||||||David Loshin|||||||||||||||||||992356797|42
||Big Data applications;Optimization;Servers;Protocols;Multimedia communication;SDN;Network Virtualization;Software Defined Networks;Big data||74-77|2017 14th International Conference on Smart Cities: Improving Quality of Life Using ICT & IoT (HONET-ICT)|Big data applications depend on underlying networks that make the transfer of information possible. These networks may be real (conventional) or virtual (in case of services hosted in data centers). Either way, the responsibility of smooth execution of the application, despite increasing traffic volume, lies with the service provider. The service providers face many challenges with respect to providing a high quality of service. It is therefore in the best interest of the service providers that efficiency of the applications is increased. SDN has the potential to improve big data application performance. In this paper we have a look at the recent advancements in technology that helps improve big data applications using SDN and discuss our observations.|10.1109/HONET.2017.8102206|||||2017|Benefits of SDN for Big data applications|Alqarni, Mohammed A.|inproceedings|8102206||Oct||19494106|||||||||||||||||||||||||993460332|1008564358
BDAW '16|Blagoevgrad, Bulgaria|Real-Time Analysis, Big Data, Data Volume, Medical Data, Healthcare Analytics|11||Proceedings of the International Conference on Big Data and Advanced Wireless Technologies|We are inundated in a flood of data today. Data is being collected at a rapid scale from variety of sources like healthcare, e-commerce, social networking and so on. Decisions which were earlier made on assumptions can now be made on the data itself. It's a well known fact that volume, variety, velocity and veracity are the challenges associated in handling Big Data. The dynamic nature of the Internet and the velocity factor pose humongous challenges in retrieving patterns from the data. Coping up with noisy data which occurs at a rapid rate is still an open challenge. We have handled the issues associated with variety and veracity. After reviewing the existing system, it was found that there is no significant research model towards addressing data velocity problem exclusively taking case study of healthcare analytics.Hence, this paper presents a novel framework TAF or Temporal Analysis Framework that mainly targets at handling the incoming speed of data and redundancies in Healthcare Analytics. The proposed system uses real-time data analysis that significantly handles the data velocity along with retention of minimal error. The study outcome was assessed to find minimal algorithm complexities compared to any system that doesn't use this approach of self-adaptable real-time data analysis.|10.1145/3010089.3010095|https://doi.org/10.1145/3010089.3010095|New York, NY, USA|Association for Computing Machinery|9781450347792|2016|TAF: Temporal Analysis Framework for Handling Data Velocity in Healthcare Analytics|Sindhu, C. S. and Hegde, Nagaratna P.|inproceedings|10.1145/3010089.3010095|10||||||||||||||||||||||||||||994066129|42
||Big data, Social media data, Urban landscape sustainability, Smart city, Urban planning||123142||The future of humanity depends increasingly on the performance of cities. Big data provide new and powerful ways of studying and improving coupled urban environmental, social, and economic systems to achieve urban sustainability. However, the term big data has been defined variably, and its urban applications have so far been sporadic in terms of research topic and location. A comprehensive review of big data-based urban environment, society, and sustainability (UESS) research is much needed. The aim of this study was to summarize the big data-based UESS research using a systematic review approach in combination with bibliometric and thematic analyses. The results showed that the numbers of publications and citations of related articles have been increasing exponentially in recent years. The most frequently used big data in UESS research are human behavior data, and the major analytical methods are of five types: classification, clustering, regression, association rules, and social network analysis. The major research topics of big data-based UESS research include urban mobility, urban land use and planning, environmental sustainability, public health and safety, social equity, tourism, resources and energy utilization, real estate, and retail, accommodation and catering. Big data benefit UESS research by proving a people-oriented perspective, timely and real-time information, and fine-resolution spatial dynamics. In addition, several obstacles were identified to applying big data in UESS research, which are related to data quality and acquisition, data storage and management, data security and privacy, data cleaning and preprocessing, and data analysis and information mining. To move forward, future research should integrate multiple big data sources, develop and utilize new methods such as deep learning and cloud computing, and expand the application fields to focus on the interactions between human activities and urban environments. This review can contribute to understanding the current situation of big data-based UESS research, and provide a reference for studies of this topic in the future.|https://doi.org/10.1016/j.jclepro.2020.123142|https://www.sciencedirect.com/science/article/pii/S0959652620331875||||2020|A systematic review of big data-based urban sustainability research: State-of-the-science and future directions|Lingqiang Kong and Zhifeng Liu and Jianguo Wu|article|KONG2020123142|||Journal of Cleaner Production|09596526||273|||||19167|1,937|Q1|200|5126|10603|304498|103295|10577|9,56|59,40|United Kingdom|Western Europe|1993-2021|Environmental Science (miscellaneous) (Q1); Industrial and Manufacturing Engineering (Q1); Renewable Energy, Sustainability and the Environment (Q1); Strategy and Management (Q1)|170,352|9.297|0.18299|994624579|1121054297
||Feist Publishing, Inc. v. Rural Telephone Service Co., Data Quality Act, Freedom of Information Act, limited data use agreements, tort, patents, intellectual property, informed consent, data ownership, copyright, infringement, fair use||183-199|Principles of Big Data|Big Data projects always incur some legal risk. It is impossible to know all the data contained in a Big Data project, and it is impossible to know every purpose to which Big Data is used. Hence, the entities that produce Big Data may unknowingly contribute to a variety of illegal activities, chiefly copyright and other intellectual property infringements, breaches of confidentiality, and privacy invasions. In addition, issues of data quality, data availability, and data documentation may contribute to the legal or regulatory disqualification of Big Data as a resource suitable for its intended purposes. In this chapter, four issues will be discussed in detail: (1) responsibility for the accuracy of the contained data; (2) rights to create, use, and share the data held in the resource; (3) intellectual property encumbrances incurred from the use of standards required for data representation and data exchange; and (4) protections for individuals whose personal information is used in the resource. Big Data managers contend with a wide assortment of legal issues, but these four problems never seem to go away.|https://doi.org/10.1016/B978-0-12-404576-7.00013-7|https://www.sciencedirect.com/science/article/pii/B9780124045767000137|Boston|Morgan Kaufmann|978-0-12-404576-7|2013|Chapter 13 - Legalities|Jules J. Berman|incollection|BERMAN2013183||||||||||Jules J. Berman|||||||||||||||||||994705431|42
||DGGS, Data model, Big data, Spatial data, Analytics, Environment||214-228||Discrete global grid systems (DGGS) have been proposed as a data model for a digital earth framework. We introduce a new data model and analytics system called IDEAS – integrated discrete environmental analysis system to create an operational DGGS-based GIS which is suitable for large scale environmental modelling and analysis. Our analysis demonstrates that DGGS-based GIS is feasible within a relational database environment incorporating common data analytics tools. Common GIS operations implemented in our DGGS data model outperformed the same operations computed using traditional geospatial data types. A case study into wildfire modelling demonstrates the capability for data integration and supporting big data geospatial analytics. These results indicate that DGGS data models have significant capability to solve some of the key outstanding problems related to geospatial data analytics, providing a common representation upon which fast and scalable algorithms can be built.|https://doi.org/10.1016/j.isprsjprs.2020.02.009|https://www.sciencedirect.com/science/article/pii/S0924271620300502||||2020|An integrated environmental analytics system (IDEAS) based on a DGGS|Colin Robertson and Chiranjib Chaudhuri and Majid Hojati and Steven A. Roberts|article|ROBERTSON2020214|||ISPRS Journal of Photogrammetry and Remote Sensing|09242716||162|||||29161|2,960|Q1|138|264|677|16114|7306|668|10,56|61,04|Netherlands|Western Europe|1989-2020|Atomic and Molecular Physics, and Optics (Q1); Computer Science Applications (Q1); Computers in Earth Sciences (Q1); Engineering (miscellaneous) (Q1); Geography, Planning and Development (Q1)|18,026|8.979|0.02145|996772397|660578442
||Big Data Warehouse, Big Data, ETL||817-824||Autonomous driving is assisted by different sensors, each providing information about certain parameters. What we are looking for is an integrated perspective of all these parameters to drive us into better decisions. To achieve this goal, a system that can handle these Big Data issues regarding volume, velocity and variety is needed. This paper aims to design and develop a real-time Big Data Warehouse repository, integrating the data generated by the multiple sensors developed in the context of IVS (In-Vehicle Sensing) systems; the data to be stored in this repository should be merged, which will imply its processing, consolidation and preparation for the analytical mechanisms that will be required. This multisensory fusion is important because it allows the integration of different perspectives in terms of sensor data, since they complement each other. Therefore, it can enrich the entire analysis process at the decision-making level, for instance, understanding what is going on inside the cockpit.|https://doi.org/10.1016/j.procs.2022.08.099|https://www.sciencedirect.com/science/article/pii/S1877050922008377||||2022|Big Data Analytics for Vehicle Multisensory Anomalies Detection|Ana Xavier Fernandes and Pedro Guimarães and Maribel Yasmina Santos|article|FERNANDES2022817|||Procedia Computer Science|18770509||204||International Conference on Industry Sciences and Computer Science Innovation|||19700182801|0,334|-|76|1907|6483|38511|13722|6359|2,09|20,19|Netherlands|Western Europe|2010-2020|Computer Science (miscellaneous)||||998326115|2108686752
||System, Bid price evaluation, Construction project, Big data||606-614||Tender price evaluation of construction project is one of the most important works for the clients to control project cost in the bidding stage. However,the previously underutilization of project cost data made the tender price evaluation of new projects lack of effective evaluation criterion, which brings challenge to cost control. With the improvement of companies’ information technology application and the advent of big data era, the project cost-related data can be completely and systematically recorded in real time, as well as fully utilized to support decision-making for construction project cost management. In this paper, a system for tender price evaluation of construction project based on big data is presented, aiming to use related technique of big data to analysis project cost data to give a reasonable cost range, which contributes to obtaining the evaluation criterion to support the tender price controls. The paper introduced the data sources, data extraction, data storage and data analysis of the system respectively. A case study is conducted in a metro station project to evaluate the system. The results show that the system based on big data is significant for tender price evaluation in construction project.|https://doi.org/10.1016/j.proeng.2015.10.114|https://www.sciencedirect.com/science/article/pii/S1877705815032154||||2015|A System for Tender Price Evaluation of Construction Project Based on Big Data|Yongcheng Zhang and Hanbin Luo and Yi He|article|ZHANG2015606|||Procedia Engineering|18777058||123||Selected papers from Creative Construction Conference 2015|||18700156717|0,320|-|74|0|5873|0|9870|5804|1,88|0,00|Netherlands|Western Europe|2009-2019|Engineering (miscellaneous)||||1002030110|506091674
|||9|226–234|DG.O 2022: The 23rd Annual International Conference on Digital Government Research|Decision-making has become more critical for organizations in the 21st century. The citizens’ countless needs and the emerging problems (internal and external) faced by governments increase the complexity of government decisions worldwide. The research question guiding this attempt is: How is government decision-making grounded on artificial intelligence (AI)? Based on the PRISMA approach and empirical analysis of some international cases are adopted. The authors analyze different organizational and environmental factors, the objectives, benefits, and risks of AI-supported decision-making. The findings show an increasing interest in the research on government decision-making based on AI. Finally, there is the potential of AI to support decision-making for the benefit of citizens and public value generation, collaboratively between governments, industry, and society. Future work will further analyze AI-based decision-making in government in depth.||https://doi.org/10.1145/3543434.3543445|New York, NY, USA|Association for Computing Machinery|9781450397490|2022|Mind the Gap: Towards an Understanding of Government Decision-Making Based on Artificial Intelligence|Valle-Cruz, David and Garc\'{\i}a-Contreras, Rigoberto and Mu\~{n}oz-Ch\'{a}vez, J. Patricia|inbook|10.1145/3543434.3543445|||||||||||||||||||||||||||||1004496322|42
UMAP '21|Utrecht, Netherlands|user modeling, business analytics, personalization, data visualizations, human factors, adaptation|7|291–297|Proceedings of the 29th ACM Conference on User Modeling, Adaptation and Personalization|In a data driven economy where data volume and dimensions are explosively increasing, businesses rely on business intelligence and analytics (BI&amp;A) platforms for analysing their data and coming to beneficial decisions. With the ever-growing generation of data, the process of data analysis is becoming more complicated for the business users, as the exploration of more demanding use cases increases. While the existing BI&amp;A platforms provide myriads of data visualizations that support data exploration, none of those account for the user’s individual differences, needs or requirements, and thus may hinder the user’s understanding of visual data and consequently their decision-making processes. This work embarks on an interdisciplinary endeavour to introduce a human-centred adaptive data visualizations framework in the context of business, as the core of an adaptive data analytics platform, that aims to enhance the business user’s decision making by increasing her understanding of data. The framework is built using a multi-dimensional human-centred user model that goes beyond traditional user characteristics and accounts for cognitive factors, domain expertise and experience and factors related to the business context i.e., data, visualizations and tasks; a data visualization engine that will recommend to the unique-user the best-fit data visualizations based on the abovementioned user model; and an intelligent data analytics component that enhances the efficiency and effectiveness of the data exploration process by leveraging user interactions during the explorations to further inform the user model on the user’s expertise and experience.|10.1145/3450613.3459657|https://doi.org/10.1145/3450613.3459657|New York, NY, USA|Association for Computing Machinery|9781450383660|2021|Adaptive Visualizations for Enhanced Data Understanding and Interpretation|Amyrotos, Christos|inproceedings|10.1145/3450613.3459657|||||||||||||||||||||||||||||1006460073|42
ASIST '16|Copenhagen, Denmark|knowledge transfer, user groups, ASIS&amp;T publications, publication format and processes|10||Proceedings of the 79th ASIS&amp;T Annual Meeting: Creating Knowledge, Enhancing Lives through Information &amp; Technology|This study reports the results of a 2016 online survey on perceptions and uses of ASIS&amp;T publications. The 190 survey respondents represented 26 countries and 5 continents, with 77% of participants coming from academia rather than practitioners. Among the emerging themes were calls for a wider scope of research from information science to be reflected in the publications (including JASIS&amp;T and the ASIS&amp;T Proceedings), and ongoing challenges in the role of the Bulletin as a bridge between research and practice. The study provides insights into the scholarly publishing practices of the ASIS&amp;T community and highlights key issues for the future direction of ASIS&amp;T's scholarly communication.|||USA|American Society for Information Science||2016|Needs Assessment of ASIS&amp;T Publications: Bridging Information Research and Practice|Tang, Rong and Mon, Lorri and Beheshti, Jamshid and Li, Yuelin and Pollock, Danielle and Ni, Chaoqun and Chu, Samuel and Xiao, Lu and Caffrey, Julia and Gentry, Steven|inproceedings|10.5555/3017447.3017501|54||||||||||||||||||||||||||||1006616521|42
PSBD '14|Shanghai, China|privacy-preserving data publishing, academic data publishing, privacy of big data, interaction network inference|8|3–10|Proceedings of the First International Workshop on Privacy and Secuirty of Big Data|We address the publication of a large academic information dataset addressing privacy issues. We evaluate anonymization techniques achieving the intended protection, while retaining the utility of the anonymized data. The released data could help infer behaviors and subsequently find solutions for daily planning activities, such as cafeteria attendance, cleaning schedules or student performance, or study interaction patterns among an academic population. However, the nature of the academic data is such that many implicit social interaction networks can be derived from the anonymized datasets, raising the need for researching how anonymity can be assessed in this setting.|10.1145/2663715.2669610|https://doi.org/10.1145/2663715.2669610|New York, NY, USA|Association for Computing Machinery|9781450315838|2014|Evaluating the Impact of Anonymization on Large Interaction Network Datasets|Silva, M\'{a}rio J. and Rijo, Pedro and Francisco, Alexandre|inproceedings|10.1145/2663715.2669610|||||||||||||||||||||||||||||1008954501|42
||Image quality;Adaptation models;Image edge detection;Object detection;Streaming media;Throughput;Internet of Things;IoT;Edge Computing;P2P;Video Stream;Object Detection||1427-1434|2021 IEEE 23rd Int Conf on High Performance Computing & Communications; 7th Int Conf on Data Science & Systems; 19th Int Conf on Smart City; 7th Int Conf on Dependability in Sensor, Cloud & Big Data Systems & Application (HPCC/DSS/SmartCity/DependSys)|These days, sensors and cameras are being deployed on an increasingly large scale. Furthermore, the rapid development of machine learning models for computer vision now presents novel opportunities for the use of artificial intelligence (AI) and Internet of Things (IoT) combinations in various application scenarios. However, challenges remain in supporting low-latency video streaming from distributed mobile IoT devices under dynamic network environments, and overcoming video data quality degradation that results from weather “noise”, which reduces the accuracy of AI-based data analyses such as object detection. In this paper, we propose a live video stream processing system for supporting intelligent services that integrates the following features. First, to cope with dynamic networks and achieve low latency, our approach employs a peer-to-peer (P2P)-based virtual network at the edge and a multi-tiered architecture composed of IoT cameras, edge, and cloud servers. Second, we construct a flexible messaging system for video analysis built upon SINETStream, which is a messaging system that adopts a topic-based pub/sub model. Third, we implement a framework that can remove weather-related (rain, snow, and fog) noise by applying weather classification and adaptive noise removal models that improve the accuracy of video analysis from data collected outdoors. The latency, throughput, and image quality benchmark experiments conducted to validate the feasibility of our proposed system showed that the process resulted in image quality improvements of approximately 30% (on average).|10.1109/HPCC-DSS-SmartCity-DependSys53884.2021.00214|||||2021|Intelligent Live Video Streaming for Object Detection|Chen, Mingkang and Sun, Jingtao and Aida, Kento and Figueiredo, Renato J. and Ku, Yun-Jung and Subratie, Kensworth|inproceedings|9781127||Dec|||||||||||||||||||||||||||1011596885|42
||Internet of things, Data quality, Data cleaning, Outlier detection||57-81||In the Internet of Things (IoT), data gathered from a global-scale deployment of smart-things, are the base for making intelligent decisions and providing services. If data are of poor quality, decisions are likely to be unsound. Data quality (DQ) is crucial to gain user engagement and acceptance of the IoT paradigm and services. This paper aims at enhancing DQ in IoT by providing an overview of its state-of-the-art. Data properties and their new lifecycle in IoT are surveyed. The concept of DQ is defined and a set of generic and domain-specific DQ dimensions, fit for use in assessing IoT's DQ, are selected. IoT-related factors endangering the DQ and their impact on various DQ dimensions and on the overall DQ are exhaustively analyzed. DQ problems manifestations are discussed and their symptoms identified. Data outliers, as a major DQ problem manifestation, their underlying knowledge and their impact in the context of IoT and its applications are studied. Techniques for enhancing DQ are presented with a special focus on data cleaning techniques which are reviewed and compared using an extended taxonomy to outline their characteristics and their fitness for use for IoT. Finally, open challenges and possible future research directions are discussed.|https://doi.org/10.1016/j.jnca.2016.08.002|https://www.sciencedirect.com/science/article/pii/S1084804516301564||||2016|Data quality in internet of things: A state-of-the-art survey|Aimad Karkouch and Hajar Mousannif and Hassan {Al Moatassime} and Thomas Noel|article|KARKOUCH201657|||Journal of Network and Computer Applications|10848045||73|||||||||||||||||||||||1015554720|2040591560
||Big data;Quality of service;Market research;Monitoring;Satellites;Delays;Prediction methods;Network Diagnosis;Big Data;Traffic Prediction;Large Scale Network;Complex Network||1204-1208|2015 4th International Conference on Computer Science and Network Technology (ICCSNT)|The increasing demand on higher transmission speed and shorter delay in wired networks becomes critical in recent communication networks. However, the capacity of transmission link is limited by the method of transmission. In this paper, aiming at the situation of large scale networks, an overview of the network optimization based on big data and traffic prediction is given in our proposed work. In wired networks, how to make full use of the transmission bandwidth and provide more reliable QoS is in great demand. Based on the network topology in our facility, we make a summary of current diagnosis method of the network and then propose the future possible way to solve the network malfunction based on big data through network log and complex monitors, then we make an overview of the diagnosis method based on traffic prediction, which could effectively make full use of bandwidth and avoid collision of the network.|10.1109/ICCSNT.2015.7490949|||||2015|The diagnosis of wired network malfunctions based on big data and traffic prediction: An overview|Yuan Gao and Hong Ao and Kang Wang and Weigui Zhou and Yi Li|inproceedings|7490949||Dec||||01|||||||||||||||||||||||1016030138|42
||Social networks, Big data, Content analysis, Sentiment analysis, Systematic literature review||101517||Social Networking Services (SNSs) connect people worldwide, where they communicate through sharing contents, photos, videos, posting their first-hand opinions, comments, and following their friends. Social networks are characterized by velocity, volume, value, variety, and veracity, the 5 V’s of big data. Hence, big data analytic techniques and frameworks are commonly exploited in Social Network Analysis (SNA). By the ever-increasing growth of social networks, the analysis of social data, to describe and find communication patterns among users and understand their behaviors, has attracted much attention. In this paper, we demonstrate how big data analytics meets social media, and a comprehensive review is provided on big data analytic approaches in social networks to search published studies between 2013 and August 2020, with 74 identified papers. The findings of this paper are presented in terms of main journals/conferences, yearly distributions, and the distribution of studies among publishers. Furthermore, the big data analytic approaches are classified into two main categories: Content-oriented approaches and network-oriented approaches. The main ideas, evaluation parameters, tools, evaluation methods, advantages, and disadvantages are also discussed in detail. Finally, the open challenges and future directions that are worth further investigating are discussed.|https://doi.org/10.1016/j.tele.2020.101517|https://www.sciencedirect.com/science/article/pii/S0736585320301763||||2021|Big data analytics meets social media: A systematic review of techniques, open issues, and future directions|Sepideh {Bazzaz Abkenar} and Mostafa {Haghi Kashani} and Ebrahim Mahdipour and Seyed Mahdi Jameii|article|BAZZAZABKENAR2021101517|||Telematics and Informatics|07365853||57|||||20896|1,567|Q1|66|96|483|6939|3832|438|7,45|72,28|United Kingdom|Western Europe|1984-2020|Communication (Q1); Computer Networks and Communications (Q1); Electrical and Electronic Engineering (Q1); Law (Q1)|5,351|6.182|0.00899|1017436163|1579285336
|||3|101–103|Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice|||https://doi.org/10.1145/3447404.3447411|New York, NY, USA|Association for Computing Machinery|9781450390293|2021|Ethics and Statistics|McMenemy, David|inbook|10.1145/3447404.3447411|||||||||1||||||||||||||||||||1018654091|42
||Collaboration;Ontologies;Dynamic scheduling;Vectors;Web services;Multi-Agent;Collaboration;Mechanism design||2277-2284|2013 IEEE 10th International Conference on High Performance Computing and Communications & 2013 IEEE International Conference on Embedded and Ubiquitous Computing|With the emergence of Big Data in Internet, composing existing web services for satisfying new requirements, such as data quality enhancing, effective data choosing, knowledge discovering etc, has gained daily expanding attentions and interests. Many efforts have been pursued for supporting the essential activities in service composition. However, the existing techniques only focus on passive services which are waiting there for being discovered and invoked. We argue that it might be more attractive when Web services become active entities (Service Agent) distributed in Internet which can recognize the newly emergent requirements and compete with others for realize (part of) the requirements. Retreating or refinement of Big data will hardly be accomplished by one or two data handling center, Service Agent collaboration would be a competitive method for the big data handling problem. Mostly more than one service agents have to collaborate to satisfy requirements in current internet environment especially with social networks. That could be called as the requirement driven agent collaboration. Research on such collaboration might be useful for the previous problem. We have given a preliminary model for the requirement driven agent collaboration based on a function ontology and the automated mechanism design in the earlier work. This paper extended the Function Ontology, and enhanced the AMD model. That makes the interactions in MAS generated by agent collaboration can be described. A negotiation frame for the evaluation and choice of collaboration solutions is also given in this paper. It helps the requester evaluate the possible MAS systems, and helps the service agents make decisions to choose a good enough solution by negotiation. According to the dependencies provided in Function Ontology, a specification is given to describe the execution process of the chosen MAS. And also a method is given to translate the specification to BPEL which is more standard, acceptable, and easier to understood.|10.1109/HPCC.and.EUC.2013.327|||||2013|AMD Based Service Agent Collaboration and Specification|Zheng, Liwei|inproceedings|6832210||Nov|||||||||||||||||||||||||||1020366051|42
DSIT 2021|Shanghai, China|Mass Customization, Data Quality, LiDAR Camera Technology, Horizontal Integration|5|69–73|2021 4th International Conference on Data Science and Information Technology|To solve the problem of Mass Customization Data Quantity and Quality Problem, a Horizontal Integration Scheme of MC is proposed. By using LiDAR technology to scan and identify parts information, the machining route and required parts of the workpiece are automatically planned by comparing and matching with the documents using STEP-NC standard, to realize the efficient acquisition and utilization of MC data and ensure the automation of enterprise production.|10.1145/3478905.3478920|https://doi.org/10.1145/3478905.3478920|New York, NY, USA|Association for Computing Machinery|9781450390248|2021|Research on Horizontal Integration Scheme for Mass Customization Data Quantity and Quality Problem: Horizontal Integration Scheme for MC|Fei, Yiming and Yuan, Xiaoyue and Ren, Mengmeng and Fan, Shuhai|inproceedings|10.1145/3478905.3478920|||||||||||||||||||||||||||||1020519458|42
CSIIRW '13|Oak Ridge, Tennessee, USA|analytics, complex event processing, orchestration, Hadoop, big data, security, intelligence, trust|4||Proceedings of the Eighth Annual Cyber Security and Information Intelligence Research Workshop|Cyberspaces require both the implementation of customized functional requirements and the enforcement of policy constraints to be trustworthy. In tailored, distributed and adaptive environments (spaces), monitoring to ensure this enforcement is especially difficult given the wide spectrum of activities performed and the evolving range of threats. Spaces must be monitored from a multitude of perspectives, each of which will generate a vast quantity of disparate information, including structured, semi-structured and unstructured data. However, existing security toolsets and offerings are not yet well equipped to analyze these kinds of data with the necessary speed and agility. Big Data technologies, such as Hadoop, enable the analysis of large and unstructured data sources. We propose security operations teams extend their existing security infrastructure with emerging Big Data analytics and Complex Event Processing platforms. To help them do so, we introduce a conceptual blueprint for the analytics solution. We also present an Orchestrated Analytical Security operational and organizational framework that helps organizations understand how analytical security not only provides monitoring but also creates actionable intelligence from data.|10.1145/2459976.2459991|https://doi.org/10.1145/2459976.2459991|New York, NY, USA|Association for Computing Machinery|9781450316873|2013|Enabling Trustworthy Spaces via Orchestrated Analytical Security|Howes, Joshua and Solderitsch, James and Chen, Ignatius and Craighead, Jont\'{e}|inproceedings|10.1145/2459976.2459991|13||||||||||||||||||||||||||||1020740895|42
||Big Data;Cyber-physical systems;Safety;Real-time systems;Sensors;Big data;cyber-physical system (CPS);security;real-time;standardization;infrastructure;data quality||72-78|2017 8th IEEE Annual Information Technology, Electronics and Mobile Communication Conference (IEMCON)|The use of big data for cyber-physical systems (CPS) is gaining more importance due to the ever-increasing amount of collectable data. Due to the decreasing cost of sensors and the growth of embedded systems, which are increasingly used in the industries as well as in the private sectors, new methods are needed to evaluate and process the collected data. Therefore, in this paper we proposed a framework to handle big data for cyber-physical systems. The framework considered the possible solutions that would be standardization, cloud computing, online and data stream learning, a methodology to process data and multi-agent systems for CPS. Furthermore, we examine the security challenges and big data issues of cyber-physical systems.|10.1109/IEMCON.2017.8117153|||||2017|A framework to handle big data for cyber-physical systems|ur Rehman, Shafiq and Hark, Andre and Gruhn, Volker|inproceedings|8117153||Oct|||||||||||||||||||||||||||1021786671|42
||Service-oriented architecture;web crawler;sensor network;Sensor Observation Service (SOS);Tomact;Istsos||1-7|2015 23rd International Conference on Geoinformatics|Along with the development of the sensor system and sensor network, the wide applications of sensor networks have arisen at the historic moment. In reality, all kinds of sensors monitor every aspect of our life, which provides various services and brings the challenge: how to effectively integrate those distributed sensor resources and then can be used to find more advanced information or implement the sharing of resources are the big problems to be solved. Based on the framework of Sensor Web Enablement(SWE) which was proposed by Open GIS Consortium (OGC)and combined with the function of web crawler, we study and find Sensor Observation Service (SOS) service which is the core components of the SWE then we design a system based on the web crawler technology and the Istituto Scienze della Terra Sensor Observation Service (Istsos) architecture. The design of sensor network technology integration architecture includes three parts. The layer of data access which is the lowest layer encapsulates the access to the database or other source of resources. The layer of business logic it provides the core operation of component which was named Request Operator, this layer is used for processing various requests from the lowest layer in order to return the classes of listening. The layer of web and the client is connected, which can provide some thin client of SOS. The published server includes the ability of new services creation, addition of new sensors and relative metadata, visualization, and manipulation of stored observations, registration of new measures and setting of system properties like observable properties and data quality codes. In order to get sensor data, web crawler technology is used in our research, which can make us get sensor data from the target website, and the standardized sensor data is gotten by filtering the original data and then the data is uploaded to the database of Istsos with the standardized format. At last, the implementation of SOS architecture has been configured. The test's results show that the integrated architecture of services can effectively obtain the required sensor data and display them graphically.|10.1109/GEOINFORMATICS.2015.7378670|||||2015|The integration technology of sensor network based on web crawler|Yan Zhou and Haitian Xie|inproceedings|7378670||June||2161024X|||||||21100285411|0,132|Q4|8|0|212|0|76|209|0,30|0,00|United States|Northern America|2013, 2016-2018|Computer Networks and Communications (Q4); Computer Vision and Pattern Recognition (Q4); Electrical and Electronic Engineering (Q4); Geography, Planning and Development (Q4); Information Systems (Q4); Software (Q4)||||1023100270|493973147
||training programs, property-based debugging, neural networks||||Nowadays, we are witnessing an increasing effort to improve the performance and trustworthiness of Deep Neural Networks (DNNs), with the aim to enable their adoption in safety critical systems such as self-driving cars or aircraft collision-avoidance systems. Multiple testing techniques are proposed to generate test cases that can expose inconsistencies in the behavior of DNN models. These techniques assume implicitly that the training program is bug-free and appropriately configured. However, satisfying this assumption for a novel problem requires significant engineering work to prepare the data, design the DNN, implement the training program, and tune the hyperparameters in order to produce the model for which current automated test data generators search for corner-case behaviors. All these model training steps can be error-prone. Therefore, it is crucial to detect and correct errors throughout all the engineering steps of DNN-based software systems and not only on the resulting DNN model. In this paper, we gather a catalog of training issues and based on their symptoms and their effects on the behavior of the training program, we propose practical verification routines to detect the aforementioned issues, automatically, by continuously validating that some important properties of the learning dynamics hold during the training. Then, we design, TheDeepChecker, an end-to-end property-based debugging approach for DNN training programs and implement it as a TensorFlow-based library. As an empirical evaluation, we conduct a case study to assess the effectiveness of TheDeepChecker on synthetic and real-world buggy DL programs and compare its performance to that of the Amazon SageMaker Debugger (SMD). Results show that TheDeepChecker’s on-execution validation of DNN-based program’s properties through three sequential phases (pre-, on-, and post-fitting), succeeds in revealing several coding bugs and system misconfigurations errors, early on and at a low cost. Moreover, our property-based approach outperforms the SMD’s offline rules verification on training logs in terms of detection accuracy for unstable learning issues and coverage of additional DL bugs.|10.1145/3529318|https://doi.org/10.1145/3529318|New York, NY, USA|Association for Computing Machinery||2022|Testing Feedforward Neural Networks Training Programs|Braiek, Houssem Ben and Khomh, Foutse|article|10.1145/3529318||may|ACM Trans. Softw. Eng. Methodol.|1049331X||||Just Accepted|||||||||||||||||||||1023176448|1429302734
||Big data, Supply chain management, Operations management, Value creation, Business analytics, Capabilities||539-548||Big data has played an increasingly important role in using data to improve business value. In response to several big data challenges, the purpose of this study is to identify firm-level capabilities required to create value from big data. The adjacent theories of business process management and IT business value underpinned the study, together with an in-depth case study that led to the identification of twenty-four types of capabilities related to IT, process, performance, human, strategic, and organizational practices. The findings confirmed the application of practices and capabilities of adjacent theories, as well as certain practices and attributes that were both changed and reinforced at the intersection of big data. As an outstanding additional support to the extant big data studies, this work empirically confirms and portrays hitherto unexplored capabilities of big data and set their roles, thus providing a holistic overview of firm-level capabilities that are required for big data value creation.|https://doi.org/10.1016/j.jbusres.2020.07.036|https://www.sciencedirect.com/science/article/pii/S0148296320304859||||2021|Firm-level capabilities towards big data value creation|Morten Brinch and Angappa Gunasekaran and Samuel {Fosso Wamba}|article|BRINCH2021539|||Journal of Business Research|01482963||131|||||20550|2,049|Q1|195|878|1342|73221|11507|1315|7,38|83,40|United States|Northern America|1973-2021|Marketing (Q1)|46,935|7.550|0.03523|1024204885|1502892296
||Big data, Veracity, Granularity, Heterogeneous datasets, Humanitarian, Value||453-468||This paper examines the challenges of leveraging big data in the humanitarian sector in support of UN Sustainable Development Goal 17 “Partnerships for the Goals”. The full promise of Big Data is underpinned by a tacit assumption that the heterogeneous ‘exhaust trail’ of data is contextually relevant and sufficiently granular to be mined for value. This promise, however, relies on relationality – that patterns can be derived from combining different pieces of data that are of corresponding detail or that there are effective mechanisms to resolve differences in detail. Here, we present empirical work integrating eight heterogeneous datasets from the humanitarian domain to provide evidence of the inherent challenge of complexity resulting from differing levels of data granularity. In clarifying this challenge, we explore the reasons why it is manifest, discuss strategies for addressing it and, as our principal contribution, identify five propositions to guide future research.|https://doi.org/10.1016/j.jbusres.2020.09.035|https://www.sciencedirect.com/science/article/pii/S0148296320306172||||2021|Exploring future challenges for big data in the humanitarian domain|David Bell and Mark Lycett and Alaa Marshan and Asmat Monaghan|article|BELL2021453|||Journal of Business Research|01482963||131|||||20550|2,049|Q1|195|878|1342|73221|11507|1315|7,38|83,40|United States|Northern America|1973-2021|Marketing (Q1)|46,935|7.550|0.03523|1025409935|1502892296
||||241-246||AI and big data technologies have been increasingly deployed to process complex, heterogeneous, high-resolution environmental data, and generate results at greater speeds and higher accuracies to facilitate environmental decision-making. However, current attempts to develop reliable AI and big data technologies for environmental decision-making are still inadequate. In this special issue, AI for Social Good: AI and Big Data Approaches for Environmental Decision-Making, we attempt to address the following important questions: What are the conditions for AI and big data technologies to facilitate environmental decision-making? How can AI and big data be used to facilitate environmental decision-making? Do AI and big data serve those most at risk of environmental pollution? Who should own and govern AI and big data? This special issue brings together researchers in relevant fields of AI and environmental science to address these pertinent questions. First, we will review the existing works which attempt to address these four questions. Second, we summarize the significance and novelty of six articles included in our special issue in addressing these four questions. Finally, we highlight the important principles of AI for Social Good, which can help distinguish good from bad environmental decisions based on AI and big data technologies.|https://doi.org/10.1016/j.envsci.2021.09.001|https://www.sciencedirect.com/science/article/pii/S1462901121002471||||2021|AI for Social Good: AI and Big Data Approaches for Environmental Decision-Making|Victor O.K. Li and Jacqueline C.K. Lam and Jiahuan Cui|article|LI2021241|||Environmental Science & Policy|14629011||125|||||||||||||||||||||||1029162636|1729298893
||Adaptation models;Speech;Acoustics;Computational modeling;Engines;Big data;Speech recognition;Big Data;Cloud Computing;Hadoop;Speech Recognition;Scalability;Personalization||64-73|2014 IEEE/ACM International Symposium on Big Data Computing|We observe that the recent advances in big data computing have empowered the personalization of service including model-based services such as speech recognition, face recognition, and context-aware service. Various sources of user's logs can be utilized in remodeling, adapting, and personalizing pretrained models to improve the quality of service. We propose a system that can support store/retrieve data and process them in a scalable manner on top of Samsung' big data infrastructure. An automatic speech recognition (ASR) service such as Samsung's S-Voice, Apple's SIRI is one of the representative examples. Recently advances in ASR married with big data technologies drive more personalized services in many areas of services. A speaker adaptation is now a well-accepted technology that requires huge computation cost in creating a personalized acoustic model and corresponding language model over several billions of Samsung product users. We implement a personalized and scalable ASR system powered by the big data infrastructure which brings data-driven personalized opportunities to voice-enabled services such as voice-to-text transcriber, voice-enabled web search in a peta bytes scale. We verify the feasibility of speaker adaptation based on 107 testers' recordings and obtain about 10% of recognition accuracy. An optimal set of performance optimization is suggested to have the best performance such as workflow compaction, file compression, best file system selection among several distributed file systems.|10.1109/BDC.2014.11|||||2014|A Practical Approach to Scalable Big Data Computing for the Personalization of Services at Samsung|Ahnn, Jong Hoon|inproceedings|7321730||Dec|||||||||||||||||||||||||||1031047994|42
BDIOT2017|London, United Kingdom|Advanced multitenant Hadoop, Metadata, Big data, Open data, Data authorization|4|48–51|Proceedings of the International Conference on Big Data and Internet of Thing|Nowadays, there has been an immense amount of data coming from various devices sensors, social networks and IoT services. Among these data, open data is playing more and more important role in practice. Many individuals and organizations collect a broad range of different types of data in order to perform their analytic tasks. However, the current open data platforms still have many limitations. Among the drawbacks, data management, an important process of analytic service development, needs to be improved significantly. The main reason is that the emergence of massive data explosion coming from various sources has been making the process become more and more complicated and costly. Therefore, we propose here a system related to the field of data management to allow multitenant users to find and access easily their desired data as well as metadata. It also helps improve the performance of platform.|10.1145/3175684.3175719|https://doi.org/10.1145/3175684.3175719|New York, NY, USA|Association for Computing Machinery|9781450354301|2017|Advanced Multitenant Hadoop in Smart Open Data Platform|Nguyen, Minh Chau and Won, Hee Sun|inproceedings|10.1145/3175684.3175719|||||||||||||||||||||||||||||1031579418|42
ICICM '18|Edinburgh, United Kingdom|safety service, big data governance, community safety, scenario model|6|44–49|Proceedings of the 8th International Conference on Information Communication and Management|In community safety service, big data governance is the prime mode to achieve community safety big data sharing and service value increasing. Although existing researches have preliminarily established the general big data governance framework, identification and governance of specific sharing problems lack comprehensive and systematic scenario description. Applying software engineering method, this paper proposes a kind of scenario expression model of big data governance in community safety service. Considering the common features of big data governance scenarios, construct the meta-scenario model of big data governance in community safety services. Considering the scenario expression difference under different levels, scales and particle sizes, construct the full view scenario model of big data governance in community safety services by meta-models nesting to complete the scenario expression under different applying situation. Finally, a use case is proposed to verify the rationality and effectiveness of the scenario expression models.|10.1145/3268891.3268892|https://doi.org/10.1145/3268891.3268892|New York, NY, USA|Association for Computing Machinery|9781450365024|2018|Full View Scenario Model of Big Data Governance in Community Safety Service|Liu, Zhao-ge and Li, Xiang-yang|inproceedings|10.1145/3268891.3268892|||||||||||||||||||||||||||||1032477357|42
||Big Data, business intelligence, BI, analytics, predictive analytics, program manager, program management, project manager, project management, challenges, data warehouse, data warehousing, enterprise data warehouse, EDW||77-82|Enterprise Business Intelligence and Data Warehousing|The Big Data era is creating seismic shifts in how we approach enterprise business intelligence and data warehousing. This final chapter discusses considerations related to technology and architecture, analytics-oriented requirements collection, and organizational structure.|https://doi.org/10.1016/B978-0-12-801540-7.00008-1|https://www.sciencedirect.com/science/article/pii/B9780128015407000081|Boston|Morgan Kaufmann|978-0-12-801540-7|2015|Chapter 8 - Considerations for the Big Data Era|Alan Simon|incollection|SIMON201577||||||||||Alan Simon|||||||||||||||||||1035715617|42
CompSysTech '19|Ruse, Bulgaria|Emerging Architectures, Big Data, GATE Platform, Smart City, Big Data Value Chain|8|261–268|Proceedings of the 20th International Conference on Computer Systems and Technologies|Today we experience a data-driven society. All human activities, industrial processes and research lead to data generation of unprecedented scale, spurring new products, services and businesses. Big Data and its application have been a target for European Commission -- with more than 100 FP7 and about 50 H2020 funded projects under Big Data domain. GATE project aims to establish and sustain in the long run a Centre of Excellence as collaborative environment for conducting Big Data research and innovation, facilitated by GATE platform and Innovation Labs. This paper proposes a conceptual architecture of GATE platform, that is holistic, symbiotic, open, evolving and data-integrated. It is also modular and with component-based design that allows to position a mix of products and tools from different providers. GATE platform will enable start-ups, SMEs and large enterprises, as well as other organizations in a wide range of sectors, to build advanced Data driven services and applications. The usability of the proposed architecture is proven through a development of a sample time series data visualization application. Its architecture follows the proposed one through implementation of required components using open technology stack.|10.1145/3345252.3345282|https://doi.org/10.1145/3345252.3345282|New York, NY, USA|Association for Computing Machinery|9781450371490|2019|Conceptual Architecture of GATE Big Data Platform|Petrova-Antonova, Dessislava and Krasteva, Iva and Ilieva, Sylvia and Pavlova, Irena|inproceedings|10.1145/3345252.3345282|||||||||||||||||||||||||||||1035921812|42
ICSIM 2022|Yokohama, Japan|Kafka, anomaly detection, isomerism and heterogeneous data, serialization and deserialization|5|95–99|2022 The 5th International Conference on Software Engineering and Information Management (ICSIM)|With the development of big data technology, data accessed by big data platforms maintain the features of mass, isomerism, heterogeneous, and streaming. Therefore, how to access the varied data sources of isomerism and heterogeneous data and how to process and analyze the data become the current challenges. In this paper, we design and implement a data aggregation and anomaly detection system for isomerism and heterogeneous data. The system proposes a novel isomerism and heterogeneous data access sub-system. The sub-system applies improved Avro as the unified data description format and presents different storage algorithms for data serialization to raise the data adaption efficiency. The system adopts Kafka as the message middleware for data aggregation and distribution. Also, we design the anomaly detection and alarming sub-system for detecting the anomalies of streaming data on time and notifying the users. The data aggregation and anomaly detection system has passed all the tests and applied in small and medium-sized enterprises.|10.1145/3520084.3520099|https://doi.org/10.1145/3520084.3520099|New York, NY, USA|Association for Computing Machinery|9781450395519|2022|Data Aggregation and Anomaly Detection System for Isomerism and Heterogeneous Data|Li, Yunze and Wu, Yuxuan and Tang, Ruisen|inproceedings|10.1145/3520084.3520099|||||||||||||||||||||||||||||1036278950|42
||Traffic, Time series, Air quality maps, Real-time data, Spatio-temporal data, Smart cities||100292||This paper presents a system that employs information visualization techniques to analyze urban traffic data and the impact of traffic emissions on urban air quality. Effective visualizations allow citizens and public authorities to identify trends, detect congested road sections at specific times, and perform monitoring and maintenance of traffic sensors. Since road transport is a major source of air pollution, also the impact of traffic on air quality has emerged as a new issue that traffic visualizations should address. Trafair Traffic Dashboard exploits traffic sensor data and traffic flow simulations to create an interactive layout focused on investigating the evolution of traffic in the urban area over time and space. The dashboard is the last step of a complex data framework that starts from the ingestion of traffic sensor observations, anomaly detection, traffic modeling, and also air quality impact analysis. We present the results of applying our proposed framework on two cities (Modena, in Italy, and Santiago de Compostela, in Spain) demonstrating the potential of the dashboard in identifying trends, seasonal events, abnormal behaviors, and understanding how urban vehicle fleet affects air quality. We believe that the framework provides a powerful environment that may guide the public decision-makers through effective analysis of traffic trends devoted to reducing traffic issues and mitigating the polluting effect of transportation.|https://doi.org/10.1016/j.bdr.2021.100292|https://www.sciencedirect.com/science/article/pii/S221457962100109X||||2022|Big Data Analytics and Visualization in Traffic Monitoring|Chiara Bachechi and Laura Po and Federica Rollo|article|BACHECHI2022100292|||Big Data Research|22145796||27|||||21100356018|0,565|Q2|25|11|76|547|324|69|4,06|49,73|United States|Northern America|2014-2020|Computer Science Applications (Q2); Information Systems (Q2); Information Systems and Management (Q2); Management Information Systems (Q2)|586|3.578|0.00126|1036802067|1627174784
||||100342||In recent years, more and more single cell technologies have been developed. A vast amount of single cell omics data has been generated by large projects, such as Human Cell Atlas (HCA), Mouse Cell Atlas, Mouse RNA Atlas, Mouse ATAC Atlas, and Plant Cell Atlas. Based on these single cell big data, thousands of bioinformatics algorithms for quality control, clustering, cell type annotation, developmental inference, cell-cell transition, cell-cell interaction, and spatial analysis are developed. With powerful experimental single cell technology and state-of-the-art big data analysis methods based on Artificial Intelligence (AI), the molecular landscape at the single cell level can be revealed. With spatial transcriptomics and single cell multi-omics, even the spatial dynamic multi-level regulatory mechanisms can be deciphered. Such single cell technologies have many successful applications in oncology, assisted reproduction, embryonic development, and plant breeding. We not only reviewed the experimental and bioinformatics methods for single cell research, but also discussed their applications in various fields and forecasted the future directions for single cell technologies. We believe that spatial transcriptomics and single cell multi-omics will become the next booming business for the mechanism research and commercial industry.|https://doi.org/10.1016/j.xinn.2022.100342|https://www.sciencedirect.com/science/article/pii/S2666675822001382||||2022|Single cell technologies: From research to application|Lu Wen and Guoqiang Li and Tao Huang and Wei Geng and Hao Pei and Jialiang Yang and Miao Zhu and Pengfei Zhang and Rui Hou and Geng Tian and Wentao Su and Jian Chen and Dake Zhang and Pingan Zhu and Wei Zhang and Xiuxin Zhang and Ning Zhang and Yunlong Zhao and Xin Cao and Guangdun Peng and Xianwen Ren and Nan Jiang and Caihuan Tian and Zijiang Chen|article|WEN2022100342|||The Innovation|26666758|||||||||||||||||||||||||1037987543|1734043345
||Big data, Enriched sea salp optimization, Wearable sensors, Health care monitoring systems||101010||The rapid development of communication technologies and expert systems have resulted in a large volume of medical data. Big data such as clinical data, omics data, and electronic health data are difficult to manage in real-time due to noise, large size, different formats, missing values and large features. Hence, it is more difficult for the health monitoring system to extract the correct information. Low quality and noisy data can lead to unnecessary treatment. To overcome these issues, we proposed Enriched Salp Swarm Optimization based Bidirectional Long Short Term Memory (ESSOBiLSTM) to monitor health. This method consists of four layers, such as the data collection layer, data storage layer, data analytics, and presentation layer. The initial layer handles a variety of information from main sources: wearable sensor devices (WSD), social network data, and medical records (MR). The second layer stores all the collected data from WSD, MR, and social network data to the cloud server through the wireless network. The proposed framework for performing big data analytics steps like preprocessing, filtering, dimensionality reduction, and classification is performed in the third layer. In the final layer, the doctor analyzes the patient's condition based on the classification results of the enriched SSO-BiLSTM. Based on the evaluation report, the proposed ESSOBiLSTM gives an accuracy of 85%, precision of 80%, RMSE of 0.6, MAE of 0.58, recall of 85% and F-measure of 79%. As a result, ESSOBiLSTM has proven to be more effective in monitoring health in large datasets.|https://doi.org/10.1016/j.imu.2022.101010|https://www.sciencedirect.com/science/article/pii/S2352914822001538||||2022|Deep enriched salp swarm optimization based bidirectional -long short term memory model for healthcare monitoring system in big data|Geetika Dhand and Kavita Sheoran and Parul Agarwal and Siddhartha Sankar Biswas|article|DHAND2022101010|||Informatics in Medicine Unlocked|23529148||32|||||21100780477|0,440|Q3|21|208|219|9529|798|217|3,37|45,81|United Kingdom|Western Europe|2015-2020|Health Informatics (Q3)||||1038591104|1420175481
ISAIMS 2020|Beijing, China|Medical sciences, Artificial intelligence, Bibliometric analysis, Development status, Information technology|8|257–264|Proceedings of the 2020 International Symposium on Artificial Intelligence in Medical Sciences|Objective: To compare the development status of artificial intelligence (AI) in medicine among the United States (US), China and India with bibliometric analysis. Methods: Articles involving AI in medicine published from 2015 to 2019 were retrieved on March 30, 2020 from Web of Science Core Collection. The country-level and the institution-level performance of the US, China and India in the field of AI in medicine were compared with indicators including the amount of papers, 5-year Compound Annual Growth Rate (CAGR) of the amount of papers, the amount of highly-cited papers, the proportion of highly-cited papers and the average citations per paper. In addition, the research hotspots and international cooperation of the three countries in recent 5 years were compared by conducting keywords co-occurrence analysis and co-authorship analysis in VOSviewer. Results: From 2015 to 2019, The US has published 7838 papers and 154 highly-cited papers in the field of AI in medicine, with an average citations per paper to be 9.3, and the proportion of highly-cited papers to be 2.0 %. China has output 6635 papers and 73 highly-cited papers in this field, with an average citations per paper to be 5.3, and the proportion of highly-cited papers to be 1.1%. India has output 3895 papers and 22 highly-cited papers in this field, with an average citations per paper to be 3.6, and the proportion of highly-cited papers to be 0.6%. The 5-year CAGR of the US, China and India in the period of 2015~2019 were 16.0%, 25.4% and 2.4%, respectively. At the institutional level, most of these indicators were significantly better for the US institutions than for Chinese and Indian ones. There were four research hotspots in this field, namely medical imaging technology, health big data mining, disease prediction with biomarkers and genetic information, and early diagnosis of neurological disease. The three countries focused on different hotspots, with China focusing relatively less on health big data mining, while the US and India being complementary to each other. As to international cooperation, the average links per paper to other countries were 0.60, 0.40 and 0.20, respectively, for the US, China and India. Conclusions: In the field of AI in medicine, the US, with a number of competitive institutions in AI and medical researches, is taking a definitely leading role, having conducted many innovative researches and cooperated extensively with other countries. China is taking the second leading role at the country level, with top institutions somewhat less productive than those in the US. India is the third productive country, with top institutions obvious less productive than those in the US, and with research hotspots exactly complementary to the US.|10.1145/3429889.3429938|https://doi.org/10.1145/3429889.3429938|New York, NY, USA|Association for Computing Machinery|9781450388603|2020|Artificial Intelligence in Medicine in the United States, China and India|Chen, Juan and Lu, Yan and Zhang, Ting and Ouyang, Zhaolian|inproceedings|10.1145/3429889.3429938|||||||||||||||||||||||||||||1040052746|42
||Machine learning, Chemical, Toxicity, Environmental health, Big data||129487||Over the past few decades, data-driven machine learning (ML) has distinguished itself from hypothesis-driven studies and has recently received much attention in environmental toxicology. However, the use of ML in environmental toxicology remains in the early stages, with knowledge gaps, technical bottlenecks in data quality, high-dimensional/heterogeneous/small-sample data analysis and model interpretability, and a lack of an in-depth understanding of environmental toxicology. Given the above problems, we review the recent progress in the literature and highlight state-of-the-art toxicological studies using ML (such as learning and predicting toxicity in complicated biosystems and multiple-factor environmental scenarios of long-term and large-scale pollution). Beyond predicting simple biological endpoints by integrating untargeted omics and adverse outcome pathways, ML development should focus on revealing toxicological mechanisms. The integration of data-driven ML with other methods (e.g., omics analysis and adverse outcome pathway frameworks) endows ML with widely promising application in revealing toxicological mechanisms. High-quality databases and interpretable algorithms are urgently needed for toxicology and environmental science. Addressing the core issues and future challenges for ML in this review may narrow the knowledge gap between environmental toxicity and computational science and facilitate the control of environmental risk in the future.|https://doi.org/10.1016/j.jhazmat.2022.129487|https://www.sciencedirect.com/science/article/pii/S0304389422012808||||2022|Machine learning in the identification, prediction and exploration of environmental toxicology: Challenges and perspectives|Xiaotong Wu and Qixing Zhou and Li Mu and Xiangang Hu|article|WU2022129487|||Journal of Hazardous Materials|03043894||438|||||25858|2,034|Q1|284|2143|2994|126809|31399|2975|10,39|59,17|Netherlands|Western Europe|1975-2021|Environmental Chemistry (Q1); Environmental Engineering (Q1); Health, Toxicology and Mutagenesis (Q1); Pollution (Q1); Waste Management and Disposal (Q1)|137,983|10.588|0.07094|1044067583|586168767
||Urban areas;Big Data;Data analysis;Transportation;Cloud computing;Data mining;Europe;Big data;cloud computing;data analytics;data privacy;data quality;distributed environment;public transport management;smart city||117652-117677||Smart urban transportation management can be considered as a multifaceted big data challenge. It strongly relies on the information collected into multiple, widespread, and heterogeneous data sources as well as on the ability to extract actionable insights from them. Besides data, full stack (from platform to services and applications) Information and Communications Technology (ICT) solutions need to be specifically adopted to address smart cities challenges. Smart urban transportation management is one of the key use cases addressed in the context of the EUBra-BIGSEA (Europe-Brazil Collaboration of Big Data Scientific Research through Cloud-Centric Applications) project. This paper specifically focuses on the City Administration Dashboard, a public transport analytics application that has been developed on top of the EUBra-BIGSEA platform and used by the Municipality stakeholders of Curitiba, Brazil, to tackle urban traffic data analysis and planning challenges. The solution proposed in this paper joins together a scalable big and fast data analytics platform, a flexible and dynamic cloud infrastructure, data quality and entity matching algorithms as well as security and privacy techniques. By exploiting an interoperable programming framework based on Python Application Programming Interface (API), it allows an easy, rapid and transparent development of smart cities applications.|10.1109/ACCESS.2019.2936941|||||2019|An Integrated Big and Fast Data Analytics Platform for Smart Urban Transportation Management|Fiore, Sandro and Elia, Donatello and Pires, Carlos Eduardo and Mestre, Demetrio Gomes and Cappiello, Cinzia and Vitali, Monica and Andrade, Nazareno and Braz, Tarciso and Lezzi, Daniele and Moraes, Regina and Basso, Tania and Kozievitch, Nádia P. and Fonseca, Keiko Verônica Ono and Antunes, Nuno and Vieira, Marco and Palazzo, Cosimo and Blanquer, Ignacio and Meira, Wagner and Aloisio, Giovanni|article|8809689|||IEEE Access|21693536||7|||||21100374601|0,587|Q1|127|18036|24267|771081|116691|24200|4,48|42,75|United States|Northern America|2013-2020|Computer Science (miscellaneous) (Q1); Engineering (miscellaneous) (Q1); Materials Science (miscellaneous) (Q2)|105,968|3.367|0.15396|1046486556|1905633267
||Big data, Graph data, Spatiotemporal compression, Cloud computing, Scheduling||1563-1583||It is well known that processing big graph data can be costly on Cloud. Processing big graph data introduces complex and multiple iterations that raise challenges such as parallel memory bottlenecks, deadlocks, and inefficiency. To tackle the challenges, we propose a novel technique for effectively processing big graph data on Cloud. Specifically, the big data will be compressed with its spatiotemporal features on Cloud. By exploring spatial data correlation, we partition a graph data set into clusters. In a cluster, the workload can be shared by the inference based on time series similarity. By exploiting temporal correlation, in each time series or a single graph edge, temporal data compression is conducted. A novel data driven scheduling is also developed for data processing optimisation. The experiment results demonstrate that the spatiotemporal compression and scheduling achieve significant performance gains in terms of data size and data fidelity loss.|https://doi.org/10.1016/j.jcss.2014.04.022|https://www.sciencedirect.com/science/article/pii/S002200001400066X||||2014|A spatiotemporal compression based approach for efficient big data processing on Cloud|Chi Yang and Xuyun Zhang and Changmin Zhong and Chang Liu and Jian Pei and Kotagiri Ramamohanarao and Jinjun Chen|article|YANG20141563|||Journal of Computer and System Sciences|00220000|8|80||Special Issue on Theory and Applications in Parallel and Distributed Computing Systems|||||||||||||||||||||1046955861|1380600830
||Connected health, Integrated care, Personal health system, Electronic health||22-27||The increasingly aging population in Europe and worldwide brings up the need for the restructuring of healthcare. Technological advancements in electronic health can be a driving force for new health management models, especially in chronic care. In a patient-centered e-health management model, communication and coordination between patient, healthcare professionals in primary care and hospitals can be facilitated, and medical decisions can be made timely and easily communicated. Bringing the right information to the right person at the right time is what connected health aims at, and this may set the basis for the investigation and deployment of the integrated care models. In this framework, an overview of the main technological axes and challenges around connected health technologies in chronic disease management are presented and discussed. A central concept is personal health system for the patient/citizen and three main application areas are identified. The connected health ecosystem is making progress, already shows benefits in (a) new biosensors, (b) data management, (c) data analytics, integration and feedback. Examples are illustrated in each case, while open issues and challenges for further research and development are pinpointed.|https://doi.org/10.1016/j.maturitas.2015.03.015|https://www.sciencedirect.com/science/article/pii/S0378512215006052||||2015|Connected health and integrated care: Toward new models for chronic disease management|Ioanna G. Chouvarda and Dimitrios G. Goulis and Irene Lambrinoudaki and Nicos Maglaveras|article|CHOUVARDA201522|||Maturitas|03785122|1|82||PERSONALIZED HEALTHCARE FOR MIDLIFE AND BEYOND|||27660|1,346|Q1|105|146|527|5638|2319|475|3,97|38,62|Ireland|Western Europe|1978-2021|Biochemistry, Genetics and Molecular Biology (miscellaneous) (Q1); Obstetrics and Gynecology (Q1)|9,715|4.342|0.01085|1050617483|222761207
||Agriculture supply chain, GIS analytics, Big data analytics, Internet of things, Drones, Smart farming||103-120||The world population is estimated to reach nine billion by 2050. Many challenges are adding pressure on the current agriculture supply chains that include shrinking land sizes, ever increasing demand for natural resources and environmental issues. The agriculture systems need a major transformation from the traditional practices to precision agriculture or smart farming practices to overcome these challenges. Geographic information system (GIS) is one such technology that pushes the current methods to precision agriculture. In this paper, we present a systematic literature review (SLR) of 120 research papers on various applications of big GIS analytics (BGA) in agriculture. The selected papers are classified into two broad categories; the level of analytics and GIS applications in agriculture. The GIS applications viz., land suitability, site search and selection, resource allocation, impact assessment, land allocation, and knowledge-based systems are considered in this study. The outcome of this study is a proposed BGA framework for agriculture supply chain. This framework identifies big data analytics to play a significant role in improving the quality of GIS application in agriculture and provides the researchers, practitioners, and policymakers with guidelines on the successful management of big GIS data for improved agricultural productivity.|https://doi.org/10.1016/j.compag.2018.10.001|https://www.sciencedirect.com/science/article/pii/S0168169918311311||||2018|Big GIS analytics framework for agriculture supply chains: A literature review identifying the current trends and future perspectives|Rohit Sharma and Sachin S. Kamble and Angappa Gunasekaran|article|SHARMA2018103|||Computers and Electronics in Agriculture|01681699||155|||||30441|1,208|Q1|115|648|1300|28725|9479|1298|7,27|44,33|Netherlands|Western Europe|1985-2020|Agronomy and Crop Science (Q1); Animal Science and Zoology (Q1); Computer Science Applications (Q1); Forestry (Q1); Horticulture (Q1)|17,657|5.565|0.01646|1054087702|1531073408
||Linked Open Data, Multidimensional modelling, Conceptual modelling, RDF Data Cube vocabulary, Semantic web, Big data||103378||Most organisations using Open Data currently focus on data processing and analysis. However, although Open Data may be available online, these data are generally of poor quality, thus discouraging others from contributing to and reusing them. This paper describes an approach to publish statistical data from public repositories by using Semantic Web standards published by the W3C, such as RDF and SPARQL, in order to facilitate the analysis of multidimensional models. We have defined a framework based on the entire lifecycle of data publication including a novel step of Linked Open Data assessment and the use of external repositories as knowledge base for data enrichment. As a result, users are able to interact with the data generated according to the RDF Data Cube vocabulary, which makes it possible for general users to avoid the complexity of SPARQL when analysing data. The use case was applied to the Barcelona Open Data platform and revealed the benefits of the application of our approach, such as helping in the decision-making process.|https://doi.org/10.1016/j.csi.2019.103378|https://www.sciencedirect.com/science/article/pii/S0920548919300480||||2020|Adding value to Linked Open Data using a multidimensional model approach based on the RDF Data Cube vocabulary|Pilar Escobar and Gustavo Candela and Juan Trujillo and Manuel Marco-Such and Jesús Peral|article|ESCOBAR2020103378|||Computer Standards & Interfaces|09205489||68|||||24303|0,556|Q1|63|36|246|2302|1013|243|3,71|63,94|Netherlands|Western Europe|1985-2020|Law (Q1); Hardware and Architecture (Q2); Software (Q2)||||1054449011|827980402
||Feature extraction;Mutual information;Sparks;Algorithm design and analysis;Redundancy;Big Data;Classification algorithms;feature selection;filter method;parallel computing;apache spark;mRMR;SVM||16-20|2017 IEEE 4th International Conference on Soft Computing & Machine Intelligence (ISCMI)|Recently, enormous volumes of data are generated in information systems. That's why data mining area is facing new challenges of transforming this “big data” into useful knowledge. In fact, “big data” relies low density of information (low data quality) and data redundancy, which negatively affect the data mining process. Therefore, when the number of variables describing the data is high, features selection methods are crucial for selecting relevant data. Features selection is the process of identifying the most relevant variables and removing those are redundant and irrelevant. In this paper, we propose a parallel, scalable feature selection algorithm based on mRMR (Max-Relevance and Min-Redundancy) in Spark, an in-memory parallel computing framework specialized in computation for large distributed datasets. Our experiments using real-world data of high dimensionality demonstrated that our proposition scale well and efficiently with large datasets.|10.1109/ISCMI.2017.8279590|||||2017|A large-scale filter method for feature selection based on spark|Marone, Reine Marie and Camara, Fodé and Ndiaye, Samba|inproceedings|8279590||Nov|||||||||||||||||||||||||||1054745937|42
||Big data analytics, Graph theory, Machine learning, Default prediction, SHAP value||114840||With the unprecedented increase in data all over the world, financial sector such as companies and industries try to remain competitive by transforming themselves into data-driven organizations. By analyzing a huge amount of financial data, companies are able to obtain valuable information to determine their strategic plans such as risk control, crisis management, or growth management. However, as the amount of data increase dramatically, traditional data analytic platforms confront with storing, managing, and analyzing difficulties. Emerging Big Data Analytics (BDA) overcome these problems by providing decentralized and distributed processing. In this study, we propose two new models for default prediction. In the first model, called DPModel-1, statistical (logistic regression), and machine learning methods (decision tree, random forest, gradient boosting) are employed to predict company default. Derived from the first model, we propose DPModel-2 based on graph theory. DPModel-2 also comprises new variables obtained from the trading interactions of companies. In both models, grid search optimization and SHapley Additive exPlanations (SHAP) value are utilized in order to determine the best hyperparameters and make the models interpretable, respectively. By leveraging balance sheet, credit, and invoice datasets, default prediction is realized for about one million companies in Turkey between the years 2010–2018. The default rates of companies range between 3%-6% by year. The experimental results are conducted on a BDA platform. According to the DPModel-1 results, the highest AUC score is ensured by random forest with 0.87. In addition, the results are improved for each technique separately by adjusting new variables with graph theory. According to DPModel-2 results, the best AUC score is achieved by random forest with 0.89.|https://doi.org/10.1016/j.eswa.2021.114840|https://www.sciencedirect.com/science/article/pii/S0957417421002815||||2021|Big data analytics for default prediction using graph theory|Mustafa Yıldırım and Feyza Yıldırım Okay and Suat Özdemir|article|YILDIRIM2021114840|||Expert Systems with Applications|09574174||176|||||24201|1,368|Q1|207|770|1945|42314|17345|1943|8,67|54,95|United Kingdom|Western Europe|1990-2021|Artificial Intelligence (Q1); Computer Science Applications (Q1); Engineering (miscellaneous) (Q1)|55,444|6.954|0.04053|1058279352|1377770283
iiWAS2019|Munich, Germany|Big Data Source Selection, Source reliability, Big Data integration, Data quality|6|611–616|Proceedings of the 21st International Conference on Information Integration and Web-Based Applications &amp; Services|Big Data presents promising technological and economical opportunities. In fact, it has become the raw material of production for many organizations. Data is available in large quantities, and it continues generating abundantly. However, not all the data will have valuable knowledge. Unreliable sources provide misleading and biased information, and even reliable sources could suffer from low data quality.In this paper, we propose a novel methodology for the selectability of data sources, by both considering the presence and the absence of users' preferences. The proposed model integrates multiple factors that affect the reliability of data sources, including their quality, gain, cost and coverage. Experimental results on real world data-sets, show its capability to find the subset of relevant and reliable sources with the lowest cost.|10.1145/3366030.3366121|https://doi.org/10.1145/3366030.3366121|New York, NY, USA|Association for Computing Machinery|9781450371797|2020|Data Source Selection in Big Data Context|Safhi, Hicham Moad and Frikh, Bouchra and Ouhbi, Brahim|inproceedings|10.1145/3366030.3366121|||||||||||||||||||||||||||||1059869078|42
|||||Data Cleaning|Data quality is one of the most important problems in data management, since dirty data often leads to inaccurate data analytics results and incorrect business decisions. Poor data across businesses and the U.S. government are reported to cost trillions of dollars a year. Multiple surveys show that dirty data is the most common barrier faced by data scientists. Not surprisingly, developing effective and efficient data cleaning solutions is challenging and is rife with deep theoretical and engineering problems.This book is about data cleaning, which is used to refer to all kinds of tasks and activities to detect and repair errors in the data. Rather than focus on a particular data cleaning task, we give an overview of the endto- end data cleaning process, describing various error detection and repair methods, and attempt to anchor these proposals with multiple taxonomies and views. Specifically, we cover four of the most common and important data cleaning tasks, namely, outlier detection, data transformation, error repair (including imputing missing values), and data deduplication. Furthermore, due to the increasing popularity and applicability of machine learning techniques, we include a chapter that specifically explores how machine learning techniques are used for data cleaning, and how data cleaning is used to improve machine learning models.This book is intended to serve as a useful reference for researchers and practitioners who are interested in the area of data quality and data cleaning. It can also be used as a textbook for a graduate course. Although we aim at covering state-of-the-art algorithms and techniques, we recognize that data cleaning is still an active field of research and therefore provide future directions of research whenever appropriate.||https://doi.org/10.1145/3310205.3310208|New York, NY, USA|Association for Computing Machinery|9781450371520|2019|Outlier Detection||inbook|10.1145/3310205.3310208|||||||||||||||||||||||||||||1062479405|42
||Data analytics, wearables, sleep quality, statistical methods||242-249||Physical exercise and sleep have independent, yet synergistic, impacts on the health. However, the effects of acute exercise level on sleep quality have not been well investigated. We utilize statistical methods to investigate the differences of exercise level between the good and bad sleep nights. Our results present a complex interrelation between physical exercise and sleep quality with analyzing large personal data sets collected from wearables. As far as we know, this is the first study to investigate insights of interrelation of physical exercise and sleep quality based on a big volume of data collected from wearable devices of real users.|https://doi.org/10.1016/j.procs.2019.08.035|https://www.sciencedirect.com/science/article/pii/S1877050919309494||||2019|How Physical Exercise Level Affects Sleep Quality? Analyzing Big Data Collected from Wearables|Xiaoli Liu and Satu Tamminen and Topi Korhonen and Juha Röning|article|LIU2019242|||Procedia Computer Science|18770509||155||The 16th International Conference on Mobile Systems and Pervasive Computing (MobiSPC 2019),The 14th International Conference on Future Networks and Communications (FNC-2019),The 9th International Conference on Sustainable Energy Information Technology|||19700182801|0,334|-|76|1907|6483|38511|13722|6359|2,09|20,19|Netherlands|Western Europe|2010-2020|Computer Science (miscellaneous)||||1065914481|2108686752
||outliers, quality-control, principal components, gap filling||2128-2140||This paper focuses on techniques for dealing with imperfect data in a frame of early warning system (EWS). Despite the fact that data may be technically damaged by presenting noise, outliers or missing values, met-ocean simulation systems have to deal with them to provide data transaction between models, real time data assimilation, calibration, etc. In this context data quality-control becomes one of the most important parts of EWS. St. Petersburg FWS was considered as an example of EWS. Quality control in St. Petersburg FWS contains blocks of technical control, human mistakes control, statistical control of simulated fields, statistical control and restoration of measurements and control using alternative models. Domain specific quality control was presented as two types of procedures based on theoretically proved methods were applied. The first procedure is based on probabilistic model of dynamical system, where processes are spatially interrelated and could be implemented in a form of multivariate regression (MRM). The second procedure is based on principal component analysis extended for taking into account temporal relations in data set (ePCA).|https://doi.org/10.1016/j.procs.2016.05.532|https://www.sciencedirect.com/science/article/pii/S1877050916310225||||2016|Data Quality Control for St. Petersburg Flood Warning System|Jose Luis Araya Lopez and Anna V. Kalyuzhnaya and Sergey S. Kosukhin and Sergey V. Ivanov|article|LOPEZ20162128|||Procedia Computer Science|18770509||80||International Conference on Computational Science 2016, ICCS 2016, 6-8 June 2016, San Diego, California, USA|||19700182801|0,334|-|76|1907|6483|38511|13722|6359|2,09|20,19|Netherlands|Western Europe|2010-2020|Computer Science (miscellaneous)||||1069163172|2108686752
||Training;Ethics;Decision making;Big Data;Software;Software reliability;Software measurement;Data quality;Data bias;Data ethics;Algorithm fairness;Automated decision-making||4287-4296|2021 IEEE International Conference on Big Data (Big Data)|"\"Bias in the data used to train decision-making systems is a relevant socio-technical issue that emerged in recent years, and it still lacks a commonly accepted solution. Indeed, the \"\"bias in-bias out\"\" problem represents one of the most significant risks of discrimination, which encompasses technical fields, as well as ethical and social perspectives. We contribute to the current studies of the issue by proposing a data quality measurement approach combined with risk management, both defined in ISO/IEC standards. For this purpose, we investigate imbalance in a given dataset as a potential risk factor for detecting discrimination in the classification outcome: specifically, we aim to evaluate whether it is possible to identify the risk of bias in a classification output by measuring the level of (im)balance in the input data. We select four balance measures (the Gini, Shannon, Simpson, and Imbalance ratio indexes) and we test their capability to identify discriminatory classification outputs by applying such measures to protected attributes in the training set. The results of this analysis show that the proposed approach is suitable for the goal highlighted above: the balance measures properly detect unfairness of software output, even though the choice of the index has a relevant impact on the detection of discriminatory outcomes, therefore further work is required to test more in-depth the reliability of the balance measures as risk indicators. We believe that our approach for assessing the risk of discrimination should encourage to take more conscious and appropriate actions, as well as to prevent adverse effects caused by the \"\"bias in-bias out\"\" problem.\""|10.1109/BigData52589.2021.9671443|||||2021|Detecting Discrimination Risk in Automated Decision-Making Systems with Balance Measures on Input Data|Mecati, Mariachiara and Vetrò, Antonio and Torchiano, Marco|inproceedings|9671443||Dec|||||||||||||||||||||||||||1071375942|42
||Big Data;Tools;Databases;Electronic mail;Reliability;Temperature measurement;Null value;big data quality;big data validation;data checklist;big data tool;case study||281-286|2017 IEEE Third International Conference on Big Data Computing Service and Applications (BigDataService)|With the advent of big data, data is being generated, collected, transformed, processed and analyzed at an unprecedented scale. Since data is created at a fast velocity and with a large variety, the quality of big data is far from perfect. Recent studies have shown that poor quality can bring serious erroneous data costs on the result of big data analysis. Data validation is an important process to recognize and improve data quality. In this paper, a case study that is relevant to big data quality is designed to study original big data quality, data quality dimension, data validation process and tools.|10.1109/BigDataService.2017.44|||||2017|Big Data Validation Case Study|Xie, Chunli and Gao, Jerry and Tao, Chuanqi|inproceedings|7944952||April|||||||||||||||||||||||||||1071967610|42
|||7|29–35||Welcome to ACM SIGMOD Record series of interviews with distinguished members of the database community. I'm Marianne Winslett, and today we're at the 2017 SIGMOD and PODS conference in Chicago. I have here with me Mike Franklin, who is the chair of the Computer Science department at the University of Chicago. Before that, for many years, Mike was a professor at Berkeley where he also served as a chair of the Computer Science division. Mike was a co-founder and director of the Algorithms, Machines, and People Lab, better known as the AMPLab. He is an ACM fellow, a two-time winner of the SIGMOD Ten Year Test of Time Award, and a founder of the successful startup, Truviso. Mike's Ph.D. is from the University of Wisconsin Madison. So, Mike, welcome!|10.1145/3377391.3377398|https://doi.org/10.1145/3377391.3377398|New York, NY, USA|Association for Computing Machinery||2019|Michael Franklin Speaks Out on Data Science|Winslett, Marianne and Braganholo, Vanessa|article|10.1145/3377391.3377398||dec|SIGMOD Rec.|01635808|3|48|September 2019||||13622|0,372|Q2|142|39|81|808|127|57|1,67|20,72|United States|Northern America|1969, 1973-1978, 1981-2020|Information Systems (Q2); Software (Q2)|1,819|0.775|7.5E-4|1071980253|962972343
CSAI2019|Normal, IL, USA|Data asset, Distributed, Shared, Construction, Analysis of big data|5|54–58|Proceedings of the 2019 3rd International Conference on Computer Science and Artificial Intelligence|A data revolution has been leading by big data, which have the extremely profound influence on the economic, social development and public life. This paper introduces the meaning of big data, and discusses the innovation and opportunity of enterprise under the perspective of big data. According to the information architecture, this paper supplies the basic construction of enterprise big data analysis platform, and suggests the strategy of application, which have certain realistic directive significance.|10.1145/3374587.3374650|https://doi.org/10.1145/3374587.3374650|New York, NY, USA|Association for Computing Machinery|9781450376273|2020|Construction and Application of Big Data Analysis Platform for Enterprise|Shen, Shaoyi and Li, Bin and Li, Situo|inproceedings|10.1145/3374587.3374650|||||||||||||||||||||||||||||1072168027|42
Mobihoc '18|Los Angeles, CA, USA|Mobile data crowdsourcing, data quality, incentive mechanism|10|161–170|Proceedings of the Eighteenth ACM International Symposium on Mobile Ad Hoc Networking and Computing|"\"Mobile data crowdsourcing has found a broad range of applications (e.g., spectrum sensing, environmental monitoring) by leveraging the \"\"wisdom\"\" of a potentially large crowd of \"\"workers\"\" (i.e., mobile users). A key metric of crowdsourcing is data accuracy, which relies on the quality of the participating workers' data (e.g., the probability that the data is equal to the ground truth). However, the data quality of a worker can be its own private information (which the worker learns, e.g., based on its location) that it may have incentive to misreport, which can in turn mislead the crowdsourcing requester about the accuracy of the data. This issue is further complicated by the fact that the worker can also manipulate its effort made in the crowdsourcing task and the data reported to the requester, which can also mislead the requester. In this paper, we devise truthful crowdsourcing mechanisms for Quality, Effort, and Data Elicitation (QEDE), which incentivize strategic workers to truthfully report their private worker quality and data to the requester, and make truthful effort as desired by the requester. The truthful design of the QEDE mechanisms overcomes the lack of ground truth and the coupling in the joint elicitation of worker quality, effort, and data. Under the QEDE mechanisms, we characterize the socially optimal and the requester's optimal task assignments, and analyze their performance. We show that the requester's optimal assignment is determined by the largest \"\"virtual valuation\"\" rather than the highest quality among workers, which depends on the worker's quality and the quality's distribution. We evaluate the QEDE mechanisms using simulations which demonstrate the truthfulness of the mechanisms and the performance of the optimal task assignments.\""|10.1145/3209582.3209599|https://doi.org/10.1145/3209582.3209599|New York, NY, USA|Association for Computing Machinery|9781450357708|2018|Incentivizing Truthful Data Quality for Quality-Aware Mobile Data Crowdsourcing|Gong, Xiaowen and Shroff, Ness|inproceedings|10.1145/3209582.3209599|||||||||||||||||||||||||||||1077165299|42
CASCON '13|Ontario, Canada||15|177–191|Proceedings of the 2013 Conference of the Center for Advanced Studies on Collaborative Research|Modeling the strategic objectives has been shown to be useful both for understanding a business as well as planning and guiding the overall activities within an enterprise. Business strategy is modeled according to human expertise, setting up the goals as well as the indicators that monitor activities and goals. However, usually indicators provide high-level aggregated views of data, making it difficult to pinpoint problems within specific sub-areas until they have a significant impact into the aggregated value. By the time these problems become evident, they have already hindered the performance of the organization. However, performing a detailed analysis manually can be a daunting task, due to the size of the data space. In order to solve this problem, we propose a user-driven method to analyze the data related to each business indicator by means of data mining. We illustrate our approach with a real world example based on the Europe 2020 framework. Our approach allows us not only to identify latent problems, but also to highlight deviations from anticipated trends that may represent opportunities and exceptional situations, thereby enabling an organization to take advantage of them.|||USA|IBM Corp.||2013|Monitoring and Diagnosing Indicators for Business Analytics|Zoumpatianos, Konstantinos and Palpanas, Themis and Mylopoulos, John and Mat\'{e}, Alejandro and Trujillo, Juan|inproceedings|10.5555/2555523.2555543|||||||||||||||||||||||||||||1078304874|42
||Wearable Sensing, Respiration Signal, Machine Learning, Conversation Modeling|27|||Monitoring of in-person conversations has largely been done using acoustic sensors. In this paper, we propose a new method to detect moment-by-moment conversation episodes by analyzing breathing patterns captured by a mobile respiration sensor. Since breathing is affected by physical and cognitive activities, we develop a comprehensive method for cleaning, screening, and analyzing noisy respiration data captured in the field environment at individual breath cycle level. Using training data collected from a speech dynamics lab study with 12 participants, we show that our algorithm can identify each respiration cycle with 96.34% accuracy even in presence of walking. We present a Conditional Random Field, Context-Free Grammar (CRF-CFG) based conversation model, called rConverse, to classify respiration cycles into speech or non-speech, and subsequently infer conversation episodes. Our model achieves 82.7% accuracy for speech/non-speech classification and it identifies conversation episodes with 95.9% accuracy on lab data using a leave-one-subject-out cross-validation. Finally, the system is validated against audio ground-truth in a field study with 32 participants. rConverse identifies conversation episodes with 71.7% accuracy on 254 hours of field data. For comparison, the accuracy from a high-quality audio-recorder on the same data is 71.9%.|10.1145/3191734|https://doi.org/10.1145/3191734|New York, NY, USA|Association for Computing Machinery||2018|RConverse: Moment by Moment Conversation Detection Using a Mobile Respiration Sensor|Bari, Rummana and Adams, Roy J. and Rahman, Md. Mahbubur and Parsons, Megan Battles and Buder, Eugene H. and Kumar, Santosh|article|10.1145/3191734|2|mar|Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.||1|2|March 2018||||||||||||||||||||||1079851802|42
ICBDE'19|London, United Kingdom|Association Rules, Accuracy, Bayesian Networks, Radom Forests, Precision|7|59–65|Proceedings of the 2019 International Conference on Big Data and Education|The information from Higher Education Institutions (HEIs) is primarily relevant for decision maker and educators. This study tackles e-learners behaviour using machine learning, particularly association rules and classifiers. Learners are characterized by a set of behaviours and attitudes that determine their learning abilities and skills. Learning from data generated by online learners may have significant impacts, however, few studies cover this resource from machine learning perspectives. We examine different data mining techniques including Random Forests, Logistic Regressions and Bayesian Networks as classifiers used for predicting e-learners' classes (High, Medium and Low). The novelty of this study is that it explores and compares classifiers performance on the behaviour of online learners on four variables: raise hands, visiting IT resources, view announcement and discussion impact on e-learners. The results of this study indicate an 80% accuracy level obtained by Bayesian Networks; in contrast, the Random Forests have only 63% accuracy level and Logistic Regressions for 58%.|10.1145/3322134.3322145|https://doi.org/10.1145/3322134.3322145|New York, NY, USA|Association for Computing Machinery|9781450361866|2019|Understanding E-Learners' Behaviour Using Data Mining Techniques|Al Fanah, Muna and Ansari, Muhammad Ayub|inproceedings|10.1145/3322134.3322145|||||||||||||||||||||||||||||1082029377|42
||Data privacy;Big data;Computer security;Data quality||113-118||This article summarizes the key findings of a Canadian Anonymization Network study of several large data custodians who utilize deidentification and similar privacy-enhancing processes prior to engaging in analytics, secondary uses, and disclosure of personal information.|10.1109/MSEC.2021.3126185|||||2022|The Practices and Challenges of Generating Nonidentifiable Data|Kardash, Adam and Morin, Suzanne|article|9693431||Jan|IEEE Security & Privacy|15584046|1|20|||||28916|0,530|Q1|76|62|253|718|678|212|3,12|11,58|United States|Northern America|2003-2020|Law (Q1); Computer Networks and Communications (Q2); Electrical and Electronic Engineering (Q2)||||1082508445|1429805806
KDD '21|Virtual Event, Singapore|data governance, data auditing, data pricing, data asset, distributed ledger technology, privacy|2|4185–4186|Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining|"\"Today's computing is characterized by an increasing degree of complexity, comprehensiveness and collaboration. The complexity can be observed by the wide application of gigantic models with a huge number of parameters and structures of an unprecedented level of sophistication. The comprehensiveness is best illustrated by the high heterogeneity of data both in terms of format and source. The collaboration, finally, becomes an obvious trend when computing systems grow more open and decentralized in which various entities interact to achieve collective intelligence with the presence of potentially malicious behavior. Trust, therefore, has become critical at multiple levels: At model level to assure its integrity, fairness and interpretability; At data level to safeguard data quality, compliance and privacy; At system level to govern resilience, performance and incentive. Moreover, the notion of trust has long been discussed in different domains in both academia and industry with different definition and understanding. The Third International Workshop on Smart Data for Blockchain and Distributed Ledger (SDBD'21) will be held as a joint workshop with the special-themed \"\"Trust Day\"\" of KDD 2021, which has therefore aimed to bring together researchers, practitioners and experts from various communities to exchange and explore ideas, frontiers, opportunities and challenges under the broad theme of \"\"trust\"\" in a highly interdisciplinary manner.\""|10.1145/3447548.3469441|https://doi.org/10.1145/3447548.3469441|New York, NY, USA|Association for Computing Machinery|9781450383325|2021|The Third International Workshop on Smart Data for Blockchain and Distributed Ledger (SDBD2021): Joint Workshop with SIGKDD 2021 Trust Day|Zhu, Feida and Pei, Jian|inproceedings|10.1145/3447548.3469441|||||||||||||||||||||||||||||1083467239|42
|||5|56–60||Twenty-five Semantic Web and Database researchers met at the 2011 STI Semantic Summit in Riga, Latvia July 6-8, 2011[1] to discuss the opportunities and challenges posed by Big Data for the Semantic Web, Semantic Technologies, and Database communities. The unanimous conclusion was that the greatest shared challenge was not only engineering Big Data, but also doing so meaningfully. The following are four expressions of that challenge from different perspectives.|10.1145/2094114.2094129|https://doi.org/10.1145/2094114.2094129|New York, NY, USA|Association for Computing Machinery||2012|The Meaningful Use of Big Data: Four Perspectives -- Four Challenges|Bizer, Christian and Boncz, Peter and Brodie, Michael L. and Erling, Orri|article|10.1145/2094114.2094129||jan|SIGMOD Rec.|01635808|4|40|December 2011||||13622|0,372|Q2|142|39|81|808|127|57|1,67|20,72|United States|Northern America|1969, 1973-1978, 1981-2020|Information Systems (Q2); Software (Q2)|1,819|0.775|7.5E-4|1085676129|962972343
||Smart product-service systems, Digital technology, Sustainable innovation, Fuzzy delphi method, Decision-making trial and evaluation laboratory (DEMATEL), Diffusion of innovation theory||108244||This study adopts the diffusion of innovation theory as to develop the smart product service system model in banking industry due to prior studies are lacking in identifying the attributes. The smart product service system functions are bearing high uncertainty and system complexity; hence, the hybrid method of fuzzy Delphi method and fuzzy decision-making trial and evaluation laboratory to construct a valid hierarchical model and identified the causal interrelationships among the attributes. The smart product service system hierarchical model with eight aspects and 41 criteria are proposed enriching the existing literature and that identify appropriate strategies to achieve operational performance. The results show that seven aspects and 22 criteria are determined as the valid hierarchical model. The institutional compression, digital platform operation, and e-knowledge management are the causing aspects helps to form smart product service system operational performance in high uncertainty. For practices, the banking decision-makers should develop innovative actions relied on the forcible compression, cyber-physical systems, industrial big data, cloud service allocation and sharing, and transparency improvement as they are most importance criteria playing a decisive role in a successful SPSS. This provides guidelines for banking industry practice in Taiwan encouraging the miscellany of digital technology accomplishment for sustainable target.|https://doi.org/10.1016/j.ijpe.2021.108244|https://www.sciencedirect.com/science/article/pii/S0925527321002206||||2021|Smart product service system hierarchical model in banking industry under uncertainties|Ming-Lang Tseng and Tat-Dat Bui and Shulin Lan and Ming K. Lim and Abu Hashan Md Mashud|article|TSENG2021108244|||International Journal of Production Economics|09255273||240|||||19165|2,406|Q1|185|327|891|21712|8124|881|8,31|66,40|Netherlands|Western Europe|1991-2021|Business, Management and Accounting (miscellaneous) (Q1); Economics and Econometrics (Q1); Industrial and Manufacturing Engineering (Q1); Management Science and Operations Research (Q1)|32,606|7.885|0.0228|1086370096|850534974
||Big data, power quality disturbance detection, intrusion detection, islanding detection, feature extraction, classification, data analytics, forecasting, visualization, smart meters, demand response||253-285|The Power Grid|A traditional power grid, also known as the legacy grid, collects data at a few locations on the grid to monitor grid performance and forecast energy requirements on a macro level. A smart grid is the next generation of the electric power grid; it includes technologies for real-time data acquisition from various sections of the grid and provides a means for two-way communication between energy suppliers and consumers. Compared to a legacy grid, the smart grid generates large volumes of data that can be exploited for power quality event monitoring, intrusion detection, islanding detection, price forecasting, and energy forecasting at a much more granular level. These large volumes of data have to be analyzed in real-time and with high accuracy in order assist in decision making for power system operations and optimal power flow. This poses a big data challenge, which to be implemented successfully requires changes in infrastructure and data analysis methods. This chapter describes the smart grid and its associated big data and discusses methods for informative feature extraction from raw data, event monitoring, and energy consumption forecasting using these features and visualization methods to assist with data interpretation and decision making.|https://doi.org/10.1016/B978-0-12-805321-8.00009-4|https://www.sciencedirect.com/science/article/pii/B9780128053218000094||Academic Press|978-0-12-805321-8|2017|9 - Big Data and Monitoring the Grid|Sonal K. Phan and Cathy Chen|incollection|PHAN2017253||||||||||Brian W. D’Andrade|||||||||||||||||||1086610510|42
WSC '17|Las Vegas, Nevada||13||Proceedings of the 2017 Winter Simulation Conference|Manufacturing systems incorporate many semi-independent, yet strongly interacting processes, usually exhibiting some stochastic behavior. As a consequence, overall system behavior, in the long run but also in the short run, is very difficult to predict. Not surprisingly, both practitioners and academics recognized in the 1950's the potential value of discrete event simulation technology in supporting manufacturing system decision-making. This short history is one perspective on the development and evolution of discrete event simulation technology and applications, specifically focusing on manufacturing applications. This assessment is based on an examination of the literature, our own experiences, and interviews with leading practitioners. History is interesting, but it's useful only if it helps us see a way forward, so we offer some opinions on the state of the research and practice of simulation in manufacturing, and the opportunities to further advance the field.||||IEEE Press|9781538634271|2017|History and Perspective of Simulation in Manufacturing|McGinnis, Leon F. and Rose, Oliver|inproceedings|10.5555/3242181.3242206|24||||||||||||||||||||||||||||1086624276|42
||Land, Housing, House prices, Housing boom and bust, Financial crisis||224-246||We use a new property-level data set and an innovative methodology to estimate the price of land from 2000 to 2013 for nearly the universe of detached single-family homes in the Washington, DC metro area and to characterize the boom-bust cycle in land and house prices at a fine geography. The results show that land prices were more volatile than house prices everywhere, but especially so in the areas where land was inexpensive in 2000. We demonstrate that the change in the land share of house value during the boom was a significant predictor of the decline in house prices during the bust, highlighting the value of focusing on land in assessing house-price risk.|https://doi.org/10.1016/j.regsciurbeco.2017.06.006|https://www.sciencedirect.com/science/article/pii/S0166046216301508||||2017|Residential land values in the Washington, DC metro area: New insights from big data|Morris A. Davis and Stephen D. Oliner and Edward J. Pinto and Sankar Bokka|article|DAVIS2017224|||Regional Science and Urban Economics|01660462||66|||||||||||||||||||||||1086657486|896581477
||Big data management, graph data, generators, synthetic data, benchmarks|30|||The abundance of interconnected data has fueled the design and implementation of graph generators reproducing real-world linking properties or gauging the effectiveness of graph algorithms, techniques, and applications manipulating these data. We consider graph generation across multiple subfields, such as Semantic Web, graph databases, social networks, and community detection, along with general graphs. Despite the disparate requirements of modern graph generators throughout these communities, we analyze them under a common umbrella, reaching out the functionalities, the practical usage, and their supported operations. We argue that this classification is serving the need of providing scientists, researchers, and practitioners with the right data generator at hand for their work. This survey provides a comprehensive overview of the state-of-the-art graph generators by focusing on those that are pertinent and suitable for several data-intensive tasks. Finally, we discuss open challenges and missing requirements of current graph generators along with their future extensions to new emerging fields.|10.1145/3379445|https://doi.org/10.1145/3379445|New York, NY, USA|Association for Computing Machinery||2020|Graph Generators: State of the Art and Open Challenges|Bonifati, Angela and Holubov\'{a}, Irena and Prat-P\'{e}rez, Arnau and Sakr, Sherif|article|10.1145/3379445|36|apr|ACM Comput. Surv.|03600300|2|53|March 2021||||23038|2,079|Q1|163|112|365|15859|6978|365|19,16|141,60|United States|Northern America|1969-2020|Computer Science (miscellaneous) (Q1); Theoretical Computer Science (Q1)|10,790|10.282|0.01438|1089669970|1517405264
||TBM performance prediction, Deep belief network (DBN), Yingsong Water Diversion Project, Field penetration index prediction||103636||This work explores the potential for predicting TBM performance using deep learning. It focuses on a 17.5-km-long tunnel excavated for the Yingsong Water Diversion Project in Northeastern China with its 728 days’ continuous monitoring of mechanical data. The prediction uses the deep belief network (DBN) proposed by Hinton et al. (2006),on the penetration rate, cutter rotation speed, torque, and thrust force. Field Penetration Index (FPI) is introduced to quantify TBM performance in the field. The DBN algorithm trains on nth number of preceding elements and predicts the performance of the n + 1th element. Prior to the implementation of the DBN, a pilot test was performed to find the optimal values for the network structural parameters (number of input nodes, number of hidden layers, number of nodes in the hidden layers, and learning rate). Predictions on FPIs in all the three rock types were then proceeded with good agreement with the field measured data. The mean relative errors for the predicted measured FPIs are generally less than 0.15 and the correlation coefficients (R) can be higher than 0.78. The predicted and measured FPI values along the length of the tunnel graphically follow the same trends. These results confirm the usefulness of big data and the deep learning in predicting TBM performance.|https://doi.org/10.1016/j.tust.2020.103636|https://www.sciencedirect.com/science/article/pii/S0886779820305903||||2021|Tunnel boring machines (TBM) performance prediction: A case study using big data and deep learning|Shangxin Feng and Zuyu Chen and Hua Luo and Shanyong Wang and Yufei Zhao and Lipeng Liu and Daosheng Ling and Liujie Jing|article|FENG2021103636|||Tunnelling and Underground Space Technology|08867798||110|||||14642|2,172|Q1|98|375|946|16500|6440|942|6,53|44,00|United Kingdom|Western Europe|1986-2020|Building and Construction (Q1); Geotechnical Engineering and Engineering Geology (Q1)|16,336|5.915|0.0149|1089865234|701092564
||Big data, Frameworks, Healthcare, Healthcare systems||33-56|Demystifying Big Data, Machine Learning, and Deep Learning for Healthcare Analytics|Today, the term big data involves various applications, technologies, architectures, services, and standards in the healthcare domain. The advanced technology and healthcare systems have been extremely knotted together in recent times. As the adoption of wearable biosensors and their applications have begun across the world, eHealth and mHealth have emerged. Hence, wearable sensor devices produce organized and unorganized big data that cannot be easily processed and analyzed due to its complexity; that hinders effective medical decision making. Modern advances in healthcare systems have increased the size of health records such as through electronic health records, patient care, clinical reports, regulations, and compliance requirements. However, the present data-processing technologies are not capable of handling the growing amount of large datasets. This chapter discusses theoretical illustrations that pinpoint various aspects of big data-related frameworks and proposes a large data-based conceptual framework in healthcare systems.|https://doi.org/10.1016/B978-0-12-821633-0.00003-9|https://www.sciencedirect.com/science/article/pii/B9780128216330000039||Academic Press|978-0-12-821633-0|2021|Chapter 3 - Big data-based frameworks for healthcare systems|Aboobucker Ilmudeen|incollection|ILMUDEEN202133||||||||||Pradeep N and Sandeep Kautish and Sheng-Lung Peng|||||||||||||||||||1091918712|42
||Social networking (online);Data models;Blogs;Data integrity;Sentiment analysis;Information integrity;Big Data;Coherence;Social media;big data;microblogging platforms;topic modeling;data cleansing;data quality;topic coherence;purity||105328-105351||With the emergence of microblogging platforms and social media applications, large amounts of user-generated data in the form of comments, reviews, and brief text messages are produced every day. Microblog data is typically of poor quality; hence improving the quality of the data is a significant scientific and practical challenge. In spite of the relevance of the problem, there has been not much work so far, especially in regard to microblog data quality for Short-Text Topic Modelling (STTM) purposes. This paper addresses this problem and proposes an approach called the Social Media Data Cleansing Model (SMDCM) to improve data quality for STTM. We evaluate SMDCM using six topic modelling methods, namely the Latent Dirichlet Allocation (LDA), Word-Network Topic Model (WNTM), Pseudo-document-based Topic Modelling (PTM), Biterm Topic Model (BTM), Global and Local word embedding-based Topic Modeling (GLTM), and Fuzzy Topic modelling (FTM). We used the Real-world Cyberbullying Twitter (RW-CB-Twitter) and the Cyberbullying Mendeley (CB-MNDLY) datasets in the evaluation. The results proved the efficiency of the GLTM and WNTM over the other STTM models when applying the SMDCM techniques, which achieved optimum topic coherence and high accuracy values on RW-CB-Twitter and CB-MNDLY datasets.|10.1109/ACCESS.2022.3211396|||||2022|Enhancing Big Social Media Data Quality for Use in Short-Text Topic Modeling|Murshed, Belal Abdullah Hezam and Abawajy, Jemal and Mallappa, Suresha and Saif, Mufeed Ahmed Naji and Al-Ghuribi, Sumaia Mohammed and Ghanem, Fahd A.|article|9906976|||IEEE Access|21693536||10|||||21100374601|0,587|Q1|127|18036|24267|771081|116691|24200|4,48|42,75|United States|Northern America|2013-2020|Computer Science (miscellaneous) (Q1); Engineering (miscellaneous) (Q1); Materials Science (miscellaneous) (Q2)|105,968|3.367|0.15396|1092032842|1905633267
SIET '21|Malang, Indonesia||6|100–105|6th International Conference on Sustainable Information Engineering and Technology 2021|The development of information technology encourages the government to digitize business processes. It is generated with a large and varied volume from various data sources so that advanced data analytics (DA) is required to overcome this to support organization's data driven decision making. It's necessary to prepare DA based on DA readiness model so that the implementation of DA can run successfully. Whereas currently, there is limited study and no standard model for DA readiness. The focus of this study is to propose model readiness of implementing data analytics that is suitable in Indonesian government. The model refers to DA readiness model based on literature review on 15 papers relevant to DA readiness. Then it's verified by 7 experts. Furthermore, online survey was conducted to test the model that affects the readiness of implementing data analytics in Indonesian government. The survey results were analyzed using factor analysis. As a result, DA readiness model contains 4 dimensions, 11 factors, and 78 indicators where its dimensions consist of information system, organizational and cultural, organization structure and resource readiness. This model can describe 85% of the data analysis readiness requirements in the Indonesian government. In order to implement data analytics successfully, the government needs to improve the readiness of information systems, organizational and cultural, organizational structures, and resources.|10.1145/3479645.3479669|https://doi.org/10.1145/3479645.3479669|New York, NY, USA|Association for Computing Machinery|9781450384070|2021|Data Analytics Readiness Model in Indonesian Government|Sulistyowati, Ira and Fransisca, Dyna and Ruldeviyani, Yova|inproceedings|10.1145/3479645.3479669|||||||||||||||||||||||||||||1092480780|42
||Bayesian estimation, Data quality, Data rectification, Filtering, Fourier analysis, Gaussian and non-Gaussian noise, Kalman filtering, Model-based denoising, Multiscale analysis, Off-line and online denoising, Outliers, Smoothing, Wavelet analysis, Wavelet thresholding, Windowed Fourier analysis||179-204|Comprehensive Chemometrics (Second Edition)|This article introduces the methods of Fourier and wavelet analysis for enhancing data quality in typical chemometric and process analytics applications. Fourier analysis has been popular for many decades but is best suited for enhancing signals where most features are localized in frequency. In contrast, wavelet analysis is appropriate for signals that contain features localized in both time and frequency domains. It also retains the benefits of Fourier analysis such as orthonormality and computational efficiency. Practical algorithms for off-line and on-line denoising are described and compared via simple examples. These algorithms can be used for off-line or on-line applications in order to mitigate the impact of additive Gaussian as well as non-Gaussian noise.|https://doi.org/10.1016/B978-0-12-409547-2.14874-7|https://www.sciencedirect.com/science/article/pii/B9780124095472148747|Oxford|Elsevier|978-0-444-64166-3|2020|3.10 - Data Quality and Denoising: A Review☆|M.S. Reis and P.M. Saraiva and B.R. Bakshi|incollection|REIS2020179|||||||||Second Edition|Steven Brown and Romà Tauler and Beata Walczak|||||||||||||||||||1093705302|42
||Clustering algorithms;Partitioning algorithms;Algorithm design and analysis;Accuracy;Feature extraction;Integrated circuits;Time complexity;missing values;data imputation;stacked auto-encoder;incremental clustering||1725-1730|2015 IEEE 17th International Conference on High Performance Computing and Communications, 2015 IEEE 7th International Symposium on Cyberspace Safety and Security, and 2015 IEEE 12th International Conference on Embedded Software and Systems|With the explosive increase of data volume, the research of data quality and data usability draws extensive attention. In this work, we focus on one aspect of data usability -- incomplete data imputation, and present a novel missing value imputation method using stacked auto-encoder and incremental clustering (SAICI). Specifically, SAICI's functionality rests on four pillars: (i) a distinctive value assigned to impute missing values initially, (ii) the stacked auto-encoder(SAE) applied to locate principal features, (iii) a new incremental clustering utilized to partition incomplete data set, and (iv) the top nearest neighbors' weighted values designed to refill the missing values. Most importantly, stages (ii)~(iv) iterate until convergence condition is satisfied. Experimental results demonstrate that the proposed scheme not only imputes the missing data values effectively, but also has better time performance. Moreover, this work is suitable for distributed data processing framework, which can be applied to the imputation of incomplete big data.|10.1109/HPCC-CSS-ICESS.2015.103|||||2015|A Hybrid Method for Incomplete Data Imputation|Zhao, Liang and Chen, Zhikui and Yang, Zhennan and Hu, Yueming|inproceedings|7336420||Aug|||||||||||||||||||||||||||1097101080|42
||Big data management, Storage, Big data, Processing, Security||151-166||The rapid growth of emerging applications and the evolution of cloud computing technologies have significantly enhanced the capability to generate vast amounts of data. Thus, it has become a great challenge in this big data era to manage such voluminous amount of data. The recent advancements in big data techniques and technologies have enabled many enterprises to handle big data efficiently. However, these advances in techniques and technologies have not yet been studied in detail and a comprehensive survey of this domain is still lacking. With focus on big data management, this survey aims to investigate feasible techniques of managing big data by emphasizing on storage, pre-processing, processing and security. Moreover, the critical aspects of these techniques are analyzed by devising a taxonomy in order to identify the problems and proposals made to alleviate these problems. Furthermore, big data management techniques are also summarized. Finally, several future research directions are presented.|https://doi.org/10.1016/j.jnca.2016.04.008|https://www.sciencedirect.com/science/article/pii/S1084804516300583||||2016|A survey of big data management: Taxonomy and state-of-the-art|Aisha Siddiqa and Ibrahim Abaker Targio Hashem and Ibrar Yaqoob and Mohsen Marjani and Shahabuddin Shamshirband and Abdullah Gani and Fariza Nasaruddin|article|SIDDIQA2016151|||Journal of Network and Computer Applications|10848045||71|||||||||||||||||||||||1098632555|2040591560
||Bi-directional estimation, Building energy, Deep learning, Electric power, Missing data, Transfer learning||109941||Improving the energy efficiency of the buildings is a worldwide hot topic nowadays. To assist comprehensive analysis and smart management, high-quality historical data records of the energy consumption is one of the key bases. However, the energy data records in the real world always contain different kinds of problems. The most common problem is missing data. It is also one of the most frequently reported data quality problems in big data/machine learning/deep learning related literature in energy management. However, limited studied have been conducted to comprehensively discuss different kinds of missing data situations, including random missing, continuous missing, and large proportionally missing. Also, the methods used in previous literature often rely on linear statistical methods or traditional machine learning methods. Limited study has explored the feasibility of advanced deep learning and transfer learning techniques in this problem. To this end, this study proposed a methodology, namely the hybrid Long Short Term Memory model with Bi-directional Imputation and Transfer Learning (LSTM-BIT). It integrates the powerful modeling ability of deep learning networks and flexible transferability of transfer learning. A case study on the electric consumption data of a campus lab building was utilized to test the method. Results show that LSTM-BIT outperforms other methods with 4.24% to 47.15% lower RMSE under different missing rates.|https://doi.org/10.1016/j.enbuild.2020.109941|https://www.sciencedirect.com/science/article/pii/S0378778819333717||||2020|A bi-directional missing data imputation scheme based on LSTM and transfer learning for building energy data|Jun Ma and Jack C.P. Cheng and Feifeng Jiang and Weiwei Chen and Mingzhu Wang and Chong Zhai|article|MA2020109941|||Energy and Buildings|03787788||216|||||29359|1,737|Q1|184|705|2402|39338|15832|2394|6,33|55,80|Netherlands|Western Europe|1970, 1977-2020|Building and Construction (Q1); Civil and Structural Engineering (Q1); Electrical and Electronic Engineering (Q1); Mechanical Engineering (Q1)|51,254|5.879|0.04387|1099475461|1347637955
||Solid modeling;Oils;Organizations;Big Data;Solids;Monitoring;Meteorology;heterogeneous data;noise filter;convergence;climate change||284-287|2020 IEEE International Women in Engineering (WIE) Conference on Electrical and Computer Engineering (WIECON-ECE)|In this research, we are working with Big Data for obtaining, preparing and analyzing data-based information to make use of the data retrieved which will benefit any organization. It is a progressing part of all divisions of industry and business. All organizations in any field, for example, oil, money, fabricating hardware and so forth produce big data, which can show incredibly helpful designs to business directors to make and develop their organizations, when the information is gathered and analyzed accurately. It permits us to gather, store, and decipher immense measures of big data to produce useful outcomes. Data quality is affected by the information that is gathered to be analyzed as that data will make sure whether in the long run a specific method of conducting the ongoing process is useful or not. Consequently, the consistency of big data very important. Here, we propose that the various types of raw information should be analyzed to expand its precision in the pre-handling stage, as those pieces of information are not utilized later in the process. During investments, we break down and model the big data to decrease overhead expenses to create and add to a solid understanding of results to improve information consistency.|10.1109/WIECON-ECE52138.2020.9397990|||||2020|Pre-Processing Data In Weather Monitoring Application By Using Big Data Quality Framework|Labeeb, Kashshaf and Chowdhury, Kuraish Bin Quader and Riha, Rabea Basri and Abedin, Mohammad Zoynul and Yesmin, Sarmila and Khan, Mohammad Nasfikur Rahman|inproceedings|9397990||Dec|||||||||||||||||||||||||||1101892820|42
||Business;Data models;Terminology;Big data;Data warehouses;Wheels;Standards organizations;Data Modeling;Project Methodology;Data Governance;Metadata;Information Catalog||2056-2065|2015 IEEE International Conference on Big Data (Big Data)|This paper discusses an integrated methodology to structure and formalize business requirements in large data-intensive projects, e.g. data warehouses implementations, turning them into precise and unambiguous data definitions suitable to facilitate harmonization and assignment of data governance responsibilities. We place a business information model in the center - used end-to-end from analysis, design, development, testing to data quality checks by data stewards. In addition, we show that the approach is suitable beyond traditional data warehouse environments, applying it also to big data landscapes and data science initiatives - where business requirements analysis is often neglected. As proper tool support has turned out to be inevitable in many real-world settings, we also discuss software requirements and their implementation in the Accurity Glossary tool. The approach is evaluated based on a large banking data warehouse project the authors are currently involved in.|10.1109/BigData.2015.7363987|||||2015|Business information modeling: A methodology for data-intensive projects, data science and big data governance|Priebe, Torsten and Markus, Stefan|inproceedings|7363987||Oct|||||||||||||||||||||||||||1102321892|42
DSIT 2020|Xiamen, China|Education, Data-mining, Cultivation of high-level talents|8|239–246|Proceedings of the 3rd International Conference on Data Science and Information Technology|The cultivation of high-level talents in either scientific or engineering domain is of significant importance to the development of a country. From the perspective of data science, this paper takes the group of academicians of the Chinese Academy of Sciences and the Chinese Academy of Engineering as an example to explore the method of cultivating high-end talents. Through multidimensional data analysis, it is of great significance to explore the spatial pattern and time evolution characteristics of the first generation of top natural science talents in China, to deepen the understanding of the growth and education laws of talents, optimize top-level design, and implement targeted scientific policies. It is found that Chinese culture and Chinese native universities have made significant contributions for the early scientific talents of China, and the analysis method of data science can be used to facilitate the education innovation.|10.1145/3414274.3414509|https://doi.org/10.1145/3414274.3414509|New York, NY, USA|Association for Computing Machinery|9781450376044|2020|Multidimensional Data Mining on the Early Scientific Talents of China|Liu, Xingchen and Zhao, Boyu and Qian, Haotian and Liu, Yuhang|inproceedings|10.1145/3414274.3414509|||||||||||||||||||||||||||||1104050836|42
CAIH2020|Taiyuan, China|Diabetes, corpus construction, frequently-asked questions, visualization|7|60–66|Proceedings of the 2020 Conference on Artificial Intelligence and Healthcare|In recent years, the prevalence of diabetes has been increasing rapidly worldwide. With the advancement of information technology, automated question-answering services for healthcare, which are commonly based on annotated corpus in health domain, have positive effects on health knowledge spread and daily health management for high-risk populations. This paper proposes to construct a large scale diabetes corpus of frequently-asked questions for automated question-answering services and evaluations. Concentrating on the characteristics of diabetes-related factors that reflect conditions of diabetes, this work establishes an annotated dataset containing professional question &amp; answer pairs about diabetes and their annotated question target categories. The corpus is applicable for various question-answering applications, supporting users to retrieve needed information, arrange diets, adhere to scientific medication as well as prevent and control disease complications.|10.1145/3433996.3434008|https://doi.org/10.1145/3433996.3434008|New York, NY, USA|Association for Computing Machinery|9781450388641|2020|The Construction of a Diabetes-Oriented Frequently Asked Question Corpus for Automated Question-Answering Services|Guo, Xusheng and Liang, Likeng and Liu, Yuanxia and Weng, Heng and Hao, Tianyong|inproceedings|10.1145/3433996.3434008|||||||||||||||||||||||||||||1106414693|42
BDCA'17|Tetouan, Morocco|System of Systems, Data lifecycle, Creation/Reception Process, Seven views, BPMN, Collect system, Requirement Model, Big data|7||Proceedings of the 2nd International Conference on Big Data, Cloud and Applications|In Big data era, managing data requires sufficient tools, last computer science evolution and developed methodologies. To be able to satisfy customer and the big need of information, multiple methods are developed to handle the complexity as well as the huge amount of data in different phases of data lifecycle. We notice for each complicated situation in data lifecycle we focus more particularly to develop storage or Analysis processes. For this reason in this paper, we try to have a different approach to resolve basic issues on targeting the first phase of data lifecycle, which is data collect. We present it as a System of systems, since the complexity of each phase of data lifecycle. In this research, we are interested by the collect system and particularly the process of Creation/Reception of data for which we model the requirements in order to manage smart data at the first level of the cycle. To build this model, we follow a methodology that required three major steps. Starting with requirement identification to defining criterion for each requirement, and in the last step will provide requirement modeling. This research highlight the importance of managing data collect to identify and restrict the issues of big data era.|10.1145/3090354.3090358|https://doi.org/10.1145/3090354.3090358|New York, NY, USA|Association for Computing Machinery|9781450348522|2017|Data Collect Requirements Model|Tikito, I. and Souissi, N.|inproceedings|10.1145/3090354.3090358|4||||||||||||||||||||||||||||1106878891|42
CHI '20|Honolulu, HI, USA|visualization, empirical study, literature review, debugging databases, survey|16|1–16|Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems|Database management systems (or DBMSs) have been around for decades, and yet are still difficult to use, particularly when trying to identify and fix errors in user programs (or queries). We seek to understand what methods have been proposed to help people debug database queries, and whether these techniques have ultimately been adopted by DBMSs (and users). We conducted an interdisciplinary review of 112 papers and tools from the database, visualisation and HCI communities. To better understand whether academic and industry approaches are meeting the needs of users, we interviewed 20 database users (and some designers), and found surprising results. In particular, there seems to be a wide gulf between users' debugging strategies and the functionality implemented in existing DBMSs, as well as proposed in the literature. In response, we propose new design guidelines to help system designers to build features that more closely match users debugging strategies.|10.1145/3313831.3376485|https://doi.org/10.1145/3313831.3376485|New York, NY, USA|Association for Computing Machinery|9781450367080|2020|Debugging Database Queries: A Survey of Tools, Techniques, and Users|Gathani, Sneha and Lim, Peter and Battle, Leilani|inproceedings|10.1145/3313831.3376485|||||||||||||||||||||||||||||1107011912|42
||Data Quality Act, Freedom of Information Act, FOIA, Limited Data Use Agreements, and Madey v. Duke, Tort, Patents, Intellectual property, Informed consent, Data ownership, Copyright, Infringement, Fair use||395-417|Principles and Practice of Big Data (Second Edition)|Big Data projects always incur some legal risk. It is impossible to know all the data contained in a Big Data project, and it is impossible to know every purpose to which Big Data is used. Hence, the entities that produce Big Data may unknowingly contribute to a variety of illegal activities, chiefly: copyright and other intellectual property infringements, breaches of confidentiality, and privacy invasions. In addition, issues of data quality, data availability, and data documentation may contribute to the legal or regulatory disqualification of a Big Data resource. In this chapter, four issues will be discussed in detail: (1) responsibility for the accuracy of the contained data; (2) obtaining the rights to create, use, and share the data held in the resource; (3) intellectual property encumbrances incurred from the use of standards required for data representation and data exchange; and (4) protections for individuals whose personal information is used in the resource. Big Data managers contend with a wide assortment of legal issues, but these four issues will never go away.|https://doi.org/10.1016/B978-0-12-815609-4.00019-4|https://www.sciencedirect.com/science/article/pii/B9780128156094000194||Academic Press|978-0-12-815609-4|2018|19 - Legalities|Jules J. Berman|incollection|BERMAN2018395|||||||||Second Edition|Jules J. Berman|||||||||||||||||||1110733249|42
||Single cell, RNA-seq, Big data, Transcriptional heterogeneity, Signal normalization||21-30||The rapid growth of single-cell RNA-seq studies (scRNA-seq) demands efficient data storage, processing, and analysis. Big-data technology provides a framework that facilitates the comprehensive discovery of biological signals from inter-institutional scRNA-seq datasets. The strategies to solve the stochastic and heterogeneous single-cell transcriptome signal are discussed in this article. After extensively reviewing the available big-data applications of next-generation sequencing (NGS)-based studies, we propose a workflow that accounts for the unique characteristics of scRNA-seq data and primary objectives of single-cell studies.|https://doi.org/10.1016/j.gpb.2016.01.005|https://www.sciencedirect.com/science/article/pii/S1672022916000437||||2016|Single-cell Transcriptome Study as Big Data|Pingjian Yu and Wei Lin|article|YU201621|||Genomics, Proteomics & Bioinformatics|16720229|1|14|||||89440|3,114|Q1|49|36|158|1874|1152|135|6,41|52,06|China|Asiatic Region|2003-2020|Biochemistry (Q1); Computational Mathematics (Q1); Genetics (Q1); Medicine (miscellaneous) (Q1); Molecular Biology (Q1)||||1111223881|1963566185
https://doi.org/10.1016/j.jss.2019.01.051|https://www.sciencedirect.com/science/article/pii/S0164121219300172||||2019|Architectural Tactics for Big Data Cybersecurity Analytics Systems: A Review|Faheem Ullah and Muhammad {Ali Babar}|article|ULLAH201981|||Journal of Systems and Software|01641212||151||||||||||||||||||||||||||||||1111852116|42
Advances in ubiquitous sensing applications for healthcare||Big data analytics, Classification, Healthcare, Privacy preservation, Recommendation system||227-246|Big Data Analytics for Intelligent Healthcare Management|In today's digital world, healthcare is one of the core areas in the medical domain. A healthcare system is required to analyze a large amount of patient data, which helps to derive insights and predictions of disease. This system should be intelligent and able to predict the patient's health condition by analyzing the patient's lifestyle, physical health records, and social activities. The health recommendation system (HRS) is becoming an important platform for healthcare services. In this context, health intelligent systems have become indispensable tools in decision-making processes in the healthcare sector. The main objective is to ensure the availability of valuable information at the right time by ensuring information quality, trustworthiness, authentication, and privacy. As people use social networks to learn about their health condition, so the HRS is very important to derive outcomes such as recommending diagnosis, health insurance, clinical pathway-based treatment methods, and alternative medicines based on the patient's health profile. In this chapter, we discuss recent research that targeted utilization of large volumes of medical data while combining multimodal data from disparate sources, which reduces the workload and cost in healthcare. In the healthcare sector, big data analytics using a recommendation system has an important role in terms of decision-making processes regarding the patient's health. This chapter presents a proposed intelligent HRS that provides an insight into how to use big data analytics for implementing an effective health recommendation engine and shows how to transform the healthcare industry from the traditional scenario to more personalized paradigm in a tele-health environment. Our proposed intelligent HRS resulted in lower MAE value when compared to existing approaches.|https://doi.org/10.1016/B978-0-12-818146-1.00009-X|https://www.sciencedirect.com/science/article/pii/B978012818146100009X||Academic Press|978-0-12-818146-1|2019|Chapter 9 - Intelligence-Based Health Recommendation System Using Big Data Analytics|Abhaya Kumar Sahoo and Sitikantha Mallik and Chittaranjan Pradhan and Bhabani Shankar Prasad Mishra and Rabindra Kumar Barik and Himansu Das|incollection|SAHOO2019227||||||||||Nilanjan Dey and Himansu Das and Bighnaraj Naik and Himansu Sekhar Behera|||||||||||||||||||1112149008|42
||Business rules, Data quality, Data quality evaluation, Data quality measurement, Data quality characteristics, Data quality properties, ISO/IEC 25012, ISO/IEC 25024||102058||Data quality evaluation is built upon data quality measurement results. “Data quality evaluation” uses the “data quality rules” representing the risk appetite of the organization to decide on the usability of the data; “data quality measurement” uses the business rules describing the “data requirements” or “data specifications” to determine the validity of the data. Consequently, to conduct meaningful and useful data quality evaluations, business rules must be first completely identified and captured at the beginning of the evaluation to perform sound measurements. We propose that the evaluation leads to better and more interpretable and useful results when the potential contribution of these business rules to the measurement of the data quality characteristics is first evaluated, avoiding the inclusion in the evaluation of those not having potential contribution and the resulting waste of resources. Considering this, we feel that for a better management of business rules for data quality evaluation, it makes sense to group all business rules having an important contribution to the evaluation of data quality characteristics, something that other business rules management methodologies have not covered yet. Through our experiences in conducting industrial projects of data quality evaluations we identified six problems when collecting and grouping the business rules. These problems make data quality evaluation processes less efficient and more costly. The main contribution of this paper is a methodology to systematically collect, group and validate the business rules to avoid or to alleviate these problems. For the sake of generalization, comparability, and reusability, we propose to do the grouping for data quality characteristics and properties defined in ISO/IEC 25012 and ISO/IEC 25024, respectively. Lastly, we validate the methodology in three case studies of real projects. From this validation, it is possible to raise the conclusion that the methodology is useful, applicable in the real world, and valid to capture and group the business rules used as a basis for data quality evaluation.|https://doi.org/10.1016/j.is.2022.102058|https://www.sciencedirect.com/science/article/pii/S0306437922000485||||2022|BR4DQ: A methodology for grouping business rules for data quality evaluation|Ismael Caballero and Fernando Gualo and Moisés Rodríguez and Mario Piattini|article|CABALLERO2022102058|||Information Systems|03064379||109|||||12305|0,547|Q2|85|100|271|4739|1027|255|3,39|47,39|United Kingdom|Western Europe|1975-2021|Hardware and Architecture (Q2); Information Systems (Q2); Software (Q2)|2,604|2.309|0.00331|1112812452|303930735
||MALDI-ToF, Pattern recognition, Quality control, Comparative intensity data, Automated processing||100194||Urine from first trimester pregnancies has been found to be rich in information related to aneuploidies and other clinical conditions. Mass spectral analysis derived from matrix assisted laser desorption ionization (MALDI) time of flight (ToF) data has been proven to be a cost effective method for clinical diagnostics. However, urine mass spectra are complex and require data modelling frameworks. Therefore, computational approaches that systematically analyse big data generated from MALDI-ToF mass spectra are essential. To address this issue, we developed an automated workflow that successfully processed large data sets from MALDI-ToF which is 100-fold faster than using a common software tool. Our method performs accurate data quality control decisions, and generates a comparative analysis to extract peak intensity patterns from a data set. We successfully applied our framework to the identification of peak intensity patterns for Trisomy 21 and Trisomy 18 gestations on data sets from maternal pregnancy urines obtained in the UK and China. The results from our automated comparative analysis have shown characteristic patterns associated with aneuploidies in the first trimester pregnancy. Moreover, we have shown that the intensity patterns depended on the population origin, gestational age, and MALDI-ToF instrument.|https://doi.org/10.1016/j.imu.2019.100194|https://www.sciencedirect.com/science/article/pii/S2352914819300851||||2019|An automated workflow for MALDI-ToF mass spectra pattern identification on large data sets: An application to detect aneuploidies from pregnancy urine|Ricardo J. Pais and R. Zmuidinaite and S.A. Butler and R.K. Iles|article|PAIS2019100194|||Informatics in Medicine Unlocked|23529148||16|||||21100780477|0,440|Q3|21|208|219|9529|798|217|3,37|45,81|United Kingdom|Western Europe|2015-2020|Health Informatics (Q3)||||1116791778|1420175481
|||36|341–376|Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice|||https://doi.org/10.1145/3447404.3447424|New York, NY, USA|Association for Computing Machinery|9781450390293|2021|Personal Context from Mobile Phones—Case Study 5|Wiese, Jason|inbook|10.1145/3447404.3447424|||||||||1||||||||||||||||||||1119335353|42
|San Francisco, CA, USA|||||It is our great pleasure to welcome you to The Web Conference 2019. The Web Conference is the premier venue focused on understanding the current state and the evolution of the Web through the lens of computer science, computational social science, economics, policy, and many other disciplines. The 2019 edition of the conference is a reflection point as we celebrate the 30th anniversary of the Web.|||New York, NY, USA|Association for Computing Machinery|9781450366748|2019|WWW '19: The World Wide Web Conference||proceedings|10.1145/3308558|||||||||||||||||||||||||||||1124883317|42
JCDL '16|Newark, New Jersey, USA|bibliometrics, natural language processing, information retrieval, digital libraries, text mining|2|299–300|Proceedings of the 16th ACM/IEEE-CS on Joint Conference on Digital Libraries|The large scale of scholarly publications poses a challenge for scholars in information-seeking and sensemaking. Bibliometric, information retrieval~(IR), text mining and NLP techniques could help in these activities, but are not yet widely used in digital libraries. This workshop is intended to stimulate IR researchers and digital library professionals to elaborate on new approaches in natural language processing, information retrieval, scientometric and recommendation techniques which can advance the state-of-the-art in scholarly document understanding, analysis and retrieval at scale.|10.1145/2910896.2926734|https://doi.org/10.1145/2910896.2926734|New York, NY, USA|Association for Computing Machinery|9781450342292|2016|Joint Workshop on Bibliometric-Enhanced Information Retrieval and Natural Language Processing for Digital Libraries (BIRNDL 2016)|Cabanac, Guillaume and Chandrasekaran, Muthu Kumar and Frommholz, Ingo and Jaidka, Kokil and Kan, Min-Yen and Mayr, Philipp and Wolfram, Dietmar|inproceedings|10.1145/2910896.2926734|||||||||||||||||||||||||||||1125655970|42
SCA '19|Casablanca, Morocco|service company, supply chain, SCM, digital, prospective approach|8||Proceedings of the 4th International Conference on Smart City Applications|"\"Supply Chain Management (SCM) was born and developed first in an industrial context. In the field of services, little research has addressed the issue of the company's SCM. According to [1] \"\"service logistics is an approach that stabilizes and guarantees the continuity of flows: it is then oriented more towards the service provided than towards reducing traffic costs\"\". The SCM of services is of increasing interest to companies facing strong competition, market globalization and rapid changes in information and communication technologies. This evolution has led to a rapid integration of new digital practices in this field.So, how is the digitalization of the SCM of service companies looking today and what will be the future trends? On the one hand, with the help of the literature review, we seek to identify the concept of the SCM in services and its specificities, then that of digitization of the SCM and its organizational dimension. On the other hand, we are attempting a prospective approach to the current practices and digitalization prospects of the service company's SCM.\""|10.1145/3368756.3369005|https://doi.org/10.1145/3368756.3369005|New York, NY, USA|Association for Computing Machinery|9781450362894|2019|The Digitalization of the Supply Chain Management of Service Companies: A Prospective Approach|Bentalha, Badr and Hmioui, Aziz and Alla, Lhoussaine|inproceedings|10.1145/3368756.3369005|29||||||||||||||||||||||||||||1125937391|42
||Absolute temperature, Northern Hemisphere, Valid station, Data quality, Seasonal bias, Extreme values distribution, Missing records, Big data analysis||70-83||Starting from a set of 6190 meteorological stations we are choosing 6130 of them and only for Northern Hemisphere we are computing average values for absolute annual Mean, Minimum, Q1, Median, Q3, Maximum temperature plus their standard deviations for years 1800–2013, while we use 4887 stations and 389467 rows of complete yearly data. The data quality and the seasonal bias indices are defined and used in order to evaluate our dataset. After the year 1969 the data quality is monotonically decreasing while the seasonal bias is positive in most of the cases. An Extreme Value Distribution estimation is performed for minimum and maximum values, giving some upper bounds for both of them and indicating a big magnitude for temperature changes. Finally suggestions for improving the quality of meteorological data are presented.|https://doi.org/10.1016/j.jastp.2015.03.009|https://www.sciencedirect.com/science/article/pii/S1364682615000577||||2015|Extraction of the global absolute temperature for Northern Hemisphere using a set of 6190 meteorological stations from 1800 to 2013|Demetris T. Christopoulos|article|CHRISTOPOULOS201570|||Journal of Atmospheric and Solar-Terrestrial Physics|13646826||128|||||28436|0,515|Q2|89|180|628|8954|1256|620|1,89|49,74|United Kingdom|Western Europe|1997-2020|Geophysics (Q2); Atmospheric Science (Q3); Space and Planetary Science (Q3)|7,321|1.735|0.00523|1126763675|30653751
||supervised learning, output noise, covering distance filtering, generalization error bound, optimal sample selection|66|||The existence of output noise will bring difficulties to supervised learning. Noise filtering, aiming to detect and remove polluted samples, is one of the main ways to deal with the noise on outputs. However, most of the filters are heuristic and could not explain the filtering in uence on the generalization error (GE) bound. The hyper-parameters in various filters are specified manually or empirically, and they are usually unable to adapt to the data environment. The filter with an improper hyper-parameter may overclean, leading to a weak generalization ability. This paper proposes a unified framework of optimal sample selection (OSS) for the output noise filtering from the perspective of error bound. The covering distance filter (CDF) under the framework is presented to deal with noisy outputs in regression and ordinal classification problems. Firstly, two necessary and sufficient conditions for a fixed goodness of fit in regression are deduced from the perspective of GE bound. They provide the unified theoretical framework for determining the filtering effectiveness and optimizing the size of removed samples. The optimal sample size has the adaptability to the environmental changes in the sample size, the noise ratio, and noise variance. It offers a choice of tuning the hyper-parameter and could prevent filters from overcleansing. Meanwhile, the OSS framework can be integrated with any noise estimator and produces a new filter. Then the covering interval is proposed to separate low-noise and high-noise samples, and the effectiveness is proved in regression. The covering distance is introduced as an unbiased estimator of high noises. Further, the CDF algorithm is designed by integrating the cover distance with the OSS framework. Finally, it is verified that the CDF not only recognizes noise labels correctly but also brings down the prediction errors on real apparent age data set. Experimental results on benchmark regression and ordinal classification data sets demonstrate that the CDF outperforms the state-of-the-art filters in terms of prediction ability, noise recognition, and efficiency.||||JMLR.org||2022|A Unified Sample Selection Framework for Output Noise Filtering: An Error-Bound Perspective|Jiang, Gaoxia and Wang, Wenjian and Qian, Yuhua and Liang, Jiye|article|10.5555/3546258.3546276|18|jul|J. Mach. Learn. Res.|15324435|1|22|January 2021||||||||||||||||||||||1126793600|1642452964
ICSE '22|Pittsburgh, Pennsylvania|descriptive, data science processes, data science pipelines, predictive|13|2091–2103|Proceedings of the 44th International Conference on Software Engineering|Increasingly larger number of software systems today are including data science components for descriptive, predictive, and prescriptive analytics. The collection of data science stages from acquisition, to cleaning/curation, to modeling, and so on are referred to as data science pipelines. To facilitate research and practice on data science pipelines, it is essential to understand their nature. What are the typical stages of a data science pipeline? How are they connected? Do the pipelines differ in the theoretical representations and that in the practice? Today we do not fully understand these architectural characteristics of data science pipelines. In this work, we present a three-pronged comprehensive study to answer this for the state-of-the-art, data science in-the-small, and data science in-the-large. Our study analyzes three datasets: a collection of 71 proposals for data science pipelines and related concepts in theory, a collection of over 105 implementations of curated data science pipelines from Kaggle competitions to understand data science in-the-small, and a collection of 21 mature data science projects from GitHub to understand data science in-the-large. Our study has led to three representations of data science pipelines that capture the essence of our subjects in theory, in-the-small, and in-the-large.|10.1145/3510003.3510057|https://doi.org/10.1145/3510003.3510057|New York, NY, USA|Association for Computing Machinery|9781450392211|2022|The Art and Practice of Data Science Pipelines: A Comprehensive Study of Data Science Pipelines in Theory, in-the-Small, and in-the-Large|Biswas, Sumon and Wardat, Mohammad and Rajan, Hridesh|inproceedings|10.1145/3510003.3510057|||||||||||||||||||||||||||||1127786669|42
||Reliability;Time series analysis;Web services;Computer network reliability;Meteorology;Big data;Quality of service;Temporal evolution regularities;online reliability prediction;big service;convolutional neural networks||398-411||Service computing is an emerging technology in System of Systems Engineering (SoS Engineering or SoSE), which regards a System as a Service, and aims at constructing a robust and value-added complex system by outsourcing external component systems through service composition. The burgeoning Big Service computing just covers the significant challenges in constructing and maintaining a stable service-oriented SoS. A service-oriented SoS runs under a volatile and uncertain environment. As a step toward big service, service fault tolerance (FT) can guarantee the run-time quality of a service-oriented SoS. To successfully deploy FT in an SoS, online reliability time series prediction, which aims at predicting the reliability in near future for a service-oriented SoS arises as a grand challenge in SoS research. In particular, we need to tackle a number of big data related issues given the large and fast increasing size of the historical data that will be used for prediction purpose. The decision-making of prediction solution space be more complex. To provide highly accurate prediction results, we tackle the prediction challenges by identifying the evolution regularities of component systems' running states via different machine learning models. We present in this paper the motifs-based Dynamic Bayesian Networks (or m_DBNs) to perform one-step-ahead online reliability time series prediction. We also propose a multi-steps trajectory DBNs (or multi_DBNs) to further improve the accuracy of future reliability prediction. Finally, a Convolutional Neural Networks (CNN)-based prediction approach is developed to deal with the big data challenges. Extensive experiments conducted on real-world Web services demonstrate that our models outperform other well-known approaches consistently.|10.1109/TSC.2016.2633264|||||2019|Learning the Evolution Regularities for BigService-Oriented Online Reliability Prediction|Wang, Hongbing and Wang, Lei and Yu, Qi and Zheng, Zibin|article|7762161||May|IEEE Transactions on Services Computing|19391374|3|12|||||18300156728|1,207|Q1|70|141|364|3692|1950|261|4,91|26,18|United States|Northern America|2008-2020|Computer Networks and Communications (Q1); Computer Science Applications (Q1); Hardware and Architecture (Q1); Information Systems and Management (Q1)|3,719|8.216|0.00578|1130311056|1598291178
||Information theory, Occupant behaviour, Energy efficiency, Machine learning, Data length, Big data, Indoor environmental quality, Continuous monitoring||112197||Scientific literature about building occupants’ behaviour and the related energy performance analyses document about several strategies to monitor window operation, including different sensors and data series lengths. In this framework, the primary goal of this study is to propose effective guidelines for minimum experiment durations and their reliability. A six-year-long database from a living laboratory was used as a benchmark; and a recursive strategy enabled to split it into more than 2,500 subsets, supporting two main steps. First, information theory concepts were used to calculate uncertainty and subsets’ divergence were compared to the full database. Second, the subsets were used to train deep neural networks and evaluate the influence of monitoring lengths combined with different kinds of environmental data (i.e. indoor or outdoor). From the information-theoretic metrics, the results support that indoor-related variables can reduce most of the uncertainty related to window operation. Besides, subsets influenced by autumn and winter diverge the most compared to the full database. Considering the modelling approach, the results demonstrated that by including indoor-related variables, higher shares of reliably-performing models were achieved, and smaller subsets were needed. Seasonality has also played a major role along these lines. As a consequence, the conclusions supported the feasibility of nine-month-long field studies, starting in summer or spring, when indoor and outdoor variables are monitored.|https://doi.org/10.1016/j.enbuild.2022.112197|https://www.sciencedirect.com/science/article/pii/S0378778822003681||||2022|Are years-long field studies about window operation efficient? a data-driven approach based on information theory and deep learning|Mateus Bavaresco and Ioannis Kousis and Ilaria Pigliautile and Anna {Laura Pisello} and Cristina Piselli and Enedir Ghisi|article|BAVARESCO2022112197|||Energy and Buildings|03787788||268|||||29359|1,737|Q1|184|705|2402|39338|15832|2394|6,33|55,80|Netherlands|Western Europe|1970, 1977-2020|Building and Construction (Q1); Civil and Structural Engineering (Q1); Electrical and Electronic Engineering (Q1); Mechanical Engineering (Q1)|51,254|5.879|0.04387|1132509096|1347637955
SCA '18|Tetouan, Morocco|data mining, big data analytics, Smart sustainable cities|10||Proceedings of the 3rd International Conference on Smart City Applications|There has recently been much enthusiasm about the possibilities created by the big data deluge to better understand, monitor, analyze, and plan modern cities to improve their contribution to the goals of sustainable development. Indeed, much of our knowledge of urban sustainability has been gleaned from studies that are characterized by data scarcity. Therefore, this paper endeavors to develop a systematic framework for urban sustainability analytics based on a cross-industry standard process for data mining. The intention is to enable well-informed decision-making and enhanced insights in relation to diverse urban domains. We argue that there is tremendous potential to transform and advance the knowledge of smart sustainable cities through the creation of a big data deluge that seeks to provide much more sophisticated, wider-scale, finer-grained, real-time understanding, and control of various aspects of urbanity in the undoubtedly upcoming Exabyte Age.|10.1145/3286606.3286788|https://doi.org/10.1145/3286606.3286788|New York, NY, USA|Association for Computing Machinery|9781450365628|2018|The Big Data Deluge for Transforming the Knowledge of Smart Sustainable Cities: A Data Mining Framework for Urban Analytics|Bibri, Simon Elias and Krogstie, John|inproceedings|10.1145/3286606.3286788|11||||||||||||||||||||||||||||1132687648|42
IWCTS'18|Seattle, WA, USA|OpenStreetMap, Hierarchical Classification, Street Networks, Semantics, Conditional Random Fields|10|29–38|Proceedings of the 11th ACM SIGSPATIAL International Workshop on Computational Transportation Science|This paper presents an intuitive and novel method for improving the semantic quality of streets in crowdsourced maps. Two factors negatively affecting the quality are incorrect and ambiguous semantics. Toward overcoming these, a multi-layer CRF based model is proposed that performs a simultaneous hierarchical classification of streets into fine-grained (crowdsourced; therefore, rich but ambiguous) and coarse-grained (familiar and standard) semantics. Inference is performed using Lazy Flipper algorithm which is fast for street network consisting of several hundred thousand streets. The model achieves a classification accuracy of 61% for fine-grained classification and 77% for coarse-grained classification respectively.|10.1145/3283207.3283210|https://doi.org/10.1145/3283207.3283210|New York, NY, USA|Association for Computing Machinery|9781450360371|2018|A Multi-Layer CRF Based Methodology for Improving Crowdsourced Street Semantics|Jilani, Musfira and Corcoran, Padraig and Bertolotto, Michela|inproceedings|10.1145/3283207.3283210|||||||||||||||||||||||||||||1133586245|42
||Smart cities;Streaming media;Cloud computing;Big Data;Quality of service;Mobile communication;Urban areas||31-37||Recent advances in networking, caching, and computing have significant impacts on the developments of smart cities. Nevertheless, these important enabling technologies have traditionally been studied separately in the existing works on smart cities. In this article, we propose an integrated framework that can enable dynamic orchestration of networking, caching, and computing resources to improve the performance of applications for smart cities. Then we present a novel big data deep reinforcement learning approach. Simulation results with different system parameters are presented to show the effectiveness of the proposed scheme.|10.1109/MCOM.2017.1700246|||||2017|Software-Defined Networks with Mobile Edge Computing and Caching for Smart Cities: A Big Data Deep Reinforcement Learning Approach|He, Ying and Yu, F. Richard and Zhao, Nan and Leung, Victor C. M. and Yin, Hongxi|article|8198798||Dec|IEEE Communications Magazine|15581896|12|55|||||||||||||||||||||||1135274652|160873938
ICISCAE 2021|Dalian, China||7|207–213|2021 4th International Conference on Information Systems and Computer Aided Education|After experiencing the stages of document delivery service, information service and knowledge service, the traditional product design creative demand perception technology has gradually transformed to the cloud service stage driven by new technologies such as big data and cloud computing. With the advent of the era of big data and artificial intelligence, multi-source heterogeneous and massive data resources have specific fusion characteristics and application trends. The generation of new artificial intelligence technologies and methods is to respond to the above characteristics and trends. Multi-source heterogeneous resources and vast amounts of data driven product design requirements perception from the user requirements perception, perception technology content and creative product design requirements capturing perception technology scenario-based push these three core function implementation requirements perception technology cloud service mode, implement the product design requirements perception technology innovation process. Of this study was to study the cloud service situation and feedback the fusion of product design requirements perception technology, cloud service components analysis demand perception technology and its features, with case studies to explore data driven era product design demand perception technology to the cloud service model transformation of ideas, requirements for perception technology transformation provides scientific theory and practice.|10.1145/3482632.3482675|https://doi.org/10.1145/3482632.3482675|New York, NY, USA|Association for Computing Machinery|9781450390255|2021|Cloud Service Context and Feedback Fusion of Product Design Creative Demand Perception Technology|Sun, Wen|inproceedings|10.1145/3482632.3482675|||||||||||||||||||||||||||||1135537810|42
||NASA;Manuals;Big Data;Servers;Decision trees;Anomaly detection;Random forests;Anomaly detection;Autocorrelation;Data quality tests;Explainability;LSTM-Autoencoder;Time series||5068-5077|2020 IEEE International Conference on Big Data (Big Data)|Data quality significantly impacts the results of data analytics. Researchers have proposed machine learning based anomaly detection techniques to identify incorrect data. Existing approaches fail to (1) identify the underlying domain constraints violated by the anomalous data, and (2) generate explanations of these violations in a form comprehensible to domain experts. We propose IDEAL, which is an LSTM-Autoencoder based approach that detects anomalies in multivariate time-series data, generates domain constraints, and reports subsequences that violate the constraints as anomalies. We propose an automated autocorrelation-based windowing approach to adjust the network input size, thereby improving the correctness and performance of constraint discovery over manual and brute-force approaches. The anomalies are visualized in a manner comprehensible to domain experts in the form of decision trees extracted from a random forest classifier. Domain experts can then provide feedback to retrain the learning model and improve the accuracy of the process. We evaluate the effectiveness of IDEAL using datasets from Yahoo servers, NASA Shuttle, and Colorado State University Energy Institute. We demonstrate that IDEAL can detect previously known anomalies from these datasets. Using mutation analysis, we show that IDEAL can detect different types of injected faults. We also demonstrate that the accuracy improves after incorporating domain expert feedback.|10.1109/BigData50022.2020.9378192|||||2020|An Autocorrelation-based LSTM-Autoencoder for Anomaly Detection on Time-Series Data|Homayouni, Hajar and Ghosh, Sudipto and Ray, Indrakshi and Gondalia, Shlok and Duggan, Jerry and Kahn, Michael G.|inproceedings|9378192||Dec|||||||||||||||||||||||||||1135598047|42
WWW '22|Virtual Event, Lyon, France|Graph representation learning, Graph augmentation learning, Graph neural networks|10|1063–1072|Companion Proceedings of the Web Conference 2022|Graph Augmentation Learning (GAL) provides outstanding solutions for graph learning in handling incomplete data, noise data, etc. Numerous GAL methods have been proposed for graph-based applications such as social network analysis and traffic flow forecasting. However, the underlying reasons for the effectiveness of these GAL methods are still unclear. As a consequence, how to choose optimal graph augmentation strategy for a certain application scenario is still in black box. There is a lack of systematic, comprehensive, and experimentally validated guideline of GAL for scholars. Therefore, in this survey, we in-depth review GAL techniques from macro (graph), meso (subgraph), and micro (node/edge) levels. We further detailedly illustrate how GAL enhance the data quality and the model performance. The aggregation mechanism of augmentation strategies and graph learning models are also discussed by different application scenarios, i.e., data-specific, model-specific, and hybrid scenarios. To better show the outperformance of GAL, we experimentally validate the effectiveness and adaptability of different GAL strategies in different downstream tasks. Finally, we share our insights on several open issues of GAL, including heterogeneity, spatio-temporal dynamics, scalability, and generalization.|10.1145/3487553.3524718|https://doi.org/10.1145/3487553.3524718|New York, NY, USA|Association for Computing Machinery|9781450391306|2022|Graph Augmentation Learning|Yu, Shuo and Huang, Huafei and Dao, Minh N. and Xia, Feng|inproceedings|10.1145/3487553.3524718|||||||||||||||||||||||||||||1136392951|42
||Agile methodologies, Business intelligence (BI), Analytics and big data, Lifecycle for BI and Big Data||700-710||Agile methodologies were introduced in 2001. Since this time, practitioners have applied Agile methodologies to many delivery disciplines. This article explores the application of Agile methodologies and principles to business intelligence delivery and how Agile has changed with the evolution of business intelligence. Business intelligence has evolved because the amount of data generated through the internet and smart devices has grown exponentially altering how organizations and individuals use information. The practice of business intelligence delivery with an Agile methodology has matured; however, business intelligence has evolved altering the use of Agile principles and practices. The Big Data phenomenon, the volume, variety, and velocity of data, has impacted business intelligence and the use of information. New trends such as fast analytics and data science have emerged as part of business intelligence. This paper addresses how Agile principles and practices have evolved with business intelligence, as well as its challenges and future directions.|https://doi.org/10.1016/j.ijinfomgt.2016.04.013|https://www.sciencedirect.com/science/article/pii/S026840121630233X||||2016|A review and future direction of agile, business intelligence, analytics and data science|Deanne Larson and Victor Chang|article|LARSON2016700|||International Journal of Information Management|02684012|5|36|||||15631|2,770|Q1|114|224|382|20318|5853|373|16,16|90,71|United Kingdom|Western Europe|1970, 1986-2021|Artificial Intelligence (Q1); Computer Networks and Communications (Q1); Information Systems (Q1); Information Systems and Management (Q1); Library and Information Sciences (Q1); Management Information Systems (Q1); Marketing (Q1)|12,245|14.098|0.01167|1139236575|747927863
||crowdsourcing, incentive mechanism, Stackelberg game, crowdsensing|13|1732–1744||Smartphones are programmable and equipped with a set of cheap but powerful embedded sensors, such as accelerometer, digital compass, gyroscope, GPS, microphone, and camera. These sensors can collectively monitor a diverse range of human activities and the surrounding environment. Crowdsensing is a new paradigm which takes advantage of the pervasive smartphones to sense, collect, and analyze data beyond the scale of what was previously possible. With the crowdsensing system, a crowdsourcer can recruit smartphone users to provide sensing service. Existing crowdsensing applications and systems lack good incentive mechanisms that can attract more user participation. To address this issue, we design incentive mechanisms for crowdsensing. We consider two system models: the crowdsourcer-centric model where the crowdsourcer provides a reward shared by participating users, and the user-centric model where users have more control over the payment they will receive. For the crowdsourcer-centric model, we design an incentive mechanism using a Stackelberg game, where the crowdsourcer is the leader while the users are the followers. We show how to compute the unique Stackelberg Equilibrium, at which the utility of the crowdsourcer is maximized, and none of the users can improve its utility by unilaterally deviating from its current strategy. For the user-centric model, we design an auction-based incentive mechanism, which is computationally efficient, individually rational, profitable, and truthful. Through extensive simulations, we evaluate the performance and validate the theoretical properties of our incentive mechanisms.|10.1109/TNET.2015.2421897|https://doi.org/10.1109/TNET.2015.2421897||IEEE Press||2016|Incentive Mechanisms for Crowdsensing: Crowdsourcing with Smartphones|Yang, Dejun and Xue, Guoliang and Fang, Xi and Tang, Jian|article|10.1109/TNET.2015.2421897||jun|IEEE/ACM Trans. Netw.|10636692|3|24|June 2016||||27237|1,022|Q1|174|187|653|6994|3573|652|5,40|37,40|United States|Northern America|1993-2020|Computer Networks and Communications (Q1); Computer Science Applications (Q1); Electrical and Electronic Engineering (Q1); Software (Q1)||||1141195600|1453185914
|||12|18–29||Implicit Requirements (IMR) identification is part of the Requirements Engineering (RE) phase in Software Engineering during which data is gathered to create SRS (Software Requirements Specifications) documents. As opposed to explicit requirements clearly stated, IMRs constitute subtle data and need to be inferred. Research has shown that IMRs are crucial to the success of software development. Many software systems can encounter failures due to lack of IMR data management. SRS documents are large, often hundreds of pages, due to which manually identifying IMRs by human software engineers is not feasible. Moreover, such data is evergrowing due to the expansion of software systems. It is thus important to address the crucial issue of IMR data management. This article presents a survey on IMRs in SRS documents with the definition and overview of IMR data, detailed taxonomy of IMRs with explanation and examples, practices in managing IMR data, and tools for IMR identification. In addition to reviewing classical and state-of-the-art approaches, we highlight trends and challenges and point out open issues for future research. This survey article is interesting based on data quality, hidden information retrieval, veracity and salience, and knowledge discovery from large textual documents with complex heterogeneous data.|10.1145/3552490.3552494|https://doi.org/10.1145/3552490.3552494|New York, NY, USA|Association for Computing Machinery||2022|Management of Implicit Requirements Data in Large SRS Documents: Taxonomy and Techniques|Dave, Dev and Celestino, Angelica and Varde, Aparna S. and Anu, Vaibhav|article|10.1145/3552490.3552494||jul|SIGMOD Rec.|01635808|2|51|June 2022||||13622|0,372|Q2|142|39|81|808|127|57|1,67|20,72|United States|Northern America|1969, 1973-1978, 1981-2020|Information Systems (Q2); Software (Q2)|1,819|0.775|7.5E-4|1143652096|962972343
|||||Data Cleaning|Data quality is one of the most important problems in data management, since dirty data often leads to inaccurate data analytics results and incorrect business decisions. Poor data across businesses and the U.S. government are reported to cost trillions of dollars a year. Multiple surveys show that dirty data is the most common barrier faced by data scientists. Not surprisingly, developing effective and efficient data cleaning solutions is challenging and is rife with deep theoretical and engineering problems.This book is about data cleaning, which is used to refer to all kinds of tasks and activities to detect and repair errors in the data. Rather than focus on a particular data cleaning task, we give an overview of the endto- end data cleaning process, describing various error detection and repair methods, and attempt to anchor these proposals with multiple taxonomies and views. Specifically, we cover four of the most common and important data cleaning tasks, namely, outlier detection, data transformation, error repair (including imputing missing values), and data deduplication. Furthermore, due to the increasing popularity and applicability of machine learning techniques, we include a chapter that specifically explores how machine learning techniques are used for data cleaning, and how data cleaning is used to improve machine learning models.This book is intended to serve as a useful reference for researchers and practitioners who are interested in the area of data quality and data cleaning. It can also be used as a textbook for a graduate course. Although we aim at covering state-of-the-art algorithms and techniques, we recognize that data cleaning is still an active field of research and therefore provide future directions of research whenever appropriate.||https://doi.org/10.1145/3310205.3310214|New York, NY, USA|Association for Computing Machinery|9781450371520|2019|Conclusion and Future Thoughts||inbook|10.1145/3310205.3310214|||||||||||||||||||||||||||||1149893572|42
||Sensors;Data integrity;Data privacy;Task analysis;Mobile handsets;Roads;Monitoring;Mobile crowdsensing systems;privacy preservation;data quality;untruthful reporting||647-661||The proliferation of mobile smart devices with ever improving sensing capacities means that human-centric Mobile Crowdsensing Systems (MCSs) can economically provide a large scale and flexible sensing solution. The use of personal mobile devices is a sensitive issue, therefore it is mandatory for practical MCSs to preserve private information (the user's true identity, precise location, etc.) while collecting the required sensing data. However, well intentioned privacy protection techniques also conceal autonomous, or even malicious, behaviors of device owners (termed as self-interested), where the objectivity and accuracy of crowdsensing data can therefore be severely threatened. The issue of data quality due to untruthful reporting in privacy-preserving MCSs has been yet to produce solutions. Bringing together game theory, algorithmic mechanism design, and truth discovery, we develop a mechanism to guarantee and enhance the quality of crowdsensing data without jeopardizing the privacy of MCS participants. Together with solid theoretical justifications, we evaluate the performance of our proposal with extensive real-world MCS trace-driven simulations. Experimental results demonstrate the effectiveness of our mechanism on both enhancing the quality of the crowdsensing data and eliminating the motivation of MCS participants, even when their privacy is well protected, to report untruthfully.|10.1109/TMC.2019.2943468|||||2021|On the Data Quality in Privacy-Preserving Mobile Crowdsensing Systems with Untruthful Reporting|Zhao, Cong and Yang, Shusen and McCann, Julie A.|article|8847467||Feb|IEEE Transactions on Mobile Computing|15580660|2|20|||||||||||||||||||||||1152670204|1408838020
||Semantic web, IT professionals' perspective technology adoption, Technology-organization-environment framework, Innovation diffusion theory||18-33||The scale and complexity of big data quickly exceed the reach of direct human comprehension and increasingly require machine assistance to semantically analyze, organize, and interpret vast and diverse sources of big data in order to unlock its strategic value. Due to its volume, velocity, variety, and veracity, big data integration challenges overwhelm traditional integration approaches leaving many integration possibilities out of reach. Unlocking the value of big data requires innovative technology. Organizations must have the innovativeness and data capability to adopt the technology and harness its potential value. The Semantic Web (SW) technology has demonstrated its potential for integrating big data and has become important technology for tackling big data. Despite its importance to manage big data, little research has examined the determinants affecting SW adoption. Drawing upon the technology–organization–environment framework as a theory base, this study develops a research model explaining the factors affecting the adoption of SW technology from IT professionals' perspective, specifically in the context of corporate computing enterprises. We validate the proposed model using a set of empirical data collected from IT professionals including IT managers, system architects, software developers, and web developers. The findings suggest that perceived usefulness, perceived ease of use, organization's innovativeness, organization's data capability, and applicability to data management are important drivers of SW adoption. This study provides new insights on theories of organizational IT adoption from IT professionals' perspectives tailored to the context of SW technology.|https://doi.org/10.1016/j.chb.2018.04.014|https://www.sciencedirect.com/science/article/pii/S074756321830178X||||2018|Exploring Determinants of Semantic Web Technology Adoption from IT Professionals' Perspective: Industry Competition, Organization Innovativeness, and Data Management Capability|Dan J. Kim and John Hebeler and Victoria Yoon and Fred Davis|article|KIM201818|||Computers in Human Behavior|07475632||86|||||19419|2,108|Q1|178|385|1597|26867|14501|1573|7,83|69,78|United Kingdom|Western Europe|1985-2021|Arts and Humanities (miscellaneous) (Q1); Human-Computer Interaction (Q1); Psychology (miscellaneous) (Q1)|45,035|6.829|0.05973|1154510362|265090421
https://doi.org/10.1016/j.hlpt.2018.12.003|https://www.sciencedirect.com/science/article/pii/S2211883718301631||||2019|Bringing big data analytics closer to practice: A methodological explanation and demonstration of classification algorithms|Ofir Ben-Assuli and Tsipi Heart and Nir Shlomo and Robert Klempfner|article|BENASSULI20197|||Health Policy and Technology|22118837|1|8||||||||||||||||||||||||||||||1156958250|42
||Digital Twin, CMM, inspection, Model-based-defintion, Digital thread||216-221||The use of Digital Twin (DT) is adopted by manufacturers and have positive effects on the product manufacturing process. The aim of this paper is to define a Coordinate Measuring Machine (CMM) inspection DT model, based on inspection process digitalized functionalities, from one side, and Industry 4.0 opportunities (Digital thread, Big data, etc.), from the other side. A review about DT definition, is firstly presented. Secondly, we review related studies based on existing DT orientations and usages for CMM inspection. Thirdly, challenges related to the variation management are presented. Finally, a discussion about possible DT functionalities and opportunities is conducted then a CMM inspection DT model is presented.|https://doi.org/10.1016/j.promfg.2021.07.033|https://www.sciencedirect.com/science/article/pii/S2351978921001694||||2021|Towards the implementation of the Digital Twin in CMM inspection process: opportunities, challenges and proposals|Raoudha Gaha and Alexandre Durupt and Benoit Eynard|article|GAHA2021216|||Procedia Manufacturing|23519789||54||10th CIRP Sponsored Conference on Digital Enterprise Technologies (DET 2020) – Digital Technologies as Enablers of Industrial Competitiveness and Sustainability|||21100792109|0,504|Q2|43|1390|3628|27760|8346|3572|1,79|19,97|Netherlands|Western Europe|2015-2020|Artificial Intelligence (Q2); Industrial and Manufacturing Engineering (Q2)||||1160183727|896540749
MK Series on Business Intelligence||||173-192|Measuring Data Quality for Ongoing Improvement||https://doi.org/10.1016/B978-0-12-397033-6.00014-6|https://www.sciencedirect.com/science/article/pii/B9780123970336000146|Boston|Morgan Kaufmann|978-0-12-397033-6|2013|Chapter 13 - Directives for Data Quality Strategy|Laura Sebastian-Coleman|incollection|SEBASTIANCOLEMAN2013173||||||||||Laura Sebastian-Coleman|||||||||||||||||||1160352130|42
||artificial intelligence, automotive, big data analytics, industry 4.0, knowledge discovery, neural networks, prediction, principal component analysis||11168-11174||This article provides an artificial intelligence platform proposal for paint structure quality prediction using Big Data analytics methodologies. The whole proposal fits into the current trends that are outlined in the Industry 4.0 concept. The painting process is very complex, producing huge volumes of data, but the main problem is that the data comes from different data sources, often heterogeneous, and it is necessary to propose a way to collect and integrate them into a common repository. The motivation for this work were the industry requirements to solve specific problems that cannot be solved by standard methods but require a sophisticated and holistic approach. It is the application of artificial intelligence that suggests a solution that is not otherwise visible, and the use of standard methods would not give any satisfactory results. The result is the design of an artificial intelligence platform that has been deployed in a real manufacturing process, and the initial results confirm the correctness and validity of this step. We also present a data collection and integration architecture, which is an integral part of every big data analytics solution, and a principal component analysis that was used to reduce the dimensionality of the large number of production process data.|https://doi.org/10.1016/j.ifacol.2020.12.299|https://www.sciencedirect.com/science/article/pii/S2405896320305796||||2020|Artificial Intelligence Platform Proposal for Paint Structure Quality Prediction within the Industry 4.0 Concept|M. Kebisek and P. Tanuska and L. Spendla and J. Kotianova and P. Strelec|article|KEBISEK202011168|||IFAC-PapersOnLine|24058963|2|53||21st IFAC World Congress|||21100456158|0,308|Q3|72|115|8407|1913|9863|8351|1,13|16,63|Austria|Western Europe|2002-2019|Control and Systems Engineering (Q3)||||1161629549|676980763
||Psychological targeting, Psychological profiling, Psychologically-informed interventions, Big Data, Digital footprints, Methods, Ethics, Privacy, Contextual integrity||193-222|Measuring and Modeling Persons and Situations|Advances in the collection, storage, and processing of large amounts of user data have given rise to psychological targeting, which we define as the process of extracting individuals’ psychological characteristics from their digital footprints in order to target them with psychologically-informed interventions at scale. In this chapter, we introduce a two-stage framework of psychological targeting consisting of (1) psychological profiling and (2) psychologically-informed interventions. We summarize the most important research findings in relation to the two stages and discuss important methodological opportunities and pitfalls. To help researchers make the most of the opportunities, we also provide practical advice on how to deal with some of the potential pitfalls. Finally, we highlight ethical opportunities and challenges and offer some suggestions for addressing these challenges. If done right, psychological targeting has the potential to advance our scientific understanding of human nature and to enhance the well-being of individuals and society at large.|https://doi.org/10.1016/B978-0-12-819200-9.00015-6|https://www.sciencedirect.com/science/article/pii/B9780128192009000156||Academic Press|978-0-12-819200-9|2021|Chapter 6 - Psychological targeting in the age of Big Data|Ruth E. Appel and Sandra C. Matz|incollection|APPEL2021193||||||||||Dustin Wood and Stephen J. Read and P.D. Harms and Andrew Slaughter|||||||||||||||||||1161942950|42
||Location-based data, Crowdsourced data, Waze, Bluetooth, Big data, Smart cities, Surface streets||101518||Obtaining accurate speed and travel time information is a challenge for researchers, geographers, and transportation agencies. In the past, traffic data were usually acquired and disseminated by government agencies through fixed-location sensors. High costs, infrastructure demands, and low coverage levels of these sensor devices require agencies and researchers to look beyond the traditional approaches. With the emergence of smartphones and navigation apps, location-based and crowdsourced Big Data are receiving increased attention. In this regard, location-based big data (LocBigData) collected from probe vehicles and road users can be used to provide speed and travel time information in different locations. Examining the quality of crowdsourced data is essential for researchers and agencies before using them. This study assessed the quality of Waze speed data from surface streets and conducted a case study in Sevierville, Tennessee. Typically, examining the quality of these data in surface streets and arterials is more challenging than freeways data. This research used Bluetooth speed data as the ground truth, which is independent of Waze data. In this study, three steps of methodology were used. In the first step, Waze speed data was compared to Bluetooth data in terms of accuracy, mean difference, and distribution similarity. In the second step, a k-means algorithm was used to categorize Waze data quality, and a multinomial logistics regression model was performed to explore the significant factors that impact data quality. Finally, in the third step, machine learning techniques were conducted to predict the data quality in different conditions. The result of the comparison showed a similar pattern and a slight difference between datasets, which verified the quality of Waze speed data. The statistical model indicates that that Waze speed data are more accurate in peak hours than in night hours. Also, the traffic speed, traffic volume, and segment length have a significant association on the accuracy of Waze data on surface streets. Finally, the result of machine learning prediction showed that a KNN method performed the highest prediction accuracy of 84.5% and 82.9% of the time for training and test datasets, respectively. Overall, the study results suggest that Waze speed data is a promising data source for surface streets.|https://doi.org/10.1016/j.compenvurbsys.2020.101518|https://www.sciencedirect.com/science/article/pii/S0198971520302519||||2020|Quality of location-based crowdsourced speed data on surface streets: A case study of Waze and Bluetooth speed data in Sevierville, TN|Nima Hoseinzadeh and Yuandong Liu and Lee D. Han and Candace Brakewood and Amin Mohammadnazar|article|HOSEINZADEH2020101518|||Computers, Environment and Urban Systems|01989715||83|||||23269|1,549|Q1|92|85|327|4842|2135|324|6,11|56,96|United Kingdom|Western Europe|1980-1986, 1988-2020|Ecological Modeling (Q1); Environmental Science (miscellaneous) (Q1); Geography, Planning and Development (Q1); Urban Studies (Q1)||||1162868362|1571752529
||Big data trading, Blockchain, Smart contract, Proxy re-encryption, Price negotiation, Value reward||107994||Data are an extremely important asset. Governments around the world encourage big data sharing and trading to promote the big data economy. However, existing data trading platforms are not fully trusted. Such platforms face the problems of a single point of failure (SPOF), opaque transactions, uncontrollability, untraceability, and issues of data privacy. Several blockchain-based big data trading methods have been proposed; however, they do not adequately address the security issues introduced by dishonesty in the data provider and data agent or the fairness of data revenue distribution and price bargaining. In this paper, we propose a blockchain-based decentralized data trading system in which data trading is completed by smart contract-based data matching, price negotiation, and reward assigning. Moreover, the proposed data trading system evaluates the data quality on the basis of three metrics, records the evaluation results in a side-chain, and distributes the data users’ application revenue to the data provider according to the evaluated data quality. We verify the security, usability, and efficiency of the proposed big data trading system.|https://doi.org/10.1016/j.comnet.2021.107994|https://www.sciencedirect.com/science/article/pii/S138912862100116X||||2021|A blockchain-based trading system for big data|Donghui Hu and Yifan Li and Lixuan Pan and Meng Li and Shuli Zheng|article|HU2021107994|||Computer Networks|13891286||191|||||26811|0,798|Q1|135|383|896|19762|4981|877|5,93|51,60|Netherlands|Western Europe|1977-1984, 1989-1990, 1996-2020|Computer Networks and Communications (Q1)|11,644|4.474|0.00957|1164668526|424954694
|||4|1918–1921||The rapid accumulation of indoor positioning data is increasingly booming the interest in indoor mobility analyses. As a fundamental analysis, it is highly relevant to translate raw indoor positioning data into mobility semantics that describe what, where and when in a more concise and semantics-oriented way. Such a translation is challenging as multiple data sources are involved, raw indoor positioning data is of low quality, and translation results are hard to assess. We demonstrate a system TRIPS that streamlines the entire translation process by three functional components. The Configurator provides a standard but concise means to configure multiple input sources, including the indoor positioning data, indoor space information, and relevant contexts. The Translator cleans the indoor positioning data and exports reliable mobility semantics without manual interventions. The Viewer offers a suite of flexible operations to trace the input, output and intermediate data involved in the translation. Data analysts can interact with TRIPS to obtain the desired mobility semantics in a visual and convenient way.|10.14778/3229863.3236224|https://doi.org/10.14778/3229863.3236224||VLDB Endowment||2018|TRIPS: A System for Translating Raw Indoor Positioning Data into Visual Mobility Semantics|Li, Huan and Lu, Hua and Shi, Feichao and Chen, Gang and Chen, Ke and Shou, Lidan|article|10.14778/3229863.3236224||aug|Proc. VLDB Endow.|21508097|12|11|August 2018||||21100199855|0,946|Q1|134|246|598|12401|3759|566|5,50|50,41|United States|Northern America|2008-2020|Computer Science (miscellaneous) (Q1)|7,198|2.047|0.00891|1167093680|1216159931
||Data pre-processing, Big engineering data, Building energy management||112372||The rapid development of building energy consumption monitoring platforms makes engineering data more diverse, which facilitates the goal of reducing emissions. It is increasingly acknowledged that data preprocessing deserves the same attention as intelligent algorithms. In this work, the data quality issue of the engineering big data from non-demonstration complexes in China are analyzed thoroughly, and the analysis is based on clustering-based algorithms. We can conclude that the data of the hourly power of equipment groups are quality and stable, which is suitable for the benchmark to check other data. The quality of the data about pipes is acceptable. The number of data types about cooling towers is less, and the quality is worse. Regarding other data, the quality is unstable, so researchers should deal with those case-by-case. According to the above analysis, we proposed a convenient, rule-based data preprocessing framework that utilizes the law of physics, ensuring the strong coupling of multi-variants. After the data preprocessing, these engineering data are more reliable and can be used to improve performance or train models. Additionally, the proposed framework is more suitable for preprocessing multi-variant engineering data.|https://doi.org/10.1016/j.enbuild.2022.112372|https://www.sciencedirect.com/science/article/pii/S0378778822005436||||2022|A rule-based data preprocessing framework for chiller rooms inspired by the analysis of engineering big data|Ruikai He and Tong Xiao and Shunian Qiu and Jiefan Gu and Minchen Wei and Peng Xu|article|HE2022112372|||Energy and Buildings|03787788||273|||||29359|1,737|Q1|184|705|2402|39338|15832|2394|6,33|55,80|Netherlands|Western Europe|1970, 1977-2020|Building and Construction (Q1); Civil and Structural Engineering (Q1); Electrical and Electronic Engineering (Q1); Mechanical Engineering (Q1)|51,254|5.879|0.04387|1169958209|1347637955
||Big Data, Hydroinformatics, Visual Analytics, Visualization, Human-Computer Interaction||105396||Recent advances in information, communication, and environmental monitoring technologies have increased the availability, spatiotemporal resolution, and quality of water-related data, thereby leading to the emergence of many innovative big data applications. Among these applications, visualization and visual analytics, also known as the visual computing techniques, empower the synergy of computational methods (e.g., machine learning and statistical models) with human reasoning to improve the understanding and solution toward complex science and engineering problems. These approaches are frequently integrated with geographic information systems and cyberinfrastructure to provide new opportunities and methods for enhancing water resources management. In this paper, we present a comprehensive review of recent hydroinformatics applications that employ visual computing techniques to (1) support complex data-driven research problems, and (2) support the communication and decision-makings in the water resources management sector. Then, we conduct a technical review of the state-of-the-art web-based visualization technologies and libraries to share our experiences on developing shareable, adaptive, and interactive visualizations and visual interfaces for water resources management applications. We close with a vision that applies the emerging visual computing technologies and paradigms to develop the next generation of hydroinformatics applications.|https://doi.org/10.1016/j.envsoft.2022.105396|https://www.sciencedirect.com/science/article/pii/S1364815222001025||||2022|An overview of visualization and visual analytics applications in water resources management|Haowen Xu and Andy Berres and Yan Liu and Melissa R. Allen-Dumas and Jibonananda Sanyal|article|XU2022105396|||Environmental Modelling & Software|13648152||153|||||23295|1,828|Q1|136|223|717|14218|4373|711|5,47|63,76|Netherlands|Western Europe|1997-2020|Ecological Modeling (Q1); Environmental Engineering (Q1); Software (Q1)||||1170038702|665084965
||artificial intelligence, cardiovascular imaging, deep learning, machine learning||1317-1335||Data science is likely to lead to major changes in cardiovascular imaging. Problems with timing, efficiency, and missed diagnoses occur at all stages of the imaging chain. The application of artificial intelligence (AI) is dependent on robust data; the application of appropriate computational approaches and tools; and validation of its clinical application to image segmentation, automated measurements, and eventually, automated diagnosis. AI may reduce cost and improve value at the stages of image acquisition, interpretation, and decision-making. Moreover, the precision now possible with cardiovascular imaging, combined with “big data” from the electronic health record and pathology, is likely to better characterize disease and personalize therapy. This review summarizes recent promising applications of AI in cardiology and cardiac imaging, which potentially add value to patient care.|https://doi.org/10.1016/j.jacc.2018.12.054|https://www.sciencedirect.com/science/article/pii/S0735109719302360||||2019|Artificial Intelligence in Cardiovascular Imaging: JACC State-of-the-Art Review|Damini Dey and Piotr J. Slomka and Paul Leeson and Dorin Comaniciu and Sirish Shrestha and Partho P. Sengupta and Thomas H. Marwick|article|DEY20191317|||Journal of the American College of Cardiology|07351097|11|73|||||||||||||||||||||||1170291560|2058625437
||Agriculture, COVID-19, Major crises, Smart technology, Community marketing, Resilience||103023||The COVID-19 outbreak was an unprecedented situation that uncovered forgotten interconnections and interdependencies between agriculture, society, and economy, whereas it also brought to the fore the vulnerability of agrifood production to external disturbances. Building upon the ongoing experience of the COVID-19 pandemic, in this short communication, we discuss three potential mechanisms that, in our opinion, can mitigate the impacts of major crises or disasters in agriculture: resilience-promoting policies, community marketing schemes, and smart farming technology. We argue that resilience-promoting policies should focus on the development of crisis management plans and enhance farmers' capacity to cope with external disturbances. We also stress the need to promote community marketing conduits that ensure an income floor for farmers while in parallel facilitating consumer access to agrifood products when mainstream distribution channels under-serve them. Finally, we discuss some issues that need to be solved to ensure that smart technology and big data can help farmers overcome external shocks.|https://doi.org/10.1016/j.agsy.2020.103023|https://www.sciencedirect.com/science/article/pii/S0308521X20308842||||2021|Enhancing the ability of agriculture to cope with major crises or disasters: What the experience of COVID-19 teaches us|Evagelos D. Lioutas and Chrysanthi Charatsari|article|LIOUTAS2021103023|||Agricultural Systems|0308521X||187|||||15061|1,694|Q1|107|197|511|12414|3229|503|5,52|63,02|United Kingdom|Western Europe|1976-2020|Agronomy and Crop Science (Q1); Animal Science and Zoology (Q1)|9,779|5.370|0.00937|1180683702|947875286
WWW '12 Companion|Lyon, France|sensors, search engine, multimedia|4|283–286|Proceedings of the 21st International Conference on World Wide Web|This paper presents work in progress within the FP7 EU-funded project SMART to develop a multimedia search engine over content and information stemming from the physical world, as derived through visual, acoustic and other sensors. Among the unique features of the search engine is its ability to respond to social queries, through integrating social networks with sensor networks. Motivated by this innovation, the paper presents and discusses the state-of-the-art in participatory sensing and other technologies blending social and sensor networks.|10.1145/2187980.2188029|https://doi.org/10.1145/2187980.2188029|New York, NY, USA|Association for Computing Machinery|9781450312301|2012|Multimedia Search over Integrated Social and Sensor Networks|Soldatos, John and Draief, Moez and Macdonald, Craig and Ounis, Iadh|inproceedings|10.1145/2187980.2188029|||||||||||||||||||||||||||||1183552853|42
ArabWIC 2019|Rabat, Morocco|data lifecycle, Open Government Data, data value creation|6||Proceedings of the ArabWIC 6th Annual International Conference Research Track|"\"Government, through Open Government Data \"\"OGD, becomes one of the important producers of open data. OGD is an opportunity to create valuable services and innovative products useful for citizens as a primarily targeted consumer. However, the expected benefits of OGD are not yet met. That is to say, several research communities' studies insist on the necessity of creating valuable data in order to generate valuable services. These studies are still insufficient for a shared understanding of how OGD contribute to the creation of value. For this purpose, this paper presents a review of a set of data lifecycle models compared against their contribution to the creation of value in the context of OGD.\""|10.1145/3333165.3333180|https://doi.org/10.1145/3333165.3333180|New York, NY, USA|Association for Computing Machinery|9781450360890|2019|Open Government Data: Towards a Comparison of Data Lifecycle Models|Elmekki, Hanae and Chiadmi, Dalila and Lamharhar, Hind|inproceedings|10.1145/3333165.3333180|15||||||||||||||||||||||||||||1184793006|42
||Streaming media;Cameras;Cloud computing;Lighting;Image color analysis;Empirical mode decomposition;Feature extraction;Empirical Mode Decomposition;Local Ternary Patterns;Riesz Transform;Amplitude Spectrum;Cloud Computing;Big Data Analytics;Object Classification||18-26|2016 IEEE/ACM 3rd International Conference on Big Data Computing Applications and Technologies (BDCAT)|The recent rise in multimedia technology has made it easier to perform a number of tasks. One of these tasks is monitoring where cheap cameras are producing large amount of video data. This video data is then processed for object classification to extract useful information. However, the videodata obtained by these cheap cameras is often of low qualityand results in blur video content. Moreover, various illuminationeffects caused by lightning conditions also degradethe video quality. These effects present severe challenges forobject classification. We present a cloud-based blur and illumination invariant approach for object classification fromimages and video data. The bi-dimensional empirical modedecomposition (BEMD) has been adopted to decompose avideo frame into intrinsic mode functions (IMFs). TheseIMFs further undergo to first order Reisz transform to generatemonogenic video frames. The analysis of each IMF hasbeen carried out by observing its local properties (amplitude, phase and orientation) generated from each monogenic videoframe. We propose a stack based hierarchy of local patternfeatures generated from the amplitudes of each IMF whichresults in blur and illumination invariant object classification. The extensive experimentation on video streams aswell as publically available image datasets reveals that oursystem achieves high accuracy from 0.97 to 0.91 for increasingGaussian blur ranging from 0.5 to 5 and outperformsstate of the art techniques under uncontrolled conditions. The system also proved to be scalable with high throughputwhen tested on a number of video streams using cloud infrastructure.||||||2016|Spatial Frequency Based Video Stream Analysis for Object Classification and Recognition in Clouds|Yaseen, Muhammad Usman and Anjum, Ashiq and Antonopoulos, Nick|inproceedings|7877045||Dec|||||||||||||||||||||||||||1186011643|42
||Pitfalls, Considerations, Implementation, Migration pattern, Practitioner's perspective, Open source, Data warehouse||167-179|Software Architecture for Big Data and the Cloud|Big data solutions represent a significant challenge for some organizations. There are a huge variety of software products, deployment patterns and solution options that need to be considered to ensure a successful outcome for an organization trying to implement a big data solution. With that in mind, the chapter “Big Data: a practitioner's perspective” will focus on four key areas associated with big data that require consideration from a practical and implementation perspective: (i) Big Data is a new Paradigm – Differences with Traditional Data Warehouse, Pitfalls and Considerations; (ii) Product considerations for Big Data – Use of Open Source products for Big Data, Pitfalls and Considerations; (iii) Use of Cloud for hosting Big Data – Why use Cloud, Pitfalls and Considerations; and (iv) Big Data Implementation – Architecture definition, processing framework and migration patterns from Data Warehouse to Big Data.|https://doi.org/10.1016/B978-0-12-805467-3.00010-7|https://www.sciencedirect.com/science/article/pii/B9780128054673000107|Boston|Morgan Kaufmann|978-0-12-805467-3|2017|Chapter 10 - Big Data: A Practitioners Perspective|Darshan Lopes and Kevin Palmer and Fiona O'Sullivan|incollection|LOPES2017167||||||||||Ivan Mistrik and Rami Bahsoon and Nour Ali and Maritta Heisel and Bruce Maxim|||||||||||||||||||1186417685|42
||Europol, Big data, Privacy, Data protection, Data protection impact assessment, Risk assessment, Privacy by design, Advanced technologies, Europol Regulation, Integrated Data Management Concept (IDMC)||298-308||In the digital age, the interaction between privacy, data protection and advanced technological developments such as big data analytics has become pertinent to Europol's effectiveness in providing accurate crime analyses. For the purposes of preventing and combating crime falling within the scope of its objectives, it is imperative for Europol to employ the fullest and most up-to-date information and technical capabilities possible whilst respecting fundamental human rights. The present article addresses precisely the “paradox” of on one side protecting fundamental human rights against external terrorist and/or cybercrime intrusions, and on the other providing a privacy-conscious approach to data collection and analytics, so that Europol can even more effectively support and strengthen action in protecting society against internal threats in a proportionate, responsible and legitimate manner. The advantage proposed in this very context of large quantities of data informing strategic analysis at Europol is a purpose-oriented data protection impact assessment. Namely, the evolution from traditional instruments in the fight against organised crime and terrorism to more technologically advanced ones equally requires an alteration of the conventional notions of privacy and investigative and information-sharing methods.|https://doi.org/10.1016/j.clsr.2017.03.006|https://www.sciencedirect.com/science/article/pii/S0267364917300699||||2017|The BIG DATA Challenge: Impact and opportunity of large quantities of information under the Europol Regulation|Daniel Drewer and Vesela Miladinova|article|DREWER2017298|||Computer Law & Security Review|02673649|3|33|||||28888|0,815|Q1|38|66|242|1245|707|223|3,39|18,86|United Kingdom|Western Europe|1985-2020|Business, Management and Accounting (miscellaneous) (Q1); Computer Networks and Communications (Q1); Law (Q1)||||1189685715|1769124433
ICAIIS 2021|Chongqing, China||6||2021 2nd International Conference on Artificial Intelligence and Information Systems||10.1145/3469213.3470272|https://doi.org/10.1145/3469213.3470272|New York, NY, USA|Association for Computing Machinery|9781450390200|2021|Research and Application of Digital Collection Method of Human Movement|Hu, Yerong and He, Xiangzhen and Zhang, Yihao and Zeng, Jia and Yang, Huaiyuan and Zhou, Shuaihang|inproceedings|10.1145/3469213.3470272|71||||||||||||||||||||||||||||1192566145|42
||Big Data, social media, data analysis, public health, Internet||67-82|Participatory Health Through Social Media|Social Media (SM) can be a complementary channel of information to other official means for the health data collection such as the epidemiological surveillance activities and control carried out by health authorities. For this reason, more and more organizations, professionals, and scientific institutions are seeing the need to make the most of resources of health information based on SM platforms through the use of Big Data tools and analytics. Although there is a consensus on the potential benefits and opportunities that SM may provide when used for healthcare purposes, its use has brought unsuspected drawbacks and challenges related to the protection of personal data, it is essential to promote a wide reflection and that the authorities and governments establish, in collaboration with patients associations and professional institutions, specific ethical, legal guidelines, and use policies to the benefit of the current and future healthcare professional–patient relationship and general public.|https://doi.org/10.1016/B978-0-12-809269-9.00005-0|https://www.sciencedirect.com/science/article/pii/B9780128092699000050||Academic Press|978-0-12-809269-9|2016|Chapter 5 - Big Data For Health Through Social Media|M.A. Mayer and L. Fernández-Luque and A. Leis|incollection|MAYER201667||||||||||Shabbir Syed-Abdul and Elia Gabarron and Annie Y.S. Lau|||||||||||||||||||1194356927|42
||Ambidexterity, Industry 4.0, Business intelligence, Big data, Intellectual capital, Human intellect, Accountability and performance||121201||This study investigates the literary corpus of the role and potential of data intelligence and analytics through the lenses of artificial intelligence (AI), big data, and the human–AI interface to improve overall decision-making processes. It investigates how data intelligence and analytics improve decision-making processes in the public sector. A bibliometric analysis of a database containing 161 English-language articles published between 2017 and 2021 is performed, providing a map of the knowledge produced and disseminated in previous studies. It provides insights into key topics, citation patterns, publication activities, the status of collaborations between contributors over past studies, aggregated data intelligence, and analytics research contributions. The study provides a retrospective review of published content in the field of data intelligence and analytics. The findings indicate that field research has been concentrated mainly on emerging technologies' intelligence capabilities rather than on human–artificial intelligence in decision-making performance in the public sector. This study extends an ambidexterity theory in decision support, which enlightens how this ambidexterity can be encouraged and how it affects decision outcomes. The study emphasises the importance of the public sector adoption of data intelligence and analytics, as well as its efficiency. Furthermore, this study expands how researchers and practitioners interpret and understand data intelligence and analytics, AI, and big data for effective public sector decision-making.|https://doi.org/10.1016/j.techfore.2021.121201|https://www.sciencedirect.com/science/article/pii/S004016252100634X||||2022|Data intelligence and analytics: A bibliometric analysis of human–Artificial intelligence in public sector decision-making effectiveness|Assunta {Di Vaio} and Rohail Hassan and Claude Alavoine|article|DIVAIO2022121201|||Technological Forecasting and Social Change|00401625||174|||||14704|2,226|Q1|117|448|1099|35581|10127|1061|9,01|79,42|United States|Northern America|1970-2020|Applied Psychology (Q1); Business and International Management (Q1); Management of Technology and Innovation (Q1)|21,116|8.593|0.02416|1194887826|1949868303
ICDLT '21|Qingdao, China|Sales prediction, Transformer, Smart container|8|46–53|Proceedings of the 2021 5th International Conference on Deep Learning Technologies|With the advent of the new retail era, the value of unmanned smart container is increasingly prominent. Fast and flexible self-service is favored by consumers. How to use accumulated historical sales data to predict sales in the future is an important part of smart container operation management. Reasonable sales prediction can not only reduce the inventory cost, but also reduce the shortage loss of the container. Based on the smart container sales data of Dalian Xiaode New Retail Co., Ltd., through detailed exploratory analysis in many aspects, this paper carries out the feature selection of sales prediction, and uses random forest, XGBoost, Transformer and other algorithms to predict sales. The experimental results show that the prediction accuracy of Transformer is better than traditional algorithms, whose MAPE is 14.67% lower than that of the worst one. Transformer can be well applied in the field of sales prediction of smart container. And in this experiment, compared with Transformer using sine and cosine functions for positional encoding, Transformer encoded by position index has better prediction performance and stronger stability.|10.1145/3480001.3480017|https://doi.org/10.1145/3480001.3480017|New York, NY, USA|Association for Computing Machinery|9781450390163|2021|A Transformer Based Sales Prediction of Smart Container in New Retail Era|Jin, Ying and Gao, Ming and Yu, Jixiang|inproceedings|10.1145/3480001.3480017|||||||||||||||||||||||||||||1195539557|42
||data quality, Human-in-the-loop, deep learning|27|||Schema matching is a core task of any data integration process. Being investigated in the fields of databases, AI, Semantic Web, and data mining for many years, the main challenge remains the ability to generate quality matches among data concepts (e.g., database attributes). In this work, we examine a novel angle on the behavior of humans as matchers, studying match creation as a process. We analyze the dynamics of common evaluation measures (precision, recall, and f-measure), with respect to this angle and highlight the need for unbiased matching to support this analysis. Unbiased matching, a newly defined concept that describes the common assumption that human decisions represent reliable assessments of schemata correspondences, is, however, not an inherent property of human matchers. In what follows, we design PoWareMatch that makes use of a deep learning mechanism to calibrate and filter human matching decisions adhering to the quality of a match, which are then combined with algorithmic matching to generate better match results. We provide an empirical evidence, established based on an experiment with more than 200 human matchers over common benchmarks, that PoWareMatch predicts well the benefit of extending the match with an additional correspondence and generates high-quality matches. In addition, PoWareMatch outperforms state-of-the-art matching algorithms.|10.1145/3483423|https://doi.org/10.1145/3483423|New York, NY, USA|Association for Computing Machinery||2022|PoWareMatch: A Quality-Aware Deep Learning Approach to Improve Human Schema Matching|Shraga, Roee and Gal, Avigdor|article|10.1145/3483423|16|may|J. Data and Information Quality|19361955|3|14|September 2022||||||||||||||||||||||1196636978|833754770
WSC '17|Las Vegas, Nevada||21||Proceedings of the 2017 Winter Simulation Conference|In stochastic simulation, input modeling refers to the process of identifying and selecting the probability distributions, called input models, from which are generated the random variates that are the source of the stochastic variation in the simulation model when it is run. This article reviews the history of the development and use of such models with the main focus on discrete-event simulation (DES).||||IEEE Press|9781538634271|2017|History of Input Modeling|Cheng, Russell|inproceedings|10.5555/3242181.3242194|12||||||||||||||||||||||||||||1199116609|42
ICBDR 2018|Weihai, China|college student management, data mining, big data|4|86–89|Proceedings of the 2nd International Conference on Big Data Research|Data mining is an advanced science and technology to process data and information. It can be extracted from a large number of complex data, or find some valuable data rules and models. Education has entered the era of big data. However, the way of data processing in university educational administration is relatively backward. Aiming at this problem, this paper applies data mining technology to college teaching management, extracts useful information from the data collected by educational administration management system, and provides correct and powerful data support and guarantee for college teaching managers to make relevant decisions.|10.1145/3291801.3291834|https://doi.org/10.1145/3291801.3291834|New York, NY, USA|Association for Computing Machinery|9781450364768|2018|Application of Large Data Mining Technology in Colleges and Universities|Deze, Wang|inproceedings|10.1145/3291801.3291834|||||||||||||||||||||||||||||1199160687|42
Hybrid Computational Intelligence for Pattern Analysis||Aspect extraction, Big data, CRISP-DM, Customer relations, Data mining, Data visualization, Deep learning, Distributed computing, LSTM, Machine learning||37-61|Trends in Deep Learning Methodologies|The growth of the digital age has led to a colossal leap in data generated by the average user. This growing data has several applications: businesses can use it to give a more personalized touch to their services, governments can use it to better allocate their funds, and companies can utilize it to select the best candidates for a job. While these applications may seem extremely enticing, there are a couple of problems that must be solved first, namely, data collection and extraction of useful patterns from the data. The disciplines of data mining and big data deal with these problems, respectively. But, as we have already discussed, the amount of data is so vast that any manual approach is extremely time intensive and costly. Thus this limits the potential outcomes from this data. This problem has been solved by the application of deep learning. Deep learning has allowed us to automate processes that were not only time intensive but also mentally arduous. It has achieved better than human accuracy in several types of discriminative and recognition tasks making it a viable alternative to inefficient human labor. Deep learning plays a vital role in this analysis and has enabled several businesses to comprehend customer needs and accordingly improve their own services, thus giving them the opportunity to outdo their competitors. Similarly, deep learning has also been instrumental in analyzing the trends and associations of securities in the financial market. It has even helped to create fraud detection and loan underwriting applications, which have contributed to making financial institutions more transparent and efficient. Apart from directly improving the efficiency in these fields, deep learning has also been instrumental in improving the fields of data mining and big data. Machine learning algorithms can actually utilize the existing data to predict the unknowns, including future trends in data. Due to its potential applications the field of machine learning is deeply interconnected with data mining. Nevertheless, machine learning algorithms are often heavily dependent on the availability of huge datasets to ensure useful accuracy. Deep learning algorithms have allowed the different components of data (i.e., multimedia data) in the data mining process itself to be identified. Similarly, semantic indexing and tagging algorithms have allowed the processes of big data to speed up. In this chapter, we will discuss the applications of deep learning in these fields and give a brief overview of the concepts involved.|https://doi.org/10.1016/B978-0-12-822226-3.00002-7|https://www.sciencedirect.com/science/article/pii/B9780128222263000027||Academic Press|978-0-12-822226-3|2021|Chapter 2 - Deep learning in big data and data mining|Deepak Kumar Sharma and Bhanu Tokas and Leo Adlakha|incollection|SHARMA202137||||||||||Vincenzo Piuri and Sandeep Raj and Angelo Genovese and Rajshree Srivastava|||||||||||||||||||1200608600|42
||Big data privacy, Cloud computing (CC) applications, COVID-19, Digital transformation, Security challenge, Work from home||100059||In light of the COVID-19 outbreak caused by the novel coronavirus, companies and institutions have instructed their employees to work from home as a precautionary measure to reduce the risk of contagion. Employees, however, have been exposed to different security risks because of working from home. Moreover, the rapid global spread of COVID-19 has increased the volume of data generated from various sources. Working from home depends mainly on cloud computing (CC) applications that help employees to efficiently accomplish their tasks. The cloud computing environment (CCE) is an unsung hero in the COVID-19 pandemic crisis. It consists of the fast-paced practices for services that reflect the trend of rapidly deployable applications for maintaining data. Despite the increase in the use of CC applications, there is an ongoing research challenge in the domains of CCE concerning data, guaranteeing security, and the availability of CC applications. This paper, to the best of our knowledge, is the first paper that thoroughly explains the impact of the COVID-19 pandemic on CCE. Additionally, this paper also highlights the security risks of working from home during the COVID-19 pandemic.|https://doi.org/10.1016/j.jnlest.2020.100059|https://www.sciencedirect.com/science/article/pii/S1674862X20300665||||2021|Impact of coronavirus pandemic crisis on technologies and cloud computing applications|Ziyad R. Alashhab and Mohammed Anbar and Manmeet Mahinderjit Singh and Yu-Beng Leau and Zaher Ali Al-Sai and Sami {Abu Alhayja’a}|article|ALASHHAB2021100059|||Journal of Electronic Science and Technology|1674862X|1|19||Special Section on In Silico Research on Microbiology and Public Health|||21100432792|0,132|Q4|5|37|134|961|74|124|0,82|25,97|China|Asiatic Region|2015-2020|Computer Networks and Communications (Q4); Electrical and Electronic Engineering (Q4); Signal Processing (Q4)||||1200817607|811784945
https://doi.org/10.1016/j.patter.2021.100347|https://www.sciencedirect.com/science/article/pii/S2666389921002026||||2021|Addressing bias in big data and AI for health care: A call for open science|Natalia Norori and Qiyang Hu and Florence Marcelle Aellen and Francesca Dalia Faraci and Athina Tzovara|article|NORORI2021100347|||Patterns|26663899|10|2||||||||||||||||||||||||||||||1200898061|42
||Indexes;Lakes;Companies;Big data;Data mining;Industries;Data analysis;Web mining;Data preprocessing;Machine learning||21-30|2015 IEEE/ACM 2nd International Symposium on Big Data Computing (BDC)|Data completeness is one of the most important data quality dimensions and an essential premise in data analytics. With new emerging Big Data trends such as the data lake concept, which provides a low cost data preparation repository instead of moving curated data into a data warehouse, the problem of data completeness is additionally reinforced. While traditionally the process of filling in missing values is addressed by the data imputation community using statistical techniques, we complement these approaches by using external data sources from the data lake or even the Web to lookup missing values. In this paper we propose a novel hybrid data imputation strategy that, takes into account the characteristics of an incomplete dataset and based on that chooses the best imputation approach, i.e. either a statistical approach such as regression analysis or a Web-based lookup or a combination of both. We formalize and implement both imputation approaches, including a Web table retrieval and matching system and evaluate them extensively using a corpus with 125M Web tables. We show that applying statistical techniques in conjunction with external data sources will lead to a imputation system which is robust, accurate, and has high coverage at the same time.|10.1109/BDC.2015.38|||||2015|Towards a Hybrid Imputation Approach Using Web Tables|Ahmadov, Ahmad and Thiele, Maik and Eberius, Julian and Lehner, Wolfgang and Wrembel, Robert|inproceedings|7406326||Dec|||||||||||||||||||||||||||1202071607|42
||Deep learning;Data integrity;Text categorization;Sampling methods;Natural language processing;Noise measurement;Task analysis;Data Augmentation;Chinese;Classification||2006-2012|2020 IEEE 6th International Conference on Computer and Communications (ICCC)|In natural language processing tasks, data is very important, but data collection is not cheap. Large volume data can well serve a series of tasks, especially for deep learning tasks. Data augmentation methods are solutions to data problems, which can work well on rising data quality and quantity, such as generating text without meaning changing and expanding the diversity of data distribution. A user-friendly method of the data augmentation is to sample words in a text then augmenting them. The sampling method is often implemented by a random probability. Although the performance of this solution has been proved over the past few years, random sampling is not the best choice for the data augmentation as it has a chance of randomly introducing some noise into initial data, like stop words. The generated data could interfere with the subsequent tasks and drop the accuracy of the tasks' solutions. Hence, this paper aims to introduce a novel data augmentation method that could avoid involving such noisy data. The strategy is keywords-oriented data augmentation for Chinese (KDA). The KDA proposed in this paper indicates a method of extracting keywords based on category labels, and an augmenting method based on the keywords. In contrast to randomness, the proposed technique firstly selects the key information data, then expands the selected data. The experimental section is compared with another two typical data augmentation techniques on three Chinese data sets for text classification tasks. The result shows that the KDA technique has a better performance in the data augmentation task than the compared two.|10.1109/ICCC51575.2020.9345133|||||2020|Keywords-oriented Data Augmentation for Chinese|Yuan, Fang and Hong, Xianbin and Yuan, Cheng and Fei, Xiang and Guan, Sheng-Uei and Liu, Dawei and Wang, Wei|inproceedings|9345133||Dec|||||||||||||||||||||||||||1202429038|42
||Vibration serviceability, Online sampling, Big data, Data cleaning||107850||Vibration serviceability issue has attracted increasing attentions recently. Many studies on vibration serviceability limitations have been performed in labs using simulation. The proposed limits were incompatible and lacked details about the physiological and environmental factors because of small sample sizes and unrealistic environments. This study proposes a novel online big data approach for investigating vibration serviceability limits in real environment. A smartphone-based application (App) was designed and spread to volunteers to collect multi-source heterogeneous data including questionnaires of personal judgement on vibration level, vibration signals, environmental and biological factors in their daily life. So far, 8521 records have been received. Data cleaning was performed and a qualified database with large volume and various types of factor information was produced. Analysis of the database showed that vibration limits given by the new method were compatible with previous results, but with more abundant details that were ignored in previous studies.|https://doi.org/10.1016/j.measurement.2020.107850|https://www.sciencedirect.com/science/article/pii/S0263224120303882||||2020|Online investigation of vibration serviceability limitations using smartphones|Lei Cao and Jun Chen|article|CAO2020107850|||Measurement|02632241||162|||||15424|0,772|Q1|91|1092|2645|40953|11589|2630|4,41|37,50|Netherlands|Western Europe|1983-2021|Condensed Matter Physics (Q1); Education (Q1); Electrical and Electronic Engineering (Q1); Instrumentation (Q1); Applied Mathematics (Q2); Statistics and Probability (Q2)||||1203181487|1009388385
||database design, Conceptual modeling|15|||Recent decade has seen a dramatic change in the information systems landscape that alters the ways we design and interact with information technologies, including such developments as the rise of business analytics, user-generated content, and NoSQL databases, to name just a few. These changes challenge conceptual modeling research to offer innovative solutions tailored to these environments. Conceptual models typically represent classes (categories, kinds) of objects rather than concrete specific objects, making the class construct a critical medium for capturing domain semantics. While representation of classes may differ between grammars, a common design assumption is what we term different semantics same syntax (D3S). Under D3S, all classes are depicted using the same syntactic symbols. Following recent findings in psychology, we introduce a novel assumption semantics-contingent syntax (SCS) whereby syntactic representations of classes in conceptual models may differ based on their semantic meaning. We propose a core SCS design principle and five guidelines pertinent for conceptual modeling. We believe SCS carries profound implications for theory and practice of conceptual modeling as it seeks to better support modern information environments.|10.1145/3131780|https://doi.org/10.1145/3131780|New York, NY, USA|Association for Computing Machinery||2017|Are All Classes Created Equal? Increasing Precision of Conceptual Modeling Grammars|Lukyanenko, Roman and Samuel, Binny M.|article|10.1145/3131780|14|sep|ACM Trans. Manage. Inf. Syst.|2158656X|4|8|December 2017||||||||||||||||||||||1204852877|2097885649
https://doi.org/10.1016/j.xops.2022.100215|https://www.sciencedirect.com/science/article/pii/S266691452200104X||||2023|Visual Acuity: Assessment of Data Quality and Usability in an Electronic Health Record System|Judith E. Goldstein and Xinxing Guo and Michael V. Boland and Kerry E. Smith|article|GOLDSTEIN2023100215|||Ophthalmology Science|26669145|1|3||||||||||||||||||||||||||||||1205344974|42
||Atmospheric modeling;Neural networks;Machine learning;Predictive models;Big Data;Delays;Quantum cascade lasers;Flight Delay;Machine Learning;Aviation Data Analytics||665-672|2020 IEEE Intl Conf on Dependable, Autonomic and Secure Computing, Intl Conf on Pervasive Intelligence and Computing, Intl Conf on Cloud and Big Data Computing, Intl Conf on Cyber Science and Technology Congress (DASC/PiCom/CBDCom/CyberSciTech)|Flight delay has been a serious and widespread problem that needs to be solved. One promising solution is the flight delay prediction. Although big data analytics and machine learning have been applied successfully in many domains, their applications in aviation are limited. This paper presents a comprehensive study of flight delay spanning data pre-processing, data visualization and data mining, in which we develop several machine learning models to predict flight arrival delays. Two data sets were used, namely Airline On-Time Performance (AOTP) Data and Quality Controlled Local Climatological Data (QCLCD). This paper aims to recognize useful patterns of the flight delay from aviation data and perform accurate delay prediction. The best result for flight delay prediction (five classes) using machine learning models is 89.07% (Multilayer Perceptron). A Convolution neural network model is also built which is enlightened by the idea of pattern recognition and success of neural network method, showing a slightly better result with 89.32% prediction accuracy.|10.1109/DASC-PICom-CBDCom-CyberSciTech49142.2020.00114|||||2020|Applying Machine Learning to Aviation Big Data for Flight Delay Prediction|Jiang, Yushan and Liu, Yongxin and Liu, Dahai and Song, Houbing|inproceedings|9251206||Aug|||||||||||||||||||||||||||1209539040|42
||||150-152|||https://doi.org/10.1016/j.molp.2019.01.008|https://www.sciencedirect.com/science/article/pii/S1674205219300139||||2019|Delineation of Nitrogen Signaling Networks: Computational Approaches in the Big Data Era|Yoshiaki Ueda and Shuichi Yanagisawa|article|UEDA2019150|||Molecular Plant|16742052|2|12|||||17600155011|4,588|Q1|115|153|474|8872|4792|380|9,60|57,99|United States|Northern America|2008-2020|Molecular Biology (Q1); Plant Science (Q1)|15,778|13.164|0.02686|1209874522|1400021155
||Big Data, Software reference architecture, Semantic-aware, Data management, Data analysis||75-92||Context: Big Data systems are a class of software systems that ingest, store, process and serve massive amounts of heterogeneous data, from multiple sources. Despite their undisputed impact in current society, their engineering is still in its infancy and companies find it difficult to adopt them due to their inherent complexity. Existing attempts to provide architectural guidelines for their engineering fail to take into account important Big Data characteristics, such as the management, evolution and quality of the data. Objective: In this paper, we follow software engineering principles to refine the λ-architecture, a reference model for Big Data systems, and use it as seed to create Bolster, a software reference architecture (SRA) for semantic-aware Big Data systems. Method: By including a new layer into the λ-architecture, the Semantic Layer, Bolster is capable of handling the most representative Big Data characteristics (i.e., Volume, Velocity, Variety, Variability and Veracity). Results: We present the successful implementation of Bolster in three industrial projects, involving five organizations. The validation results show high level of agreement among practitioners from all organizations with respect to standard quality factors. Conclusion: As an SRA, Bolster allows organizations to design concrete architectures tailored to their specific needs. A distinguishing feature is that it provides semantic-awareness in Big Data Systems. These are Big Data system implementations that have components to simplify data definition and exploitation. In particular, they leverage metadata (i.e., data describing data) to enable (partial) automation of data exploitation and to aid the user in their decision making processes. This simplification supports the differentiation of responsibilities into cohesive roles enhancing data governance.|https://doi.org/10.1016/j.infsof.2017.06.001|https://www.sciencedirect.com/science/article/pii/S0950584917304287||||2017|A software reference architecture for semantic-aware Big Data systems|Sergi Nadal and Victor Herrero and Oscar Romero and Alberto Abelló and Xavier Franch and Stijn Vansummeren and Danilo Valerio|article|NADAL201775|||Information and Software Technology|09505849||90|||||18732|0,606|Q2|103|121|420|6853|2316|403|5,12|56,64|Netherlands|Western Europe|1970, 1987-2020|Computer Science Applications (Q2); Information Systems (Q2); Software (Q2)|5,172|2.730|0.00554|1210968002|132335391
||Internet of things (IoT), Machine-to-machine communication (M2M), Human to machine (H2M), Machine learning, Smart grid security, Hadoop, Data mining, Security, Privacy||309-333|From Smart Grid to Internet of Energy|The smart grid applications are related with monitoring and control operations of conventional power grid. The integration of information and communication technologies (ICT) to existing power network has leveraged interaction of different generators, controllers, monitoring and measurement devices, and intelligent loads. The two-way communication infrastructure is comprised by numerous sensor networks that increased deployment of massive data from measurement nodes to monitoring centers. Big data is a widespread concept which become a trend for massive data streams that are transferred and processed in an ecosystem. The enormous amount of data are generated, transferred and stored to improve operating and management quality of smart grid. The big data analytics are performed to improve quality of service in terms of grid operators and consumers. The big data acquisition, processing, storing, and clustering stages are widely researched by a wide variety of specialist. In this chapter, the all these stages, big data acquisition technologies, machine learning methods used in big data analytics, privacy and security of big data infrastructure are introduced in detail. The privacy preserving methods, big data processing technologies and firmware infrastructures are presented in the context of this chapter.|https://doi.org/10.1016/B978-0-12-819710-3.00008-9|https://www.sciencedirect.com/science/article/pii/B9780128197103000089||Academic Press|978-0-12-819710-3|2019|Chapter 8 - Big data, privacy and security in smart grids|Ersan Kabalci and Yasin Kabalci|incollection|KABALCI2019309||||||||||Ersan Kabalci and Yasin Kabalci|||||||||||||||||||1211574940|42
||Throughput;Clustering algorithms;Big Data;Quality of service;Data mining;Tools;Matlab;Clustering;Cosine Similarity;Gaussian Similarity;Hybrid Similarity;AODV||849-853|2020 8th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions) (ICRITO)|Clustering is an important data mining and tool for examining data. The paper compares the two techniques of clustering, in the first technique only Cosine similarity is used and in the second technique Improved Rank Similarity (Cosine Similarity + Gaussian Similarity) is used. The results are compared with various parameters constituting the Accuracy in NetBeans and QoS parameters using AODV routing protocol. The simulation is done on MATLAB, a network is created and communication from source node to target node is noted.|10.1109/ICRITO48877.2020.9197774|||||2020|Enhancement of the Accuracy and QoS in Clustering of Data|Ahlawat, Deepak and Kaur, Amandeep and Gupta, Deepali|inproceedings|9197774||June|||||||||||||||||||||||||||1212931716|42
||airport management, big data, DataOps, Data-intensive systems, cyber-physical systems, systems governance|25|||Recent advancements in information technology have ushered a new wave of systems integrating Internet technology with sensing, wireless communication, and computational resources over existing infrastructures. As a result, myriad complex, non-traditional Cyber-Physical Systems (CPS) have emerged, characterized by interaction among people, physical facilities, and embedded sensors and computers, all generating vast amounts of complex data. Such a case is encountered within a contemporary airport hall setting: passengers roaming, information systems governing various functions, and data being generated and processed by cameras, phones, sensors, and other Internet of Things technology. This setting has considerable potential of contributing to goals entertained by the CPS operators, such as airlines, airport operators/owners, technicians, users, and more. We model the airport setting as an instance of such a complex, data-intensive CPS where multiple actors and data sources interact, and generalize a methodology to support it and other similar systems. Furthermore, this article instantiates the methodology and pipeline for predictive analytics for passenger flow, as a characteristic manifestation of such systems requiring a tailored approach. Our methodology also draws from DataOps principles, using multi-modal and real-life data to predict the underlying distribution of the passenger flow on a flight-level basis (improving existing day-level predictions), anticipating when and how the passengers enter the airport and move through the check-in and baggage drop-off process. This allows to plan airport resources more efficiently while improving customer experience by avoiding passenger clumping at check-in and security. We demonstrate results obtained over a case from a major international airport in the Netherlands, improving up to 60% upon predictions of daily passenger flow currently in place.|10.1145/3432247|https://doi.org/10.1145/3432247|New York, NY, USA|Association for Computing Machinery||2021|DataOps for Cyber-Physical Systems Governance: The Airport Passenger Flow Case|Garriga, Martin and Aarns, Koen and Tsigkanos, Christos and Tamburri, Damian A. and Heuvel, Wjan Van Den|article|10.1145/3432247|36|may|ACM Trans. Internet Technol.|15335399|2|21|June 2021||||||||||||||||||||||1213358596|314651938
||Data integrity;Data models;Internet of Things;Cleaning;Meteorology;Sensors;Predictive models;data quality;internet of things;big data;machine learning||1-4|2018 14th International Conference on Emerging Technologies (ICET)|Internet of Things (IoTs) is one of the most promising fields in computer science. It consists of physical devices, automobiles, home appliances, embedded hardware, sensors and actuators which empowers these objects to interface and share information with other devices over the network. The data gathered from these devices is used to make intelligent decisions. If the data quality is poor, decisions are likely to be flawed. A little work has been carried out regarding data quality in the Internet of Things, but there is no scheme which is experimentally proved. In this paper we will identify data quality challenges in the Internet of Things domain and propose a model which ensure data quality standards provided by ISO 8000. We evaluated our model on the weather dataset and used the random forest prediction method to calculate the accuracy of our data. Results show that when compared with the baseline model the proposed system improves accuracy by 38.88%.|10.1109/ICET.2018.8603594|||||2018|Data Quality Techniques in the Internet of Things: Random Forest Regression|Farooqi, M. Mashab and Ali Khattak, Hasan and Imran, Muhammad|inproceedings|8603594||Nov|||||||||||||||||||||||||||1213386564|42
|||6|72–77|||10.1145/3423923|https://doi.org/10.1145/3423923|New York, NY, USA|Association for Computing Machinery||2020|Digital Healthcare in Latin America: The Case of Brazil and Mexico|Tentori, Monica and Ziviani, Artur and Muchaluat-Saade, D\'{e}bora C. and Favela, Jesus|article|10.1145/3423923||oct|Commun. ACM|00010782|11|63|November 2020||||||||||||||||||||||1213536596|647144465
||Big data, Deep learning, Deep belief networks, Convolutional Neural Networks||275-287||Deep learning methods are extensively applied to various fields of science and engineering such as speech recognition, image classifications, and learning methods in language processing. Similarly, traditional data processing techniques have several limitations of processing large amount of data. In addition, Big Data analytics requires new and sophisticated algorithms based on machine and deep learning techniques to process data in real-time with high accuracy and efficiency. However, recently, research incorporated various deep learning techniques with hybrid learning and training mechanisms of processing data with high speed. Most of these techniques are specific to scenarios and based on vector space thus, shows poor performance in generic scenarios and learning features in big data. In addition, one of the reason of such failure is high involvement of humans to design sophisticated and optimized algorithms based on machine and deep learning techniques. In this article, we bring forward an approach of comparing various deep learning techniques for processing huge amount of data with different number of neurons and hidden layers. The comparative study shows that deep learning techniques can be built by introducing a number of methods in combination with supervised and unsupervised training techniques.|https://doi.org/10.1016/j.compeleceng.2017.12.009|https://www.sciencedirect.com/science/article/pii/S0045790617315835||||2019|Deep learning in big data Analytics: A comparative study|Bilal Jan and Haleem Farman and Murad Khan and Muhammad Imran and Ihtesham Ul Islam and Awais Ahmad and Shaukat Ali and Gwanggil Jeon|article|JAN2019275|||Computers & Electrical Engineering|00457906||75|||||18159|0,630|Q1|64|298|1040|7464|4626|979|4,79|25,05|United Kingdom|Western Europe|1973-1984, 1986-2020|Computer Science (miscellaneous) (Q1); Control and Systems Engineering (Q2); Electrical and Electronic Engineering (Q2)||||1216111590|1285201041
||Big Data, Data Quality, Data Security, Trade-off between Quality, Security||916-922||The essence of an information system lies in the data; if it is not of good quality or not sufficiently protected, the consequences will undoubtedly be harmful. Quality and Security are two essential aspects that add value to data and their implementation has become a real need and must be adopted before any data exploitation. Due to the high volume of data, their diversity and their rapid generation, effective implementation of such systems requires well thought out mechanisms and strategies. This paper provides an overview of Data Quality and Data Security in a Big Data context. We want through this paper to highlight the conflicts that may exist during the implementation of data security and data quality management systems. Such a conflict makes the complexity greater and requires new adapted solutions.|https://doi.org/10.1016/j.procs.2019.04.127|https://www.sciencedirect.com/science/article/pii/S1877050919305915||||2019|Big Data: Trade-off between Data Quality and Data Security|M. TALHA and A. ABOU {EL KALAM} and N. ELMARZOUQI|article|TALHA2019916|||Procedia Computer Science|18770509||151||The 10th International Conference on Ambient Systems, Networks and Technologies (ANT 2019) / The 2nd International Conference on Emerging Data and Industry 4.0 (EDI40 2019) / Affiliated Workshops|||19700182801|0,334|-|76|1907|6483|38511|13722|6359|2,09|20,19|Netherlands|Western Europe|2010-2020|Computer Science (miscellaneous)||||1216318057|2108686752
||The General Data Protection Regulation (GDPR), Big Data analytics, Privacy, Security||102896||The implementation of the GDPR that aims at protecting European citizens’ privacy is still a real challenge. In particular, in Big Data systems where data are voluminous and heterogeneous, it is hard to track data evolution through its complex life cycle ranging from collection, ingestion, storage and analytics. In this context, from 2016 to 2021 research has been conducted and several security tools designed. However, they are either specific to particular applications or address partially the regulation articles. To identify the covered parts, the missed ones and the necessary metrics for comparing different works, we propose a framework for GDPR compliance. The framework identifies the main components for the regulation implementation by mapping requirements aligned with GDPR’s provisions to IT design requirements. Based on this framework, we compare the main GDPR solutions in the Big Data domain and we propose a guideline for GDPR verification and implementation in Big Data systems.|https://doi.org/10.1016/j.jisa.2021.102896|https://www.sciencedirect.com/science/article/pii/S221421262100123X||||2021|Guidelines for GDPR compliance in Big Data systems|Mouna Rhahla and Sahar Allegue and Takoua Abdellatif|article|RHAHLA2021102896|||Journal of Information Security and Applications|22142126||61|||||21100332403|0,610|Q2|40|183|297|8559|1526|292|5,43|46,77|United Kingdom|Western Europe|2013-2020|Computer Networks and Communications (Q2); Safety, Risk, Reliability and Quality (Q2); Software (Q2)|1,526|3.872|0.00209|1216660391|68968737
||Data integrity;Data models;Big Data;Biological system modeling;Ecosystems;Standards||1-6|2020 IEEE International Conference on Communications Workshops (ICC Workshops)|Internet of Things devices and data sources areseeing increased use in various application areas. The pro-liferation of cheaper sensor hardware has allowed for widerscale data collection deployments. With increased numbers ofdeployed sensors and the use of heterogeneous sensor typesthere is increased scope for collecting erroneous, inaccurate orinconsistent data. This in turn may lead to inaccurate modelsbuilt from this data. It is important to evaluate this data asit is collected to determine its validity. This paper presents ananalysis of data quality as it is represented in Internet of Things(IoT) systems and some of the limitations of this representation. The paper discusses the use of trust as a heuristic to drive dataquality measurements. Trust is a well-established metric that hasbeen used to determine the validity of a piece or source of datain crowd sourced or other unreliable data collection techniques. The analysis extends to detail an appropriate framework forrepresenting data quality effectively within the big data modeland why a trust backed framework is important especially inheterogeneously sourced IoT data streams.|10.1109/ICCWorkshops49005.2020.9145071|||||2020|Data Quality and Trust : A Perception from Shared Data in IoT|Byabazaire, John and O'Hare, Gregory and Delaney, Declan|inproceedings|9145071||June||24749133|||||||||||||||||||||||||1217991613|351935640
||Cloud computing, Service orientation, Service science, Data-as-a-service, Information-as-a-service, Analytics-as-a-service, Big data||412-421||Using service-oriented decision support systems (DSS in cloud) is one of the major trends for many organizations in hopes of becoming more agile. In this paper, after defining a list of requirements for service-oriented DSS, we propose a conceptual framework for DSS in cloud, and discus about research directions. A unique contribution of this paper is its perspective on how to servitize the product oriented DSS environment, and demonstrate the opportunities and challenges of engineering service oriented DSS in cloud. When we define data, information and analytics as services, we see that traditional measurement mechanisms, which are mainly time and cost driven, do not work well. Organizations need to consider value of service level and quality in addition to the cost and duration of delivered services. DSS in CLOUD enables scale, scope and speed economies. This article contributes new knowledge in service science by tying the information technology strategy perspectives to the database and design science perspectives for a broader audience.|https://doi.org/10.1016/j.dss.2012.05.048|https://www.sciencedirect.com/science/article/pii/S0167923612001595||||2013|Leveraging the capabilities of service-oriented decision support systems: Putting analytics and big data in cloud|Haluk Demirkan and Dursun Delen|article|DEMIRKAN2013412|||Decision Support Systems|01679236|1|55|||||21933|1,564|Q1|151|115|342|7152|2672|338|7,04|62,19|Netherlands|Western Europe|1985-2020|Arts and Humanities (miscellaneous) (Q1); Developmental and Educational Psychology (Q1); Information Systems (Q1); Information Systems and Management (Q1); Management Information Systems (Q1)|13,580|5.795|0.0081|1218983015|1234879127
||Data integrity;Data models;Big Data;Biological system modeling;Measurement;Standards;Internet of Things;Data Quality;Internet of Things (IoT);Trust;Big Data Model;Machine learning||1-9|2020 29th International Conference on Computer Communications and Networks (ICCCN)|Recent developments in Internet of Things have heightened the need for data sharing across application domains to foster innovation. As most of these IoT deployments are based on heterogeneous sensor types, there is increased scope for sharing erroneous, inaccurate or inconsistent data. This in turn may lead to inaccurate models built from this data. It is important to evaluate this data as it is collected to establish its quality. This paper presents an analysis of data quality as it is represented in Internet of Things (IoT) systems and some of the limitations of this representation. The paper then introduces the use of trust as a heuristic to drive data quality measurements. Trust is a well-established metric that has been used to determine the validity of a piece or source of data in crowd sourced or other unreliable data collection techniques. The analysis extends to detail an appropriate framework for representing data quality within the big data model. To demonstrate the application of a trust backed framework, we used data collected from a IoT deployment of sensors to measure air quality in which a low cost sensor was co-located with a gold reference sensor. Using data streams modeled based on a dataset from an IoT deployment, our initial results show that the framework's trust score are consistent with the accuracy measure of the machine learning models.|10.1109/ICCCN49398.2020.9209633|||||2020|Using Trust as a Measure to Derive Data Quality in Data Shared IoT Deployments|Byabazaire, John and O’Hare, Gregory and Delaney, Declan|inproceedings|9209633||Aug||26379430|||||||||||||||||||||||||1219739012|947309988
ICSEW'20|Seoul, Republic of Korea|Actionable emotion, Case study, Context information, Implicit user feedback, Physiological data|7|563–569|Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops|Ensuring the quality of user experience is very important for increasing the acceptance likelihood of software applications, which can be affected by several contextual factors that continuously change over time (e.g., emotional state of end-user). Due to these changes in the context, software continually needs to adapt for delivering software services that can satisfy user needs. However, to achieve this adaptation, it is important to gather and understand the user feedback. In this paper, we mainly investigate whether physiological data can be considered and used as a form of implicit user feedback. To this end, we conducted a case study involving a tourist traveling abroad, who used a wearable device for monitoring his physiological data, and a smartphone with a mobile app for reminding him to take his medication on time during four days. Through the case study, we were able to identify some factors and activities as emotional triggers, which were used for understanding the user context. Our results highlight the importance of having a context analyzer, which can help the system to determine whether the detected stress could be considered as actionable and consequently as implicit user feedback.|10.1145/3387940.3391466|https://doi.org/10.1145/3387940.3391466|New York, NY, USA|Association for Computing Machinery|9781450379632|2020|Understanding Implicit User Feedback from Multisensorial and Physiological Data: A Case Study|Suni-Lopez, Franci and Condori-Fernandez, Nelly and Catala, Alejandro|inproceedings|10.1145/3387940.3391466|||||||||||||||||||||||||||||1222976180|42
ICEGOV2019|Melbourne, VIC, Australia|Information and communication technology (ICT), Transformative governance, E-governance|11|65–75|Proceedings of the 12th International Conference on Theory and Practice of Electronic Governance|This article first introduce a new government initiative emerging after the US presidential election in 2008. Comparing to the more descriptive definitions of e-government, supporters of these new government initiatives emphasize the transformative and normative aspect of the newest generation of Information and Communication Technology (ICTs). They argue that the new initiative redefines how government should operate and transform state-citizen relationships. To understand the core of this initiative and whether it offers new opportunities to solve public problems, we collected and analyzed research papers published in the e-governance area between 2008 and 2017. Our analysis demonstrates that the use of new generation of ICTs has promoted the government information infrastructure. In other words, the application of new ICTs enables the government to accumulate and use a large amount of data, so that the government makes better decisions. The advancement of open data, the wide use of social media, and the potential of data analytics have also generated pressure to address challenging questions and issues in e-democracy. However, the analysis leads us to deliberate on whether the use of new generation of ICTs worldwide have actually achieved their goal. In the conclusion, we present challenges to be addressed before new innovative ICTs realize their potential towards better public governance.|10.1145/3326365.3326374|https://doi.org/10.1145/3326365.3326374|New York, NY, USA|Association for Computing Machinery|9781450366441|2019|What is the Role of New Generation of ICTs in Transforming Government Operation and Redefining State-Citizen Relationship in the Last Decade?|Liu, Shuhua Monica and Pan, Liting and Lei, Yupei|inproceedings|10.1145/3326365.3326374|||||||||||||||||||||||||||||1223417542|42
||Data intelligence, Systems implementation, Data analytics, Success factors, Public sector, AHP||121180||This study aims to fill a gap in the literature by identifying, defining, and evaluating the critical success factors that impact the implementation of data intelligence in the public sector. Fourteen factors were identified, and then divided into three categories: organization, process, and technology. We used the analytical hierarchy process, a quantitative method of decision-making, to evaluate the importance of the factors presented in the study using data collected from nine experts. The results showed that technology, as a category, is the most important. The analysis also indicated that project management, information systems & data, and data quality are the most important factors among all fourteen critical success factors. We discuss the implications of the analysis for practitioners and researchers in the paper.|https://doi.org/10.1016/j.techfore.2021.121180|https://www.sciencedirect.com/science/article/pii/S0040162521006132||||2021|Evaluating the critical success factors of data intelligence implementation in the public sector using analytical hierarchy process|Mohammad I. Merhi|article|MERHI2021121180|||Technological Forecasting and Social Change|00401625||173|||||14704|2,226|Q1|117|448|1099|35581|10127|1061|9,01|79,42|United States|Northern America|1970-2020|Applied Psychology (Q1); Business and International Management (Q1); Management of Technology and Innovation (Q1)|21,116|8.593|0.02416|1223488723|1949868303
CIKM '15|Melbourne, Australia|problem scale reduction, recursive method, consistency assurance, truth discovery|10|503–512|Proceedings of the 24th ACM International on Conference on Information and Knowledge Management|Many real-world applications rely on multiple data sources to provide information on their interested items. Due to the noises and uncertainty in data, given a specific item, the information from different sources may conflict. To make reliable decisions based on these data, it is important to identify the trustworthy information by resolving these conflicts, i.e., the truth discovery problem. Current solutions to this problem detect the veracity of each value jointly with the reliability of each source for each data item. In this way, the efficiency of truth discovery is strictly confined by the problem scale, which in turn limits truth discovery algorithms from being applicable on a large scale. To address this issue, we propose an approximate truth discovery approach, which divides sources and values into groups according to a user-specified approximation criterion. The groups are then used for efficient inter-value influence computation to improve the accuracy. Our approach is applicable to most existing truth discovery algorithms. Experiments on real-world datasets show that our approach improves the efficiency compared to existing algorithms while achieving similar or even better accuracy. The scalability is further demonstrated by experiments on large synthetic datasets.|10.1145/2806416.2806444|https://doi.org/10.1145/2806416.2806444|New York, NY, USA|Association for Computing Machinery|9781450337946|2015|Approximate Truth Discovery via Problem Scale Reduction|Wang, Xianzhi and Sheng, Quan Z. and Fang, Xiu Susie and Li, Xue and Xu, Xiaofei and Yao, Lina|inproceedings|10.1145/2806416.2806444|||||||||||||||||||||||||||||1223680240|42
CSAE '18|Hohhot, China|data quality, Data governance, governance framework|5||Proceedings of the 2nd International Conference on Computer Science and Application Engineering|With1 the further development of information technology, data has become one of the core resources of enterprises. In the current era of big data, data governance has gradually become an important means for enterprises to make intelligent decisions, helping enterprises to occupy a favorable position in a highly competitive market environment. This paper briefly describes the current situation of enterprise big data governance, and analyzes the problems of current data governance from the perspectives of enterprise management and data itself. To deal with these problems, this paper proposes several suggestions and strategies from the perspectives of organization and management system construction, construction of a data standard system, improve the level of data quality management, data technology surpport, etc. Fully combining the advanced theories and methods of domestic data governance, this paper designs a data governance framework from the perspective of data application and innovation. The framework includes supervision and control modules, data governance staff organization modules, application and service modules, and data processing and integration modules. And the framework is applied to the data governance of electric power enterprises, which has important reference significance and value for enterprises to carry out data governance.|10.1145/3207677.3278000|https://doi.org/10.1145/3207677.3278000|New York, NY, USA|Association for Computing Machinery|9781450365123|2018|Research and Application of Enterprise Big Data Governance|Ke, Changwen and Wang, Kuisheng|inproceedings|10.1145/3207677.3278000|29||||||||||||||||||||||||||||1224177695|42
||Web-based modelling, Big Data, Web services, OGC standards||185-198||Recent evolutions in computing science and web technology provide the environmental community with continuously expanding resources for data collection and analysis that pose unprecedented challenges to the design of analysis methods, workflows, and interaction with data sets. In the light of the recent UK Research Council funded Environmental Virtual Observatory pilot project, this paper gives an overview of currently available implementations related to web-based technologies for processing large and heterogeneous datasets and discuss their relevance within the context of environmental data processing, simulation and prediction. We found that, the processing of the simple datasets used in the pilot proved to be relatively straightforward using a combination of R, RPy2, PyWPS and PostgreSQL. However, the use of NoSQL databases and more versatile frameworks such as OGC standard based implementations may provide a wider and more flexible set of features that particularly facilitate working with larger volumes and more heterogeneous data sources.|https://doi.org/10.1016/j.envsoft.2014.10.007|https://www.sciencedirect.com/science/article/pii/S1364815214002965||||2015|Web technologies for environmental Big Data|Claudia Vitolo and Yehia Elkhatib and Dominik Reusser and Christopher J.A. Macleod and Wouter Buytaert|article|VITOLO2015185|||Environmental Modelling & Software|13648152||63|||||23295|1,828|Q1|136|223|717|14218|4373|711|5,47|63,76|Netherlands|Western Europe|1997-2020|Ecological Modeling (Q1); Environmental Engineering (Q1); Software (Q1)||||1224219171|665084965
||Big data, Comparative effectiveness, Orthopedics, Total joint arthroplasty, Administrative database, Clinical database||725-732|||https://doi.org/10.1016/j.ocl.2016.05.009|https://www.sciencedirect.com/science/article/pii/S0030589816300554||||2016|Big Data, Big Problems: Incorporating Mission, Values, and Culture in Provider Affiliations|Steven H. Shaha and Zain Sayeed and Afshin A. Anoushiravani and Mouhanad M. El-Othmani and Khaled J. Saleh|article|SHAHA2016725|||Orthopedic Clinics of North America|00305898|4|47||Sports-Related Injuries|||12469|1,177|Q1|86|61|186|2288|437|153|2,01|37,51|United Kingdom|Western Europe|1970-2020|Orthopedics and Sports Medicine (Q1)|3,693|2.472|0.00273|1227848908|916843836
|||21|409–429|Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice|||https://doi.org/10.1145/3447404.3447428|New York, NY, USA|Association for Computing Machinery|9781450390293|2021|Driver Cognitive Load Classification Based on Physiological Data—Case Study 7|He, Dengbo and Risteska, Martina and Donmez, Birsen and Chen, Kaiyang|inbook|10.1145/3447404.3447428|||||||||1||||||||||||||||||||1229629284|42
CompEd '19|Chengdu,Sichuan, China|computing curriculum, computing for data science, data science|2|192–193|Proceedings of the ACM Conference on Global Computing Education|This panel brings the workings and results of the ACM Education Council Task Force on Data Science Education. The task force has gathered information on existing programs and has reviewed documents such as the result of the National Academies deliberations on data science. The task force is charged with exploring the role of computer science in data science education, understanding that data science is an inherently interdisciplinary field and not exclusively a computer science field. The panel will present a summary of the task force findings by two members of the task force and perspectives from leaders in data-intensive applications from China. The goal of the panel is to present the findings, but also to obtain perspectives from the attendees in order to enrich the task force's work.|10.1145/3300115.3312508|https://doi.org/10.1145/3300115.3312508|New York, NY, USA|Association for Computing Machinery|9781450362597|2019|Panel: The Computing in Data Science|Cassel, Lillian and Hongzhi, Wang|inproceedings|10.1145/3300115.3312508|||||||||||||||||||||||||||||1229659871|42
||Big data, Predictive analytics, Retailing, Pricing||79-95||The paper examines the opportunities in and possibilities arising from big data in retailing, particularly along five major data dimensions—data pertaining to customers, products, time, (geo-spatial) location and channel. Much of the increase in data quality and application possibilities comes from a mix of new data sources, a smart application of statistical tools and domain knowledge combined with theoretical insights. The importance of theory in guiding any systematic search for answers to retailing questions, as well as for streamlining analysis remains undiminished, even as the role of big data and predictive analytics in retailing is set to rise in importance, aided by newer sources of data and large-scale correlational techniques. The Statistical issues discussed include a particular focus on the relevance and uses of Bayesian analysis techniques (data borrowing, updating, augmentation and hierarchical modeling), predictive analytics using big data and a field experiment, all in a retailing context. Finally, the ethical and privacy issues that may arise from the use of big data in retailing are also highlighted.|https://doi.org/10.1016/j.jretai.2016.12.004|https://www.sciencedirect.com/science/article/pii/S0022435916300835||||2017|The Role of Big Data and Predictive Analytics in Retailing|Eric T. Bradlow and Manish Gangwar and Praveen Kopalle and Sudhir Voleti|article|BRADLOW201779|||Journal of Retailing|00224359|1|93||The Future of Retailing|||22990|3,184|Q1|136|66|98|4311|839|87|4,51|65,32|United Kingdom|Western Europe|1993-2020|Marketing (Q1)|10,594|5.245|0.00458|1230107796|1572306299
||Measurement;Time series analysis;Feature extraction;Phylogeny;Mutual information;Big Data;Data mining;Similarity measures;Robinson Foulds;Variation of Information;Engineering data;Multivariate numerical data;Dimensional inconsistency||1528-1535|2018 IEEE 20th International Conference on High Performance Computing and Communications; IEEE 16th International Conference on Smart City; IEEE 4th International Conference on Data Science and Systems (HPCC/SmartCity/DSS)|When developing multivariate data classification and clustering methodologies for data mining, it is clear that most literature contributions only really consider data that contain consistently the same attributes. There are however many cases in current big data analytics applications where for same topic and even same source data sets there are differing attributes being measured, for a multitude of reasons (whether the specific design of an experiment or poor data quality and consistency). We define this class of data a dimensionally inconsistent multivariate data, a topic that can be considered a subclass of the Big Data Variety research. This paper explores some classification methodologies commonly used in multivariate classification and clustering tasks and considers how these traditional methodologies could be adapted to compare dimensionally inconsistent data sets. The study focuses on adapting two similarity measures: Robinson-Foulds tree distance metrics and Variation of Information; for comparing clustering of hierarchical cluster algorithms (such clusters are derived from the raw multivariate data). The results from experiments on engineering data highlight that adapting pairwise measures to exclude non-common attributes from the traditional distance metrics may not be the best method of classification. We suggest that more specialised metrics of similarity are required to address challenges presented by dimensionally inconsistent multivariate data, with specific applications for big engineering data analytics.|10.1109/HPCC/SmartCity/DSS.2018.00251|||||2018|Exploring Methods for Comparing Similarity of Dimensionally Inconsistent Multivariate Numerical Data|Micic, Natasha and Neagu, Daniel and Torgunov, Denis and Campean, Felician|inproceedings|8622989||June|||||||||||||||||||||||||||1230374227|42
SBD '16|San Francisco, California|CiteSeerX, citation graph, scholarly big data, digital library search engine, semantic entity extraction|6||Proceedings of the International Workshop on Semantic Big Data|Scholarly big data is, for many, an important instance of Big Data. Digital library search engines have been built to acquire, extract, and ingest large volumes of scholarly papers. This paper provides an overview of the scholarly big data released by CiteSeerX, as of the end of 2015, and discusses various aspects such as how the data is acquired, its size, general quality, data management, and accessibility. Preliminary results on extracting semantic entities from body text of scholarly papers with Wikifier show biases towards general terms appearing in Wikipedia and against domain specific terms. We argue that the latter will play a more important role in extracting important facts from scholarly papers.|10.1145/2928294.2928306|https://doi.org/10.1145/2928294.2928306|New York, NY, USA|Association for Computing Machinery|9781450342995|2016|CiteSeerX Data: Semanticizing Scholarly Papers|Wu, Jian and Liang, Chen and Yang, Huaiyu and Giles, C. Lee|inproceedings|10.1145/2928294.2928306|2||||||||||||||||||||||||||||1230881815|42
||Blockchain, Life cycle assessment, Supply chain sustainability, Environmental sustainability, Operational excellence||104512||Life cycle assessment (LCA) is widely used for assessing the environmental impacts of a product or service. Collecting reliable data is a major challenge in LCA due to the complexities involved in the tracking and quantifying inputs and outputs at multiple supply chain stages. Blockchain technology offers an ideal solution to overcome the challenge in sustainable supply chain management. Its use in combination with internet-of-things (IoT) and big data analytics and visualization can help organizations achieve operational excellence in conducting LCA for improving supply chain sustainability. This research develops a framework to guide the implementation of Blockchain-based LCA. It proposes a system architecture that integrates the use of Blockchain, IoT, and big data analytics and visualization. The proposed implementation framework and system architecture were validated by practitioners who were experienced with Blockchain applications. The research also analyzes system implementation costs and discusses potential issues and solutions, as well as managerial and policy implications.|https://doi.org/10.1016/j.resconrec.2019.104512|https://www.sciencedirect.com/science/article/pii/S0921344919304185||||2020|Blockchain-based life cycle assessment: An implementation framework and system architecture|Abraham Zhang and Ray Y Zhong and Muhammad Farooque and Kai Kang and V G Venkatesh|article|ZHANG2020104512|||Resources, Conservation and Recycling|09213449||152|||||||||||||||||||||||1239482694|905300119
||Building life span, Life cycle cost, Life cycle assessment, Big data, Machine learning, Deep neural network||108267||Life cycle assessment (LCA) and life cycle cost (LCC) are two primary methods used to assess the environmental and economic feasibility of building construction. An estimation of the building's life span is essential to carrying out these methods. However, given the diverse factors that affect the building's life span, it was estimated typically based on its main structural type. However, different buildings have different life spans. Simply assuming that all buildings with the same structural type follow an identical life span can cause serious estimation errors. In this study, we collected 1,812,700 records describing buildings built and demolished in South Korea, analysed the actual life span of each building, and developed a building life-span prediction model using deep-learning and traditional machine learning. The prediction models examined in this study produced root mean square errors of 3.72–4.6 and the coefficients of determination of 0.932–0.955. Among those models, a deep-learning based prediction model was found the most powerful. As anticipated, the conventional method of determining a building's life expectancy using a discrete set of specific factors and associated assumptions of life span did not yield realistic results. This study demonstrates that an application of deep learning to the LCA and LCC of a building is a promising direction, effectively guiding business planning and critical decision making throughout the construction process.|https://doi.org/10.1016/j.buildenv.2021.108267|https://www.sciencedirect.com/science/article/pii/S0360132321006673||||2021|Building life-span prediction for life cycle assessment and life cycle cost using machine learning: A big data approach|Sukwon Ji and Bumho Lee and Mun Yong Yi|article|JI2021108267|||Building and Environment|03601323||205|||||26874|1,736|Q1|154|754|1692|43938|12000|1687|6,90|58,27|United Kingdom|Western Europe|1976-2020|Building and Construction (Q1); Civil and Structural Engineering (Q1); Environmental Engineering (Q1); Geography, Planning and Development (Q1)|38,699|6.456|0.02925|1239747490|723885742
||redundancy elimination, quality of service, Mobile crowdsensing, cost-effectiveness, Internet of Things|26|||Mobile crowdsensing serves as a critical building block for emerging Internet of Things (IoT) applications. However, the sensing devices continuously generate a large amount of data, which consumes much resources (e.g., bandwidth, energy, and storage) and may sacrifice the Quality-of-Service (QoS) of applications. Prior work has demonstrated that there is significant redundancy in the content of the sensed data. By judiciously reducing redundant data, data size and load can be significantly reduced, thereby reducing resource cost and facilitating the timely delivery of unique, probably critical information and enhancing QoS. This article presents a survey of existing works on mobile crowdsensing strategies with an emphasis on reducing resource cost and achieving high QoS. We start by introducing the motivation for this survey and present the necessary background of crowdsensing and IoT. We then present various mobile crowdsensing strategies and discuss their strengths and limitations. Finally, we discuss future research directions for mobile crowdsensing for IoT. The survey addresses a broad range of techniques, methods, models, systems, and applications related to mobile crowdsensing and IoT. Our goal is not only to analyze and compare the strategies proposed in prior works, but also to discuss their applicability toward the IoT and provide guidance on future research directions for mobile crowdsensing.|10.1145/3185504|https://doi.org/10.1145/3185504|New York, NY, USA|Association for Computing Machinery||2018|A Survey of Mobile Crowdsensing Techniques: A Critical Component for The Internet of Things|Liu, Jinwei and Shen, Haiying and Narman, Husnu S. and Chung, Wingyan and Lin, Zongfang|article|10.1145/3185504|18|jun|ACM Trans. Cyber-Phys. Syst.|2378962X|3|2|July 2018||||||||||||||||||||||1240271067|1535435511
ICISCAE 2021|Dalian, China||4|960–963|2021 4th International Conference on Information Systems and Computer Aided Education|This paper constructs the logic structure of big data platform of physical teaching information system in Colleges and universities through Hadoop distributed system, which ensures the realization of management and monitoring function, data collection function, data storage and query function, data calculation function, security and management function. The security design is carried out from access control, data authorization and management, Hadoop security configuration to deal with the possible security problems of big data platform.|10.1145/3482632.3483061|https://doi.org/10.1145/3482632.3483061|New York, NY, USA|Association for Computing Machinery|9781450390255|2021|The Architecture and Security Design of Big Data Platform of the Physical Teaching Information System in Colleges and Universities|Han, Caibao|inproceedings|10.1145/3482632.3483061|||||||||||||||||||||||||||||1240645356|42
||Metadata;Business;Big Data applications;Organizations;Interoperability;Knowledge based systems;Standards organizations||57-96|Data Lakes|The notion of metadata has been used in information management long before the emergence of computer science, in fields related to documentation or cataloguing. The National Information Security Organization classification of metadata encompasses the following four types: descriptive metadata, structural metadata, administrative metadata, and markup languages. A metadata schema consists of a labeling, marking or encoding system, used for storing information about the way data is organized and structured. Business metadata define the information content that the data provide in a business context. Business metadata are an important aspect in any successful information governance program. Navigational integration metadata describe the data linkage and data movement within the environments. Operational metadata describes the data integration applications and supporting job runs. Primary sources of operational metadata include data integration job logs and data quality checks. In a data lake ecosystem, as well as other information systems, metadata are varied and pervasive.|10.1002/9781119720430.ch4|https://ieeexplore.ieee.org/document/9822310||Wiley|9781119720423|2020|Metadata in Data Lake Ecosystems|Zgolli, Asma and Collet, Christine and Madera, Cédrine|inbook|9822310|||||||||||||||||||||||||||||1241003313|42
||Big data, Remote sensing, Machine learning, Wildfire prediction, Data mining, Artificial intelligence||130-146||Wildfires, whether natural or caused by humans, are considered among the most dangerous and devastating disasters around the world. Their complexity comes from the fact that they are hard to predict, hard to extinguish and cause enormous financial losses. To address this issue, many research efforts have been conducted in order to monitor, predict and prevent wildfires using several Artificial Intelligence techniques and strategies such as Big Data, Machine Learning, and Remote Sensing. The latter offers a rich source of satellite images, from which we can retrieve a huge amount of data that can be used to monitor wildfires. The method used in this paper combines Big Data, Remote Sensing and Data Mining algorithms (Artificial Neural Network and SVM) to process data collected from satellite images over large areas and extract insights from them to predict the occurrence of wildfires and avoid such disasters. For this reason, we implemented a methodology that serves this purpose by building a dataset based on Remote Sensing data related to the state of the crops (NDVI), meteorological conditions (LST), as well as the fire indicator “Thermal Anomalies”, these data, were acquired from “MODIS” (Moderate Resolution Imaging Spectroradiometer), a key instrument aboard the Terra and Aqua satellites. This dataset is available on GitHub via this link (https://github.com/ouladsayadyounes/Wildfires). Experiments were made using the big data platform “Databricks”. Experimental results gave high prediction accuracy (98.32%). These results were assessed using several validation strategies (e.g., classification metrics, cross-validation, and regularization) as well as a comparison with some wildfire early warning systems.|https://doi.org/10.1016/j.firesaf.2019.01.006|https://www.sciencedirect.com/science/article/pii/S0379711218303941||||2019|Predictive modeling of wildfires: A new dataset and machine learning approach|Younes Oulad Sayad and Hajar Mousannif and Hassan {Al Moatassime}|article|SAYAD2019130|||Fire Safety Journal|03797112||104|||||28425|0,958|Q1|78|260|442|8386|1566|437|3,13|32,25|United Kingdom|Western Europe|1977-2020|Chemistry (miscellaneous) (Q1); Materials Science (miscellaneous) (Q1); Physics and Astronomy (miscellaneous) (Q1); Safety, Risk, Reliability and Quality (Q1)|5,861|2.764|0.00398|1241158206|1399047854
|||14|1959–1972||Mobile crowdsensing has found a variety of applications e.g., spectrum sensing, environmental monitoring by leveraging the “wisdom” of a potentially large crowd of mobile users. An important metric of a crowdsensing task is data accuracy, which relies on the data quality of the participating users’ data e.g., users’ received SNRs for measuring a transmitter’s transmit signal strength. However, the quality of a user can be its private information which, e.g., may depend on the user’s location that it can manipulate to its own advantage, which can mislead the crowdsensing requester about the knowledge of the data’s accuracy. This issue is exacerbated by the fact that the user can also manipulate its effort made in the crowdsensing task, which is a hidden action that could result in the requester having incorrect knowledge of the data’s accuracy. In this paper, we devise truthful crowdsensing mechanisms for Quality and Effort Elicitation QEE, which incentivize strategic users to truthfully reveal their private quality and truthfully make efforts as desired by the requester. The QEE mechanisms achieve the truthful design by overcoming the intricate dependency of a user’s data on its private quality and hidden effort. Under the QEE mechanisms, we show that the crowdsensing requester’s optimal RO effort assignment assigns effort only to the best user that has the smallest “virtual valuation”, which depends on the user’s quality and the quality’s distribution. We also show that, as the number of users increases, the performance gap between the RO effort assignment and the socially optimal effort assignment decreases, and converges to 0 asymptotically. We further discuss some extensions of the QEE mechanisms. Simulation results demonstrate the truthfulness of the QEE mechanisms and the system efficiency of the RO effort assignment.|10.1109/TNET.2019.2934026|https://doi.org/10.1109/TNET.2019.2934026||IEEE Press||2019|Truthful Mobile Crowdsensing for Strategic Users With Private Data Quality|Gong, Xiaowen and Shroff, Ness B.|article|10.1109/TNET.2019.2934026||oct|IEEE/ACM Trans. Netw.|10636692|5|27|October 2019||||27237|1,022|Q1|174|187|653|6994|3573|652|5,40|37,40|United States|Northern America|1993-2020|Computer Networks and Communications (Q1); Computer Science Applications (Q1); Electrical and Electronic Engineering (Q1); Software (Q1)||||1244704899|1453185914
BDCAT '19|Auckland, New Zealand|mapreduce framework, p2p botnet, big data, data reduction, netflow, data compacting, data compression, botnet|4|81–84|Proceedings of the 6th IEEE/ACM International Conference on Big Data Computing, Applications and Technologies|Big data analytics helps us to find potentially valuable knowledge, but as the size of the dataset increases, the computing cost also grows exponentially. In our previous work, BotCluster, we had designed a pre-processing filtering pipeline, including whitelist filter and flow loss-response rate (FLR) filter, for data reduction, which intended to wipe out irrelative noises and reduce the computing overhead. However, we still face a data redundancy phenomenon in which some of the same feature vectors repeatedly emerged. In this paper, we propose a data compacting approach aimed to reduce the input volume and keep enough representative feature vectors to fit DBSCAN's (Density-based spatial clustering of applications with noise) criteria. It purges the redundant vectors according to a purging threshold and keeps the primary representatives. Experimental results have shown that the average data reduction ratio is about 81.34%, while the precision has only slightly decreased by 1.6% on average, and the results still have 99.88% of IPs overlapped with the previous system.|10.1145/3365109.3368778|https://doi.org/10.1145/3365109.3368778|New York, NY, USA|Association for Computing Machinery|9781450370165|2019|A Data Compacting Technique to Reduce the NetFlow Size in Botnet Detection with BotCluster|Wang, Chun-Yu and Fuh, Shih-Hao and Lo, Ta-Chun and Cheng, Qi-Jun and Chen, Yu-Cheng and Cho, Feng-Min and Chang, Jyh-Biau and Shieh, Ce-Kuen|inproceedings|10.1145/3365109.3368778|||||||||||||||||||||||||||||1245108355|42
||Neural networks;Image recognition;Feature extraction;Forgery;Machine learning;convolutional neural network;handwriting;deep learning;signature authentication;signature verification;machine learning;image classifier||2644-2651|2017 IEEE International Conference on Big Data (Big Data)|Many studies have been conducted on Handwritten Signature Verification. Researchers have taken many different approaches to accurately identify valid signatures from skilled forgeries, which closely resemble the real signature. The purpose of this paper is to suggest a method for validating written signatures on bank checks. This model uses a convolutional neural network (CNN) to analyze pixels from a signature image to recognize abnormalities. We believe the feature extraction capabilities of a CNN can optimize processing time and feature analysis of signature verification. Unique characteristics from signatures can be accurately and rapidly analyzed with multiple layers of receptive fields and hidden layers. Our method was able to correctly detect the validity of the inputted signature approximately 83 percent of the time. We tested our method using the SIGCOMP 2011 dataset. The main contribution of this method is to detect and decrease fraud committed, especially in the banking industry. Future uses of signature verification could include legal documents and the justice system.|10.1109/BigData.2017.8258225|||||2017|Toward data quality analytics in signature verification using a convolutional neural network|Tayeb, Shahab and Pirouz, Matin and Cozzens, Brittany and Huang, Richard and Jay, Maxwell and Khembunjong, Kyle and Paliskara, Sahan and Zhan, Felix and Zhang, Mark and Zhan, Justin and Latifi, Shahram|inproceedings|8258225||Dec|||||||||||||||||||||||||||1246441163|42
||Data analysis;Automation;Data integrity;Optimization methods;Big Data;Solids;Task analysis||1644-1653|2020 IEEE International Conference on Big Data (Big Data)|"\"Data Quality (DQ) has been one of the key focuses as Data Analytics and Artificial Intelligence (AI) fields continue to grow. Yet, data quality analysis has mostly been a disjointed, ad-hoc, and cumbersome process in the overall data analysis workflow. There have been ongoing attempts to formalize this process, but the solutions that have come out are not universally applicable. Most of the proposed solutions try to address the problem of data quality from a limited perspective and suc-cessfully address only a subset of all challenges. These solutions fail to translate to other domains due to a lack of structure. In this paper, we present DQLearn, a toolkit for structured data quality learning. We start by presenting the core principle on which we build our library and introduce the four components that provide a solid base to address the needs of the data quality problem. Then, we showcase our automation structure - \"\"Workflows\"\", and the two optimization techniques equipped with it, that help the users to structure their learning problem very easily. Next, we discuss four important scenarios of the DQ Workflows in the overall life-cycle. Finally, we demonstrate the utility of the proposed toolkit with public datasets and show benchmark results from optimization experiments.\""|10.1109/BigData50022.2020.9378296|||||2020|DQLearn : A Toolkit for Structured Data Quality Learning|Shrivastava, Shrey and Patel, Dhaval and Zhou, Nianjun and Iyengar, Arun and Bhamidipaty, Anuradha|inproceedings|9378296||Dec|||||||||||||||||||||||||||1246506410|42
CHIWORK 2022|Durham, NH, USA|employers, ethics, employees, trust, neurotechnology, privacy, future of work, neuroethics|19||2022 Symposium on Human-Computer Interaction for Work|Advances in automation and autonomous systems means that the future of work will involve even more cognitive effort. For those in already cognitively demanding work, many of us aim to optimise our effort and productivity to achieve more in work, and ideally to rest outside of work. Neuroergonomics research is concerned with how neurotechnology will help improve work to be manageable and safe, often in e.g. safety critical work, operators experience high demands and mental workload. Meanwhile, Neuroethics is concerned with the largely unregulated future of this industry, involving technologies that are not technically medical devices, but will involve invasive forms of personal data. This work aims to explicate the privacy, trust, and ethical concerns that workers have about employers using neurotechnology to manage their work-forces. An online survey and themes drawn from interviews with factory and office workers are presented. We conclude by discussing these concerns and how they might affect the rapidly expanding neurotechnology industry.|10.1145/3533406.3533423|https://doi.org/10.1145/3533406.3533423|New York, NY, USA|Association for Computing Machinery|9781450396554|2022|Understanding the Ethical Concerns for Neurotechnology in the Future of Work|Martinez, Wendy and Benerradi, Johann and Midha, Serena and Maior, Horia A. and Wilson, Max L.|inproceedings|10.1145/3533406.3533423|17||||||||||||||||||||||||||||1246899047|42
||Scattering;Optical sensors;Optical imaging;Fish;Backscatter;deep-sea observation;vision data quality;automatic judging||4789-4791|2017 IEEE International Conference on Big Data (Big Data)|Deep-sea study by optical observation method is an interdisciplinary subject and faces plenty of difficulties. To find out the influences of vision data quality, characteristic of vision data for deep-sea observation is analyzed, and a deep-sea landing experiment has been implemented. Data quality analyzing based on real deep-sea vision data that collected in the in-situ observation platform is actualized. It is expected that the research on influence mechanism of deep-sea vision quality is beneficial to the detection of region of interest, the judging of animal existence, the classification of species, and the trajectories labeling. Further analyzing on unsupervised deep-sea vision data quality control is necessary.|10.1109/BigData.2017.8258543|||||2017|The influences of deep-sea vision data quality on observational analysis|Liu, Lixin and Chen, Jun|inproceedings|8258543||Dec|||||||||||||||||||||||||||1249724139|42
||Safety big data (SBD), Safety small data (SSD), Safety science, Big data application, Method, Principle, Prospect and challenge||60-71||It is clear that big data has numerous potential impacts in many fields. However, few papers discussed its applications in the field of safety science research. Additionally, there exist many problems that cannot be ignored when big data is applied to safety science, most outstanding of which is lack of universal supporting theory that guides how to apply big data to safety science research like methods, principles and approaches, etc. In other terms, it is not enough for big data to be viewed asa strong enabler for safety science applications mainly due to lack of universal and basic theory from the perspective of safety science. Considering the above analyzes, the two key objectives of this paper are: (1) to propose the connotation of safety big data (SBD) and its applying rules, methods and principles, and (2) to put forward some application prospects and challenges of big data to safety science research seen from theoretical research. First, by comparing SBD and traditional safety small data (SSD) from four aspects including theoretical research, typical research method, specific analysis method and processing mode, this paper puts forward the definition and connotation of SBD. Subsequently this paper further summarizes and extracts the application rules and methods of SBD. And then nine principles of SBD are explored and their relationship and application are addressed from the view of theory architecture and working framework in data processing flow. At last, this paper also discusses the potential applications and some hot issues of SBD. Overall, this paper will play an essential role in supporting the application of SBD. In addition, it will fill in the theory gaps in the field of SBD beyond traditional safety statistics, and further enriches the contents of safety science.|https://doi.org/10.1016/j.ssci.2017.08.012|https://www.sciencedirect.com/science/article/pii/S0925753517304320||||2018|Methodologies, principles and prospects of applying big data in safety science research|Qiumei Ouyang and Chao Wu and Lang Huang|article|OUYANG201860|||Safety Science|09257535||101|||||12332|1,178|Q1|111|451|1047|26276|5909|1020|5,49|58,26|Netherlands|Western Europe|1991-2020|Public Health, Environmental and Occupational Health (Q1); Safety Research (Q1); Safety, Risk, Reliability and Quality (Q1)|17,184|4.877|0.01488|1254612679|1544923827
||Data integrity;Instruments;Atmospheric measurements;Dictionaries;Big Data;Portals;scientific data workflows;data quality;provenance;atmospheric science||3260-3266|2019 IEEE International Conference on Big Data (Big Data)|Data quality assessment, management and improvement is an integral part of any big data intensive scientific research to ensure accurate, reliable, and reproducible scientific discoveries. The task of maintaining the quality of data, however, is non-trivial and poses a challenge for a program like the Department of Energy's Atmospheric Radiation Measurement (ARM) that collects data from hundreds of instruments across the world, and distributes thousands of streaming data products that are continuously produced in near-real-time for an archive 1.7 Petabyte in size and growing. In this paper, we present a computational data processing workflow to address the data quality issues via an easy and intuitive web-based portal that allows reporting of any quality issues for any site, facility or instruments at a granularity down to individual variables in the data files. This portal allows instrument specialists and scientists to provide corrective actions in the form of symbolic equations. A parallel processing framework applies the data improvement to a large volume of data in an efficient, parallel environment, while optimizing data transfer and file I/O operations; corrected files are then systematically versioned and archived. A provenance tracking module tracks and records any change made to the data during its entire life cycle which are communicated transparently to the scientific users. Developed in Python using open source technologies, this software architecture enables fast and efficient management and improvement of data in an operational data center environment.|10.1109/BigData47090.2019.9006358|||||2019|Provenance–aware workflow for data quality management and improvement for large continuous scientific data streams|Kumar, Jitendra and Crow, Michael C. and Devarakonda, Ranjeet and Giansiracusa, Michael and Guntupally, Kavya and Olatt, Joseph V. and Price, Zach and Shanafield, Harold A. and Singh, Alka|inproceedings|9006358||Dec|||||||||||||||||||||||||||1255052352|42
ICMVA 2022|Singapore, Singapore|Data mining, Big data, Data analysis, Data space, Data visualization|5|56–60|2022 the 5th International Conference on Machine Vision and Applications (ICMVA)|Abstract: The development of visualization technology and data mining technology provides powerful means and tools for the visual analysis of diversified data. In this era of information and data explosion, various information resources are very rich, but due to the large amount of data, the characteristics of the data are not so obvious. This requires us to study the corresponding methods and means to extract the characteristics of data. Data visualization technology has developed rapidly under this background. This paper introduces the basic theory of data visualization technology, and selects the GDP data of the world's major economies over the years as a visualization example.|10.1145/3523111.3523119|https://doi.org/10.1145/3523111.3523119|New York, NY, USA|Association for Computing Machinery|9781450395670|2022|Analysis and Example Implementation of Data Visualization Technology|Meng, Xianyu and Ma, Liangli and Zhou, Yingxue|inproceedings|10.1145/3523111.3523119|||||||||||||||||||||||||||||1256111509|42
|||12|120–131||This paper explores an analysis-aware data cleaning architecture for a large class of SPJ SQL queries. In particular, we propose QuERy, a novel framework for integrating entity resolution (ER) with query processing. The aim of QuERy is to correctly and efficiently answer complex queries issued on top of dirty data. The comprehensive empirical evaluation of the proposed solution demonstrates its significant advantage in terms of efficiency over the traditional techniques for the given problem settings.|10.14778/2850583.2850587|https://doi.org/10.14778/2850583.2850587||VLDB Endowment||2015|QuERy: A Framework for Integrating Entity Resolution with Query Processing|Altwaijry, Hotham and Mehrotra, Sharad and Kalashnikov, Dmitri V.|article|10.14778/2850583.2850587||nov|Proc. VLDB Endow.|21508097|3|9|November 2015||||21100199855|0,946|Q1|134|246|598|12401|3759|566|5,50|50,41|United States|Northern America|2008-2020|Computer Science (miscellaneous) (Q1)|7,198|2.047|0.00891|1257652857|1216159931
|||4|1314–1317||Ontology-based data access (OBDA) is a novel paradigm for accessing large data repositories through an ontology, that is a formal description of a domain of interest. Supporting the management of OBDA applications poses new challenges, as it requires to provide effective tools for (i) allowing both expert and non-expert users to analyze the OBDA specification, (ii) collaboratively documenting the ontology, (iii) exploiting OBDA services, such as query answering and automated reasoning over ontologies, e.g., to support data quality check, and (iv) tuning the OBDA application towards optimized performances. To fulfill these challenges, we have built a novel system, called MASTRO STUDIO, based on a tool for automated reasoning over ontologies, enhanced with a suite of tools and optimization facilities for managing OBDA applications. To show the effectiveness of MASTRO STUDIO, we demonstrate its usage in one OBDA application developed in collaboration with the Italian Ministry of Economy and Finance.|10.14778/2536274.2536304|https://doi.org/10.14778/2536274.2536304||VLDB Endowment||2013|Mastro Studio: Managing Ontology-Based Data Access Applications|Civili, Cristina and Console, Marco and De Giacomo, Giuseppe and Lembo, Domenico and Lenzerini, Maurizio and Lepore, Lorenzo and Mancini, Riccardo and Poggi, Antonella and Rosati, Riccardo and Ruzzi, Marco and Santarelli, Valerio and Savo, Domenico Fabio|article|10.14778/2536274.2536304||aug|Proc. VLDB Endow.|21508097|12|6|August 2013||||21100199855|0,946|Q1|134|246|598|12401|3759|566|5,50|50,41|United States|Northern America|2008-2020|Computer Science (miscellaneous) (Q1)|7,198|2.047|0.00891|1258455746|1216159931
||Big Data;Manufacturing;Industries;Sensors;Data integrity;Real-time systems;Cloud computing;Cyber-Physical System;Internet of Things;Industry 4.0;cloud computing;big data ecosystem;data quality||173-178|2019 IEEE International Conference on Industrial Cyber Physical Systems (ICPS)|The Industry Internet of Things (IIoT) and the Industry Cyber-Physical System (ICPS) for real industry are becoming vitally necessary in the smart manufacturing environment. Very large number of intelligent sensors are being available generating an exploding amount of data. Several issues come with the big data in real industry, including the a grand-scale connected network construction with the data security and access protocol issues, data quality with considerable noise when gathered from industrial factories, efficient data storage, smart interconnection with cloud services, and real-time analytics requirements. This paper proposes an integrated CPS based architecture for smart manufacturing and provides the deployment details, addressing all the potential problems in an appropriate way. It has been successfully implemented in a real industry environment, and won the Best Industry Application of IoT at the BigInsights Data & AI Innovation Awards.|10.1109/ICPHYS.2019.8780271|||||2019|Implementation of Industrial Cyber Physical System: Challenges and Solutions|Yu, Wenjin and Dillon, Tharam and Mostafa, Fahed and Rahayu, Wenny and Liu, Yuehua|inproceedings|8780271||May|||||||||||||||||||||||||||1263385974|42
||Big data, Cartography, Crisis, Crowdsourcing, Disaster, Emergency, Geospatial web, Relief, Spatial data, Web mapping||121-128|International Encyclopedia of Human Geography (Second Edition)|Humanitarian mapping refers to the production of spatial data and cartographic products to improve situational awareness and decision-making around humanitarian issues from acute events such as natural disasters and public health emergencies to longer term events such as refugee crises and political unrest. Mapping is a key part of the broader area of humanitarian information management, which has traditionally been undertaken by governments and international humanitarian organizations. As a core aspect of the field of digital humanitarianism, mapping activities are now widely undertaken by smaller organizations and networks of volunteers who produce spatial data and maps on the ground and remotely via the use of Web mapping and mobile phone technologies. Big data based on location and behavioral attributes produced online and through interaction with digital systems and networks can also be exploited to enhance information environments. Together, these new developments signal new possibilities for improved risk and crisis management, based on up-to-date high resolution spatial and temporal evidence. Research in human geography, geographic information science, and related disciplines focuses on tracing benefits such as increased speed and low costs, as well as the risks of relying on distributed volunteers and new sources of data of questionable accuracy and validity.|https://doi.org/10.1016/B978-0-08-102295-5.10559-1|https://www.sciencedirect.com/science/article/pii/B9780081022955105591|Oxford|Elsevier|978-0-08-102296-2|2020|Humanitarian Mapping|Jonathan Cinnamon|incollection|CINNAMON2020121|||||||||Second Edition|Audrey Kobayashi|||||||||||||||||||1266371189|42
|||3|51–53||The 2nd International Workshop on Business intelligencE and the WEB (BEWEB) was co-located with the EDBT/ICDT 2011 Joint Conference in Uppsala (Sweden) on March 25, 2011. BEWEB intends to be an international forum for researchers and practitioners to exchange ideas on how to leverage the huge amount of data that is available on the Web in BI applications and on how to apply Web engineering methods and techniques to the design of BI applications. This report summarizes the 2011 edition of BEWEB.|10.1145/2380776.2380789|https://doi.org/10.1145/2380776.2380789|New York, NY, USA|Association for Computing Machinery||2012|Report of the International Workshop on Business Intelligence and the Web: BEWEB 2011|Maz\'{o}n, Jose-Norberto and Garrig\'{o}s, Irene and Daniel, Florian and Castellanos, Malu|article|10.1145/2380776.2380789||oct|SIGMOD Rec.|01635808|3|41|September 2012||||13622|0,372|Q2|142|39|81|808|127|57|1,67|20,72|United States|Northern America|1969, 1973-1978, 1981-2020|Information Systems (Q2); Software (Q2)|1,819|0.775|7.5E-4|1268894785|962972343
||Standards;Uncertainty;Big Data;Metrology;Reliability;Measurement uncertainty;Biomedical measurement;Big data;data quality;data traceability;metrology;standard reference data;uncertainty||36294-36299||In the era of big data, the scientific and social demand for quality data is aggressive and urgent. This paper sheds light on the expanded role of metrology of verifying validated procedures of data production and developing adequate uncertainty evaluation methods to ensure the trustworthiness of data and information. In this regard, I explore the mechanism of the national standard reference data (SRD) program of Korea, which connects various scientific and social sectors to metrology by applying useful metrological concepts and methods to produce reliable data and convert such data into national standards. In particular, the changing interpretation of metrological key concepts, such as “measurement,” “traceability,” and “uncertainty,” will be explored and reconsidered from the perspective of data quality assurance. As a result, I suggest the concept of “data traceability” with “the matrix of data quality evaluation” according to the elements of a data production system and related evaluation criteria. To conclude, I suggest social and policy implications for the new role of metrology and standards for producing and disseminating reliable knowledge sources from big data.|10.1109/ACCESS.2019.2904286|||||2019|Big Data Quality Assurance Through Data Traceability: A Case Study of the National Standard Reference Data Program of Korea|Lee, Doyoung|article|8667300|||IEEE Access|21693536||7|||||21100374601|0,587|Q1|127|18036|24267|771081|116691|24200|4,48|42,75|United States|Northern America|2013-2020|Computer Science (miscellaneous) (Q1); Engineering (miscellaneous) (Q1); Materials Science (miscellaneous) (Q2)|105,968|3.367|0.15396|1268970102|1905633267
||Data quality measurement, Uncertainty modelling, Insufficient information, Possibility theory||95-111||Recently, a fundamental study on measurement of data quality introduced an ordinal-scaled procedure of measurement. Besides the pure ordinal information about the level of quality, numerical information is induced when considering uncertainty involved during measurement. In the case where uncertainty is modelled as probability, this numerical information is ratio-scaled. An essential property of the mentioned approach is that the application of a measure on a large collection of data can be represented efficiently in the sense that (i) the representation has a low storage complexity and (ii) it can be updated incrementally when new data are observed. However, this property only holds when the evaluation of predicates is clear and does not deal with uncertainty. For some dimensions of quality, this assumption is far too strong and uncertainty comes into play almost naturally. In this paper, we investigate how the presence of uncertainty influences the efficiency of a measurement procedure. Hereby, we focus specifically on the case where uncertainty is caused by insufficient information and is thus modelled by means of possibility theory. It is shown that the amount of data that reaches a certain level of quality, can be summarized as a possibility distribution over the set of natural numbers. We investigate an approximation of this distribution that has a controllable loss of information, allows for incremental updates and exhibits a low space complexity.|https://doi.org/10.1016/j.ijar.2018.03.007|https://www.sciencedirect.com/science/article/pii/S0888613X17307478||||2018|An incremental approach for data quality measurement with insufficient information|A. Bronselaer and J. Nielandt and G. {De Tré}|article|BRONSELAER201895|||International Journal of Approximate Reasoning|0888613X||96|||||24286|1,039|Q1|97|129|496|6026|2071|475|4,55|46,71|United States|Northern America|1974, 1987-2020|Applied Mathematics (Q1); Artificial Intelligence (Q1); Software (Q1); Theoretical Computer Science (Q1)|4,819|3.816|0.00487|1269465140|1158612006
||Leadership effectiveness, Leadership processes, Machine Learning, Artificial intelligence, Causality, Experiments, Big Data, Heterogeneous treatment effects||101426||Machine Learning (ML) techniques offer exciting new avenues for leadership research. In this paper we discuss how ML techniques can be used to inform predictive and causal models of leadership effects and clarify why both types of model are important for leadership research. We propose combining ML and experimental designs to draw causal inferences by introducing a recently developed technique to isolate “heterogeneous treatment effects.” We provide a step-by-step guide on how to design studies that combine field experiments with the application of ML to establish causal relationships with maximal predictive power. Drawing on examples in the leadership literature, we illustrate how the suggested approach can be applied to examine the impact of, for example, leadership behavior on follower outcomes. We also discuss how ML can be used to advance leadership research from theoretical, methodological and practical perspectives and consider limitations.|https://doi.org/10.1016/j.leaqua.2020.101426|https://www.sciencedirect.com/science/article/pii/S1048984320300539||||2020|Determining causal relationships in leadership research using Machine Learning: The powerful synergy of experiments and data science|Allan Lee and Ilke Inceoglu and Oliver Hauser and Michael Greene|article|LEE2020101426|||The Leadership Quarterly|10489843|||||||21149|4,989|Q1|151|78|150|9416|1576|142|9,94|120,72|United States|Northern America|1990-2020|Applied Psychology (Q1); Business and International Management (Q1); Organizational Behavior and Human Resource Management (Q1); Sociology and Political Science (Q1)|14,059|10.517|0.00895|1271317444|711124800
||Data integrity;Big Data;Decision making;Organizations;Standards organizations;data quality;judicial data governance;quality measurement||66-70|2019 IEEE 19th International Conference on Software Quality, Reliability and Security Companion (QRS-C)|With the development of Smart Court 3.0, the amount of judicial data that can be stored and processed by the computer is increasing rapidly. People gradually realize that judicial data contains tremendous social and business value. However, we need stronger ability to handle with and apply massive, multi-source and heterogeneous judicial data. A complete data governance system should be built in order to make full use of the value of data assets. In such a data governance system, data quality control is one of the key steps of data governance, and also the bottleneck of data service development, because data quality determines the upper limit of data application. This paper proposes a judicial data quality measurement framework by analyzing some judicial business data, followed by a data governance method driven by it.|10.1109/QRS-C.2019.00026|||||2019|Quality Driven Judicial Data Governance|He, Tieke and Chen, Shenghao and Hao, Lian and Liu, Jia|inproceedings|8859426||July|||||||||||||||||||||||||||1272085151|42
||Mobile edge computing, Tasks offloading, Data quality, Optimal stopping theory, Sequential decision making||462-479||An important use case of the Mobile Edge Computing (MEC) paradigm is task and data offloading. Computational offloading is beneficial for a wide variety of mobile applications on different platforms including autonomous vehicles and smartphones. With the envision deployment of MEC servers along the roads and while mobile nodes are moving and having certain tasks (or data) to be offloaded to edge servers, choosing an appropriate time and an ideally suited MEC server to guarantee the Quality of Service (QoS) is challenging. We tackle the data quality-aware offloading sequential decision making problem by adopting the principles of Optimal Stopping Theory (OST) to minimize the expected processing time. A variety of OST stochastic models and their applications to the offloading decision making problem are investigated and assessed. A performance evaluation is provided using simulation approach and real world data sets together with the assessment of baseline deterministic and stochastic offloading models. The results show that the proposed OST models can significantly minimize the expected processing time for analytics task execution and can be implemented in the mobile nodes efficiently.|https://doi.org/10.1016/j.future.2020.12.017|https://www.sciencedirect.com/science/article/pii/S0167739X2033079X||||2021|Data quality-aware task offloading in Mobile Edge Computing: An Optimal Stopping Theory approach|Ibrahim Alghamdi and Christos Anagnostopoulos and Dimitrios P. Pezaros|article|ALGHAMDI2021462|||Future Generation Computer Systems|0167739X||117|||||12264|1,262|Q1|119|798|1935|36054|16877|1868|9,11|45,18|Netherlands|Western Europe|1984-2021|Computer Networks and Communications (Q1); Hardware and Architecture (Q1); Software (Q1)||||1277974896|562237118
ICEMIS '19|Astana, Kazakhstan|security and privacy, IoT classification, vulnerability, challenges in IoT, research issues, IoT services, S/W weakness, IoT architecture|9||Proceedings of the 5th International Conference on Engineering and MIS|Computing environment in IoT (Internet of Things) is surrounded with huge amounts of heterogeneous data fulfilling many services in everyone's daily life. Since, communication process in IoT takes place using different devices such as smart phones, sensors, mobile devices, household devices, embedded equipment etc. With the use of these variety of devices, the exchange of data in open internet environment is prone to vulnerabilities. The main cause for these vulnerabilities is the weaknesses in the design of software components and hardware components. Bridging communications gaps in the IoT is a complex process as the data is from heterogeneous sources. An effort is made in this paper to discuss various challenges that are being faced in security and privacy of data. This will be very much helpful for researchers who want to pursue research.|10.1145/3330431.3330457|https://doi.org/10.1145/3330431.3330457|New York, NY, USA|Association for Computing Machinery|9781450372121|2019|A Recent Survey on Challenges in Security and Privacy in Internet of Things|Aljawarneh, Shadi and Radhakrishna, Vangipuram and Kumar, Gunupudi Rajesh|inproceedings|10.1145/3330431.3330457|25||||||||||||||||||||||||||||1278131657|42
||Support vector machines;Data models;Predictive models;Training;Data mining;Feature extraction;Power grids;missing data prediction;machine learning;support vector machine (SVM);power transformer||417-422|2015 IEEE 17th International Conference on High Performance Computing and Communications, 2015 IEEE 7th International Symposium on Cyberspace Safety and Security, and 2015 IEEE 12th International Conference on Embedded Software and Systems|Big data techniques has been applied to power grid for the evaluation and prediction of grid conditions. However, the raw data quality rarely can meet the requirement of precise data analytics since raw data set usually contains samples with missing data to which the common data mining models are sensitive. Though classic interpolation or neural network methods can been used to fill the gaps of missing data, their predicted data often fail to fit the rules of power grid conditions. This paper presents a machine learning framework (OR_MLF) to improve the prediction accuracy for datasets with missing data points, which mainly combines preprocessing, optimizing support vector machine (OSVM) and refining SVM (RSVM). On top of the OSVM engine, the scheme introduces dedicated data training strategies. First, the original data originating from data generation facilities is preprocessed through standardization. Traditional SVM is then trained to obtain a preliminary prediction model. Next, the optimized SVM predictors are achieved with new training data set, which is extracted based on the preliminary prediction model. Finally, the missing data prediction result depending on OSVM is selectively inputted into the traditional SVM and the refined SVM is lastly accomplished. We test the OR_MLF framework on missing data prediction of power transformers in power grid system. The experimental results show that the predictors based on the proposed framework achieve lower mean square error than traditional ones. Therefore, the framework OR_MLF would be a good candidate to predict the missing data in power grid system.|10.1109/HPCC-CSS-ICESS.2015.16|||||2015|Improving Power Grid Monitoring Data Quality: An Efficient Machine Learning Framework for Missing Data Prediction|Shi, Weiwei and Zhu, Yongxin and Zhang, Jinkui and Tao, Xiang and Sheng, Gehao and Lian, Yong and Wang, Guoxing and Chen, Yufeng|inproceedings|7336197||Aug|||||||||||||||||||||||||||1280729616|42
IUI '16|Sonoma, California, USA|exploratory analysis, visual analytics, intelligent visual interfaces, visualization|11|85–95|Proceedings of the 21st International Conference on Intelligent User Interfaces|Large and high-dimensional real-world datasets are being gathered across a wide range of application disciplines to enable data-driven decision making. Interactive data visualization can play a critical role in allowing domain experts to select and analyze data from these large collections. However, there is a critical mismatch between the very large number of dimensions in complex real-world datasets and the much smaller number of dimensions that can be concurrently visualized using modern techniques. This gap in dimensionality can result in high levels of selection bias that go unnoticed by users. The bias can in turn threaten the very validity of any subsequent insights. In this paper, we present Adaptive Contextualization (AC), a novel approach to interactive visual data selection that is specifically designed to combat the invisible introduction of selection bias. Our approach (1) monitors and models a user's visual data selection activity, (2) computes metrics over that model to quantify the amount of selection bias after each step, (3) visualizes the metric results, and (4) provides interactive tools that help users assess and avoid bias-related problems. We also share results from a user study which demonstrate the effectiveness of our technique.|10.1145/2856767.2856779|https://doi.org/10.1145/2856767.2856779|New York, NY, USA|Association for Computing Machinery|9781450341370|2016|Adaptive Contextualization: Combating Bias During High-Dimensional Visualization and Data Selection|Gotz, David and Sun, Shun and Cao, Nan|inproceedings|10.1145/2856767.2856779|||||||||||||||||||||||||||||1281682226|42
ICBDC '22|Shenzhen, China|multi-dimensional indicators, multi-head attention, Electric power data, data quality evaluation|5|1–5|Proceedings of the 7th International Conference on Big Data and Computing|High-quality power data is the basis for reliable operation of power systems, efficient data processing, and effective mining of the potential value of power data. How to use big data, artificial intelligence and other technologies to evaluate the quality of power data is a hot research topic in the field of electric power. At present, most of the power data quality evaluation methods are simple and lack the research of general data quality evaluation model. Therefore, this paper proposes a multi-dimensional data quality evaluation model based on a multi-head attention mechanism. The model measures multiple indicators such as completeness, accuracy, smoothness, and correlation. The corresponding methods are used to quantify these indicators to form a data quality evaluation index system oriented to multi-dimensional indicators; then, an application feedback mechanism based on a multi-head attention network is used to correct the calculation weights and score outputs, so as to achieve the evaluation of power data quality. Finally, the validation analysis is carried out based on the electricity data of a region in China. The experimental results show that the proposed method can effectively evaluate the quality of electric power data.|10.1145/3545801.3545802|https://doi.org/10.1145/3545801.3545802|New York, NY, USA|Association for Computing Machinery|9781450396097|2022|A Multi-Head Attention Mechanism Base Multi-Dimensional Data Quality Evaluation Model|Liu, Xiaobao and Li, Qi and Zhu, Shaosong and Wang, Cong and Meng, Lingzhen|inproceedings|10.1145/3545801.3545802|||||||||||||||||||||||||||||1288558544|42
https://doi.org/10.1016/S1473-3099(21)00585-5|https://www.sciencedirect.com/science/article/pii/S1473309921005855||||2022|Using big data and mobile health to manage diarrhoeal disease in children in low-income and middle-income countries: societal barriers and ethical implications|Karen H Keddy and Senjuti Saha and Samuel Kariuki and John Bosco Kalule and Farah Naz Qamar and Zoya Haq and Iruka N Okeke|article|KEDDY2022e130|||The Lancet Infectious Diseases|14733099|5|22||||||||||||||||||||||||||||||1290942838|42
||Genome Sequence Archive, GSA, Big data, Raw sequence data, INSDC||14-18||With the rapid development of sequencing technologies towards higher throughput and lower cost, sequence data are generated at an unprecedentedly explosive rate. To provide an efficient and easy-to-use platform for managing huge sequence data, here we present Genome Sequence Archive (GSA; http://bigd.big.ac.cn/gsa or http://gsa.big.ac.cn), a data repository for archiving raw sequence data. In compliance with data standards and structures of the International Nucleotide Sequence Database Collaboration (INSDC), GSA adopts four data objects (BioProject, BioSample, Experiment, and Run) for data organization, accepts raw sequence reads produced by a variety of sequencing platforms, stores both sequence reads and metadata submitted from all over the world, and makes all these data publicly available to worldwide scientific communities. In the era of big data, GSA is not only an important complement to existing INSDC members by alleviating the increasing burdens of handling sequence data deluge, but also takes the significant responsibility for global big data archive and provides free unrestricted access to all publicly available data in support of research activities throughout the world.|https://doi.org/10.1016/j.gpb.2017.01.001|https://www.sciencedirect.com/science/article/pii/S1672022917300025||||2017|GSA: Genome Sequence Archive*|Yanqing Wang and Fuhai Song and Junwei Zhu and Sisi Zhang and Yadong Yang and Tingting Chen and Bixia Tang and Lili Dong and Nan Ding and Qian Zhang and Zhouxian Bai and Xunong Dong and Huanxin Chen and Mingyuan Sun and Shuang Zhai and Yubin Sun and Lei Yu and Li Lan and Jingfa Xiao and Xiangdong Fang and Hongxing Lei and Zhang Zhang and Wenming Zhao|article|WANG201714|||Genomics, Proteomics & Bioinformatics|16720229|1|15|||||89440|3,114|Q1|49|36|158|1874|1152|135|6,41|52,06|China|Asiatic Region|2003-2020|Biochemistry (Q1); Computational Mathematics (Q1); Genetics (Q1); Medicine (miscellaneous) (Q1); Molecular Biology (Q1)||||1291146858|1963566185
|||4|1298–1301||The formulation of hypotheses based on patterns found in data is an essential component of scientific discovery. As larger and richer data sets become available, new scalable and user-friendly tools for scientific discovery through data analysis are needed. We demonstrate Scolopax, which explores the idea of a search engine for hypotheses. It has an intuitive user interface that supports sophisticated queries. Scolopax can explore a huge space of possible hypotheses, returning a ranked list of those that best match the user preferences. To scale to large and complex data sets, Scolopax relies on parallel data management and mining techniques. These include model training, efficient model summary generation, and novel parallel join techniques that together with traditional approaches such as clustering manipulate massive model-summary collections to find the most interesting hypotheses. This demonstration of Scolopax uses a real observational data set, provided by the Cornell Lab of Ornithology. It contains more than 3.3 million bird sightings reported by citizen scientists and has almost 2500 attributes. Conference attendees have the opportunity to make novel discoveries in this data set, ranging from identifying variables that strongly affect bird populations in specific regions to detecting more sophisticated patterns such as habitat competition and migration.|10.14778/2536274.2536300|https://doi.org/10.14778/2536274.2536300||VLDB Endowment||2013|Scolopax: Exploratory Analysis of Scientific Data|Okcan, Alper and Riedewald, Mirek and Panda, Biswanath and Fink, Daniel|article|10.14778/2536274.2536300||aug|Proc. VLDB Endow.|21508097|12|6|August 2013||||21100199855|0,946|Q1|134|246|598|12401|3759|566|5,50|50,41|United States|Northern America|2008-2020|Computer Science (miscellaneous) (Q1)|7,198|2.047|0.00891|1291149261|1216159931
||Organizational sustainability modeling (OSM), Comparison between Cloud and non-Cloud storage platforms, Real Cloud case studies, Data analysis and visualization||56-76||When comparing Cloud and non-Cloud Storage it can be difficult to ensure that the comparison is fair. In this paper we examine the process of setting up such a comparison and the metric used. Performance comparisons on Cloud and non-Cloud systems, deployed for biomedical scientists, have been conducted to identify improvements of efficiency and performance. Prior to the experiments, network latency, file size and job failures were identified as factors which degrade performance and experiments were conducted to understand their impacts. Organizational Sustainability Modeling (OSM) is used before, during and after the experiments to ensure fair comparisons are achieved. OSM defines the actual and expected execution time, risk control rates and is used to understand key outputs related to both Cloud and non-Cloud experiments. Forty experiments on both Cloud and non-Cloud systems were undertaken with two case studies. The first case study was focused on transferring and backing up 10,000 files of 1 GB each and the second case study was focused on transferring and backing up 1000 files 10 GB each. Results showed that first, the actual and expected execution time on the Cloud was lower than on the non-Cloud system. Second, there was more than 99% consistency between the actual and expected execution time on the Cloud while no comparable consistency was found on the non-Cloud system. Third, the improvement in efficiency was higher on the Cloud than the non-Cloud. OSM is the metric used to analyze the collected data and provided synthesis and insights to the data analysis and visualization of the two case studies.|https://doi.org/10.1016/j.future.2015.10.003|https://www.sciencedirect.com/science/article/pii/S0167739X15003167||||2016|A model to compare cloud and non-cloud storage of Big Data|Victor Chang and Gary Wills|article|CHANG201656|||Future Generation Computer Systems|0167739X||57|||||12264|1,262|Q1|119|798|1935|36054|16877|1868|9,11|45,18|Netherlands|Western Europe|1984-2021|Computer Networks and Communications (Q1); Hardware and Architecture (Q1); Software (Q1)||||1291480482|562237118
GECCO '22|Boston, Massachusetts|neural networks, real estate valuation, case-based reasoning, evolutionary algorithms|9|1130–1138|Proceedings of the Genetic and Evolutionary Computation Conference|Human lives are increasingly influenced by algorithms, which therefore need to meet higher standards not only in accuracy but also with respect to explainability. This is especially true for high-stakes areas such as real estate valuation. Unfortunately, the methods applied there often exhibit a trade-off between accuracy and explainability.One explainable approach is case-based reasoning (CBR), where each decision is supported by specific previous cases. However, such methods can be wanting in accuracy. The unexplainable machine learning approaches are often observed to provide higher accuracy but are not scrutable in their decision-making.In this paper, we apply evolutionary algorithms (EAs) to CBR predictors in order to improve their performance. In particular, we deploy EAs to the similarity functions (used in CBR to find comparable cases), which are fitted to the data set at hand. As a consequence, we achieve higher accuracy than state-of-the-art deep neural networks (DNNs), while keeping interpretability and explainability.These results stem from our empirical evaluation on a large data set of real estate offers where we compare known similarity functions, their EA-improved counterparts, and DNNs. Surprisingly, DNNs are only on par with standard CBR techniques. However, using EA-learned similarity functions does yield an improved performance.|10.1145/3512290.3528801|https://doi.org/10.1145/3512290.3528801|New York, NY, USA|Association for Computing Machinery|9781450392372|2022|Towards Explainable Real Estate Valuation via Evolutionary Algorithms|Angrick, Sebastian and Bals, Ben and Hastrich, Niko and Kleissl, Maximilian and Schmidt, Jonas and Dosko\v{c}, Vanja and Molitor, Louise and Friedrich, Tobias and Katzmann, Maximilian|inproceedings|10.1145/3512290.3528801|||||||||||||||||||||||||||||1292117410|42
||Big data outsourcing, Big data sharing, Big data management, SALSA encryption with MapReduce, Fractal index tree, SHA-3||3121-3135||With the rapid growth of data sources, Big data security in Cloud is a big challenge. Different issues have ascended in the area of Big data security such as infrastructure security, data privacy, data management and data integrity. Currently, Big data processing, analytics and storage is secured using cryptography algorithms, which are not appropriate for Big data protection over Cloud. In this paper, we present a solution for addressing the main issues in Big data security over Cloud. We propose a novel system architecture called the Secure Authentication and Data Sharing in Cloud (SADS-Cloud). There are three processes involved in this paper including (i). Big Data Outsourcing, (ii). Big Data Sharing and (iii). Big Data Management. In Big data outsourcing, the data owners are registered to a Trust Center using SHA-3 hashing algorithm. The MapReduce model is used to split the input file into fixed-size of blocks of data and SALSA20 encryption algorithm is applied over each block. In Big data sharing, data users participate in a secure file retrieval. For this purpose, user's credentials (ID, password, secure ID, and current timestamp, email id) are hashed and compared with that stored in a database. In Big data management, there are three important processes implemented to organize data. They are as follows: Compression using Lemperl Ziv Markow Algorithm (LZMA), Clustering using Density-based Clustering of Applications with Noise (DBSCAN), and Indexing using Fractal Index Tree. The proposed scheme for these processes are implemented using Java Programming and performance tested for the following metrics: Information Loss, Compression Ratio, Throughput, Encryption Time and Decryption Time.|https://doi.org/10.1016/j.jksuci.2020.05.005|https://www.sciencedirect.com/science/article/pii/S1319157820303700||||2022|A novel system architecture for secure authentication and data sharing in cloud enabled Big Data Environment|Uma Narayanan and Varghese Paul and Shelbi Joseph|article|NARAYANAN20223121|||Journal of King Saud University - Computer and Information Sciences|13191578|6, Part B|34|||||||||||||||||||||||1292506937|1257083324
||Blockchain, Big data, Vertical applications, Smart city, Smart healthcare, Smart transportation, Security||209-226||Big data has generated strong interest in various scientific and engineering domains over the last few years. Despite many advantages and applications, there are many challenges in big data to be tackled for better quality of service, e.g., big data analytics, big data management, and big data privacy and security. Blockchain with its decentralization and security nature has the great potential to improve big data services and applications. In this article, we provide a comprehensive survey on blockchain for big data, focusing on up-to-date approaches, opportunities, and future directions. First, we present a brief overview of blockchain and big data as well as the motivation behind their integration. Next, we survey various blockchain services for big data, including blockchain for secure big data acquisition, data storage, data analytics, and data privacy preservation. Then, we review the state-of-the-art studies on the use of blockchain for big data applications in different domains such as smart city, smart healthcare, smart transportation, and smart grid. For a better understanding, some representative blockchain-big data projects are also presented and analyzed. Finally, challenges and future directions are discussed to further drive research in this promising area.|https://doi.org/10.1016/j.future.2022.01.017|https://www.sciencedirect.com/science/article/pii/S0167739X22000243||||2022|A survey on blockchain for big data: Approaches, opportunities, and future directions|N. Deepa and Quoc-Viet Pham and Dinh C. Nguyen and Sweta Bhattacharya and B. Prabadevi and Thippa Reddy Gadekallu and Praveen Kumar Reddy Maddikunta and Fang Fang and Pubudu N. Pathirana|article|DEEPA2022209|||Future Generation Computer Systems|0167739X||131|||||12264|1,262|Q1|119|798|1935|36054|16877|1868|9,11|45,18|Netherlands|Western Europe|1984-2021|Computer Networks and Communications (Q1); Hardware and Architecture (Q1); Software (Q1)||||1292699722|562237118
||Energy modelling, Climate policy, Energy policy, Decarbonisation, Energy data, Big data||991-1002||Energy economy models are central to decision making on energy and climate issues in the 21st century, such as informing the design of deep decarbonisation strategies under the Paris Agreement. Designing policies that are aimed at achieving such radical transitions in the energy system will require ever more in-depth modelling of end-use demand, efficiency and fuel switching, as well as an increasing need for regional, sectoral, and agent disaggregation to capture technological, jurisdictional and policy detail. Building and using these models entails complex trade-offs between the level of detail, the size of the system boundary, and the available computing resources. The availability of data to characterise key energy system sectors and interactions is also a key driver of model structure and parameterisation, and there are many blind spots and design compromises that are caused by data scarcity. We may soon, however, live in a world of data abundance, potentially enabling previously impossible levels of resolution and coverage in energy economy models. But while big data concepts and platforms have already begun to be used in a number of selected energy research applications, their potential to improve or even completely revolutionise energy economy modelling has been almost completely overlooked in the existing literature. In this paper, we explore the challenges and possibilities of this emerging frontier. We identify critical gaps and opportunities for the field, as well as developing foundational concepts for guiding the future application of big data to energy economy modelling, with reference to the existing literature on decision making under uncertainty, scenario analysis and the philosophy of science.|https://doi.org/10.1016/j.apenergy.2019.02.002|https://www.sciencedirect.com/science/article/pii/S0306261919302922||||2019|Prospects for energy economy modelling with big data: Hype, eliminating blind spots, or revolutionising the state of the art?|Francis G.N. Li and Chris Bataille and Steve Pye and Aidan O'Sullivan|article|LI2019991|||Applied Energy|03062619||239|||||28801|3,035|Q1|212|1729|5329|100144|56804|5304|10,59|57,92|United Kingdom|Western Europe|1975-2020|Building and Construction (Q1); Energy (miscellaneous) (Q1); Management, Monitoring, Policy and Law (Q1); Mechanical Engineering (Q1)|122,712|9.746|0.15329|1293340661|892883729
SIGMIS-CPR'20|Nuremberg, Germany|big data, predictive analytics, challenges, supply chain management, transport logistics|8|144–151|Proceedings of the 2020 on Computers and People Research Conference|The field of Predictive Analytics (PA) provides the possibility to utilize large amounts of data to improve forecasting, data-driven decision-making, and competitive advantage. Especially the transport logistics sector, which is characterized by high business-related uncertainties, time-sensitivity, and volatility, highly benefits from accurate resource and production planning. While success factors and framework conditions of applying PA are well-investigated on a theoretical SCM level, findings on internal and external challenges of transport logistics organizations remain scarce. Therefore, based on a multiple case approach, this study offers in-depth insights into six real-world cases of freight forwarders, ocean carriers, and air carriers. The results uncover both internal and external challenges. From the internal perspective, the biggest challenges are related to the technical implementation including the acquisition of globally generated, internal and external data and its harmonization. In addition, stakeholder management and target setting impede the development of PA. Regarding external challenges, relational and external conditions hamper the application. Therefore, especially actions of third-party institutions in terms of standardization and security enhancements are required. This study contributes to the existing literature in various ways as the systematic identification addresses real-world issues of PA in the neglected but crucial area of transport logistics, discussing urgent research needs and highlighting potential solutions. Additionally, the results offer valuable guidance for managers when implementing PA in transport logistics.|10.1145/3378539.3393864|https://doi.org/10.1145/3378539.3393864|New York, NY, USA|Association for Computing Machinery|9781450371308|2020|Challenges of Applying Predictive Analytics in Transport Logistics|Birkel, Hendrik and Kopyto, Matthias and Lutz, Corinna|inproceedings|10.1145/3378539.3393864|||||||||||||||||||||||||||||1298663405|42
IMIP '21|Tianjin, China|Sharing, Opening, Healthcare big data, Grading, Classification|6|116–121|2021 3rd International Conference on Intelligent Medicine and Image Processing|This paper expounds the connotations and features of healthcare big data, as well as the concepts and logical relations for its classification, grading, sharing and opening. It also summarizes overseas policies related thereto and their status quo, and emphatically introduces main policies, laws, regulations and national standards regarding the classification, grading, sharing and opening of healthcare big data in China, as well as practices in three places represented by Shandong Province, Guangdong Province and Fuzhou. Finally, principles and practical suggestions for the classification, grading, sharing and opening of healthcare big data were proposed based on current polices, regulations and standards|10.1145/3468945.3468964|https://doi.org/10.1145/3468945.3468964|New York, NY, USA|Association for Computing Machinery|9781450390057|2021|An Empirical Study on the Classification, Grading, Sharing and Opening of Healthcare Big Data Based on Current Policies and Standards|Hou, Hanfang and Fu, Qiang and Zhang, Yang|inproceedings|10.1145/3468945.3468964|||||||||||||||||||||||||||||1301312784|42
||Internet of things, Systematic survey, Big data, Data mining||19-47||In recent years, the Internet of Things (IoT) has emerged as a new opportunity. Thus, all devices such as smartphones, transportation facilities, public services, and home appliances are used as data creator devices. All the electronic devices around us help our daily life. Devices such as wrist watches, emergency alarms, and garage doors and home appliances such as refrigerators, microwaves, air conditioning, and water heaters are connected to an IoT network and controlled remotely. Methods such as big data and data mining can be used to improve the efficiency of IoT and storage challenges of a large data volume and the transmission, analysis, and processing of the data volume on the IoT. The aim of this study is to investigate the research done on IoT using big data as well as data mining methods to identify subjects that must be emphasized more in current and future research paths. This article tries to achieve the goal by following the conference and journal articles published on IoT-big data and also IoT-data mining areas between 2010 and August 2017. In order to examine these articles, the combination of Systematic Mapping and literature review was used to create an intended review article. In this research, 44 articles were studied. These articles are divided into three categories: Architecture & Platform, framework, and application. In this research, a summary of the methods used in the area of IoT-big data and IoT-data mining is presented in three categories to provide a starting point for researchers in the future.|https://doi.org/10.1016/j.comnet.2018.04.001|https://www.sciencedirect.com/science/article/pii/S1389128618301579||||2018|Systematic survey of big data and data mining in internet of things|Shabnam Shadroo and Amir Masoud Rahmani|article|SHADROO201819|||Computer Networks|13891286||139|||||26811|0,798|Q1|135|383|896|19762|4981|877|5,93|51,60|Netherlands|Western Europe|1977-1984, 1989-1990, 1996-2020|Computer Networks and Communications (Q1)|11,644|4.474|0.00957|1302685098|424954694
BiDEDE '22|Philadelphia, Pennsylvania|digital twin, consistency checking, industry 4.0|7||Proceedings of The International Workshop on Big Data in Emergent Distributed Environments|An essential component of today's industry is data, which is generated during manufacturing. The goal of industry 4.0 is efficient collection, processing and analysis of this data. In our work, we address these three tasks and present an extensible system to solve them. To the best of our knowledge, the combination of a consistency checker (CC) for data preparation and a digital twin (DT) for analysis activities represents a novel approach. Consistency checking in combination with a DT leads to increased data quality, which in turn has a positive effect on analyses, like reducing errors to decrease costs, identifying relevant parameters to increase the productivity, and determining the bottleneck of a manufacturing line for enhanced production planning.|10.1145/3530050.3532928|https://doi.org/10.1145/3530050.3532928|New York, NY, USA|Association for Computing Machinery|9781450393461|2022|Enhancing Data Quality and Process Optimization for Smart Manufacturing Lines in Industry 4.0 Scenarios|Paasche, Simon and Groppe, Sven|inproceedings|10.1145/3530050.3532928|9||||||||||||||||||||||||||||1304848608|42
DSMM'16|San Francisco, CA, USA|data integrity, supervisory data, investment advisers, hedge funds, ontologies, knowledge representation, data integration, Financial regulation|6||Proceedings of the Second International Workshop on Data Science for Macro-Modeling|Form PF, mandated by the 2010 Dodd-Frank Act, is financial regulators' primary source for supervisory data on the risk exposures of hedge funds. Investment advisers use the Securities and Exchange Commission's (SEC) Form ADV to register, and then submit Form PF to report on each of the funds that they advise. These forms embed significant internal structure that is amenable to knowledge representation via formal ontologies, which would facilitate key tasks, such as data integration and the assurance of data integrity. We argue that ontologies should be a core and integral component of information management for financial regulatory reporting. We have tested the approach by designing, developing, and integrating ontologies in OWL/RDF in prototype to consistently describe Form ADV and Form PF with precise semantics. Preliminary results indicate that this technique is feasible in practice for data search and analysis, and will yield useful functionality. We also outline directions for future research.|10.1145/2951894.2951901|https://doi.org/10.1145/2951894.2951901|New York, NY, USA|Association for Computing Machinery|9781450344074|2016|An Ontology of Form PF|Fan, Liju and Flood, Mark D.|inproceedings|10.1145/2951894.2951901|9||||||||||||||||||||||||||||1305620502|42
||5G healthcare, Deep learning, Big data, Human disease prediction||401-420|Blockchain Applications for Healthcare Informatics|The importance of healthcare technologies has been made clear in the current pandemic. Healthcare informatics plays an important role in facilitating healthcare and providing healthcare services in real time. Healthcare informatics has developed from Healthcare 1.0 to Healthcare 4.0 in the last few decades. The data generated from the various sources are stored as electronic health record. These data are collected in different forms and formats. The inconsistent data could be handled using various techniques of big data. The information obtained from big data analytics can be used for the prediction of diseases or conditions using artificial intelligence, machine learning, and deep learning techniques. 5G plays an important role in healthcare informatics by enabling real-time remote monitoring and improving augmented reality, virtual reality, and spatial computing. With 5G technologies, a large number of devices can be connected using high-performance computing over large distances to provide healthcare services. Blockchain is applied in healthcare for health record management, insurance claims, drug tracking, authentication, and ensuring the integrity of medical data. Deep learning techniques can be applied to ever-changing data for the detection and prevention of disease. For the classification challenge, deep convolutional neural networks using pictures of diseased regions are often utilized. In many research techniques, AlexNet and GoogLeNet have been utilized to identify plant diseases. This chapter discusses the state of the art for detecting human sickness as well as the associated 5G healthcare framework for improving it.|https://doi.org/10.1016/B978-0-323-90615-9.00016-5|https://www.sciencedirect.com/science/article/pii/B9780323906159000165||Academic Press|978-0-323-90615-9|2022|18 - 5G-enabled deep learning-based framework for healthcare mining: State of the art and challenges|Rahil Parmar and Dhruval Patel and Naitik Panchal and Uttam Chauhan and Jitendra Bhatia|incollection|PARMAR2022401||||||||||Sudeep Tanwar|||||||||||||||||||1305854887|42
||Big data, Classification, Contingency analysis, Critical, Data collection, Data mining, Data processing, Data visualization, Decision tree, Different load conditions, Energy systems, Machine learning, Non critical, Powersystem, Semi critical, Severity prediction, Structured data, Testing data set, Training data set, Voltage stability, Voltage stability index, Volume of data||151-184|Smart Electrical and Mechanical Systems|This work discusses the use of big data and machine learning to predict the severity of a system breakdown caused by an n−1 transmission line condition. The contingency analysis is a key part of traditional energy management systems. The severity of the line is identified by computing the Line Voltage Stability Index The large amount of data handling will be involved during the contingency study. These data need to be processed and analyzed properly by data handling technique and the use of machine learning tools arrive required information in the system. The severity of transmission lines is predicted and compared using classification approaches. To create large data, MATLAB simulation results will be used and the machine learning tool, Weka is used to analyze the data and forecast transmission line. The standard IEEE 30 bus system is considered to understand the proposed methodology.|https://doi.org/10.1016/B978-0-323-90789-7.00004-X|https://www.sciencedirect.com/science/article/pii/B978032390789700004X||Academic Press|978-0-323-90789-7|2022|Chapter Seven - Role of big data analytic and machine learning in power system contingency analysis|Ravi V. Angadi and Suresh Babu Daram and P.S. Venkataramu|incollection|ANGADI2022151||||||||||Rakesh Sehgal and Neeraj Gupta and Anuradha Tomar and Mukund Dutt Sharma and Vigna Kumaran|||||||||||||||||||1306123791|42
||Meta learning, Machine learning, Microservice, Web-based applications, Big data||100432||For a given specific machine learning task, very often several machine learning algorithms and their right configurations are tested in a trial-and-error approach, until an adequate solution is found. This wastes human resources for constructing multiple models, requires a data analytics expert and is time-consuming. Meta learning addresses these problems and supports non-expert users by recommending a promising learning algorithm based on meta features computed from a given dataset. In the present paper, a new concept for enhancing the predictive performance of meta learning classification models by generating new meta examples is introduced. Our concept is realized and evaluated in a microservice-based meta learning framework. This framework makes use of a powerful Big Data software stack, container visualization, modern web technologies and a microservice architecture. In this demonstration and for evaluation purpose, time series model selection is taken as a use case for applying meta learning. It is shown that the proposed microservice-based meta learning framework introduces an excellent performance in assigning the adequate forecasting model for the chosen time series datasets. Moreover, our new concept for generating new meta examples enhances the predictive performance of the meta learner up to 16.77% and 27.07% in the case of using the original and encoded representation forms of meta features respectively. The recommendation of the most appropriate forecasting model results in a well acceptable low framework overhead demonstrating that the framework can provide an efficient approach to solve the problem of model selection in the context of Big Data.|https://doi.org/10.1016/j.iot.2021.100432|https://www.sciencedirect.com/science/article/pii/S2542660521000767||||2021|An extended Meta Learning Approach for Automating Model Selection in Big Data Environments using Microservice and Container Virtualizationz Technologies|Shadi Shahoud and Moritz Winter and Hatem Khalloof and Clemens Duepmeier and Veit Hagenmeyer|article|SHAHOUD2021100432|||Internet of Things|25426605||16|||||||||||||||||||||||1306376674|2077936548
||Clustering algorithms;Big Data;Quality of service;Monitoring;Reliability;Optimization;Service Identification;User Requirements;PSO Algorithm;Quality of Experience (QoE)||800-807|2017 IEEE International Conference on Web Services (ICWS)|Service identification meets with new challenges with overwhelming rise of categories and numbers of services in big data scenarios. Most of the current service identification approaches have paid little attention to the granularity of indicator for service identification, neither do they provide with any trustworthy monitoring mechanism during the process of service identification. To address the problems above, we propose a user requirements based service identification approach for big data (URBSI-BD). In the proposed URBSI-BD, we firstly cluster massive services with BIRCH clustering algorithm to obtain a number of service sets. We then employ PSO algorithm with MapReduce mechanism to achieve a fine-grained evaluation of indicator for service identification. Based on the integration, candidate services which can better meet with user requirements will be selected. Finally, we use Beth trust model on the quality of experience of users and set up a monitoring mechanism to better obtain required services. Simulation results and analysis demonstrate that the proposed approach has better performance in service identification compared with other current approaches in big data scenarios.|10.1109/ICWS.2017.11|||||2017|User Requirements Based Service Identification for Big Data|Wang, Haiyan and Zhang, Han|inproceedings|8029838||June|||||||||||||||||||||||||||1307000243|42
||data quality, referential integrity, system of origination, data profiling||32-43|Information Management|We might as well not do any data storage if we are not storing and passing high quality data. This chapter defines data quality and a program to maintain high standards throughout the enterprise.|https://doi.org/10.1016/B978-0-12-408056-0.00004-7|https://www.sciencedirect.com/science/article/pii/B9780124080560000047|Boston|Morgan Kaufmann|978-0-12-408056-0|2014|Chapter Four - Data Quality: Passing the Standard|William McKnight|incollection|MCKNIGHT201432||||||||||William McKnight|||||||||||||||||||1308348527|42
||||303-304|||https://doi.org/10.1016/j.dhjo.2015.04.003|https://www.sciencedirect.com/science/article/pii/S1936657415000515||||2015|What are the implications of the big data paradigm shift for disability and health?|Suzanne McDermott and Margaret A. Turk|article|MCDERMOTT2015303|||Disability and Health Journal|19366574|3|8|||||11300153406|0,967|Q1|36|132|307|4567|796|278|2,29|34,60|United States|Northern America|2008-2020|Medicine (miscellaneous) (Q1); Public Health, Environmental and Occupational Health (Q1)|2,139|2.554|0.00403|1312805021|326697304
ICEEL 2018|Bali, Indonesia|Smart campus, Comprehensive evaluation, Campus information portal, Index system|7|73–79|Proceedings of the 2018 2nd International Conference on Education and E-Learning|"\"As the internet wave swept the world, \"\"Internet plus education\"\" came into being. Smart campus design and construction has since become a research hotspot. The Campus Information Portal (CIP) plays an increasingly important role in the management of smart campuses. That is why, conducting a comprehensive evaluation study on the construction level of campus information portals is necessary. By combining CIP's own characteristics and incorporating intelligent needs, a comprehensive evaluation index system for CIP was developed. An Analytic Hierarchy Process (AHP) was used to determine index weights, while a Fuzzy Comprehensive Evaluation (FCE) was used to calculate the quantitative scores of the evaluation objects. We selected 10 representative Chinese universities for a comprehensive CIP evaluation and experimental analysis. We analyze the final results of the study, evaluate the validity of our process and methods and finally provide guidance for the construction of a smart campus information portal.\""|10.1145/3291078.3291083|https://doi.org/10.1145/3291078.3291083|New York, NY, USA|Association for Computing Machinery|9781450365772|2018|Evaluation of the Smart Campus Information Portal|Zhicheng, Dai and Feng, Liu|inproceedings|10.1145/3291078.3291083|||||||||||||||||||||||||||||1313006621|42
||class separability index, time series dynamics, ordinal patterns transformation, Internet of Things, Bandt-Pompe transformation, time series classification|30|||This article proposes TSCLAS, a time series classification strategy for the Internet of Things (IoT) data, based on the class separability analysis of their temporal dynamics. Given the large number and incompleteness of IoT data, the use of traditional classification algorithms is not possible. Thus, we claim that solutions for IoT scenarios should avoid using raw data directly, preferring their transformation to a new domain. In the ordinal patterns domain, it is possible to capture the temporal dynamics of raw data to distinguish them. However, to be applied to this challenging scenario, TSCLAS follows a strategy for selecting the best parameters for the ordinal patterns transformation based on maximizing the class separability of the time series dynamics. We show that our method is competitive compared to other classification algorithms from the literature. Furthermore, TSCLAS is scalable concerning the length of time series and robust to the presence of missing data gaps on them. By simulating missing data gaps as long as 50% of the data, our method could beat the accuracy of the compared classification algorithms. Besides, even when losing in accuracy, TSCLAS presents lower computation times for both training and testing phases.|10.1145/3533049|https://doi.org/10.1145/3533049|New York, NY, USA|Association for Computing Machinery||2022|A Classification Strategy for Internet of Things Data Based on the Class Separability Analysis of Time Series Dynamics|Borges, Jo\~{a}o B. and Ramos, Heitor S. and Loureiro, Antonio A. F.|article|10.1145/3533049|23|jul|ACM Trans. Internet Things|26911914|3|3|August 2022||||||||||||||||||||||1314280771|2142544135
ICCAI '20|Tianjin, China|forecasting, real-time traffic condition, big data, video surveillance system, Urban road system|7|30–36|Proceedings of the 2020 6th International Conference on Computing and Artificial Intelligence|Real-time and reliable traffic flow estimation is the basis of urban traffic management and control. However, the existing research focuses on how to use the historical data of surveillance intersection to predict future traffic conditions. As we know, there are few effective algorithms to infer the real-time traffic state of non-surveillance intersections from limited road surveillance by using traffic information in the urban road system. In this paper, we introduce a new solution to solve the prediction task of traffic flow analysis by using traffic data, especially taxi historical data, traffic network data and intersection historical data. The proposed solution takes advantage of GCN and CGAN, and we improved the Unet to realize an important part of the generator. Then, we capture the relationship between the intersections with surveillance and the intersections without surveillance by floating taxi-cabs covered in the whole city. The framework of CGAN can adjust the weights and enhance the inference ability to generate complete traffic status under current conditions. The experimental results show that our method is superior to other methods on the accuracy of traffic volume inference.|10.1145/3404555.3404621|https://doi.org/10.1145/3404555.3404621|New York, NY, USA|Association for Computing Machinery|9781450377089|2020|Traffic Condition Prediction of Urban Roads Based on Neural Network|Zhu, Ruyi|inproceedings|10.1145/3404555.3404621|||||||||||||||||||||||||||||1314541059|42
JCDL '16|Newark, New Jersey, USA|internet archive, web archiving|2|293–294|Proceedings of the 16th ACM/IEEE-CS on Joint Conference on Digital Libraries|This workshop will explore integration of Web archiving and digital libraries, so the complete life cycle involved is covered: creation/authoring, uploading/publishing in the Web (2.0), (focused) crawling, indexing, exploration (searching, browsing), archiving (of events), etc. It will include particular coverage of current topics of interest, like: big data, mobile web archiving, and systems (e.g., Memento, SiteStory, Hadoop processing).|10.1145/2910896.2926735|https://doi.org/10.1145/2910896.2926735|New York, NY, USA|Association for Computing Machinery|9781450342292|2016|WADL 2016: Third International Workshop on Web Archiving and Digital Libraries|Fox, Edward A. and Xie, Zhiwu and Klein, Martin|inproceedings|10.1145/2910896.2926735|||||||||||||||||||||||||||||1314988767|42
||Time complexity;Clustering algorithms;Algorithm design and analysis;Estimation;Big Data;Data mining;big data;cluster-based best match scanning;data imputation;k-NN||232-238|2017 3rd International Conference on Big Data Computing and Communications (BIGCOM)|High-quality data are the prerequisite for analyzing and using big data to guarantee the value of the data. Missing values in data is a common yet challenging problem in data analytics and data mining, especially in the era of big data. Amount of missing values directly affects the data quality. Therefore, it is critical to properly recover missing values in the dataset. This paper presents a new imputation algorithm called Cluster-based Best Match Scanning (CBMS) designed for Big Data. It is a modification of k-NN imputation. CBMS focuses on recovering continuous numeric missing values, and aims at balancing computational complexity and accuracy. As an imputation algorithm, it can potentially reduce the time complexity of k-NN from O(n^2*d) to O(n^1.5*d), and also reduce the space/memory usage, while perform no worse than k-NN imputation. On top of that CBMS is highly parallelizable.Simulation of CBMS is conducted on smart meter reading data. Data is manually divided into training set and testing set, and testing accuracy is evaluated by computing the mean absolute deviation. Comparison with linear interpolation and k-NN imputation is made to demonstrate the power and effectiveness of our proposed CBMS algorithm.|10.1109/BIGCOM.2017.48|||||2017|Cluster-Based Best Match Scanning for Large-Scale Missing Data Imputation|Yu, Weiqing and Zhu, Wendong and Liu, Guangyi and Kan, Bowen and Zhao, Ting and Liu, He|inproceedings|8113071||Aug|||||||||||||||||||||||||||1316523039|42
||Heuristic algorithms;Remuneration;Real-time systems;Databases;Big Data;data quality management; data currency; dynamic determining||227-242||Data quality is an important aspect in data application and management, and currency is one of the major dimensions influencing its quality. In real applications, datasets timestamps are often incomplete and unavailable, or even absent. With the increasing requirements to update real-time data, existing methods can fail to adequately determine the currency of entities. In consideration of the velocity of big data, we propose a series of efficient algorithms for determining the currency of dynamic datasets, which we divide into two steps. In the preprocessing step, to better determine data currency and accelerate dataset updating, we propose the use of a topological graph of the processing order of the entity attributes. Then, we construct an Entity Query B-Tree (EQB-Tree) structure and an Entity Storage Dynamic Linked List (ES-DLL) to improve the querying and updating processes of both the data currency graph and currency scores. In the currency determination step, we propose definitions of the currency score and currency information for tuples referring to the same entity and use examples to discuss methods and algorithms for their computation. Based on our experimental results with both real and synthetic data, we verify that our methods can efficiently update data in the correct order of currency.|10.23919/TST.2017.7914196|||||2017|Efficient currency determination algorithms for dynamic data|Ding, Xiaoou and Wang, Hongzhi and Gao, Yitong and Li, Jianzhong and Gao, Hong|article|7914196||June|Tsinghua Science and Technology|10070214|3|22|||||25522|0,428|Q1|43|69|200|2169|564|199|2,79|31,43|China|Asiatic Region|2003-2021|Multidisciplinary (Q1)|1,474|2.016|0.00116|1317108871|1752837065
BDE 2019|Hong Kong, Hong Kong|quality data resource integration, standard system, aerospace products, system planning|7|16–22|Proceedings of the 2019 International Conference on Big Data Engineering|"\"The integration and application of aerospace product quality data resources is an important way to carry out quality improvement, quality evaluation and precise management. Standardization is the basis for promoting quality data resources integration. The unified and normative standard system is the guarantee for efficient development of integration standards. Firstly, we analyzed the features of quality data resources according to the status quo of integration. Integration structure of quality data resources in terms of vertical and horizontal integration was proposed by adopting the methods of \"\"decomposition-integration\"\" and \"\"classification-association\"\". Secondly, we constructed a three-dimensions architecture of quality data resource integration using the method of system engineering methodology, from the layer dimension (basis, common, special), technical dimension (description, collection, storage, transmission, processing, comprehensive management) and category dimension (rocket, spacecraft). Thirdly, we worked out 20 lists about basis and common standard by adopting the top-down approach. Some standard development suggestions are proposed based on the characteristics of quality data resources and standard research strategies. Finally, we applied the quality problem data resource standard construction and application to verify the proposed method.\""|10.1145/3341620.3341624|https://doi.org/10.1145/3341620.3341624|New York, NY, USA|Association for Computing Machinery|9781450360913|2019|Study on Standard System of Aerospace Quality Data Resources Integration under the Background of Big Data|Jia, Fengsheng and Gao, Yang and Wang, Yuming|inproceedings|10.1145/3341620.3341624|||||||||||||||||||||||||||||1320723422|42
IPEC '22|Dalian, China||6|225–230|Proceedings of the 3rd Asia-Pacific Conference on Image Processing, Electronics and Computers|In modern society with the good and fast development of mobile e-commerce, the commodity information and the user behavior data accompanying it show an explosive and sudden growth trend, which also leads to the emergence of information overload on the e-commerce platform, and the proposed personalized recommendation system for e-commerce users largely alleviates this problem mentioned above. The personalized recommendation system for e-commerce users aims to solve the information overload of e-commerce platform by analyzing the user behavior data of e-commerce platform, so as to explore the interest preference of e-commerce platform users and make active recommendation of advertising content related to e-commerce platform. Although the research on recommendation algorithms for e-commerce platforms has made great progress, there are still challenges in terms of sparse data, static user features and interpretability of e-commerce platform recommendation results in terms of big data feature recognition. Therefore, in this paper, a hybrid recommendation algorithm based on the forgetting curve of e-commerce platform and the automatic feature construction of e-commerce platform is studied in the e-commerce scenario of e-commerce platform, combined with the e-commerce data collected in real field, for the sparsity of e-commerce platform data, the interpretability of recommendation results and the static nature of e-commerce platform user features.|10.1145/3544109.3544151|https://doi.org/10.1145/3544109.3544151|New York, NY, USA|Association for Computing Machinery|9781450395786|2022|Research on Recommendation Strategy of E-Commerce User Portrait Based on User Dynamic Interest Factor Hybrid Recommendation Algorithm|Zhang, Jun and Liu, Longlong|inproceedings|10.1145/3544109.3544151|||||||||||||||||||||||||||||1322926733|42
||Error correction;Algorithm design and analysis;Sequential analysis;Assembly;Bioinformatics;Genomics;Benchmark testing;error correction;mapreduce;genome assembly;next-generation sequencing||717-722|2013 IEEE International Conference on Big Data|Next-generation sequencing (NGS) technologies produce huge amounts of data. These sequencing data unavoidably are accompanied by the occurrence of sequencing errors which constitutes one of the major problems of further analyses. Error correction is indeed one of the critical steps to the success of NGS applications such as de novo genome assembly and DNA resequencing as illustrated in literature. However, it requires computing time and memory space heavily. To design an algorithm to improve data quality by efficiently utilizing on-demand computing resources in the cloud is a challenge for biologists and computer scientists. In this study, we present an error-correction algorithm, called the CloudRS algorithm, for correcting errors in NGS data. The CloudRS algorithm aims at emulating the notion of error correction algorithm of ALLPATHS-LG on the Hadoop/ MapReduce framework. It is conservative in correcting sequencing errors to avoid introducing false decisions, e.g., when dealing with reads from repetitive regions. We also illustrate several probabilistic measures we introduce into CloudRS to make the algorithm more efficient without sacrificing its effectiveness. Running time of using up to 80 instances each with 8 computing units shows satisfactory speedup. Experiments of comparing with other error correction programs show that CloudRS algorithm performs lower false positive rate for most evaluation benchmarks and higher sensitivity on genome S. cerevisiae. We demonstrate that CloudRS algorithm provides significant improvements in the quality of the resulting contigs on benchmarks of NGS de novo assembly.|10.1109/BigData.2013.6691642|||||2013|CloudRS: An error correction algorithm of high-throughput sequencing data based on scalable framework|Chen, Chien-Chih and Chang, Yu-Jung and Chung, Wei-Chun and Lee, Der-Tsai and Ho, Jan-Ming|inproceedings|6691642||Oct|||||||||||||||||||||||||||1323514267|42
|||13|1458–1470||We establish a robust schema design framework for data with missing values. The framework is based on the new notion of an embedded functional dependency, which is independent of the interpretation of missing values, able to express completeness and integrity requirements on application data, and capable of capturing many redundant data value occurrences. We establish axiomatic and algorithmic foundations for reasoning about embedded functional dependencies. These foundations allow us to establish generalizations of Boyce-Codd and Third normal forms that do not permit any redundancy in any future application data, or minimize their redundancy across dependency-preserving decompositions, respectively. We show how to transform any given schema into application schemata that meet given completeness and integrity requirements and the conditions of the generalized normal forms. Data over those application schemata are therefore fit for purpose by design. Extensive experiments with benchmark schemata and data illustrate our framework, and the effectiveness and efficiency of our algorithms, but also provide quantified insight into database schema design trade-offs.|10.14778/3342263.3342626|https://doi.org/10.14778/3342263.3342626||VLDB Endowment||2019|Embedded Functional Dependencies and Data-Completeness Tailored Database Design|Wei, Ziheng and Link, Sebastian|article|10.14778/3342263.3342626||jul|Proc. VLDB Endow.|21508097|11|12|July 2019||||21100199855|0,946|Q1|134|246|598|12401|3759|566|5,50|50,41|United States|Northern America|2008-2020|Computer Science (miscellaneous) (Q1)|7,198|2.047|0.00891|1326566704|1216159931
||Big Data;Monitoring;Decision making;Data science;Conferences;Spatiotemporal phenomena;Magnetic heads||4-4|2020 IEEE International Conference on Big Data (Big Data)|"\"Summary form only given, as follows. The complete presentation was not made available for publication as part of the conference proceedings. Data are being generated, collected, and analyzed today at an unprecedented scale, and data-driven decision making is sweeping through all aspects of society. As the use of big data has grown, so too have concerns that poor-quality data, prevalent in large data sets, can have serious adverse consequences on data-driven decision making. Responsible data science thus requires a recognition of the importance of veracity, the fourth \"\"V\"\" of big data. In this talk, we first present a vision of high-quality big data and highlight the substantial challenges that the first three V’s, volume, velocity, and variety, bring to dealing with veracity in big data. We then present the FIT Family of adaptive, data-driven statistical tools that we have designed, developed, and deployed at AT&T for continuous data quality monitoring of a large and diverse collection of continuously evolving data. These tools monitor data movement to discover missing, partial, duplicated, and delayed data; identify changes in the content of spatiotemporal streams; and pinpoint anomaly hotspots based on persistence, pervasiveness, and priority. We conclude with lessons from FIT relevant to big data quality that are cause for optimism.\""|10.1109/BigData50022.2020.9378181|||||2020|Towards High-Quality Big Data: Lessons from FIT|Srivastava, Divesh|inproceedings|9378181||Dec|||||||||||||||||||||||||||1330408086|42
||Safety analytics, Data analytics, Readiness assessment, Occupational health||105569||Big data and analytics have shown promise in predicting safety incidents and identifying preventative measures directed towards specific risk variables. However, the safety industry is lagging in big data utilization due to various obstacles, which may include lack of data readiness (e.g., disparate databases, missing data, low validity) and personnel competencies. This paper provides a primer on the application of big data to safety. We then describe a safety analytics readiness assessment framework that highlights system requirements and the challenges that safety professionals may encounter in meeting these requirements. The proposed framework suggests that safety analytics readiness depends on (a) the quality of the data available, (b) organizational norms around data collection, scaling, and nomenclature, (c) foundational infrastructure, including technological platforms and skills required for data collection, storage, and analysis of health and safety metrics, and (d) measurement culture, or the emergent social patterns between employees, data acquisition, and analytic processes. A safety-analytics readiness assessment can assist organizations with understanding current capabilities so measurement systems can be matured to accommodate more advanced analytics for the ultimate purpose of improving decisions that mitigate injury and incidents.|https://doi.org/10.1016/j.ssci.2021.105569|https://www.sciencedirect.com/science/article/pii/S0925753521004112||||2022|Advancing safety analytics: A diagnostic framework for assessing system readiness within occupational safety and health|Maira E. Ezerins and Timothy D. Ludwig and Tara O'Neil and Anne M. Foreman and Yalçın Açıkgöz|article|EZERINS2022105569|||Safety Science|09257535||146|||||12332|1,178|Q1|111|451|1047|26276|5909|1020|5,49|58,26|Netherlands|Western Europe|1991-2020|Public Health, Environmental and Occupational Health (Q1); Safety Research (Q1); Safety, Risk, Reliability and Quality (Q1)|17,184|4.877|0.01488|1330966111|1544923827
||motivation, data-processing science, reproducibility, human data interventions, research data management, reuse|26|||As science becomes increasingly data-intensive, the requirements for comprehensive Research Data Management (RDM) grow. This often overwhelms scientists, requiring more workload and training. The failure to conduct effective RDM leads to producing research artefacts that cannot be reproduced or reused. Past research placed high value on supporting data science workers, but focused mainly on data production, collection, processing, and sensemaking. In order to understand practices and needs of data science workers in relation to documentation, preservation, sharing, and reuse, we conducted a cross-domain study with 15 scientists and data managers from diverse scientific domains. We identified five core concepts which describe requirements, drivers, and boundaries in the development of commitment for RDM, essential for generating reproducible research artefacts: Practice, Adoption, Barriers, Education, and Impact. Based on those concepts, we introduce a stage-based model of personal RDM commitment evolution. The model can be used to drive the design of future systems that support a transition to open science. We discuss infrastructure, policies, and motivations involved at the stages and transitions in the model. Our work supports designers in understanding the constraints and challenges involved in designing for reproducibility in an age of data-driven science.|10.1145/3415212|https://doi.org/10.1145/3415212|New York, NY, USA|Association for Computing Machinery||2020|'Yes, I Comply!': Motivations and Practices around Research Data Management and Reuse across Scientific Fields|Feger, Sebastian S. and Wozniak, Pawe\l{} W. and Lischke, Lars and Schmidt, Albrecht|article|10.1145/3415212|141|oct|Proc. ACM Hum.-Comput. Interact.||CSCW2|4|October 2020||||||||||||||||||||||1338091857|42
||ethnography, speculative design, data visualization, data work, business intelligence|31|||This paper studies data work in an organizational context, and suggests speculative data work as a useful concept and the speculative dashboard as a design concept, to better understand and support cooperative work. Drawing on fieldwork in a Danish public sector organisation, the paper identifies and conceptualizes the speculative data work performed around processes of digitalization and the push to become data-driven. The speculative dashboard is proposed as a design concept and opportunity for design, using practices from speculative design and research to facilitate speculation about data?its sources, visualizations, practices and infrastructures. It does so by hacking the 'genre' of the business intelligence data dashboard, and using it as a framework for the juxtaposition of different kinds of data, facilitating and encouraging speculation on alternative visions for data types and use. The paper contributes an empirical study of organizational use of and attitudes towards data, informing a novel design method and concept for co-speculating on alternative visions of and for organizational data.|10.1145/3434173|https://doi.org/10.1145/3434173|New York, NY, USA|Association for Computing Machinery||2021|Speculative Data Work &amp; Dashboards: Designing Alternative Data Visions|Hockenhull, Michael and Cohn, Marisa Leavitt|article|10.1145/3434173|264|jan|Proc. ACM Hum.-Comput. Interact.||CSCW3|4|December 2020||||||||||||||||||||||1338278610|42
||Implication problem, Null marker, Armstrong database, Extremal combinatorics, Key, Data profiling, Discovery, SQL, Index, Axiomatization|26|571–596||Driven by the dominance of the relational model and the requirements of modern applications, we revisit the fundamental notion of a key in relational databases with NULL. In SQL, primary key columns are NOT NULL, and UNIQUE constraints guarantee uniqueness only for tuples without NULL. We investigate the notions of possible and certain keys, which are keys that hold in some or all possible worlds that originate from an SQL table, respectively. Possible keys coincide with UNIQUE, thus providing a semantics for their syntactic definition in the SQL standard. Certain keys extend primary keys to include NULL columns and can uniquely identify entities whenever feasible, while primary keys may not. In addition to basic characterization, axiomatization, discovery, and extremal combinatorics problems, we investigate the existence and construction of Armstrong tables, and describe an indexing scheme for enforcing certain keys. Our experiments show that certain keys with NULLs occur in real-world data, and related computational problems can be solved efficiently. Certain keys are therefore semantically well founded and able to meet Codd's entity integrity rule while handling high volumes of incomplete data from different formats.|10.1007/s00778-016-0430-9|https://doi.org/10.1007/s00778-016-0430-9|Berlin, Heidelberg|Springer-Verlag||2016|Possible and Certain Keys for SQL|"\"K\"\"{o}hler, Henning and Leck, Uwe and Link, Sebastian and Zhou, Xiaofang\""|article|10.1007/s00778-016-0430-9||aug|The VLDB Journal|10668888|4|25|August    2016||||13646|0,653|Q1|90|64|117|4824|544|113|4,46|75,38|United States|Northern America|1992-2020|Hardware and Architecture (Q1); Information Systems (Q1)|2,106|2.868|0.0023|1340032882|924310569
||Intelligent traffic systems, Data stream, Traffic prediction, Traffic visualization||33-46||Information extraction using distributed sensors has been widely used to obtain information knowledge from various regions or areas. Vehicle traffic data extraction is one of the ways to gather information in order to get the traffic condition information. This research intends to predict and visualize the traffic conditions in a particular road region. Traffic data was obtained from Department of Transport UK. These data are collected using hundreds of sensors for 24 h. Thus, the size of data is very huge. In order to get the behavior of the traffic condition, we need to analyze the huge dataset which was obtained from the sensors. The uses of conventional data mining methods are not sufficient to use, due to the process of knowledge building that should store data temporary in the memory. The fact that data is continuously becoming larger over time, therefore we need to find a method that could automatically adapt to process data in the form of streams. We use method called FIMT-DD (Fast Incremental Model Trees-Drift Detection) to analyze and predict the very large traffic dataset. Based on the prediction system that we have developed, we also visualize the prediction of traffic flow condition within generated sensor point in the real map simulation.|https://doi.org/10.1016/j.knosys.2015.10.028|https://www.sciencedirect.com/science/article/pii/S0950705115004165||||2016|Traffic big data prediction and visualization using Fast Incremental Model Trees-Drift Detection (FIMT-DD)|Ari Wibisono and Wisnu Jatmiko and Hanief Arief Wisesa and Benny Hardjono and Petrus Mursanto|article|WIBISONO201633|||Knowledge-Based Systems|09507051||93|||||24772|1,587|Q1|121|716|1187|36777|11094|1181|9,42|51,36|Netherlands|Western Europe|1987-2020|Artificial Intelligence (Q1); Information Systems and Management (Q1); Management Information Systems (Q1); Software (Q1)|22,261|8.038|0.02794|1340788447|1341138576
HealthDL'20|Toronto, ON, Canada|Mobile Health (mHealth), Machine Learning Models|1|1|Proceedings of Deep Learning for Wellbeing Applications Leveraging Mobile Devices and Edge Computing|Mobile sensor big data collected from smartphones, smartwatches, fitness trackers, and other wearables can be mined for signatures (called mHealth biomarkers) of subtle changes in daily behaviors (e.g., mobility, gait, sleep, etc.) and/or physiology (e.g., heart function, breathing, sweating, etc.). Clinical adoption of these mHealth biomarkers can lead to potent temporally-precise interventions, enabling patients to initiate and sustain the healthy lifestyle choices and treatment regimes that are necessary to prevent and/or successfully manage the growing burden of multiple chronic conditions.However, for any new biomarker to be successfully used for clinical diagnosis or treatment, its clinical utility must be established. mHealth biomarkers are usually derived by training a machine learning (ML) algorithm on mobile sensor data. The published models differ in feature construction (domain-derived features fed to a supervised ML model vs. data-driven features discovered by a deep learning (DL) model), data collection setting (lab vs. field), data selection and preparation (covering all twenty-four hours of the day vs. awake hours, vs. only when performing certain tasks), data labeling (retrospective self-reported aggregate labels vs. time-synchronized labels from first-person video), data size and diversity (e.g., number of participants, gender, ethnicity, age group, number of days, hours per day, etc.), experiment design (cross-validation vs. cross-subject validation), and performance (e.g., accuracy, F1 score, confusion matrix, AUC, etc.). As a result, there is wide diversity in published models on their potential for reusability, generalizability, and eventual clinical utility.This talk will describe an aspirational framework for specificity, sensitivity, generalizability, and reusability of mHealth biomarkers with some concrete performance targets so that they have a higher chance of widespread clinical utility. It will draw upon the presenter's decade-long transdisciplinary research experience in developing machine learning models to detect a wide variety of daily behaviors such as stress, speaking, smoking, and brushing from wearable physiological and inertial sensors.The talk will use the analogy of five nines (i.e., 99.999%) paradigm in the area of service-level agreements to quantify high-availability. Similar to how these managed services are expected to be available 24-7-365, with the downtime limited to 5.26 minutes per year, we can express the performance requirements of mHealth biomarkers that are expected to detect subtle signs of health and behaviour deterioration anytime and anywhere. Five nines guarantee for the detection of a health event (e.g., fall, stress) translates to one false positive every 100 person-days, if the model runs on 1,000 minutes of sensor data collected each day. Achieving five nines to claim the detection of non-event (e.g., smoking abstinence) is even more challenging, as there are several other failure scenarios for missing an event, in addition to model failure, such as the non-wearing of sensors when performing the event of interest, poor data quality, mismatch of model to where (on the body) and how the sensor is worn (e.g., smartwatch on non-dominant hand), battery failure, and data loss. To achieve generalizability, the model performance must be achieved on independent test data that covers all aspects of daily life, without any data selection. Finally, for the model to be used by others for real-life, the model should not only be accessible to the community, it should also be possible to train and test the model on different datasets by independent non-ML-expert researchers.|10.1145/3396868.3402495|https://doi.org/10.1145/3396868.3402495|New York, NY, USA|Association for Computing Machinery|9781450380126|2020|Sensitivity, Specificity, Generalizability, and Reusability Aspirations for Machine Learning (ML) Models in MHealth|Kumar, Santosh|inproceedings|10.1145/3396868.3402495|||||||||||||||||||||||||||||1345805029|42
SenSys '17|Delft, Netherlands|Wireless Sensor Network, Noise Filter, Motion Sensing, Health Care Information Systems, Clinical Decision Support System|14||Proceedings of the 15th ACM Conference on Embedded Network Sensor Systems|"\"Recent advances in machine learning based data analytics are opening opportunities for designing effective clinical decision support systems (CDSS) which can become the \"\"third-eye\"\" in the current clinical procedures and diagnosis. However, common patient movements in hospital wards may lead to faulty measurements in physiological sensor readings, and training a CDSS from such noisy data can cause misleading predictions, directly leading to potentially dangerous clinical decisions. In this work, we present MediSense, a system to sense, classify, and identify noise-causing motions and activities that affect physiological signal when made by patients on their hospital beds. Essentially, such a system can be considered as \"\"glasses\"\" for the clinical third eye in correctly observing medical data. MediSense combines wirelessly connected embedded platforms for motion detection with physiological signal data collected from patients to identify faulty physiological signal measurements and filters such noisy data from being used in CDSS training or testing datasets. We deploy our system in real intensive care units (ICUs), and evaluate its performance from real patient traces collected at these ICUs through a 4-month pilot study at the Ajou University Hospital Trauma Center, a major hospital facility located in Suwon, South Korea. Our results show that MediSense successfully classifies patient motions on the bed with &gt;90% accuracy, shows 100% reliability in determining the locations of beds within the ICU, and each bed-attached sensor achieves a lifetime of more than 33 days, which satisfies the application-level requirements suggested by our clinical partners. Furthermore, a simple case-study with arrhythmia patient data shows that MediSense can help improve the clinical diagnosis accuracy.\""|10.1145/3131672.3131690|https://doi.org/10.1145/3131672.3131690|New York, NY, USA|Association for Computing Machinery|9781450354592|2017|Glasses for the Third Eye: Improving the Quality of Clinical Data Analysis with Motion Sensor-Based Data Filtering|Park, Jaeyeon and Nam, Woojin and Choi, Jaewon and Kim, Taeyeong and Yoon, Dukyong and Lee, Sukhoon and Paek, Jeongyeup and Ko, JeongGil|inproceedings|10.1145/3131672.3131690|8||||||||||||||||||||||||||||1346693995|42
||Indirect method, Reference interval, Data pre-processing, Verification, Big data||23-32||Although reference intervals (RIs) play an important role in clinical diagnosis, there remain significant differences with respect to race, gender, age and geographic location. Accordingly, the Clinical Laboratory Standards Institute (CLSI) EP28-A3c has recommended that clinical laboratories establish RIs appropriate to their subject population. Unfortunately, the traditional and direct approach to establish RIs relies on the recruitment of a sufficient number of healthy individuals of various age groups, collection and testing of large numbers of specimens and accurate data interpretation. The advent of the big data era has, however, created a unique opportunity to “mine” laboratory information. Unfortunately, this indirect method lacks standardization, consensus support and CLSI guidance. In this review we provide a historical perspective, comprehensively assess data processing and statistical methods, and post-verification analysis to validate this big data approach in establishing laboratory specific RIs.|https://doi.org/10.1016/j.cca.2022.01.001|https://www.sciencedirect.com/science/article/pii/S0009898122000018||||2022|Big data and reference intervals|Dan Yang and Zihan Su and Min Zhao|article|YANG202223|||Clinica Chimica Acta|00098981||527|||||29286|0,924|Q2|142|555|1269|26479|4130|1202|3,35|47,71|Netherlands|Western Europe|1956-2020|Biochemistry (Q2); Biochemistry (medical) (Q2); Clinical Biochemistry (Q2); Medicine (miscellaneous) (Q2)|22,229|3.786|0.01641|1347289444|677757740
||||||"\"This document, \"\"MSIS 2016: Global Competency Model for Graduate Degree Programs in Information Systems\"\", is the latest in the series of reports that provides guidance for degree programs in the Information Systems (IS) academic discipline. The first of these reports (Ashenhurst, 1972) was published in the early 1970s, and the work has continued ever since both at the undergraduate and master's levels. The Association for Computing Machinery (ACM) has sponsored the reports from the beginning. Since the Association for Information Systems (AIS) was established in the mid-1990s, the two organizations have collaborated on the production of curriculum recommendations for the IS discipline. At the undergraduate level, both the Association for Information Technology Professionals (AITP) (formerly DPMA) and the International Federation for Information Processing (IFIP) have also made significant contributions to the curriculum recommendations.\""|||New York, NY, USA|Association for Computing Machinery|9781459354325|2017|MSIS 2016: Global Competency Model for Graduate Degree Programs in Information Systems|Topi, Heikki and Karsten, Helena and Brown, Sue A. and Carvalho, Jo\~{a}o Alvaro and Donnellan, Brian and Shen, Jun and Tan, Bernard C. Y. and Thouin, Mark F.|techreport|10.1145/3129538|||||||||||||||||||||||||||||1347530018|42
||Big data capability, Customer orientation, Entrepreneurial orientation, Technology orientation, And developmental culture||49-60||Prior research articulated the importance of developing a big data analytics capability but did not show how to cultivate this development. Drawing on the literature on this topic, this study develops the concept of Big Data capability, which enhances our understanding of Big Data practice beyond that captured in previous literature on the concept of big data analytics capability. This study further highlights the strategic implications of the concept by testing its relationship to three strategic orientations and one aspect of organizational culture. Findings show that customer, entrepreneurial, and technology orientations, and developmental culture are important contributors to the development of Big Data capability.|https://doi.org/10.1016/j.jbusres.2019.07.016|https://www.sciencedirect.com/science/article/pii/S0148296319304333||||2019|Strategic orientations, developmental culture, and big data capability|Canchu Lin and Anand Kunnathur|article|LIN201949|||Journal of Business Research|01482963||105|||||20550|2,049|Q1|195|878|1342|73221|11507|1315|7,38|83,40|United States|Northern America|1973-2021|Marketing (Q1)|46,935|7.550|0.03523|1348147862|1502892296
||Big Data platforms, Unstructured data, Text analytics, Machine learning, Virtual enterprises, Smart communities||100192||This paper presents the approach to Big Data Analytics (BDA) developed in the SIBDA (Sistema Innovativo Big Data Analytics) Project. The project aim is to study and develop innovative solutions in the field of BDA for three companies cooperating in a temporary association of enterprises. We discuss elements of Big Data tackled in the project, namely document processing, mass e-mail applications and Internet of Things sensor networks, to be integrated into a shared platform of common assets and services for the three cooperating companies. We comment about the “Big Data Journey” status in Italy reported by Osservatorio Politecnico di Milano. Then, the paper presents the SIBDA project approach and requirements, outlines the adopted architecture and provides implementation hints, along with some experiments and considerations on the use of the proposed architecture for Smart Cities and Smart Enterprises and Communities.|https://doi.org/10.1016/j.bdr.2021.100192|https://www.sciencedirect.com/science/article/pii/S2214579621000095||||2021|A Big Data Analytics Architecture for Smart Cities and Smart Companies|Mariagrazia Fugini and Jacopo Finocchi and Paolo Locatelli|article|FUGINI2021100192|||Big Data Research|22145796||24|||||21100356018|0,565|Q2|25|11|76|547|324|69|4,06|49,73|United States|Northern America|2014-2020|Computer Science Applications (Q2); Information Systems (Q2); Information Systems and Management (Q2); Management Information Systems (Q2)|586|3.578|0.00126|1348974456|1627174784
||Smart meters;Data visualization;Smart grids;Real-time systems;Image color analysis;Indexes;Data quality;Big Data;data stream;CEP;smart grids||1-5|2015 International Workshop on Computational Intelligence for Multimedia Understanding (IWCIM)|Big Data is often referred to as the 3Vs: Volume, Velocity and Variety. A 4th V (validity) was introduced to address the quality dimension. Poor data quality can be costly, lead to breaks in processes and invalidate the company's efforts on regulatory compliance. In order to process data streams in real time, a new technology called CEP (complex event processing) was developed. In France, the current deployment of smart meters will generate massive electricity consumption data. In this work, we developed a diagnostic approach to compute generic quality indicators of smart meter data streams on the fly. This solution is based on Tibco StreamBase CEP. Visualization tools were also developed in order to give a better understanding of the inter-relation between quality issues and geographical/temporal dimensions. According to the application purpose, two visualization methods can be loaded: (1) StreamBase LiveView is used to visualize quality indicators in real time; and (2) a Web application provides a posteriori and geographical analysis of the quality indicators which are plotted on a map within a color scale (lighter colors indicate good quality and darker colors indicate poor quality). In future works, new quality indicators could be added to the solution which can be applied in an operational context in order to monitor data quality from smart meters.|10.1109/IWCIM.2015.7347061|||||2015|Computing data quality indicators on Big Data streams using a CEP|Wenlu Yang and Da Silva, Alzennyr and Picard, Marie-Luce|inproceedings|7347061||Oct|||||||||||||||||||||||||||1349360265|42
||Artificial intelligence, Digitalisation, AI application, Big data, AI policy, Energy sector||100167||The utilisation of Artificial Intelligence (AI) applications in the energy sector is gaining momentum, with increasingly intensive search for suitable, high-quality and trustworthy solutions that displayed promising results in research. The growing interest comes from decision makers of both the industry and policy domains, searching for applications to increase companies’ profitability, raise efficiency and facilitate the energy transition. This paper aims to provide a novel three-dimensional (3D) indicator for AI applications in the energy sector, based on their respective maturity level, regulatory risks and potential benefits. Case studies are used to exemplify the application of the 3D indicator, showcasing how the developed framework can be used to filter promising AI applications eligible for governmental funding or business development. In addition, the 3D indicator is used to rank AI applications considering different stakeholder preferences (risk-avoidance, profit-seeking, balanced). These results allow AI applications to be better categorised in the face of rapidly emerging national and intergovernmental AI strategies and regulations that constrain the use of AI applications in critical infrastructures.|https://doi.org/10.1016/j.egyai.2022.100167|https://www.sciencedirect.com/science/article/pii/S2666546822000234||||2022|A 3D indicator for guiding AI applications in the energy sector|Hugo Quest and Marine Cauz and Fabian Heymann and Christian Rod and Lionel Perret and Christophe Ballif and Alessandro Virtuani and Nicolas Wyrsch|article|QUEST2022100167|||Energy and AI|26665468||9|||||||||||||||||||||||1349878449|492960112
||Technology acceptance model, Big data analytics system, System quality, Information quality||791-806||Research on the adoption of systems for big data analytics has drawn enormous attention in Information Systems research. This study extends big data analytics adoption research by examining the effects of system characteristics on the attitude of managers towards the usage of big data analytics systems. A research model has been proposed in this study based on an extensive review of literature pertaining to the Technology Acceptance Model, with further validation by a survey of 150 big data analytics users. Results of this survey confirm that characteristics of the big data analytics system have significant direct and indirect effects on belief in the benefits of big data analytics systems and perceived usefulness, attitude and adoption. Moreover, there are mediation effects that exist among the system characteristics, benefits of big data analytics systems, perceived usefulness and the attitude towards using big data analytics system. This study expands the existing body of knowledge on the adoption of big data analytics systems, and benefits big data analytics providers and vendors while helping in the formulation of their business models.|https://doi.org/10.1016/j.ipm.2018.01.004|https://www.sciencedirect.com/science/article/pii/S0306457317300043||||2018|An extension of the technology acceptance model in the big data analytics system implementation environment|Surabhi Verma and Som Sekhar Bhattacharyya and Saurav Kumar|article|VERMA2018791|||Information Processing & Management|03064573|5|54||In (Big) Data we trust: Value creation in knowledge organizations|||||||||||||||||||||1350640805|1769516999
||Heterogeneous networks;Backpropagation;Telecommunication traffic;Big Data;Neural networks;Traffic control;Intelligent networks;Big Data;Quality of service;Networked control systems;Recurrent neural networks||84-91||The heterogeneous network is the foundation of next-generation networks. It aims to explore the existing network resources effectively, and providing better QoS for every kind of traffic flow as far as possible. However, the diversity and dynamic nature of heterogeneous networks will bring a huge burden and big data to the network traffic control. Therefore, how to achieve efficient and intelligent network traffic control becomes the key problem of heterogeneous networks. In this article, an AI-inspired traffic control scheme is proposed. In order to realize fine-grained traffic control in heterogeneous networks, multi-dimensional (i.e., inter-layer, intra-layer, and caching and pushing) network traffic control is introduced. It is worth noting that backpropagation in deep recurrent neural networks is applied in the intra-layer such that an intelligent traffic control scheme can be derived efficiently when facing the huge traffic load in heterogeneous networks. Moreover, DBSCAN is adopted in the inter-layer, which supports efficient classification in the inter-layer. In addition, caching and pushing is adopted to make full use of network resources and provide better QoS. Simulation results demonstrate the effectiveness and practicability of the proposed scheme.|10.1109/MNET.2018.1800120|||||2018|Artificial Intelligence Inspired Multi-Dimensional Traffic Control for Heterogeneous Networks|Shen, Jian and Zhou, Tianqi and Wang, Kun and Peng, Xin and Pan, Li|article|8553660||November|IEEE Network|1558156X|6|32|||||||||||||||||||||||1351270233|1561146114
ASIST '16|Copenhagen, Denmark|program development and evaluation, data science, survey results, data analytics and evaluation|10||Proceedings of the 79th ASIS&amp;T Annual Meeting: Creating Knowledge, Enhancing Lives through Information &amp; Technology|For centuries, library and information science professionals have been responsible for curating and preserving access to information resources. The last few decades have seen an unprecedented change in how new knowledge is created, disseminated and reused both within academe and industry, which provides new opportunities to intervene within the data lifecycle. This paper documents efforts to create a graduate educational program that produces alum who understand both the social and technical aspects of data analytics and who can effectively employ data to address questions in academe and industry. We share perspectives gained from initial interviews with project partners who have data needs, and report on how those needs directly informed curricula development of the Socio-technical Data Analytics (SODA) program at the School of Information Sciences at the University of Illinois. We also provide a formative student evaluation of the program that was conducted to identify aspects that are successful, and those where further work is needed in order to help other schools who are developing similar programs that prepare a workforce who can effectively reuse data.|||USA|American Society for Information Science||2016|Preparing a Workforce to Effectively Reuse Data|Lucic, Ana and Blake, Catherine|inproceedings|10.5555/3017447.3017522|75||||||||||||||||||||||||||||1351603569|42
||||227-236||Clinical research often focuses on resource-intensive causal inference, whereas the potential of predictive analytics with constantly increasing big data sources remains largely unexplored. Basic prediction, divorced from causal inference, is much easier with big data. Emergency care may benefit from this simpler application of big data. Historically, predictive analytics have played an important role in emergency care as simple heuristics for risk stratification. These tools generally follow a standard approach: parsimonious criteria, easy computability, and independent validation with distinct populations. Simplicity in a prediction tool is valuable, but technological advances make it no longer a necessity. Emergency care could benefit from clinical predictions built using data science tools with abundant potential input variables available in electronic medical records. Patients’ risks could be stratified more precisely with large pools of data and lower resource requirements for comparing each clinical encounter to those that came before it, benefiting clinical decisionmaking and health systems operations. The largest value of predictive analytics comes early in the clinical encounter, in which diagnostic and prognostic uncertainty are high and resource-committing decisions need to be made. We propose an agenda for widening the application of predictive analytics in emergency care. Throughout, we express cautious optimism because there are myriad challenges related to database infrastructure, practitioner uptake, and patient acceptance. The quality of routinely compiled clinical data will remain an important limitation. Complementing big data sources with prospective data may be necessary if predictive analytics are to achieve their full potential to improve care quality in the emergency department.|https://doi.org/10.1016/j.annemergmed.2015.06.024|https://www.sciencedirect.com/science/article/pii/S0196064415005302||||2016|Exploring the Potential of Predictive Analytics and Big Data in Emergency Care|Alexander T. Janke and Daniel L. Overbeek and Keith E. Kocher and Phillip D. Levy|article|JANKE2016227|||Annals of Emergency Medicine|01960644|2|67|||||15220|1,241|Q1|153|392|1312|6283|2154|703|1,47|16,03|United States|Northern America|1980-2020|Emergency Medicine (Q1)|14,808|5.721|0.01573|1352673928|322775331
||Big data analytics, Machine learning, Ship performance monitoring, Energy efficiency, Emission control, Data anomaly detection||109392||Improving the operational energy efficiency of existing ships is attracting considerable interests to reduce the environmental footprint due to air emissions. As the shipping industry is entering into Shipping 4.0 with digitalization as a disruptive force, an intriguing area in the field of ship’s operational energy efficiency is big data analytics. This paper proposes a big data analytics framework for ship performance monitoring under localized operational conditions with the help of appropriate data analytics together with domain knowledge. The proposed framework is showcased through a data set obtained from a bulk carrier pertaining the detection of data anomalies, the investigation of the ship’s localized operational conditions, the identification of the relative correlations among parameters and the quantification of the ship’s performance in each of the respective conditions. The novelty of this study is to provide a KPI (i.e. key performance indicator) for ship performance quantification in order to identify the best performance trim-draft mode under the engine modes of the case study ship. The proposed framework has the features to serve as an operational energy efficiency measure to provide data quality evaluation and decision support for ship performance monitoring that is of value for both ship operators and decision-makers.|https://doi.org/10.1016/j.oceaneng.2021.109392|https://www.sciencedirect.com/science/article/pii/S0029801821008040||||2021|Advanced data analytics for ship performance monitoring under localized operational conditions|Khanh Q. Bui and Lokukaluge P. Perera|article|BUI2021109392|||Ocean Engineering|00298018||235|||||28339|1,321|Q1|100|1216|2475|51177|10979|2469|4,31|42,09|United Kingdom|Western Europe|1968-2020|Environmental Engineering (Q1); Ocean Engineering (Q1)|23,463|3.795|0.0247|1354802595|1406779183
CHI '21|Yokohama, Japan|Social Good, AI, Qualitative, HCI4D, Healthcare, India|21||Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems|There has been growing interest in the application of AI for Social Good, motivated by scarce and unequal resources globally. We focus on the case of AI in frontline health, a Social Good domain that is increasingly a topic of significant attention. We offer a thematic discourse analysis of scientific and grey literature to identify prominent applications of AI in frontline health, motivations driving this work, stakeholders involved, and levels of engagement with the local context. We then uncover design considerations for these systems, drawing from data from three years of ethnographic fieldwork with women frontline health workers and women from marginalized communities in Delhi (India). Finally, we outline an agenda for AI systems that target Social Good, drawing from literature on HCI4D, post-development critique, and transnational feminist theory. Our paper thus offers a critical and ethnographic perspective to inform the design of AI systems that target social impact.|10.1145/3411764.3445130|https://doi.org/10.1145/3411764.3445130|New York, NY, USA|Association for Computing Machinery|9781450380966|2021|AI in Global Health: The View from the Front Lines|Ismail, Azra and Kumar, Neha|inproceedings|10.1145/3411764.3445130|598||||||||||||||||||||||||||||1354876102|42
SBSI 2015|Goiania, Goias, Brazil||||Proceedings of the Annual Conference on Brazilian Symposium on Information Systems: Information Systems: A Computer Socio-Technical Perspective - Volume 1||||Porto Alegre, BRA|Brazilian Computer Society||2015|Session Details: Main Track - Management, Governance, and Government|Siqueira, Sean W. M. and Carvalho, Sergio T.|inproceedings|10.5555/2814058.3252433|||||||||||||||||||||||||||||1355173211|42
||Big Data, Healthcare, Big data analytics, disease diagnosis, predictive analysis, security, privacy||1049-1059||The paper presents a brief introduction to big data and its role in healthcare applications. It is observed that the use of big data architecture and techniques are continuously assisting in managing the expeditious data growth in healthcare industry. Here, initially an empirical study is performed to analyze the role of big data in healthcare industry. It has been observed that significant work has been done using big data in healthcare sector. Nowadays, it is intricate to envision the way the machine learning and big data can influence the healthcare industries. It has been observed that most of the authors who implemented the use of machine learning and big data analytics in disease diagnosis have not given significant weightage to the privacy and security of the data. Here, a novel design of smart and secure healthcare information system using machine learning and advanced security mechanism has been proposed to handle big data of medical industry. The innovation lies in the incorporation of optimal storage and data security layer used to maintain security and privacy. Different techniques like masking encryption, activity monitoring, granular access control, dynamic data encryption and end point validation have been incorporated. The proposed hybrid four layer healthcare model seems to be more effective disease diagnostic big data system.|https://doi.org/10.1016/j.procs.2018.05.020|https://www.sciencedirect.com/science/article/pii/S187705091830752X||||2018|Big Data and Machine Learning Based Secure Healthcare Framework|Prableen Kaur and Manik Sharma and Mamta Mittal|article|KAUR20181049|||Procedia Computer Science|18770509||132||International Conference on Computational Intelligence and Data Science|||19700182801|0,334|-|76|1907|6483|38511|13722|6359|2,09|20,19|Netherlands|Western Europe|2010-2020|Computer Science (miscellaneous)||||1356079765|2108686752
||Social housing, Data quality||196-200||The social housing sector has yet to realise the potential of high data quality. While other businesses, mainly in the private sector, reap the benefits of data quality, the social housing sector seems paralysed, as it is still struggling with recent government regulations and steep revenue reduction. This paper offers a succinct review of relevant literature on data quality and how it relates to social housing. The Housing and Development Board in Singapore offers a great example on how to integrate data quality initiatives in the social housing sector. Taking this example, the research presented in this paper is extrapolating cross-disciplinarily recommendations on how to implement data quality initiatives in social housing providers in the UK.|https://doi.org/10.1016/j.ijinfomgt.2017.09.008|https://www.sciencedirect.com/science/article/pii/S0268401216308222||||2018|Data quality challenges in the UK social housing sector|Caroline Duvier and Daniel Neagu and Crina Oltean-Dumbrava and Dave Dickens|article|DUVIER2018196|||International Journal of Information Management|02684012|1|38|||||15631|2,770|Q1|114|224|382|20318|5853|373|16,16|90,71|United Kingdom|Western Europe|1970, 1986-2021|Artificial Intelligence (Q1); Computer Networks and Communications (Q1); Information Systems (Q1); Information Systems and Management (Q1); Library and Information Sciences (Q1); Management Information Systems (Q1); Marketing (Q1)|12,245|14.098|0.01167|1357324806|747927863
ICISCAE 2021|Dalian, China||4|1967–1970|2021 4th International Conference on Information Systems and Computer Aided Education|"\"Data governance is an important part of modernizing the governance capacity of universities. Discipline data governance plays an important role in promoting the development of university disciplines, and is a key factor in improving the governance of university disciplines, the science of educational decision-making and the effectiveness of management. It is an important way to promote the \"\"precision\"\" and \"\"science\"\" of discipline governance. In this paper, we construct a model of discipline data governance based on the Contingency Theory with a view to shedding light on discipline governance.\""|10.1145/3482632.3484077|https://doi.org/10.1145/3482632.3484077|New York, NY, USA|Association for Computing Machinery|9781450390255|2021|Constructing a Computer Model for Discipline Data Governance Using the Contingency Theory and Data Mining|Wang, Chunxia and Xie, Jian|inproceedings|10.1145/3482632.3484077|||||||||||||||||||||||||||||1360870576|42
||Metadata;Business;Data visualization;Data mining;Distributed databases;Collaboration;Computer architecture;research data repository;technical framework;multidiscipline;data recommendation;data collaboration;open data||248-255|2017 IEEE 13th International Conference on e-Science (e-Science)|Research data repositories are necessary infrastructures that ensure the data generated for research are accessible, stable, reliable, and reusable. Based on years of accumulated data work experience, the Computer Network Information Center of the Chinese Academy of Sciences has built a multi-disciplinary data repository ScienceDB for research users and teams using its big data storage, analysis and computing environments. This paper firstly introduces the motivation to develop ScienceDB and gives a profile to it. Then the overall technical framework of ScienceDB is introduced, and the key technologies such as the support for multidiscipline extensibility, data collaboration and data recommendation are analyzed deeply. And then this paper presents the functions and features of ScienceDB's current version and discusses some issues such as its data policy, data quality assurance measures, and current application status. Finally, it summarizes and puts forward that it needs to carry out more in-depth research and practice of ScienceDB in order to meet the higher requirements of eScience in terms of thorough data association and fusion, data analysis and mining, data evaluation, and so on.|10.1109/eScience.2017.38|||||2017|ScienceDB: A Public Multidisciplinary Research Data Repository for eScience|Chengzan, Li and Yanfei, Hou and Jianhui, Li and Lili, Zhang|inproceedings|8109143||Oct|||||||||||||||||||||||||||1363297541|42
||Temporal knowledge discovery, Time series data mining, Big data, Building automation system, Building energy management||75-89||With the advances of information technologies, today's building automation systems (BASs) are capable of managing building operational performance in an efficient and convenient way. Meanwhile, the amount of real-time monitoring and control data in BASs grows continually in the building lifecycle, which stimulates an intense demand for powerful big data analysis tools in BASs. Existing big data analytics adopted in the building automation industry focus on mining cross-sectional relationships, whereas the temporal relationships, i.e., the relationships over time, are usually overlooked. However, building operations are typically dynamic and BAS data are essentially multivariate time series data. This paper presents a time series data mining methodology for temporal knowledge discovery in big BAS data. A number of time series data mining techniques are explored and carefully assembled, including the Symbolic Aggregate approXimation (SAX), motif discovery, and temporal association rule mining. This study also develops two methods for the efficient post-processing of knowledge discovered. The methodology has been applied to analyze the BAS data retrieved from a real building. The temporal knowledge discovered is valuable to identify dynamics, patterns and anomalies in building operations, derive temporal association rules within and between subsystems, assess building system performance and spot opportunities in energy conservation.|https://doi.org/10.1016/j.enbuild.2015.09.060|https://www.sciencedirect.com/science/article/pii/S0378778815302991||||2015|Temporal knowledge discovery in big BAS data for building energy management|Cheng Fan and Fu Xiao and Henrik Madsen and Dan Wang|article|FAN201575|||Energy and Buildings|03787788||109|||||29359|1,737|Q1|184|705|2402|39338|15832|2394|6,33|55,80|Netherlands|Western Europe|1970, 1977-2020|Building and Construction (Q1); Civil and Structural Engineering (Q1); Electrical and Electronic Engineering (Q1); Mechanical Engineering (Q1)|51,254|5.879|0.04387|1364750682|1347637955
ESEM '22|Helsinki, Finland|Story Point, Agile software., Software effort estimation|12|183–194|Proceedings of the 16th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement|Background: Previous work has provided some initial evidence that Story Point (SP) estimated by human-experts may not accurately reflect the effort needed to realise Agile software projects. Aims: In this paper, we aim to shed further light on the relationship between SP and Agile software development effort to understand the extent to which human-estimated SP is a good indicator of user story development effort expressed in terms of time needed to realise it. Method: To this end, we carry out a thorough empirical study involving a total of 37,440 unique user stories from 37 different open-source projects publicly available in the TAWOS dataset. For these user stories, we investigate the correlation between the issue development time (or its approximation when the actual time is not available) and the SP estimated by human-expert by using three widely-used correlation statistics (i.e., Pearson, Kendall and Spearman). Furthermore, we investigate SP estimations made by the human-experts in order to assess the extent to which they are consistent in their estimations throughout the project, i.e., we assess whether the development time of the issues is proportionate to the SP assigned to them. Results: The average results across the three correlation measures reveal that the correlation between the human-expert estimated SP and the approximated development time is strong for only 7% of the projects investigated, and medium (58%) or low (35%) for the remaining ones. Similar results are obtained when the actual development time is considered. Our empirical study also reveals that the estimation made is often not consistent throughout the project and the human estimator tends to misestimate in 78% of the cases. Conclusions: Our empirical results suggest that SP might not be an accurate indicator of open-source Agile software development effort expressed in terms of development time. The impact of its use as an indicator of effort should be explored in future work, for example as a cost-driver in automated effort estimation models or as the prediction target.|10.1145/3544902.3546238|https://doi.org/10.1145/3544902.3546238|New York, NY, USA|Association for Computing Machinery|9781450394277|2022|On the Relationship Between Story Points and Development Effort in Agile Open-Source Software|Tawosi, Vali and Moussa, Rebecca and Sarro, Federica|inproceedings|10.1145/3544902.3546238|||||||||||||||||||||||||||||1364800835|42
ICCIR '21|Guangzhou, China|retrofit building, machine learning, energy saving, green architecture|5|425–429|Proceedings of the 2021 1st International Conference on Control and Intelligent Robotics|Energy is material basis for social development and conflicting issue of economic development around the world. With the continuous urbanization, the energy consumption of buildings will be further increased, accounting for about 40% of the total energy consumption eventually. Thus, enhancement of energy efficiency of the buildings has become an essential issue to reduce the amount of gas emission as well as fossil fuel consumption. Delivering a high-quality built environment in an energy efficient way is the crucial key to energy conservation. Energy efficiency retrofit for buildings is considered to be a promising way to achieve energy savings. Machine learning provides the ability to learn from data using multiple computer algorithms. This paper introduces several algorithms, including random forest and Light GBM, to analyze building energy consumption based on the data from Kaggle competition, providing discussion of improvement in model efficiency and economic analysis by simulating different scenarios. In addition, sensitivity analysis is conducted to show the influence of different parameters in models and metrics to quantify the accuracy of prediction are proposed. The results of this paper can help people understand quantitative influence of different variables on energy use and energy baseline models. Future works will incorporate more data type in order to enhance the performance of prediction.|10.1145/3473714.3473788|https://doi.org/10.1145/3473714.3473788|New York, NY, USA|Association for Computing Machinery|9781450390231|2021|Potential Energy Saving Estimation for Retrofit Building with ASHRAE-Great Energy Predictor III Using Machine Learning|Zhang, Jiamin|inproceedings|10.1145/3473714.3473788|||||||||||||||||||||||||||||1368724561|42
||MDP, Electric bus, data driven, charging scheduling, charging pattern|26|||We are witnessing a rapid growth of electrified vehicles due to the ever-increasing concerns on urban air quality and energy security. Compared to other types of electric vehicles, electric buses have not yet been prevailingly adopted worldwide due to their high owning and operating costs, long charging time, and the uneven spatial distribution of charging facilities. Moreover, the highly dynamic environment factors such as unpredictable traffic congestion, different passenger demands, and even the changing weather can significantly affect electric bus charging efficiency and potentially hinder the further promotion of large-scale electric bus fleets. To address these issues, in this article, we first analyze a real-world dataset including massive data from 16,359 electric buses, 1,400 bus lines, and 5,562 bus stops. Then, we investigate the electric bus network to understand its operating and charging patterns, and further verify the necessity and feasibility of a real-time charging scheduling. With such understanding, we design busCharging, a pricing-aware real-time charging scheduling system based on Markov Decision Process to reduce the overall charging and operating costs for city-scale electric bus fleets, taking the time-variant electricity pricing into account. To show the effectiveness of busCharging, we implement it with the real-world data from Shenzhen, which includes GPS data of electric buses, the metadata of all bus lines and bus stops, combined with data of 376 charging stations for electric buses. The evaluation results show that busCharging dramatically reduces the charging cost by 23.7% and 12.8% of electricity usage simultaneously. Finally, we design a scheduling-based charging station expansion strategy to verify our busCharging is also effective during the charging station expansion process.|10.1145/3428080|https://doi.org/10.1145/3428080|New York, NY, USA|Association for Computing Machinery||2020|Pricing-Aware Real-Time Charging Scheduling and Charging Station Expansion for Large-Scale Electric Buses|Wang, Guang and Fang, Zhihan and Xie, Xiaoyang and Wang, Shuai and Sun, Huijun and Zhang, Fan and Liu, Yunhuai and Zhang, Desheng|article|10.1145/3428080|13|nov|ACM Trans. Intell. Syst. Technol.|21576904|1|12|February 2021||||||||||||||||||||||1370194312|273436860
UrbanGIS'17|Redondo Beach, CA, USA|Geospatial Data, Data Integration, Ontology, Record Linkage, Instance Matching|8||Proceedings of the 3rd ACM SIGSPATIAL Workshop on Smart Cities and Urban Analytics|To run a smart city, data is collected from disparate sources such as IoT devices, social media, private and public organizations, and government agencies. In the US, the City of Chicago has been a pioneer in the collection of data and in the development of a framework, called OpenGrid, to curate and analyze the collected data. OpenGrid is a geospatial situational awareness platform that allows policy makers, service providers, and the general public to explore city data and to perform advanced data analytics to enable planning of services, prediction of events and patterns, and identification of incidents across the city. This paper presents the instance matching module of GIVA, a Geospatial data Integration, Visualization, and Analytics platform, as applied to the integration of information related to businesses, which is spread across several datasets. In particular, we describe the integration of two datasets, Business Licenses and Food Inspections, so as to enable predictive analytics to determine which food establishments the city should inspect first. The paper describes semantic web-based instance matching mechanisms to compare the Business Names and Address fields.|10.1145/3152178.3152186|https://doi.org/10.1145/3152178.3152186|New York, NY, USA|Association for Computing Machinery|9781450354950|2017|Ontology-Based Instance Matching for Geospatial Urban Data Integration|Shivaprabhu, Vivek R. and Balasubramani, Booma Sowkarthiga and Cruz, Isabel F.|inproceedings|10.1145/3152178.3152186|8||||||||||||||||||||||||||||1371436039|42
||Big data, Web data, Web scraping, Law, Ethics||||The digital data available on the World Wide Web is currently measured in zettabytes. These vast repositories of Big Web Data are increasingly viewed as a strategic resource comparable in value to land, gold, and oil. This Big Web Data can be extracted and analyzed by organizations for the purposes of gaining a better understanding of their internal and external environment and improving organizational performance. Because of these opportunities, automated retrieval and organization of Web data (or Web Scraping) for research projects is becoming a common practice. This article educates the reader about the data-related, technical, legal, and ethical issues related to Web Scraping. Awareness of these issues can help researchers save time and other resources and, most importantly, mitigate potential risk of ethical controversies or even lawsuits in relation to the retrieval and use of Big Web Data.|https://doi.org/10.1016/j.bushor.2022.10.001|https://www.sciencedirect.com/science/article/pii/S0007681322001252||||2022|Big web data: Challenges related to data, technology, legality, and ethics|Vlad Krotov and Leigh Johnson|article|KROTOV2022|||Business Horizons|00076813|||||||20209|2,174|Q1|87|64|278|2475|2066|227|6,68|38,67|United Kingdom|Western Europe|1957-2020|Business and International Management (Q1); Marketing (Q1)|7,443|6.361|0.00571|1372276484|847373672
||Big Data;Data integrity;Error analysis;Data mining;Data analysis;Transforms;Time series analysis;Big Data;Data Pre-processing;Data Quality checks||1-4|2020 IEEE International Conference for Innovation in Technology (INOCON)|This is an era of big data, as data is growing exponentially and resources are running out of infrastructure, so it is required to accommodate all the data that gets generated. We collect data in enormous amounts to derive meaningful conclusions, perform effective data analytics and improve decision making. As we don't have enough infrastructures to support data storage for huge volumes, it is needed to clean the data in compulsion. It is a mandatory to carry out a step before doing anything with the data. We call it pre-processing of data and this is carried out in various steps. Pre-processing includes data cleaning, data integration, data filtering, and data transformation and so on. As such preprocessing is not limited to the number of steps or a number of methods or definitive methods. We must innovatively preprocess the data before it is being consumed for data analytics. It has become a responsibility for every data analyst or big data researcher to handpick data for his or her analytics. Considering all these techniques in mind we are proposing a hybrid technique to leverage various algorithms available to pre-process our data along with minor modifications such as at the run time, choosing an algorithm or technique wisely based on the data that we have.|10.1109/INOCON50539.2020.9298378|||||2020|A Hybrid Approach to Data Pre-processing Methods|Desai, Vinod and H A, Dinesha|inproceedings|9298378||Nov|||||||||||||||||||||||||||1376841405|42
PPoPP '20|San Diego, California|FPGA, scientific data, lossy compression, throughput, software-hardware co-design, compression ratio|15|74–88|Proceedings of the 25th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming|Error-bounded lossy compression is critical to the success of extreme-scale scientific research because of ever-increasing volumes of data produced by today's high-performance computing (HPC) applications. Not only can error-controlled lossy compressors significantly reduce the I/O and storage burden but they can retain high data fidelity for post analysis. Existing state-of-the-art lossy compressors, however, generally suffer from relatively low compression and decompression throughput (up to hundreds of megabytes per second on a single CPU core), which considerably restrict the adoption of lossy compression by many HPC applications especially those with a fairly high data production rate. In this paper, we propose a highly efficient lossy compression approach based on field programmable gate arrays (FPGAs) under the state-of-the-art lossy compression model SZ. Our contributions are fourfold. (1) We adopt a wavefront memory layout to alleviate the data dependency during the prediction for higher-dimensional predictors, such as the Lorenzo predictor. (2) We propose a co-design framework named waveSZ based on the wavefront memory layout and the characteristics of SZ algorithm and carefully implement it by using high-level synthesis. (3) We propose a hardware-algorithm co-optimization method to improve the performance. (4) We evaluate our proposed waveSZ on three real-world HPC simulation datasets from the Scientific Data Reduction Benchmarks and compare it with other state-of-the-art methods on both CPUs and FPGAs. Experiments show that our waveSZ can improve SZ's compression throughput by 6.9X ~ 8.7X over the production version running on a state-of-the-art CPU and improve the compression ratio and throughput by 2.1X and 5.8X on average, respectively, compared with the state-of-the-art FPGA design.|10.1145/3332466.3374525|https://doi.org/10.1145/3332466.3374525|New York, NY, USA|Association for Computing Machinery|9781450368186|2020|WaveSZ: A Hardware-Algorithm Co-Design of Efficient Lossy Compression for Scientific Data|Tian, Jiannan and Di, Sheng and Zhang, Chengming and Liang, Xin and Jin, Sian and Cheng, Dazhao and Tao, Dingwen and Cappello, Franck|inproceedings|10.1145/3332466.3374525|||||||||||||||||||||||||||||1377420690|42
LBSN '14|Dallas/Fort Worth, Texas|spatial clustering, spatiotemporal clustering, geography, human mobility, big data|8|1–8|Proceedings of the 7th ACM SIGSPATIAL International Workshop on Location-Based Social Networks|A number of natural language processing and text-mining algorithms have been developed to extract the geospatial cues (e.g., place names) to infer locations of content creators from publicly available information, such as text content, online social profiles, and the behaviors or interactions of users from social networks. These studies, however, can only successfully infer user locations at city levels with relatively decent accuracy, while much higher resolution is required for meaningful spatiotemporal analysis in geospatial fields. Additionally, geographical cues exploited by current text-based approaches are hidden in the unreliable, unstructured, informal, ungrammatical, and multilingual data, and therefore are hard to extract and make meaningful correctly. Instead of using such hidden geographic cues, this paper develops a GIS approach that can infer the true origin of tweets down to the zip code level by using and mining spatial (geo-tags) and temporal (timestamps when a message was posted) information recorded on user digital footprints. Further, individual major daily activity zones and mobility can be successfully inferred and predicted. By integrating GIS data and spatiotemporal clustering methods, this proposed approach can infer individual daily physical activity zones with spatial resolution as high as 20 m by 20 m or even higher depending on the number of digit footprints collected for social media users. The research results with detailed spatial resolution are necessary and useful for various applications such as human mobility pattern analysis, business site selection, disease control, or transportation systems improvement.|10.1145/2755492.2755494|https://doi.org/10.1145/2755492.2755494|New York, NY, USA|Association for Computing Machinery|9781450331401|2014|From Where Do Tweets Originate? A GIS Approach for User Location Inference|Huang, Qunying and Cao, Guofeng and Wang, Caixia|inproceedings|10.1145/2755492.2755494|||||||||||||||||||||||||||||1378832813|42
e-Energy '19|Phoenix, AZ, USA|non-intrusive load monitoring, file format, waveform compression, high sampling rate, Energy dataset, electricity aggregate|10|58–67|Proceedings of the Tenth ACM International Conference on Future Energy Systems|Electrical energy consumption has been an ongoing research area since the coming of smart homes and Internet of Things. Consumption characteristics and usages profiles are directly influenced by building occupants and their interaction with electrical appliances. Data analysis together with machine learning models can be utilized to extract valuable information for the benefit of occupants themselves (conserve energy and increase comfort levels), power plants (maintenance), and grid operators (stability). Public energy datasets provide a scientific foundation to develop and benchmark these algorithms and techniques. With datasets exceeding tens of terabytes, we present a novel study of five whole-building energy datasets with high sampling rates, their signal entropy, and how a well-calibrated measurement can have a significant effect on the overall storage requirements. We show that some datasets do not fully utilize the available measurement precision, therefore leaving potential accuracy and space savings untapped. We benchmark a comprehensive list of 365 file formats, transparent data transformations, and lossless compression algorithms. The primary goal is to reduce the overall dataset size while maintaining an easy-to-use file format and access API. We show that with careful selection of file format and encoding scheme, we can reduce the size of some datasets by up to 73%.|10.1145/3307772.3328285|https://doi.org/10.1145/3307772.3328285|New York, NY, USA|Association for Computing Machinery|9781450366717|2019|Waveform Signal Entropy and Compression Study of Whole-Building Energy Datasets|Kriechbaumer, Thomas and Jorde, Daniel and Jacobsen, Hans-Arno|inproceedings|10.1145/3307772.3328285|||||||||||||||||||||||||||||1381089545|42
SIGITE '14|Atlanta, Georgia, USA|analytics, information technology education, cybersecurity|6|141–146|Proceedings of the 15th Annual Conference on Information Technology Education|This paper introduces the concept of the Master of Information Security and Analytics (MISA) program for the graduate students with a background in CS, IS and IT. The 10-course graduate level program is benchmarked against existing masters programs in the areas of Information Security and Data Analytics, and an assessment was done on the estimated demand for MISA graduates in the nation. The program outcomes were then mapped against the course objectives to insure the correct mix of courses and topics. The program's admission requirement is also being discussed. This paper discusses the design process and possible ways to reduce risk in the start-up of a new degree program. How a program is marketed to prospective students and what program graduates will do after program completion is just as important as the initial design of the program. Planning for the administration of the program and the assessment process is an important phase of the initial design.|10.1145/2656450.2656453|https://doi.org/10.1145/2656450.2656453|New York, NY, USA|Association for Computing Machinery|9781450326865|2014|Designing a Graduate Program in Information Security and Analytics: Masters Program in Information Security and Analytics (MISA)|Kumar, Sathish Alampalayam|inproceedings|10.1145/2656450.2656453|||||||||||||||||||||||||||||1381377413|42
AIIPCC '19|Sanya, China|quality assessment, outlier, electrical data, data quality|6||Proceedings of the International Conference on Artificial Intelligence, Information Processing and Cloud Computing|Data quality assessment plays an important role in electricity consumption big data. It can help business people master the overall data situation, which can provide a strong guarantee for subsequent data improvement, analysis and decision. According to the electrical data quality issues, we design a rule-based data quality assessment architecture for electrical big data. It includes six types of data quality assessment indexes (such as comprehensiveness, accuracy, completeness), and the related data quality rules (such as non-empty rule and range rule), which can be used to guide the electrical data quality inspection. Meanwhile, for the accuracy, we propose an outlier detection method based on time time-relevant k-means, which is used to detect the voltage, curve and power data issues in electricity data. The experimental and simulation results show that the proposed architecture and method can work well for the comprehensive data quality assessment of electrical data.|10.1145/3371425.3371435|https://doi.org/10.1145/3371425.3371435|New York, NY, USA|Association for Computing Machinery|9781450376334|2019|A Rule Based Data Quality Assessment Architecture and Application for Electrical Data|Liu, He and Wang, Xiaohui and Lei, Shuya and Zhang, Xi and Liu, Weiwei and Qin, Ming|inproceedings|10.1145/3371425.3371435|40||||||||||||||||||||||||||||1381935664|42
||Medical services;Organizations;Standards organizations;Engines;Monitoring;Best practices;Interoperability;HL7;big data;integration;acquisition;master patient index;payer;interoperability;data quality||2453-2457|2018 IEEE International Conference on Big Data (Big Data)|Lack of interoperability between health data systems is a leading challenge for healthcare in the United States. This paper describes the challenges and lessons learned in the process of incorporating HL7 data and integration with Electronic Health Records and Health Information Exchanges from the perspective of a midsized Health Plan (Payer). As a Health Care Payer, Premera has a unique perspective regarding how health plans can provide the necessary data to complete the picture of care. This paper shares some of the best practices and focus areas for successful implementation of healthcare data integrations. This paper also focuses on integrating claims and clinical data using a master patient index as well as challenges faced in that process.Note that going forward `Health Plan' and `Payer' will be used interchangeably. Also, `Provider(s)', `Hospitals', `Healthcare Providers', `Clinics', `Provider Organizations' will be used interchangeably and in the context of this paper may mean the same.|10.1109/BigData.2018.8622349|||||2018|HL7 Data Acquisition & Integration: Challenges and Best Practices|Sinha, Shweta and Seys, Marcia|inproceedings|8622349||Dec|||||||||||||||||||||||||||1387233275|42
||Big Data;Software testing;Software;Astronomy;Data analysis;Distributed databases;software testing; big data; astronomical software||529-532|2017 IEEE International Congress on Big Data (BigData Congress)|Astronomy has been one of the first areas of science to embrace and learn from big data. The amount of data we have on our universe is doubling every year thanks to big telescopes and better light detectors. Most leading research is based on data from a handful of very expensive telescopes. Undoubtedly, the data quality is the key basis for the leading scientific findings. It is imperative that software testers understand that big data is about far more than simply data volume. This paper will analyze characteristics, types, methods, strategies, problems, challenges and propose some possible solutions of software testing for astronomical big data.|10.1109/BigDataCongress.2017.91|||||2017|Challenges of Software Testing for Astronomical Big Data|Zhou, Lixiao and Huang, Maohai|inproceedings|8029373||June|||||||||||||||||||||||||||1388306871|42
|||13|5–17|||10.1145/3092931.3092933|https://doi.org/10.1145/3092931.3092933|New York, NY, USA|Association for Computing Machinery||2017|Research Directions for Principles of Data Management (Abridged)|"\"Abiteboul, Serge and Arenas, Marcelo and Barcel\\'{o}, Pablo and Bienvenu, Meghyn and Calvanese, Diego and David, Claire and Hull, Richard and H\"\"{u}llermeier, Eyke and Kimelfeld, Benny and Libkin, Leonid and Martens, Wim and Milo, Tova and Murlak, Filip and Neven, Frank and Ortiz, Magdalena and Schwentick, Thomas and Stoyanovich, Julia and Su, Jianwen and Suciu, Dan and Vianu, Victor and Yi, Ke\""|article|10.1145/3092931.3092933||may|SIGMOD Rec.|01635808|4|45|December 2016||||13622|0,372|Q2|142|39|81|808|127|57|1,67|20,72|United States|Northern America|1969, 1973-1978, 1981-2020|Information Systems (Q2); Software (Q2)|1,819|0.775|7.5E-4|1389770204|962972343
EGOSE '15|St. Petersburg, Russian Federation|Simulation, Information modeling, Data exchange, Shared environment, Economic and mathematical modeling|7|109–115|Proceedings of the 2015 2nd International Conference on Electronic Governance and Open Society: Challenges in Eurasia|Economic and mathematical models and information models are the two main components of the information environment. These two components perform different functions - the information models is responsible for the data quality, the data delivery, and the economic and mathematical models defines data mining and intelligence. This category of models is constantly being developed often independently of each other. The information models as methods of data presentation and data integration are considered as separate from economic and mathematical modeling area. This paper discuss the relationship between the two types of models as sequence of steps for models development with the horizontal and vertical traceability. The connection between two types of models presented as the reflection of the real word logic to the data layer and after that to the software layer, and the feedback from the application to the information and to the operation logic.|10.1145/2846012.2846026|https://doi.org/10.1145/2846012.2846026|New York, NY, USA|Association for Computing Machinery|9781450340700|2015|On the Relationship Between the Information and Analytical Components in the Shared E-Government|Lipuntsov, Yuri P.|inproceedings|10.1145/2846012.2846026|||||||||||||||||||||||||||||1391833367|42
||Big Data Analytics, BDA, MBDAaaS framework, Smart manufacturing, Industry 4.0, Anomaly detection||102331||Today, in a smart manufacturing environment based on the Industry 4.0 paradigm, people, technological infrastructure and machinery equipped with sensors can constantly generate and communicate a huge volume of data, also known as Big Data. The manufacturing industry takes advantage of Big Data and analytics evolution by improving its capability to bring out valuable information and knowledge from industrial processes, production systems and sensors. The adoption of model-based frameworks in the Big Data Analytics pipeline can better address user configuration requirements (e.g. type of analysis to perform, type of algorithm to be applied) and also provide more transparency and clearness on the execution of workflows and data processing. In the current state of art, an application of a model-based framework in a manufacturing scenario is missing. Therefore, in this study, by means of a case study research focused on data from sensors associated with Computer Numerical Control machines, the configuration and execution of a Big Data Analytics pipeline with a Model-based Big Data Analytics-as-a-Service framework is described. The case study provides to theoreticians and managerial experts useful evidence for managing real-time data analytics and deploying a workflow that addresses specific analytical goals, driven by user requirements and developer models, in a complex manufacturing domain.|https://doi.org/10.1016/j.rcim.2022.102331|https://www.sciencedirect.com/science/article/pii/S0736584522000205||||2022|Model-based Big Data Analytics-as-a-Service framework in smart manufacturing: A case study|Angelo Corallo and Anna Maria Crespino and Mariangela Lazoi and Marianna Lezzi|article|CORALLO2022102331|||Robotics and Computer-Integrated Manufacturing|07365845||76|||||18080|1,561|Q1|93|139|404|6448|2949|400|7,35|46,39|United Kingdom|Western Europe|1984-1994, 1996-2021|Computer Science Applications (Q1); Control and Systems Engineering (Q1); Industrial and Manufacturing Engineering (Q1); Mathematics (miscellaneous) (Q1); Software (Q1)|6,215|5.666|0.0057|1393409873|1967447291
||Physics-informed deep learning, Prognostics and health management, Data compression, Big data||108709||The onset of the Internet of Things enables machines to be outfitted with always-on sensors that can provide health information to cloud-based monitoring systems for prognostics and health management (PHM), which greatly improves reliability and avoids downtime of machines and processes on the shop floor. On the other hand, real-time monitoring produces large amounts of data, leading to significant challenges for efficient and effective data transmission (from the shop floor to the cloud) and analysis (in the cloud). Restricted by industrial hardware capability, especially Internet bandwidth, most solutions approach data transmission from the perspective of data compression (before transmission, at local computing devices) coupled with data reconstruction (after transmission, in the cloud). However, existing data compression techniques may not adapt to domain-specific characteristics of data, and hence have limitations in addressing high compression ratios where full restoration of signal details is important for revealing machine conditions. This study integrates Deep Convolutional Autoencoders (DCAE) with local structure and physics-informed loss terms that incorporate PHM domain knowledge such as the importance of frequency content for machine fault diagnosis. Furthermore, Fault Division Autoencoder Multiplexing (FDAM) is proposed to mitigate the negative effects of multiple disjoint operating conditions on reconstruction fidelity. The proposed methods are evaluated on two case studies, and autocorrelation-based noise analysis provides insight into the relative performance across machine health and operating conditions. Results indicate that physically-informed DCAE compression outperforms prevalent data compression approaches, such as compressed sensing, Principal Component Analysis (PCA), Discrete Cosine Transform (DCT), and DCAE with a standard loss function. FDAM can further improve the data reconstruction quality for certain machine conditions.|https://doi.org/10.1016/j.ymssp.2021.108709|https://www.sciencedirect.com/science/article/pii/S0888327021010293||||2022|Physics-informed deep learning for signal compression and reconstruction of big data in industrial condition monitoring|Matthew Russell and Peng Wang|article|RUSSELL2022108709|||Mechanical Systems and Signal Processing|08883270||168|||||21080|2,275|Q1|167|538|2031|23327|16068|2017|7,95|43,36|United States|Northern America|1987-2021|Aerospace Engineering (Q1); Civil and Structural Engineering (Q1); Computer Science Applications (Q1); Control and Systems Engineering (Q1); Mechanical Engineering (Q1); Signal Processing (Q1)|30,686|6.823|0.03775|1394337269|691786838
||federated learning, GDPR, transfer learning||||Federated learning is the process of developing machine learning models over datasets distributed across data centers such as hospitals, clinical research labs, and mobile devices while preventing data leakage. This survey examines previous research and studies on federated learning in the healthcare sector across a range of use cases and applications. Our survey shows what challenges, methods, and applications a practitioner should be aware of in the topic of federated learning. This paper aims to lay out existing research and list the possibilities of federated learning for healthcare industries.|10.1145/3533708|https://doi.org/10.1145/3533708|New York, NY, USA|Association for Computing Machinery||2022|Federated Learning for Healthcare Domain - Pipeline, Applications and Challenges|Joshi, Madhura and Pal, Ankit and Sankarasubbu, Malaikannan|article|10.1145/3533708||may|ACM Trans. Comput. Healthcare|26911957||||Just Accepted|||||||||||||||||||||1394343325|1983512862
||Supply chain management, Big Data marketing, Technological innovation, Closed-loop supply chain, Overconfidence||106538||In the “Internet+” era, involving third-party Internet recycling platforms (IRPs) has revolutionized the operation models of closed-loop supply chains (CLSCs) in China. This study explores the impact of technological innovation, Big Data marketing and overconfidence on supply chain member decision-making. We propose a two-stage remanufacturing CLSC dynamic model consisting of a manufacturer, an IRP, and a supplier based on differential game theory. By comparing the optimal decisions of each member in three scenarios, we find that the IRP’s overconfident behavior is beneficial to both the manufacturer and the IRP but will damage the supplier's profit. Although a suitable cost-sharing ratio can enable the manufacturer and IRP to achieve a “win–win” situation, an excessive level of confidence will inhibit the incentives of the cost-sharing strategy, negatively affecting the manufacturer's interests. Interestingly, a cost-sharing contract will become inefficient under certain conditions, i.e., highly efficient level of technological innovation, highly efficient Big Data marketing, and a high level of overconfidence, negatively affecting the manufacturer’s interests. Additionally, technological innovation efficiency and marketing efficiency will have different effects on the IRP's recycling price. A cost-sharing contract and the IRP’s overconfidence will prompt the IRP to exert more efforts on technological innovation and Big Data marketing and to significantly reduce the manufacturing costs and recycling costs for all members. Notably, although the IRP’s overconfidence and cost-sharing strategies may damage the supplier’s profit, the total profit of the CLSC increases.|https://doi.org/10.1016/j.cie.2020.106538|https://www.sciencedirect.com/science/article/pii/S0360835220302722||||2020|Dynamic game strategies of a two-stage remanufacturing closed-loop supply chain considering Big Data marketing, technological innovation and overconfidence|Zehua Xiang and Minli Xu|article|XIANG2020106538|||Computers & Industrial Engineering|03608352||145|||||18164|1,315|Q1|128|641|1567|31833|9597|1557|5,97|49,66|United Kingdom|Western Europe|1976-2020|Computer Science (miscellaneous) (Q1); Engineering (miscellaneous) (Q1)||||1394697613|1798521593
||Surveillance capitalism, Smart meters, Energy transition, Trust, Data Ethics, Big Data||100079||In the era of information technology and big data, the extraction, commodification, and control of personal information is redefining how people relate and interact. However, the challenges that big data collection and analytics can introduce in trust-based societies, like those of Scandinavia, are not yet understood. For instance, in the energy sector, data generated through smart appliances, like smart metering devices, can have collateral implications for the end-users. In this paper, we present a systematic review of scientific articles indexed in Scopus to identify possible relationships between the practices of collecting, processing, analysing, and using people's data and people's responses to such practices. We contextualise this by looking at research about Scandinavian societies and link this to the academic literature on big data and trust, big data and smart meters, data ethics and the energy sector, surveillance capitalism, and subsequently performing a reflexive thematic analysis. We broadly situate our understanding of culture in this context on the interactions between cognitive norms, material culture, and energy practices. Our analysis identified a number of articles discussing problems and solutions to do with the practices of surveillance capitalism. We also found that research addresses these challenges in different ways. While some research focuses on technological amendments to address users’ privacy protection, only few examine the fundamental ethical questions that discuss how big data practices may change societies and increase their vulnerability. The literature suggests that even in highly trusting societies, like the ones found in Scandinavian countries, trust can be undermined and weakened.|https://doi.org/10.1016/j.egyai.2021.100079|https://www.sciencedirect.com/science/article/pii/S2666546821000331||||2021|Transformations of trust in society: A systematic review of how access to big data in energy systems challenges Scandinavian culture|Jaqueline de Godoy and Kathrin Otrel-Cass and Kristian Høyer Toft|article|GODOY2021100079|||Energy and AI|26665468||5|||||||||||||||||||||||1395283069|492960112
||Simulation, Supply Chain, Big Data, Industry 4.0||125-131||The need and potential benefits for the combined use of Simulation and Big Data in Supply Chains (SCs) has been widely recognized. Having worked on such project, some simulation experiments of the modelled SC system were conducted in SIMIO. Different circumstances were tested, including running the model based on the stored data, on statistical distributions and considering risk situations. Thus, this paper aimed to evaluate such experiments, to evaluate the performance of simulations in these contexts. After analyzing the obtained results, it was found that whilst running the model based on the real data required considerable amounts of computer memory, running the model based on statistical distributions reduced such values, albeit required considerable higher time to run a single replication. In all the tested experiments, the simulation took considerable time to run and was not smooth, which can reduce the stakeholders’ interest in the developed tool, despite its benefits for the decision-making process. For future researches, it would be beneficial to test other simulation tools and other strategies and compare those results to the ones provided in this paper.|https://doi.org/10.1016/j.promfg.2020.02.093|https://www.sciencedirect.com/science/article/pii/S2351978920306582||||2020|Are Simulation Tools Ready For Big Data? Computational Experiments with Supply Chain Models Developed in Simio|António A.C. Vieira and Luís Dias and Maribel Y. Santos and Guilherme A.B. Pereira and José Oliveira|article|VIEIRA2020125|||Procedia Manufacturing|23519789||42||International Conference on Industry 4.0 and Smart Manufacturing (ISM 2019)|||21100792109|0,504|Q2|43|1390|3628|27760|8346|3572|1,79|19,97|Netherlands|Western Europe|2015-2020|Artificial Intelligence (Q2); Industrial and Manufacturing Engineering (Q2)||||1395724450|896540749
||Big Data, Biomedical, Electronic health records, Epidemiology, Surveillance||73-82|Big Data Analytics for Healthcare|Big data has been a buzzword for academics and the healthcare industry, describing data accumulation, processing, and interpretation using analytical tools and techniques. Hitherto, identification of state of the art in big data has become a question of research for academicians due to lack of addressable studies in this regard. The potential of big data is lagging, as compared to other fields. Herein, in this chapter, we have described the potential of big data in combating infectious diseases involving personalized, participatory, predictive, and preventive models based on biomedical data also called the “omics data” and electronic health records from different authenticated sources. Our study indicates the use of various tools and techniques for data accumulation and management, thus providing an insight toward the revolution of the healthcare industry as well as the research community. Though the application of big data is still in the preliminary stage, growing Research and Development investment with its successful implementation will show enormous potential of growth in coming years. Considering all together, there is a need for the development of advanced technologies with inclusion of transparent ethical values to provide acceptance and with a moral foundation.|https://doi.org/10.1016/B978-0-323-91907-4.00009-1|https://www.sciencedirect.com/science/article/pii/B9780323919074000091||Academic Press|978-0-323-91907-4|2022|Chapter 7 - Recent advances in processing, interpreting, and managing biological data for therapeutic intervention of human infectious disease|Pritha Chakraborty and Parth Sarthi {Sen Gupta} and Shankar Dey and Nabarun {Chandra Das} and Ritwik Patra and Suprabhat Mukherjee|incollection|CHAKRABORTY202273||||||||||Pantea Keikhosrokiani|||||||||||||||||||1398605614|42
||Big data, Digital health, Family medicine, Genomics, Primary care||274-282|Systems Medicine|The chapter deals with the practical implications of big data in clinical practice, especially primary care. Family medicine has always advocated individualized approach to patient care. Medicine is changing rapidly for different reasons. One of the reasons is the development of new technologies which is going to radically change medical practice in the future. One of the key changes will involve the importance and practice of data management. The traditional data management that was based on paper records is being changed to the electronic medical record which offers great potential for patient management. This transition will also give rise to new challenges to the practising physician. We are facing the challenge of new sources of data, their increase and variety. Currently, all these data is stored in different locations and there is no consensus whether one single profession is going to take responsibility for managing the data of the patient. If this is decided, the primary care practice would seem a logical solution. In order to do this would involve challenges to healthcare policy and infrastructure. The big data approach to medical care gives rise to new ethical challenges that we would have to address. Existing and future physicians will have to be educated in order to address all these issues for the benefit of their patients. Nevertheless, the physicians should still remember that even with the vast development of precision medicine, the patient will still be more than just a collection of data.|https://doi.org/10.1016/B978-0-12-801238-3.11590-9|https://www.sciencedirect.com/science/article/pii/B9780128012383115909|Oxford|Academic Press|978-0-12-816078-7|2021|Complexity of Patient Data in Primary Care Practice|Igor Švab|incollection|SVAB2021274||||||||||Olaf Wolkenhauer|||||||||||||||||||1399235780|42
IDEAS '22|Budapest, Hungary||8|1–8|Proceedings of the 26th International Database Engineered Applications Symposium|News articles have an influence on people's belief and views about various circumstances. In this regard, some news publishers with political or ideological bias try to spread news which are distorted or totally wrong. Natural language processing was used to preprocess the text. Some general features like, number of words, sentences, stopwords, non-alphabetic words, verbs, nouns, and adjectives were identified. Word positioning was labeled to distinguish a word as a noun, a pronoun, an adjective or a verb in the sentences. Preprocessing was followed by feature extraction methods namely, count vectorizer, Term Frequency-Inverse Document Frequency (TF-IDF) vectorizer and word2vec embedding. It was observed that the results obtained by TF-IDF feature extraction method were superior compared with the other two methods. Various machine learning models were used for training the model namely, Naive Bayes, Logistic Regression, Random Forest, K-nearest neighbors (KNN), Support Vector Machine (SVM) and Recurrent Neural Network (RNN) as a deep learning model. The models were successfully tested on two datasets. On the first dataset, SVM achieved an accuracy of 98.5% and RNN achieved an accuracy of 98.03% which is much improvement over the best results of Agarwalla et al., 2019 (83.16 % accuracy). On the second dataset, SVM achieved an accuracy of 97.76%, RNN achieved 97.1% and Logistic Regression achieved 97.50% which is an improvement over the best results of Vijayraghavan et al. 2020 (94.88% accuracy).|10.1145/3548785.3548811|https://doi.org/10.1145/3548785.3548811|New York, NY, USA|Association for Computing Machinery|9781450397094|2022|Distinguishing Fake and Real News of Twitter Data with the Help of Machine Learning Techniques|Passi, Kalpdrum and Shah, Aanan|inproceedings|10.1145/3548785.3548811|||||||||||||||||||||||||||||1405904000|42
||Big data, Data complexity, Computational complexity, System complexity||59-64||In recent years, the rapid development of Internet, Internet of Things, and Cloud Computing have led to the explosive growth of data in almost every industry and business area. Big data has rapidly developed into a hot topic that attracts extensive attention from academia, industry, and governments around the world. In this position paper, we first briefly introduce the concept of big data, including its definition, features, and value. We then identify from different perspectives the significance and opportunities that big data brings to us. Next, we present representative big data initiatives all over the world. We describe the grand challenges (namely, data complexity, computational complexity, and system complexity), as well as possible solutions to address these challenges. Finally, we conclude the paper by presenting several suggestions on carrying out big data projects.|https://doi.org/10.1016/j.bdr.2015.01.006|https://www.sciencedirect.com/science/article/pii/S2214579615000076||||2015|Significance and Challenges of Big Data Research|Xiaolong Jin and Benjamin W. Wah and Xueqi Cheng and Yuanzhuo Wang|article|JIN201559|||Big Data Research|22145796|2|2||Visions on Big Data|||21100356018|0,565|Q2|25|11|76|547|324|69|4,06|49,73|United States|Northern America|2014-2020|Computer Science Applications (Q2); Information Systems (Q2); Information Systems and Management (Q2); Management Information Systems (Q2)|586|3.578|0.00126|1405989226|1627174784
||Data privacy;Partitioning algorithms;Cryptography;Smart homes;Scalability;Dictionaries;Privacy;privacy;k-anonymization;hadoop;intercloud;aging in place||424-431|2014 IEEE International Conference on Cloud Engineering|Aging-in-Place solutions are becoming increasingly prevalent in our society. New age big data technologies can harness upon enormous amount of data generated from sensors in smart homes to provide enabling services. Added care and preventive services can be furnished through interoperability and bidirectional dataflow across the value chain. However the nature of the problem domain which although allows establishing better care through sharing of information also risks disclosing complete living behavior of individuals. In this paper, we introduce and evaluate a novel scalable k-anonymization solution based upon the distributed map-reduce paradigm for preserving privacy of the shared data in a welfare intercloud. Our evaluation benchmarks both information loss and data quality metrics and demonstrates better scalability/performance than any other available solutions.|10.1109/IC2E.2014.43|||||2014|A Scalable K-Anonymization Solution for Preserving Privacy in an Aging-in-Place Welfare Intercloud|Chakravorty, Antorweep and Wlodarczyk, Tomasz Wiktor and Rong, Chunming|inproceedings|6903506||March|||||||||||||||||||||||||||1410235535|42
||Sensor calibration, Mobile sensor network, Low-cost sensors, Air quality|18|||Urban air quality information, e.g., PM2.5 concentration, is of great importance to both the government and society. Recently, there is a growing interest in developing low-cost sensors, installed on moving vehicles, for fine-grained air quality measurement. However, low-cost mobile sensors typically suffer from low accuracy and thus need careful calibration to preserve a high measurement quality. In this paper, we propose a two-phase data calibration method consisting of a linear part and a nonlinear part. We use MLS (multiple least square) to train the linear part, and use RF (random forest) to train the nonlinear part. We propose an automatic feature selection algorithm based on AIC (Akaike information criterion) for the linear model, which helps avoid overfitting due to the inclusion of inappropriate features. We evaluate our method extensively. Results show that our method outperforms existing approaches, achieving an overall accuracy improvement of 16.4% in terms of PM2.5 levels compared with state-of-the-art approach.|10.1145/3191750|https://doi.org/10.1145/3191750|New York, NY, USA|Association for Computing Machinery||2018|Calibrating Low-Cost Sensors by a Two-Phase Learning Approach for Urban Air Quality Measurement|Lin, Yuxiang and Dong, Wei and Chen, Yuan|article|10.1145/3191750|18|mar|Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.||1|2|March 2018||||||||||||||||||||||1412033450|42
||developing countries, Global development|14|||Researchers from across the social and computer sciences are increasingly using machine learning to study and address global development challenges. This article examines the burgeoning field of machine learning for the developing world (ML4D). First, we present a review of prominent literature. Next, we suggest best practices drawn from the literature for ensuring that ML4D projects are relevant to the advancement of development objectives. Finally, we discuss how developing world challenges can motivate the design of novel machine learning methodologies. This article provides insights into systematic differences between ML4D and more traditional machine learning applications. It also discusses how technical complications of ML4D can be treated as novel research questions, how ML4D can motivate new research directions, and where machine learning can be most useful.|10.1145/3210548|https://doi.org/10.1145/3210548|New York, NY, USA|Association for Computing Machinery||2018|Machine Learning for the Developing World|De-Arteaga, Maria and Herlands, William and Neill, Daniel B. and Dubrawski, Artur|article|10.1145/3210548|9|aug|ACM Trans. Manage. Inf. Syst.|2158656X|2|9|June 2018||||||||||||||||||||||1415989397|2097885649
||orchestration, deep learning, machine learning, IoT|47|||Machine Learning (ML) and Internet of Things (IoT) are complementary advances: ML techniques unlock the potential of IoT with intelligence, and IoT applications increasingly feed data collected by sensors into ML models, thereby employing results to improve their business processes and services. Hence, orchestrating ML pipelines that encompass model training and implication involved in the holistic development lifecycle of an IoT application often leads to complex system integration. This article provides a comprehensive and systematic survey of the development lifecycle of ML-based IoT applications. We outline the core roadmap and taxonomy and subsequently assess and compare existing standard techniques used at individual stages.|10.1145/3398020|https://doi.org/10.1145/3398020|New York, NY, USA|Association for Computing Machinery||2020|Orchestrating the Development Lifecycle of Machine Learning-Based IoT Applications: A Taxonomy and Survey|Qian, Bin and Su, Jie and Wen, Zhenyu and Jha, Devki Nandan and Li, Yinhao and Guan, Yu and Puthal, Deepak and James, Philip and Yang, Renyu and Zomaya, Albert Y. and Rana, Omer and Wang, Lizhe and Koutny, Maciej and Ranjan, Rajiv|article|10.1145/3398020|82|aug|ACM Comput. Surv.|03600300|4|53|July 2021||||23038|2,079|Q1|163|112|365|15859|6978|365|19,16|141,60|United States|Northern America|1969-2020|Computer Science (miscellaneous) (Q1); Theoretical Computer Science (Q1)|10,790|10.282|0.01438|1417614756|1517405264
UbiComp '15|Osaka, Japan|modeling, stress, wearable sensors, mobile health (mHealth)|12|493–504|Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing|Recent advances in mobile health have produced several new models for inferring stress from wearable sensors. But, the lack of a gold standard is a major hurdle in making clinical use of continuous stress measurements derived from wearable sensors. In this paper, we present a stress model (called cStress) that has been carefully developed with attention to every step of computational modeling including data collection, screening, cleaning, filtering, feature computation, normalization, and model training. More importantly, cStress was trained using data collected from a rigorous lab study with 21 participants and validated on two independently collected data sets --- in a lab study on 26 participants and in a week-long field study with 20 participants. In testing, the model obtains a recall of 89% and a false positive rate of 5% on lab data. On field data, the model is able to predict each instantaneous self-report with an accuracy of 72%.|10.1145/2750858.2807526|https://doi.org/10.1145/2750858.2807526|New York, NY, USA|Association for Computing Machinery|9781450335744|2015|CStress: Towards a Gold Standard for Continuous Stress Assessment in the Mobile Environment|Hovsepian, Karen and al'Absi, Mustafa and Ertin, Emre and Kamarck, Thomas and Nakajima, Motohiro and Kumar, Santosh|inproceedings|10.1145/2750858.2807526|||||||||||||||||||||||||||||1420030881|42
ARES '17|Reggio Calabria, Italy|Cyber security, Information sharing, Intrusion detection, Personal data, Alert sharing platform, Privacy|8||Proceedings of the 12th International Conference on Availability, Reliability and Security|In order to ensure confidentiality, integrity and availability (so called CIA triad) of data within network infrastructure, it is necessary to be able to detect and handle cyber security incidents. For this purpose, it is vital for Computer Security Incident Response Teams (CSIRT) to have enough data on relevant security events and threats. That is why CSIRTs share security alerts and incidents data using various sharing platforms. Even though they do so primarily to protect data and privacy of users, their use also lead to additional processing of personal data, which may cause new privacy risks. European data protection law, especially with the adoption of the new General data protection regulation, sets out very strict rules on processing of personal data which on one hand leads to greater protection of individual's rights, but on the other creates great obstacles for those who need to share any personal data. This paper analyses the General Data Protection Regulation (GDPR), relevant case-law and analyses by the Article 29 Working Party to propose optimal methods and level of personal data processing necessary for effective use of security alert sharing platforms, which would be legally compliant and lead to appropriate balance between risks.|10.1145/3098954.3105822|https://doi.org/10.1145/3098954.3105822|New York, NY, USA|Association for Computing Machinery|9781450352574|2017|Protection of Personal Data in Security Alert Sharing Platforms|Stupka, V\'{a}clav and Hor\'{a}k, Martin and Hus\'{a}k, Martin|inproceedings|10.1145/3098954.3105822|65||||||||||||||||||||||||||||1420198766|42
||social network, attribute inference, hierarchical inference||||In the social network, each user has attributes for self-description called user attributes which are semantically hierarchical. Attribute inference has become an essential way for social platforms to realize user classifications and targeted recommendations. Most existing approaches mainly focus on the flat inference problem neglecting the semantic hierarchy of user attributes which will cause serious inconsistency in multi-level tasks. In this article, we propose a multi-level model MLI, where information propagation part collects attribute information by mining the global graph structure, and the attribute correction part realizes the mutual correction between different levels of attributes. Further, we put forward the concept of generalized semantic tree, a way of representing the hierarchical structure of user attributes, whose nodes are allowed to have multiple parent nodes unlike the regular tree. Both regular and generalized semantic tree are commonly used in practice, and can be handled by our model. Besides, by making the inference start from sub-networks with sufficient attribute information, we design a “Ripple” algorithm to improve the efficiency and effectiveness of our model. For evaluation purposes, we conduct extensive verification experiments on DBLP datasets. The experimental results show the superior effect of MLI, compared with the state-of-the-art methods.|10.1145/3545797|https://doi.org/10.1145/3545797|New York, NY, USA|Association for Computing Machinery||2022|MLI: A Multi-Level Inference Mechanism for User Attributes in Social Networks|Zhang, Hang and Yang, Yajun and Wang, Xin and Gao, Hong and Hu, Qinghua|article|10.1145/3545797||jun|ACM Trans. Inf. Syst.|10468188||||Just Accepted|||18997|0,672|Q1|83|44|126|3081|850|126|7,10|70,02|United States|Northern America|1983-2020|Business, Management and Accounting (miscellaneous) (Q1); Information Systems (Q1); Computer Science Applications (Q2)|2,193|4.797|0.00183|1421231966|2120551289
ICIET '17|Tokyo, Japan|Multi Criteria Decision Making, A'WOT Analysis, Business Intelligence Architecture, Cloud Computing, IT Risk Management|5|140–144|Proceedings of the 5th International Conference on Information and Education Technology|IT infrastructure and applications in enterprise systems started with traditional client-server architecture and have gone through key paradigm shifts in infrastructure, software, enterprise, and service architectures to current age of cloud computing and internet of everything. Using strengths-weaknesses-opportunities-threats and analytical hierarchy process of multi-criteria decision making frameworks together, various aspects of cloud computing characteristics related to opportunities, benefits, costs, value and risks can be understood in a more detailed way and can be ranked. This article has combined these two frameworks for the ranking of various business intelligence architects for cloud computing by using 'business automation with sustainable hedging for information risks' framework for cloud computing from conservative perspective where risk relevancy attracts the prime focus. The results have shown that moving operational business intelligence is the best business intelligence architecture for cloud computing as its strengths are more than inherent risks as has become evident by using this approach.|10.1145/3029387.3029421|https://doi.org/10.1145/3029387.3029421|New York, NY, USA|Association for Computing Machinery|9781450348034|2017|Risk Magnification Framework for Clouds Computing Architects in Business Intelligence|Anjum, Shahid W.|inproceedings|10.1145/3029387.3029421|||||||||||||||||||||||||||||1422540464|42
MK Series on Business Intelligence||information life-cycle management, governance, program governance, data governance, data quality||241-250|Data Warehousing in the Age of Big Data|This chapter deals with how to implement information life-cycle management principles to Big Data and create a sustainable process that will ensure that business continuity is not interrupted and data is available on demand.|https://doi.org/10.1016/B978-0-12-405891-0.00012-X|https://www.sciencedirect.com/science/article/pii/B978012405891000012X|Boston|Morgan Kaufmann|978-0-12-405891-0|2013|Chapter 12 - Information Management and Life Cycle for Big Data|Krish Krishnan|incollection|KRISHNAN2013241||||||||||Krish Krishnan|||||||||||||||||||1424264367|42
https://doi.org/10.1016/j.cub.2021.06.083|https://www.sciencedirect.com/science/article/pii/S096098222100912X||||2021|Fungal biodiversity and conservation mycology in light of new technology, big data, and changing attitudes|Lotus A. Lofgren and Jason E. Stajich|article|LOFGREN2021R1312|||Current Biology|09609822|19|31||||||||||||||||||||||||||||||1424415020|42
CSAE 2019|Sanya, China|Big data, Space launch site, Application research, Launch support system|6||Proceedings of the 3rd International Conference on Computer Science and Application Engineering|At the space launch site, the big data of the launch support system comes from the construction of the launch site, the ground service, the comprehensive support process, and launch mission organization and command. The big data is extensive sources, various types, large scale, and rapid growth. The big data application can improve the data processing and management efficiency for the launch support system. Then the application can enhance the support capability of flight mission and success rate. This paper analyzes the existing data application of launch support system. The challenges and requirements of big data application are studied by the construction of intelligent launch site. The application pattern and target are put forward from four aspects of launch mission organization and command, mission application, comprehensive support, and information security. The classification of big data is proposed for a launch support system. The architecture of big data application system is designed, which meets the application pattern and target. It lays a foundation for the future big data project at the launch site.|10.1145/3331453.3360973|https://doi.org/10.1145/3331453.3360973|New York, NY, USA|Association for Computing Machinery|9781450362948|2019|Application Research of Big Data for Launch Support System at Space Launch Site|Dong, Wei and Xiao, Litian and Niu, Shengfen and Niu, Jianjun and Wang, Fei|inproceedings|10.1145/3331453.3360973|23||||||||||||||||||||||||||||1425030094|42
WSC '14|Savannah, Georgia||8|2376–2383|Proceedings of the 2014 Winter Simulation Conference|This paper aims to shed some light on how the concept of cloud manufacturing has been applied to the semiconductor manufacturing operations. It starts with describing the challenges to the semiconductor manufacturing due to evolving of outsourcing business model in global context, then discusses the different forms of cloud manufacturing and proposes the semiconductor industry oriented architecture for cloud manufacturing. Serus is used as a case study to share how the cloud manufacturing has created the values for the customer and its outsourced suppliers in the semiconductor industry.||||IEEE Press||2014|Cloud Manufacturing Application in Semiconductor Industry|Wu, Xinghao and Qiao, Fei and Poon, Kwok|inproceedings|10.5555/2693848.2694146|||||||||||||||||||||||||||||1426601010|42
||Time series analysis;Interpolation;Predictive models;Forecasting;Prediction algorithms;Extrapolation;Adaptation models;time series analysis;prediction;forecasting;interpolation;adaptive prediction algorithms;Big Data;IoT||1260-1265|2018 41st International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO)|The use of time series prediction results in benefits for an organization. Forecasting efficiency relies on applied prediction formula and quality of data received from technical devices and manually inputted. They are often of low quality, with inconsistencies. However, high data quality is crucial for efficient forecasting/prediction purposes (also event detection from time series and pattern recognition), in particular during large data sets processing (often heterogeneous, including data obtained from IoT devices). Such processing should cover inconsistency analysis, interpolation of missing/lacking data, as well as the use of data pre-transformations. The paper presents problems of inconsistent, nonstationary data prediction on the example of stock level daily forecasting. Selected methods of time series interpolation are outlined. Results of implementation of algorithms for short-term time series prediction are illustrated and discussed. Prediction quality measured based on errors values calculated both in total and in a moving window is discussed. A concept of an adaptive algorithm based on a change in the prognostic formula depending on short-term characteristics of time series is outlined.|10.23919/MIPRO.2018.8400228|||||2018|On adaptive prediction of nonstationary and inconsistent large time series data|Pełech-Pilichowski, T.|inproceedings|8400228||May|||||||||||||||||||||||||||1427136522|42
||Big Data;Maintenance engineering;Data models;Investment;Fault diagnosis;Poles and towers;data-driven;lean management;closed-loop;big data analysis||701-705|2018 China International Conference on Electricity Distribution (CICED)|This paper proposes a concept of “data-driven, lean-oriented and closed-loop” management for distribution network and explain how to implement this kind of management, as shown in fig.1 Firstly, a big data platform is constructed to integrate and combine the multi-source data. Secondly, big data analysis technologies such as data mining, machine learning and data visualization are applied to solve problems in distribution network production. For example, accurate location of the fault can be found with help of multisource information from different devices and systems. And we can also be aware of the risk points in distribution network through history data analysis. Finally, this Paper explains how to promote lean management of distribution network in the fields of asset, operation, maintenance and investment based on the big data platform and big data analysis methods. In addition, the feedback procedure sets up a bridge between application and data collecting, which further improve the data quality. Those management measure have been piloted in several cities in Jiangsu. The result proves that they can improve power supply reliability and reduce operating costs significantly. Two practical cases are given to show how they work.|10.1109/CICED.2018.8592556|||||2018|Data-driven lean Management for Distribution Network|Hao, Jiao and Jinming, Chen and Yajuan, Guo|inproceedings|8592556||Sep.||2161749X|||||||21100241614|0,133|-|10|0|520|0|245|519|0,47|0,00|United States|Northern America|2012, 2014|Control and Systems Engineering; Electrical and Electronic Engineering; Energy Engineering and Power Technology||||1427881487|856108586
|||3|78–80||This is a forum for perspectives on designing for marginalized communities worldwide. Articles will discuss design methods, theoretical/conceptual contributions, and participatory interventions with underserved communities. --- Nithya Sambasivan, Editor|10.1145/3516515|https://doi.org/10.1145/3516515|New York, NY, USA|Association for Computing Machinery||2022|All Equation, No Human: The Myopia of AI Models|Sambasivan, Nithya|article|10.1145/3516515||feb|Interactions|10725520|2|29|March - April 2022||||4000148705|0,247|Q3|46|125|292|742|503|292|1,82|5,94|United States|Northern America|1994-1995, 1997, 2006-2020|Human-Computer Interaction (Q3)||||1429753011|2035703392
|||12|2106–2117||Fuzzy join is an important primitive for data cleaning. The ability to customize fuzzy join is crucial to allow applications to address domain-specific data quality issues such as synonyms and abbreviations. While efficient indexing techniques exist for single-node implementations of customizable fuzzy join, the state-of-the-art scale-out techniques do not support customization, and exhibit poor performance and scalability characteristics. We describe the design of a scale-out fuzzy join operator that supports customization. We use a locality-sensitive-hashing (LSH) based signature scheme, and introduce optimizations that result in significant speed up with negligible impact on recall. We evaluate our implementation on the Azure Databricks version of Spark using several real-world and synthetic data sets. We observe speedups exceeding 50X compared to the best-known prior scale-out technique, and close to linear scalability with data size and number of nodes.|10.14778/3352063.3352128|https://doi.org/10.14778/3352063.3352128||VLDB Endowment||2019|Customizable and Scalable Fuzzy Join for Big Data|Chen, Zhimin and Wang, Yue and Narasayya, Vivek and Chaudhuri, Surajit|article|10.14778/3352063.3352128||aug|Proc. VLDB Endow.|21508097|12|12|August 2019||||21100199855|0,946|Q1|134|246|598|12401|3759|566|5,50|50,41|United States|Northern America|2008-2020|Computer Science (miscellaneous) (Q1)|7,198|2.047|0.00891|1432497302|1216159931
||Big data;Metadata;Distributed databases;Stakeholders;Data science;Big Data;Data Quality;Data Warehousing;Distributed Database;Metadata||2863-2871|2016 IEEE International Conference on Big Data (Big Data)|As hardware and software technologies have improved, our definition of a “manageable amount of data” has increased in its scope dramatically. The term “big data” can be applied to any of several different projects and technologies sharing the ultimate goal of supporting analysis on these large, heterogeneous, and evolving data sets. The term “data science” refers to the statistical, technical, and domain-specific knowledge required to ensure that the analysis is done properly. Techniques for managing some common causes for bad data and invalid analysis have been used in other areas, such as data warehousing and distributed database. However, big data projects face special challenges when trying to combine big data and data science without producing inaccurate, misleading, or invalid results. This paper discusses potential causes for “bad big data science”, focusing primarily on the data quality of the input data, and suggests methods for minimizing them based on techniques originally developed for data warehousing and distributed database projects.|10.1109/BigData.2016.7840935|||||2016|Bad big data science|Haug, Frank S.|inproceedings|7840935||Dec|||||||||||||||||||||||||||1434443633|42
SSDBM '14|Aalborg, Denmark|data warehouse, genome data analysis system|11||Proceedings of the 26th International Conference on Scientific and Statistical Database Management|The Integrated Microbial Genomes (IMG) system integrates microbial community aggregate genomes (metagenomes) with genomes from all domains of life. IMG provides tools for analyzing and reviewing the structural and functional annotations of metagenomes and genomes in a comparative context. At the core of the IMG system is a data warehouse that contains genome and metagenome datasets provided by scientific users, as well as public bacterial, archaeal, eukaryotic, and viral genomes from the US National Center for Biotechnology Information genomic archive and a rich set of engineered, environmental and host associated metagenomes. Genomes and metagenome datasets are processed using IMG's microbial genome and metagenome sequence data processing pipelines and then are integrated into the data warehouse using IMG's data integration toolkit. Microbial genome and metagenome application specific user interfaces provide access to different subsets of IMG's data and analysis toolkits. Genome and metagenome analysis is a gene centric iterative process that involves a sequence (composition) of data exploration and comparative analysis operations, with individual operations expected to have rapid response time.From its first release in 2005, IMG has grown from an initial content of about 300 genomes with a total of 2 million genes, to 22,578 bacterial, archaeal, eukaryotic and viral genomes, and 4,188 metagenome samples, with about 24.6 billion genes as of May 1st, 2014. IMG's database architecture is continuously revised in order to cope with the rapid increase in the number and size of the genome and metagenome datasets, maintain good query performance, and accommodate new data types. We present in this paper IMG's new database architecture developed over the past three years in the context of limited financial, engineering and data management resources customary to academic database systems. We discuss the alternative commercial and open source database management systems we considered and experimented with and describe the hybrid architecture we devised for sustaining IMG's rapid growth.|10.1145/2618243.2618244|https://doi.org/10.1145/2618243.2618244|New York, NY, USA|Association for Computing Machinery|9781450327220|2014|Maintaining a Microbial Genome &amp; Metagenome Data Analysis System in an Academic Setting|Chen, I-Min A. and Markowitz, Victor M. and Szeto, Ernest and Palaniappan, Krishna and Chu, Ken|inproceedings|10.1145/2618243.2618244|3||||||||||||||||||||||||||||1435245324|42
GeoSearch'21|Beijing, China|Geospatial search interfaces, ORNL DAAC, ARM Data Center, FAIR data principle for scientific data|4|1–4|Proceedings of the 1st ACM SIGSPATIAL International Workshop on Searching and Mining Large Collections of Geospatial Data|Several factors must be considered in designing a highly accurate, reliable, scalable, and user-friendly geospatial data search interfaces. This paper examines four critical questions that ought to be considered during design phase: (1) Is the search interface or API that provides the search capability useable by both humans and machines? (2) Are the results consistent and reliable? (3) Is the output response format free to use, community-defined, and non-propriety? (4) Does the API clearly state the usage clauses? This paper discusses how certain data repositories at the US Department of Energy's Oak Ridge National Laboratory apply FAIR data principles to enable geospatial searches and address the above-mentioned questions.|10.1145/3486640.3491391|https://doi.org/10.1145/3486640.3491391|New York, NY, USA|Association for Computing Machinery|9781450391238|2021|FAIR Interfaces for Geospatial Scientific Data Searches|Devarakonda, Ranjeet and Guntupally, Kavya and Thornton, Michele and Wei, Yaxing and Singh, Debjani and Lunga, Dalton|inproceedings|10.1145/3486640.3491391|||||||||||||||||||||||||||||1440229166|42
SIGMOD '20|Portland, OR, USA|multi-level entity matching, cluster id assignment, conflict resolution in clustering|15|2287–2301|Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data|Entity matching -- the task of clustering duplicated database records to underlying entities -- has become an increasingly critical component in modern data integration management. Amperity provides a platform for businesses to manage customer data that utilizes a machine-learning approach to entity matching, resolving billions of customer records on a daily basis. We face several challenges in deploying entity matching to industrial applications at scale, and they are less prominent in the literature. These challenges include: (1) Providing not just a single entity clustering, but supporting clusterings at multiple confidence levels to enable downstream applications with varying precision/recall trade-off needs. (2) Many customer record attributes may be systematically missing from different sources of data, creating many pairs of records in a cluster that appear to not match due to incomplete, rather than conflicting information. Allowing these records to connect transitively without introducing conflicts is invaluable to businesses because they can acquire a more comprehensive profile of their customers without incorrect entity merges. (3) How to cluster records over time and assign persistent cluster IDs that can be used for downstream use cases such as A/B tests or predictive model training; this is made more challenging by the fact that we receive new customer data every day and clusters naturally evolving over time still require persistent IDs that refer to the same entity. In this work, we describe Amperity's entity matching framework, Fusion, and how its design provides solutions to these challenges. In particular, we describe our pairwise matching model based on ordinal regression that permits a well-defined way to produce entity clusterings at different confidence levels, a novel clustering algorithm that separates conflicting record pairs in clusters while allowing for pairs that may appear dissimilar due to missing data, and a persistent ID generation algorithm which balances stability of the identifier with ever-evolving entities.|10.1145/3318464.3386143|https://doi.org/10.1145/3318464.3386143|New York, NY, USA|Association for Computing Machinery|9781450367356|2020|Entity Matching in the Wild: A Consistent and Versatile Framework to Unify Data in Industrial Applications|Yan, Yan and Meyles, Stephen and Haghighi, Aria and Suciu, Dan|inproceedings|10.1145/3318464.3386143|||||||||||||||||||||||||||||1441142776|42
ASIST '15|St. Louis, Missouri|health, forums, social question answering, big data, communities of practice|4||Proceedings of the 78th ASIS&amp;T Annual Meeting: Information Science with Impact: Research in and for the Community|Institutional data collection practices inevitably evolve over time, especially in a distributed clinical setting. Clinical and administrative data can improve health and healthcare, but only if researchers ensure that the data is well-aligned to their reuse goals and that they have adequately accounted for changes in data collection practices over time. Our goal is to understand information behaviors of health services data users as they bridge the gap between the historical data and their intended data reuse goals. This project leverages more than a decade of listserv posts related to the use of clinical and administrative data by US Department of Veterans Affairs (VA) employees, providing longitudinal insight into data reuse practices in both research and operational settings. In this paper we report the results of a pilot study that highlighted questions raised in the use of data and the knowledge engaged to answer them.|||USA|American Society for Information Science|087715547X|2015|Online Question Answering Practices to Support Healthcare Data Re-Use|Blake, Catherine and Souden, Maria and Anderson, Caryn L. and Twidale, Michael and Stelmack, Jenifer E.|inproceedings|10.5555/2857070.2857186|116||||||||||||||||||||||||||||1442484050|42
||Multimedia big data, Data acquisition, Data representation, Data reduction, Data analysis, Data security and privacy, System intelligence, Distributed system, Social media||169-195||With an exponential increase in the provisioning of multimedia devices over the Internet of Things (IoT), a significant amount of multimedia data (also referred to as multimedia big data – MMBD) is being generated. Current research and development activities focus on scalar sensor data based IoT or general MMBD and overlook the complexity of facilitating MMBD over IoT. This paper examines the unique nature and complexity of MMBD computing for IoT applications and develops a comprehensive taxonomy for MMBD abstracted into a novel process model reflecting MMBD over IoT. This process model addresses a number of research challenges associated with MMBD, such as scalability, accessibility, reliability, heterogeneity, and Quality of Service (QoS) requirements. A case study is presented to demonstrate the process model.|https://doi.org/10.1016/j.jnca.2018.09.014|https://www.sciencedirect.com/science/article/pii/S1084804518303011||||2018|Multimedia big data computing and Internet of Things applications: A taxonomy and process model|Aparna Kumari and Sudeep Tanwar and Sudhanshu Tyagi and Neeraj Kumar and Michele Maasberg and Kim-Kwang Raymond Choo|article|KUMARI2018169|||Journal of Network and Computer Applications|10848045||124|||||||||||||||||||||||1443127158|2040591560
||||1-2|||https://doi.org/10.1016/j.ejim.2018.05.019|https://www.sciencedirect.com/science/article/pii/S0953620518301961||||2018|Large databases (Big Data) and evidence-based medicine|Andrea Damiani and Graziano Onder and Vincenzo Valentini|article|DAMIANI20181|||European Journal of Internal Medicine|09536205||53|||||26617|0,894|Q2|71|384|907|8947|1904|557|2,14|23,30|Netherlands|Western Europe|1989-2020|Internal Medicine (Q2)|7,083|4.487|0.00933|1444137081|794196092
||Phylogenetic signal, mtDNA, Termite, Dictyoptera, SAMS, Rogue taxa, Long branch attraction, Signal analysis||112-122||Assessing support for molecular phylogenies is difficult because the data is heterogeneous in quality and overwhelming in quantity. Traditionally, node support values (bootstrap frequency, Bayesian posterior probability) are used to assess confidence in tree topologies. Other analyses to assess the quality of phylogenetic data (e.g. Lento plots, saturation plots, trait consistency) and the resulting phylogenetic trees (e.g. internode certainty, parameter permutation tests, topological tests) exist but are rarely applied. Here we argue that a single qualitative analysis is insufficient to assess support of a phylogenetic hypothesis and relate data quality to tree quality. We use six molecular markers to infer the phylogeny of Blattodea and apply various tests to assess relationship support, locus quality, and the relationship between the two. We use internode-certainty calculations in conjunction with bootstrap scores, alignment permutations, and an approximately unbiased (AU) test to assess if the molecular data unambiguously support the phylogenetic relationships found. Our results show higher support for the position of Lamproblattidae, high support for the termite phylogeny, and low support for the position of Anaplectidae, Corydioidea and phylogeny of Blaberoidea. We use Lento plots in conjunction with mutation-saturation plots, calculations of locus homoplasy to assess locus quality, identify long branch attraction, and decide if the tree’s relationships are the result of data biases. We conclude that multiple tests and metrics need to be taken into account to assess tree support and data robustness.|https://doi.org/10.1016/j.ympev.2018.05.007|https://www.sciencedirect.com/science/article/pii/S1055790318300186||||2018|Topological support and data quality can only be assessed through multiple tests in reviewing Blattodea phylogeny|Dominic Evangelista and France Thouzé and Manpreet Kaur Kohli and Philippe Lopez and Frédéric Legendre|article|EVANGELISTA2018112|||Molecular Phylogenetics and Evolution|10557903||128|||||||||||||||||||||||1444415490|1020979317
||Green innovation, Corporate environmental ethics, Large scale data, Human resource practices, Management commitment, Environmental and economic performance||483-498||Faced with internal and external pressure to adapt and implement environmental friendly business activities, it is becoming crucial for firms to identify practices that enhance their competitive advantage, economic, and environmental performance. Green innovation, green technologies, and the implementation of green supply chain management are examples of such practices. Green innovation and the adoption of the combination of green product innovation and green process innovation involve reduction in consumption of energy and pollution emission, recycling of wastes, sustainable utilization of resources, and green product designs. Although the extent research in this area is substantial, research on the importance of considering corporate environmental ethics, stakeholders view of green product, and demand for green products as drivers of green innovation must be conducted. Moreover, the role of large scale data, management commitment, and human resource practices play to overcome the technological challenges, achieve competitive advantage, and enhance the economic and environmental performance have yet to be addressed. This paper develops and tests a holistic model that depicts and examines the relationships among green innovation, its drivers, as well as factors that help overcome the technological challenges and influence the performance and competitive advantage of the firm. This paper is among the first works to deal with such a complex framework which considers the interrelationships among numerous constructs and their effects on competitive advantage as well as overall organizational performance. A questionnaire was designed to measure the influence of green innovation adoption/implementation and its drivers on performance and competitive advantage while taking into consideration the impact of management commitment and HR practices, as well as the use of large data on these relationships. Data collected from a sample of 215 respondents working in Middle East and North Africa (MENA) region and Golf-Cooperation Countries (GCC) were used to test the proposed relationships. The proposed model proved to be fit. The hypotheses were supported, and implications were discussed.|https://doi.org/10.1016/j.techfore.2017.12.016|https://www.sciencedirect.com/science/article/pii/S0040162517315226||||2019|Green innovation and organizational performance: The influence of big data and the moderating role of management commitment and HR practices|Abdul-Nasser El-Kassar and Sanjay Kumar Singh|article|ELKASSAR2019483|||Technological Forecasting and Social Change|00401625||144|||||14704|2,226|Q1|117|448|1099|35581|10127|1061|9,01|79,42|United States|Northern America|1970-2020|Applied Psychology (Q1); Business and International Management (Q1); Management of Technology and Innovation (Q1)|21,116|8.593|0.02416|1446162524|1949868303
||Big Data, Auditing, Accounting information systems||44-59||With corporate investment in Big Data of $34 billion in 2013 growing to $232 billion through 2016 (Gartner 2012), the Big 4 accounting firms are aiming to be at the forefront of Big Data implementations. Notably, they see Big Data as an increasingly essential part of their assurance practice. We argue that while there is a place for Big Data in auditing, its application to auditing is less clear than it is in the other fields, such as marketing and medical research. The objectives of this paper are to: (1) provide a discussion of both the inhibitors of incorporating Big Data into financial statement audits; and (3) present a research agenda to identify approaches to ameliorate those inhibitors.|https://doi.org/10.1016/j.accinf.2016.07.004|https://www.sciencedirect.com/science/article/pii/S1467089516300811||||2016|Incorporating big data in audits: Identifying inhibitors and a research agenda to address those inhibitors|Michael Alles and Glen L. Gray|article|ALLES201644|||International Journal of Accounting Information Systems|14670895||22||2015 Research Symposium on Information Integrity & Information Systems Assurance|||29806|0,897|Q1|53|21|51|1375|290|50|5,56|65,48|United States|Northern America|2000-2020|Finance (Q1); Information Systems and Management (Q1); Management Information Systems (Q1); Accounting (Q2)|1,009|4.400|5.6E-4|1446495529|818633161
https://doi.org/10.1016/j.lanwpc.2021.100110|https://www.sciencedirect.com/science/article/pii/S2666606521000195||||2021|Health-adjusted life expectancy (HALE) in Chongqing, China, 2017: An artificial intelligence and big data method estimating the burden of disease at city level|Xiaowen Ruan and Yue Li and Xiaohui Jin and Pan Deng and Jiaying Xu and Na Li and Xian Li and Yuqi Liu and Yiyi Hu and Jingwen Xie and Yingnan Wu and Dongyan Long and Wen He and Dongsheng Yuan and Yifei Guo and Heng Li and He Huang and Shan Yang and Mei Han and Bojin Zhuang and Jiang Qian and Zhenjie Cao and Xuying Zhang and Jing Xiao and Liang Xu|article|RUAN2021100110|||The Lancet Regional Health - Western Pacific|26666065||9||||||||||||||||||||||||||||||1446587860|42
ICCSIE '22|Brisbane, QLD, Australia||3|666–668|Proceedings of the 7th International Conference on Cyber Security and Information Engineering|Most traditional knowledge graph methods only learn knowledge representations from structured triples, ignoring the rich visual information in power images. To address this problem, this paper constructs a multimodal knowledge graph for power equipment defect data. The knowledge graph of power equipment images and defect texts is developed, and the InteractiveGraph graph database is used to realize knowledge storage, construct a multi-modal knowledge graph for power equipment defect data, and realize knowledge query and auxiliary decision-making.|10.1145/3558819.3565165|https://doi.org/10.1145/3558819.3565165|New York, NY, USA|Association for Computing Machinery|9781450397414|2022|Multimodal Knowledge Graph for Power Equipment Defect Data|Zhang, Tao and Ding, Jiantao and Guo, Zhiyong|inproceedings|10.1145/3558819.3565165|||||||||||||||||||||||||||||1448752390|42
||Land cover classification, Sentinel, Google Earth Engine, Big data, Remote sensing, Iran||276-288||Accurate information about the location, extent, and type of Land Cover (LC) is essential for various applications. The only recent available country-wide LC map of Iran was generated in 2016 by the Iranian Space Agency (ISA) using Moderate Resolution Imaging Spectroradiometer (MODIS) images with a considerably low accuracy. Therefore, the production of an up-to-date and accurate Iran-wide LC map using the most recent remote sensing, machine learning, and big data processing algorithms is required. Moreover, it is important to develop an efficient method for automatic LC generation for various time periods without the need to collect additional ground truth data from this immense country. Therefore, this study was conducted to fulfill two objectives. First, an improved Iranian LC map with 13 LC classes and a spatial resolution of 10 m was produced using multi-temporal synergy of Sentinel-1 and Sentinel-2 satellite datasets applied to an object-based Random forest (RF) algorithm. For this purpose, 2,869 Sentinel-1 and 11,994 Sentinel-2 scenes acquired in 2017 were processed and classified within the Google Earth Engine (GEE) cloud computing platform allowing big geospatial data analysis. The Overall Accuracy (OA) and Kappa Coefficient (KC) of the final Iran-wide LC map for 2017 was 95.6% and 0.95, respectively, indicating the considerable potential of the proposed big data processing method. Second, an efficient automatic method was developed based on Sentinel-2 images to migrate ground truth samples from a reference year to automatically generate an LC map for any target year. The OA and KC for the LC map produced for the target year 2019 were 91.35% and 0.91, respectively, demonstrating the efficiency of the proposed method for automatic LC mapping. Based on the obtained accuracies, this method can potentially be applied to other regions of interest for LC mapping without the need for ground truth data from the target year.|https://doi.org/10.1016/j.isprsjprs.2020.07.013|https://www.sciencedirect.com/science/article/pii/S0924271620302008||||2020|Improved land cover map of Iran using Sentinel imagery within Google Earth Engine and a novel automatic workflow for land cover classification using migrated training samples|Arsalan Ghorbanian and Mohammad Kakooei and Meisam Amani and Sahel Mahdavi and Ali Mohammadzadeh and Mahdi Hasanlou|article|GHORBANIAN2020276|||ISPRS Journal of Photogrammetry and Remote Sensing|09242716||167|||||29161|2,960|Q1|138|264|677|16114|7306|668|10,56|61,04|Netherlands|Western Europe|1989-2020|Atomic and Molecular Physics, and Optics (Q1); Computer Science Applications (Q1); Computers in Earth Sciences (Q1); Engineering (miscellaneous) (Q1); Geography, Planning and Development (Q1)|18,026|8.979|0.02145|1450642517|660578442
IIP'17|Bangkok, Thailand|truth discovery, weather, heterogeneous data|7||Proceedings of the 2nd International Conference on Intelligent Information Processing|In many real world applications, the same object or event may be described by multiple sources. As a result, conflicts among these sources are inevitable and these conflicts cause confusion as we have more than one value or outcome for each object. One significant problem is to resolve the confusion and to identify a piece of information which is trustworthy. This process of finding the truth from conflicting values of an object provided by multiple sources is called truth discovery or fact-finding. The main purpose of the truth discovery is to find more and more trustworthy information and reliable sources. Because the major assumption of truth discovery is on this intuitive principle, the source that provides trustworthy information is considered more reliable, and moreover, if the piece of information is from a reliable source, then it is more trustworthy. However, previously proposed truth discovery methods either do not conduct source reliability estimation at all (Voting Method), or even if they do, they do not model multiple properties of the object separately. This is the motivation for researchers to develop new techniques to tackle the problem of truth discovery in data with multiple properties. We present a method using an optimization framework which minimizes the overall weighted deviation between the truths and the multi-source observations. In this framework, different types of distance functions can be plugged in to capture the characteristics of different data types. We use weather datasets collected by four different platforms for extensive experiments and the results verify both the efficiency and precision of our methods for truth discovery.|10.1145/3144789.3144797|https://doi.org/10.1145/3144789.3144797|New York, NY, USA|Association for Computing Machinery|9781450352871|2017|Better Weather Forecasting through Truth Discovery Analysis|Zhang, Zhiqiang and Huang, Xiangbing and Iqbal, Muhammad Faisal Buland and Ye, Songtao|inproceedings|10.1145/3144789.3144797|4||||||||||||||||||||||||||||1450704062|42
ICSCA 2021|Kuala Lumpur, Malaysia|Model and Simulation, Cooperative Society, Business Intelligence, Business Forum|5|224–228|2021 10th International Conference on Software and Computer Applications|Business forums are activities between individuals and organizations that carry out the transactions on online media or within applications, which spread across countries. Along with the development of information technology towards business intelligence (BI), the business processes carried out in the business forum are modeled specifically in order to create an effort and attempt to follow the indicator and criteria from the industrial revolution 4.0. In this paper, a model is designed to combine three type of principles, namely the business forum, BI and the cooperative principle. Actually, cooperatives have been long abandoned since the existence of conventional and Islamic banking concept but it has kinship principle to divide the profits based on the size of the contribution given. Meanwhile, BI model is designed to obtain a formula from the cooperative principle, namely the residual income from operations where the transaction process is successfully implemented through the application to allocate a portion of the profits to the members based on the specified percent.|10.1145/3457784.3457820|https://doi.org/10.1145/3457784.3457820|New York, NY, USA|Association for Computing Machinery|9781450388825|2021|Model of Business Intelligence Applied the Principle of Cooperative Society in the Business Forums|Al-Khowarizmi, Al-Khowarizmi and Lubis, Muharman and Ridho Lubis, Arif and Fauzi, Fauzi and Ramadhan Nasution, Ilham|inproceedings|10.1145/3457784.3457820|||||||||||||||||||||||||||||1451593080|42
||Documentation;Data warehouses;Medical services;Big data;Databases;Cleaning;Medical diagnostic imaging;electronic health records;data quality;big data;multiple data vendors;metrics||2612-2620|2015 IEEE International Conference on Big Data (Big Data)|Currently, a large amount of data is amassed in electronic health records (EHRs). However, EHR systems are largely information silos, that is, uses of these systems are often confined to management of patient information and analytics specific to a clinician's practice. A growing trend in healthcare is combining multiple databases to support epidemiological research. The College Health Surveillance Network is the first national data warehouse containing EHR data from 31 different student health centers. Each member university contributes to the data warehouse by uploading select EHR data including patient demographics, diagnoses, and procedures to a common server on a monthly basis. In this paper, we focus on the data quality dimensions from a subsample of the data comprised of over 5.7 million patient visits for approximately 980,000 patients with 4,465 unique diagnoses from 23 of those universities. We examine the data for measures of completeness, consistency, and availability for secondary use for epidemiological research. Additionally, clinical documentation practices and EHR vendor were evaluated as potential contributors to data quality. We found that overall about 70% of the data in the data warehouse is available for secondary use, and identified clinical documentation practices that are correlated to a reduction in data quality. This suggests that automated quality control and proactive clinical documentation support could reduce ad-hoc data cleaning needs resulting in greater data availability for secondary use.|10.1109/BigData.2015.7364060|||||2015|Evaluation of data quality of multisite electronic health record data for secondary analysis|Nobles, Alicia L. and Vilankar, Ketki and Wu, Hao and Barnes, Laura E.|inproceedings|7364060||Oct|||||||||||||||||||||||||||1455123205|42
|||6|36–41|||10.1145/2641383.2641390|https://doi.org/10.1145/2641383.2641390|New York, NY, USA|Association for Computing Machinery||2014|Evaluation Methodologies in Information Retrieval Dagstuhl Seminar 13441|Agosti, Maristella and Fuhr, Norbert and Toms, Elaine and Vakkari, Pertti|article|10.1145/2641383.2641390||jun|SIGIR Forum|01635840|1|48|June 2014||||||||||||||||||||||1457013799|1301506833
ISDOC '13|Lisboa, Portugal|ERP's life cycle, ERP's, success measuring models, performance, information systems|11|16–26|Proceedings of the 2013 International Conference on Information Systems and Design of Communication|This paper addresses the problem of defining and evaluating the success of ERP throughout the life cycle of the information system. In order to solve this problem, many of the theoretical and empirical contributions on the success of the information system are analysed and discussed.This approach allows the development of a new model; especially in Delone &amp; Mclean supported research.This work will try to establish a different perspective on the success of the ERP and can be an encouragement to some organizations or the many researchers that will be engaging in these areas, in order to help achieve more clearly the expected performance in the acquisition phase of ERPs. Many times that performance does not always happen [1].|10.1145/2503859.2503863|https://doi.org/10.1145/2503859.2503863|New York, NY, USA|Association for Computing Machinery|9781450322997|2013|ERP Measure Success Model; a New Perspective|Bento, Fernando and Costa, Carlos J.|inproceedings|10.1145/2503859.2503863|||||||||||||||||||||||||||||1458862198|42
||Big data;Benchmark testing;Quality assessment;Databases;Software;Sparks;Engines;Big Data;Quality assessment;stream processing;survey;Big Data frameworks||1-4|2016 24th Telecommunications Forum (TELFOR)|Big Data refers to data volumes in the range of Exabyte (1018) and beyond. Such volumes exceed the capacity of current on-line storage and processing systems. With characteristics like volume, velocity and variety big data throws challenges to the traditional IT establishments. Computer assisted innovation, real time data analytics, customer-centric business intelligence, industry wide decision making and transparency are possible advantages, to mention few, of Big Data. There are many issues with Big Data that warrant quality assessment methods. The issues are pertaining to storage and transport, management, and processing. This paper throws light into the present state of quality issues related to Big Data. It provides valuable insights that can be used to leverage Big Data science activities.|10.1109/TELFOR.2016.7818902|||||2016|Big data and quality: A literature review|Lakshen, Guma Abdulkhader and Vraneš, Sanja and Janev, Valentina|inproceedings|7818902||Nov|||||||||||||||||||||||||||1459501157|42
SimAUD '15|Alexandria, Virginia|expert systems, design, SWIRL, SPIN, RIF, verification, performance, ontological knowledge engine, green scale tool, OWL, HCI, knowledge based rules, big data, algorithms, SPARAQL, standardization, semantic web, reliability, machine learning, linked data, sustainable data, REST, PyKE, experimentation|8|67–74|Proceedings of the Symposium on Simulation for Architecture &amp; Urban Design|"\"The era of \"\"Big Data\"\" presents new challenges and opportunities to impact how the built environment is designed and constructed. Modern design tools and material databases should be more scalable, reliable, and accessible to take full advantage of the quantity of available building data. New approaches providing well-structured information can lead to robust decision support for architectural simulations earlier in the design process; rule-based decision engines and knowledge bases are the link between current data and useful decision frameworks. Integrating distributed API-based systems means that material data silos existing in modern tools can become enriched and extensible for future use with additional data from building documents, other databases, and the minds of design professionals. The PyKE rules engine extension to the Green Scale (GS) Tool improves material searches, creates the opportunity for incorporating additional rules via a REST interface, and enables integration with the Semantic Web via Linked Data principles.\""|||San Diego, CA, USA|Society for Computer Simulation International|9781510801042|2015|Capturing an Architectural Knowledge Base Utilizing Rules Engine Integration for Energy and Environmental Simulations|Ferguson, Holly T. and Vardeman, Charles F. and Buccellato, Aimee P. C.|inproceedings|10.5555/2873021.2873031|||||||||||||||||||||||||||||1459886931|42
ICEGOV '13|Seoul, Republic of Korea|open governance, open engagement, public value, open assets, open services, government as a platform|10|77–86|Proceedings of the 7th International Conference on Theory and Practice of Electronic Governance|This experience paper is a personal thinkpiece which outlines many of the main issues and discussions taking place in Europe and elsewhere about the future of the public sector and how it can respond positively to some of the acute challenges it faces in light of the financial crisis and other global challenges. The paper examines how ICT-enabled public sector innovation highlights concepts like open governance, public value, government as a platform, open assets, open services and open engagement. It develops a vision of an 'open governance framework', moving beyond 'new public management', based on ICT-enabled societal-wide collaboration. It recognises that although the public sector can in principle create public value on its own, its potential to do so is greatly enhanced and extended by direct cooperation with other actors, or by facilitating public value creation by other actors on their own. It also examines the role of bottom-up innovation and public policy experimentation, as well as the need to focus on empowering civil servants and changing public sector working practices and mindsets.|10.1145/2591888.2591901|https://doi.org/10.1145/2591888.2591901|New York, NY, USA|Association for Computing Machinery|9781450324564|2013|ICT-Enabled Public Sector Innovation: Trends and Prospects|Millard, Jeremy|inproceedings|10.1145/2591888.2591901|||||||||||||||||||||||||||||1460791518|42
CHI '22|New Orleans, LA, USA|persuasive technology, affect, physical activity, attitude change, data storytelling, narrative visualization, solution, personality, Data Video, actionable|22||Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems|Data Videos (DVs), or animated infographics that tell stories with data, are becoming increasingly popular. Despite their potential to induce attitude change, little is explored about how to produce effective DVs. This paper describes two studies that explored factors linked to the potential of health DVs to improve viewers’ behavioural change intentions. We investigated: 1) how viewers’ affect is linked to their behavioural change intentions; 2) how these affect are linked to the viewers’ personality traits; 3) which attributes of DVs are linked to their persuasive potential. Results from both studies indicated that viewers’ negative affect lowered their behavioural change intentions. Individuals with higher neuroticism exhibited higher negative affect and were harder to convince. Finally, Study 2 proved that providing any solutions to the health problem, presented in the DV, made the viewers perceive the videos as more actionable while lowering their negative affect, and importantly, induced higher behavioural change intentions.|10.1145/3491102.3517727|https://doi.org/10.1145/3491102.3517727|New York, NY, USA|Association for Computing Machinery|9781450391573|2022|Towards Design Guidelines for Effective Health-Related Data Videos: An Empirical Investigation of Affect, Personality, and Video Content|Sallam, Samar and Sakamoto, Yumiko and Leboe-McGowan, Jason and Latulipe, Celine and Irani, Pourang|inproceedings|10.1145/3491102.3517727|342||||||||||||||||||||||||||||1461844082|42
||Cross-language information retrieval, Optical character recognition, Embedded applications||103928||Cross-Language Information Retrieval (CLIR) the purpose of another language (target language), a collection of documents written question from one language (source language).CLIR employees publish documents based on user queries, dictionary translation, machine translation methods, and promotion problems. Through various detection methods and translation, this word applies to single words, translation, transliteration of names, including transcription and translation and disambiguation. CLIR Recovery in Question Semantics Technology is the most appropriate) Translation to retrieve documents (dictionary related method) based on English Question Concentrations and Question translation. To the proposed model of translating Arabic to English, and provides the high accuracy Optical Character Recognition (OCR) errors in handling orthography, expanding outside and transliteration dubbed gives higher accuracy in resolving ambiguities. Thus, the single question expands with additional meanings and related words that improve significantly with semantic input. However, the documents related to the questions recover those cross language boundaries. The development of large data systems is completely different from the actual goal of small (traditional, structured) data system development. When the in-depth learning technology is developed, face some space and environmental barriers in different laboratory environments. To describe the requirements when running embedded applications on computers for deep learning.|https://doi.org/10.1016/j.micpro.2021.103928|https://www.sciencedirect.com/science/article/pii/S0141933121001071||||2021|Search query of English translation text based on embedded system and big data|Zhihong Li|article|LI2021103928|||Microprocessors and Microsystems|01419331||82|||||15552|0,323|Q3|38|462|428|12804|962|423|2,34|27,71|Netherlands|Western Europe|1978-2020|Artificial Intelligence (Q3); Computer Networks and Communications (Q3); Hardware and Architecture (Q3); Software (Q3)|1,490|1.525|0.00207|1461911617|1912058844
||Big data;Organizations;Conferences;Social network services;Electronic mail;Databases;Big Data;Quality;Volume;Variety;Velocity||1135-1140|2016 3rd International Conference on Computing for Sustainable Global Development (INDIACom)|Big Data approach uses an empirical process that does not lie on the understanding of underlying mechanisms, but lies on the observation of facts. Achieving high quality in Big Data is a critical issue for both the database researchers and practitioners. More explicit consideration must be given to data quality since data increasingly outlives the application for which it was initially designed. In this paper, identification of quality parameters is done which are compatible to the 3V's of big data which will further provide enhancement in achieving quality data to be stored in the repository. Good utilization of Big Data strengthens the performance and competitiveness of the firms by enabling better and faster results to its customer needs. In this paper GQM (Goal Question Metric) methodology is proposed to measure quality using metrics. It describes how to include data quality metrics to project, progress and maintain levels of quality in an organization. It helps to make a decision whether or not our current data satisfies our quality prospects.||||||2016|Identification of quality parameters associated with 3V's of Big Data|Aggarwal, Ankur|inproceedings|7724441||March|||||||||||||||||||||||||||1464382506|42
|||16|2067–2082||Automatic speech recognition (ASR) technologies have been significantly advanced in the past few decades. However, recognition of overlapped speech remains a highly challenging task to date. To this end, multi-channel microphone array data are widely used in current ASR systems. Motivated by the invariance of visual modality to acoustic signal corruption and the additional cues they provide to separate the target speaker from the interfering sound sources, this paper presents an audio-visual multi-channel based recognition system for overlapped speech. It benefits from a tight integration between a speech separation front-end and recognition back-end, both of which incorporate additional video input. A series of audio-visual multi-channel speech separation front-end components based on <italic>TF masking</italic>, <italic>Filter&amp;Sum</italic> and <italic>mask-based MVDR</italic> neural channel integration approaches are developed. To reduce the error cost mismatch between the separation and the recognition components, the entire system is jointly fine-tuned using a multi-task criterion interpolation of the scale-invariant signal to noise ratio (Si-SNR) with either the connectionist temporal classification (CTC), or lattice-free maximum mutual information (LF-MMI) loss function. Experiments suggest that: the proposed audio-visual multi-channel recognition system outperforms the baseline audio-only multi-channel ASR system by up to 8.04% (31.68% relative) and 22.86% (58.51% relative) absolute WER reduction on overlapped speech constructed using either simulation or replaying of the LRS2 dataset respectively. Consistent performance improvements are also obtained using the proposed audio-visual multi-channel recognition system when using occluded video input with the lip region randomly covered up to 60%.|10.1109/TASLP.2021.3078883|https://doi.org/10.1109/TASLP.2021.3078883||IEEE Press||2021|Audio-Visual Multi-Channel Integration and Recognition of Overlapped Speech|Yu, Jianwei and Zhang, Shi-Xiong and Wu, Bo and Liu, Shansong and Hu, Shoukang and Geng, Mengzhe and Liu, Xunying and Meng, Helen and Yu, Dong|article|10.1109/TASLP.2021.3078883||may|IEEE/ACM Trans. Audio, Speech and Lang. Proc.|23299290||29|2021||||||||||||||||||||||1464662675|1905056144
||Online healthcare communities, Physician identifying, Signaling theory, Machine learning, Topic modeling, Multi-criterion analysis||65-75||Patients face difficulties identifying appropriate doctors owing to the sizeable quantity and uneven quality of information in online healthcare communities. In studying physician searches, researchers often focus on expertise similarity matches and sentiment analyses of reviews. However, the quality is often ignored. To address patients' information needs holistically, we propose a four-dimensional IT framework based on signaling theory. The model takes expertise knowledge, online reviews, profile descriptions (e.g., hospital reputation, number of patients, city) and service quality (e.g., response speed, interaction frequency, cost) as signals that distinguish high-quality physicians. It uses machine learning approaches to derive similarity matches and sentiment analysis. It also measures the relative importance of the signals by multi-criterion analysis and derives the physician rankings through the aggregated scores. Our study revealed that the proposed approach performs better compared with the other two recommend techniques. This research expands the boundary of signaling theory to healthcare management and enriches the literature on IT use and inter-organizational systems. The proposed IT model may improve patient care, alleviate the physician-patient relationship and reduce lawsuits against hospitals; it also has practical implications for healthcare management.|https://doi.org/10.1016/j.ijinfomgt.2019.01.005|https://www.sciencedirect.com/science/article/pii/S026840121830834X||||2019|A hybrid IT framework for identifying high-quality physicians using big data analytics|Yan Ye and Yang Zhao and Jennifer Shang and Liyi Zhang|article|YE201965|||International Journal of Information Management|02684012||47|||||15631|2,770|Q1|114|224|382|20318|5853|373|16,16|90,71|United Kingdom|Western Europe|1970, 1986-2021|Artificial Intelligence (Q1); Computer Networks and Communications (Q1); Information Systems (Q1); Information Systems and Management (Q1); Library and Information Sciences (Q1); Management Information Systems (Q1); Marketing (Q1)|12,245|14.098|0.01167|1465801676|747927863
||Data Quality, Big Data, Measurement, Quality-in-Use, Model||123-130||Beyond the hype of Big Data, something within business intelligence projects is indeed changing. This is mainly because Big Data is not only about data, but also about a complete conceptual and technological stack including raw and processed data, storage, ways of managing data, processing and analytics. A challenge that becomes even trickier is the management of the quality of the data in Big Data environments. More than ever before the need for assessing the Quality-in-Use gains importance since the real contribution–business value–of data can be only estimated in its context of use. Although there exists different Data Quality models for assessing the quality of regular data, none of them has been adapted to Big Data. To fill this gap, we propose the “3As Data Quality-in-Use model”, which is composed of three Data Quality characteristics for assessing the levels of Data Quality-in-Use in Big Data projects: Contextual Adequacy, Operational Adequacy and Temporal Adequacy. The model can be integrated into any sort of Big Data project, as it is independent of any pre-conditions or technologies. The paper shows the way to use the model with a working example. The model accomplishes every challenge related to Data Quality program aimed for Big Data. The main conclusion is that the model can be used as an appropriate way to obtain the Quality-in-Use levels of the input data of the Big Data analysis, and those levels can be understood as indicators of trustworthiness and soundness of the results of the Big Data analysis.|https://doi.org/10.1016/j.future.2015.11.024|https://www.sciencedirect.com/science/article/pii/S0167739X15003817||||2016|A Data Quality in Use model for Big Data|Jorge Merino and Ismael Caballero and Bibiano Rivas and Manuel Serrano and Mario Piattini|article|MERINO2016123|||Future Generation Computer Systems|0167739X||63||Modeling and Management for Big Data Analytics and Visualization|||12264|1,262|Q1|119|798|1935|36054|16877|1868|9,11|45,18|Netherlands|Western Europe|1984-2021|Computer Networks and Communications (Q1); Hardware and Architecture (Q1); Software (Q1)||||1466153335|562237118
ICISE 2021|Shanghai, China|Digital transformation, IoT, Intelligent IoT management system|5|84–88|2021 the 6th International Conference on Information Systems Engineering|Internet of things technology, as the core technology in digital transformation, helps enterprises in digital transformation to carry out comprehensive perception, intelligent management and secure transmission. Following the information architecture of State Grid Corporation of China and combined with the business objectives and Strategies of electric power company, carry out the differentiated design of power Internet of things architecture, put forward the improvement direction of power Internet of things architecture, clarify the application scenario and future evolution route of power Internet of things business, and ensure the realization of technology.|10.1145/3503928.3503944|https://doi.org/10.1145/3503928.3503944|New York, NY, USA|Association for Computing Machinery|9781450385220|2022|Design and Research of IoT Management Architecture for Power Grid Enterprises Based on Digital Transformation: Application of IoT in Power Grid Enterprises According to Enterprise Architecture Method|Wu, Ji and Zhou, Ming and Xu, Min and Zhang, Jin and Wu, Yue and Zha, Weiwei and Zhang, Chengping|inproceedings|10.1145/3503928.3503944|||||||||||||||||||||||||||||1467089288|42
LAK19|Tempe, AZ, USA|Large lectures, quantified self, mobile application, learning analytics, live feedback|5|426–430|Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge|Learning Analytics (LA) research has demonstrated the potential of LA in detecting and monitoring cognitive-affective parameters and improving student success. But most of it has been applied to online and computerized learning environments whereas physical classrooms have largely remained outside the scope of such research. This paper attempts to bridge that gap by proposing a student feedback model in which they report on the difficult/easy and engaging/boring aspects of their lecture. We outline the pedagogical affordances of an aggregated time-series of such data and discuss it within the context of LA research.|10.1145/3303772.3303821|https://doi.org/10.1145/3303772.3303821|New York, NY, USA|Association for Computing Machinery|9781450362566|2019|DEBE Feedback for Large Lecture Classroom Analytics|Mitra, Ritayan and Chavan, Pankaj|inproceedings|10.1145/3303772.3303821|||||||||||||||||||||||||||||1468546307|42
||decision making, information disclosure, concerns, direct-to-consumer genetic testing, security, information privacy, privacy, rational, heuristics|26|||This study explores privacy concerns perceived by people with respect to having their DNA tested by direct-to-consumer (DTC) genetic testing companies such as 23andMe and Ancestry.com. Data collected from 510 respondents indicate that those who have already obtained a DTC genetic test have significantly lower levels of privacy and security concerns than those who have not obtained a DTC genetic test. Qualitative data from respondents of both these groups show that the concerns are mostly similar. However, the factors perceived to alleviate privacy concerns are more varied and nuanced amongst those who have obtained a DTC genetic test. Our data suggest that privacy concerns or lack of concerns are based on complex and multiple considerations including data ownership, access control of data and regulatory authorities of social, political and legal systems. Respondents do not engage in a full cost/benefit analysis of having their DNA tested.|10.1145/3492838|https://doi.org/10.1145/3492838|New York, NY, USA|Association for Computing Machinery||2022|Do I Spit or Do I Pass? Perceived Privacy and Security Concerns of Direct-to-Consumer Genetic Testing|Grandhi, Sukeshini A. and Plotnick, Linda|article|10.1145/3492838|19|jan|Proc. ACM Hum.-Comput. Interact.||GROUP|6|January 2022||||||||||||||||||||||1469828531|42
||Digital agriculture, Smart farming, Precision agriculture, Accuracy, Big data||623-632||The myriad potential benefits of digital farming hinge on the promise of increased accuracy, which allows ‘doing more with less’ through precise, data-driven operations. Yet, precision farming's foundational claim of increased accuracy has hardly been the subject of comprehensive examination. Drawing on social science studies of big data, this article examines digital agriculture's (in)accuracies and their repercussions. Based on an examination of the daily functioning of the various components of yield mapping, it finds that digital farming is often ‘precisely inaccurate’, with the high volume and granularity of big data erroneously equated with high accuracy. The prevailing discourse of ‘ultra-precise’ digital technologies ignores farmers' essential efforts in making these technologies more accurate, via calibration, corroboration and interpretation. We suggest that there is the danger of a ‘precision trap’. Namely, an exaggerated belief in the precision of big data that over time leads to an erosion of checks and balances (analogue data, farmer observation et cetera) on farms. The danger of ‘precision traps’ increases with the opacity of algorithms, with shifts from real-time measurement and advice towards forecasting, and with farmers' increased remoteness from field operations. Furthermore, we identify an emerging ‘precision divide’: unequally distributed precision benefits resulting from the growing algorithmic divide between farmers focusing on staple crops, catered well by technological innovation on the one hand, and farmers cultivating other crops, who have to make do with much less advanced or applicable algorithms on the other. Consequently, for the latter farms digital farming may feel more like ‘imprecision farming’.|https://doi.org/10.1016/j.jrurstud.2021.07.024|https://www.sciencedirect.com/science/article/pii/S0743016721002217||||2021|Imprecision farming? Examining the (in)accuracy and risks of digital agriculture|Oane Visser and Sarah Ruth Sippel and Louis Thiemann|article|VISSER2021623|||Journal of Rural Studies|07430167||86|||||15673|1,497|Q1|104|280|573|20744|2862|557|4,43|74,09|United Kingdom|Western Europe|1985-2020|Development (Q1); Forestry (Q1); Geography, Planning and Development (Q1); Sociology and Political Science (Q1)|9,142|4.849|0.01005|1471269124|256754271
||Big data, Economic forecasting, Data mining, Spatio-temporal||10-12||The development of big data generation, acquisition, storage, processing, and other technologies has greatly enriched our sensory world and fundamentally changed the basis of traditional economic and financial forecasting. Unexpected events in the economic and financial fields challenge our confidence in the performance of forecasting models. Obviously, the big data-driven decision theories and analysis methods are different from the traditional methods. In view of the important role of big data-driven economic and financial forecasting in social stability, innovative development, and sustainability, the research frontiers of big data-driven economic and financial forecasting in the future include: feature mining of complex economic systems with big data representation; accurate real-time correction of theories and methods of dynamic forecasting and early warning; general paradigm of big data forecasting research; formation and process of big data-driven economic and financial system management mechanism, etc. Systematic research on such issues will contribute to the formation of decision-making theories and research systems in the context of big data, thus improving the adaptability and scientificity of management decisions.|https://doi.org/10.1016/j.dsm.2021.01.001|https://www.sciencedirect.com/science/article/pii/S2666764921000011||||2021|An interview with Shouyang Wang: research frontier of big data-driven economic and financial forecasting|Shouyang Wang|article|WANG202110|||Data Science and Management|26667649|1|1|||||||||||||||||||||||1471698249|1164581282
||Big data adoption, Technology–Organization–Environment, Diffusion of Innovations||102095||Big data adoption is a process through which businesses find innovative ways to enhance productivity and predict risk to satisfy customers need more efficiently. Despite the increase in demand and importance of big data adoption, there is still a lack of comprehensive review and classification of the existing studies in this area. This research aims to gain a comprehensive understanding of the current state-of-the-art by highlighting theoretical models, the influence factors, and the research challenges of big data adoption. By adopting a systematic selection process, twenty studies were identified in the domain of big data adoption and were reviewed in order to extract relevant information that answers a set of research questions. According to the findings, Technology–Organization–Environment and Diffusion of Innovations are the most popular theoretical models used for big data adoption in various domains. This research also revealed forty-two factors in technology, organization, environment, and innovation that have a significant influence on big data adoption. Finally, challenges found in the current research about big data adoption are represented, and future research directions are recommended. This study is helpful for researchers and stakeholders to take initiatives that will alleviate the challenges and facilitate big data adoption in various fields.|https://doi.org/10.1016/j.ipm.2019.102095|https://www.sciencedirect.com/science/article/pii/S0306457319301773||||2019|Big data adoption: State of the art and research challenges|Maria Ijaz Baig and Liyana Shuib and Elaheh Yadegaridehkordi|article|BAIG2019102095|||Information Processing & Management|03064573|6|56|||||||||||||||||||||||1472225854|1769516999
||||e575-e576|||https://doi.org/10.1016/j.avsg.2020.04.022|https://www.sciencedirect.com/science/article/pii/S0890509620303411||||2020|Artificial Intelligence in Vascular Surgery: Moving from Big Data to Smart Data|Fabien Lareyre and Cédric Adam and Marion Carrier and Juliette Raffort|article|LAREYRE2020e575|||Annals of Vascular Surgery|08905096||67|||||||||||||||||||||||1475052327|94817437
||||A1-A7||We argue that there are major, persistent numerical data quality issues in IS academic research. These issues undermine the ability to replicate our research – a critical element of scientific investigation and analysis. In IS empirical and analytics research articles, the amount of space devoted to the details of data collection, validation, and/or quality pales in comparison to the space devoted to the evaluation and selection of relatively sophisticated model form(s) and estimation technique(s). Yet erudite modeling and estimation can yield no immediate value or be meaningfully replicated without high quality data inputs. The purpose of this paper is: 1) to detail potential quality issues with data types currently used in IS research, and 2) to start a wider and deeper discussion of data quality in IS research. No data type is inherently of low quality and no data type guarantees high quality. As researchers, our empirical research must always address data quality issues and provide the information necessary to determine What, When, Where, How, Who, and Which.|https://doi.org/10.1016/j.dss.2018.10.007|https://www.sciencedirect.com/science/article/pii/S0167923618301647||||2018|Numerical data quality in IS research and the implications for replication|James R. Marsden and David E. Pingry|article|MARSDEN2018A1|||Decision Support Systems|01679236||115|||||21933|1,564|Q1|151|115|342|7152|2672|338|7,04|62,19|Netherlands|Western Europe|1985-2020|Arts and Humanities (miscellaneous) (Q1); Developmental and Educational Psychology (Q1); Information Systems (Q1); Information Systems and Management (Q1); Management Information Systems (Q1)|13,580|5.795|0.0081|1475719828|1234879127
CIKM '18|Torino, Italy|data loss, experimentation trustworthiness, online controlled experiments, telemetry loss, ab testing, client experimentation|10|387–396|Proceedings of the 27th ACM International Conference on Information and Knowledge Management|Failure to accurately measure the outcomes of an experiment can lead to bias and incorrect conclusions. Online controlled experiments (aka AB tests) are increasingly being used to make decisions to improve websites as well as mobile and desktop applications. We argue that loss of telemetry data (during upload or post-processing) can skew the results of experiments, leading to loss of statistical power and inaccurate or erroneous conclusions. By systematically investigating the causes of telemetry loss, we argue that it is not practical to entirely eliminate it. Consequently, experimentation systems need to be robust to its effects. Furthermore, we note that it is nontrivial to measure the absolute level of telemetry loss in an experimentation system. In this paper, we take a top-down approach towards solving this problem. We motivate the impact of loss qualitatively using experiments in real applications deployed at scale, and formalize the problem by presenting a theoretical breakdown of the bias introduced by loss. Based on this foundation, we present a general framework for quantitatively evaluating the impact of telemetry loss, and present two solutions to measure the absolute levels of loss. This framework is used by well-known applications at Microsoft, with millions of users and billions of sessions. These general principles can be adopted by any application to improve the overall trustworthiness of experimentation and data-driven decision making.|10.1145/3269206.3271747|https://doi.org/10.1145/3269206.3271747|New York, NY, USA|Association for Computing Machinery|9781450360142|2018|Trustworthy Experimentation Under Telemetry Loss|Gupchup, Jayant and Hosseinkashi, Yasaman and Dmitriev, Pavel and Schneider, Daniel and Cutler, Ross and Jefremov, Andrei and Ellis, Martin|inproceedings|10.1145/3269206.3271747|||||||||||||||||||||||||||||1476434186|42
||incremental approach, Group level, popularity prediction, tensor analysis, information diffusion, online social networks|26|||Predicting the popularity of web contents in online social networks is essential for many applications. However, existing works are usually under non-incremental settings. In other words, they have to rebuild models from scratch when new data occurs, which are inefficient in big data environments. It leads to an urgent need for incremental prediction, which can update previous results with new data and conduct prediction incrementally. Moreover, the promising direction of group-level popularity prediction has not been well treated, which explores fine-grained information while keeping a low cost. To this end, we identify the problem of incremental group-level popularity prediction, and propose a novel model IGPP to address it. We first predict the group-level popularity incrementally by exploiting the incremental CANDECOMP/PARAFCAC (CP) tensor decomposition algorithm. Then, to reduce the cumulative error by incremental prediction, we propose three strategies to restart the CP decomposition. To the best of our knowledge, this is the first work that identifies and solves the problem of incremental group-level popularity prediction. Extensive experimental results show significant improvements of the IGPP method over other works both in the prediction accuracy and the efficiency.|10.1145/3461839|https://doi.org/10.1145/3461839|New York, NY, USA|Association for Computing Machinery||2021|Incremental Group-Level Popularity Prediction in Online Social Networks|Wang, Jingjing and Jiang, Wenjun and Li, Kenli and Wang, Guojun and Li, Keqin|article|10.1145/3461839|20|sep|ACM Trans. Internet Technol.|15335399|1|22|February 2022||||||||||||||||||||||1478917118|314651938
||Data quality, offline and online cleaning, statistical data cleaning|30|||Recent efforts in data cleaning of structured data have focused exclusively on problems like data deduplication, record matching, and data standardization; none of the approaches addressing these problems focus on fixing incorrect attribute values in tuples. Correcting values in tuples is typically performed by a minimum cost repair of tuples that violate static constraints like Conditional Functional Dependencies (which have to be provided by domain experts or learned from a clean sample of the database). In this article, we provide a method for correcting individual attribute values in a structured database using a Bayesian generative model and a statistical error model learned from the noisy database directly. We thus avoid the necessity for a domain expert or clean master data. We also show how to efficiently perform consistent query answering using this model over a dirty database, in case write permissions to the database are unavailable. We evaluate our methods over both synthetic and real data.|10.1145/2992787|https://doi.org/10.1145/2992787|New York, NY, USA|Association for Computing Machinery||2016|BayesWipe: A Scalable Probabilistic Framework for Improving Data Quality|De, Sushovan and Hu, Yuheng and Meduri, Venkata Vamsikrishna and Chen, Yi and Kambhampati, Subbarao|article|10.1145/2992787|5|oct|J. Data and Information Quality|19361955|1|8|November 2016||||||||||||||||||||||1480522969|833754770
||||139-147||Hyper-resolution datasets for urban flooding are rare. This problem prevents detailed flooding risk analysis, urban flooding control, and the validation of hyper-resolution numerical models. We employed social media and crowdsourcing data to address this issue. Natural Language Processing and Computer Vision techniques are applied to the data collected from Twitter and MyCoast (a crowdsourcing app). We found these big data based flood monitoring approaches can complement the existing means of flood data collection. The extracted information is validated against precipitation data and road closure reports to examine the data quality. The two data collection approaches are compared and the two data mining methods are discussed. A series of suggestions is given to improve the data collection strategy.|https://doi.org/10.1016/j.cageo.2017.11.008|https://www.sciencedirect.com/science/article/pii/S009830041730609X||||2018|Hyper-resolution monitoring of urban flooding with social media and crowdsourcing data|Ruo-Qian Wang and Huina Mao and Yuan Wang and Chris Rae and Wesley Shaw|article|WANG2018139|||Computers & Geosciences|00983004||111|||||110327|0,936|Q1|123|143|498|6497|1923|491|3,62|45,43|United Kingdom|Western Europe|1975-2020|Computers in Earth Sciences (Q1); Information Systems (Q1)||||1481541700|429665855
||Managerial accounting, Business analytics, Big data, Enterprise systems, Business intelligence||29-44||The nature of management accountants' responsibility is evolving from merely reporting aggregated historical value to also including organizational performance measurement and providing management with decision related information. Corporate information systems such as enterprise resource planning (ERP) systems have provided management accountants with both expanded data storage power and enhanced computational power. With big data extracted from both internal and external data sources, management accountants now could utilize data analytics techniques to answer the questions including: what has happened (descriptive analytics), what will happen (predictive analytics), and what is the optimized solution (prescriptive analytics). However, research shows that the nature and scope of managerial accounting has barely changed and that management accountants employ mostly descriptive analytics, some predictive analytics, and a bare minimum of prescriptive analytics. This paper proposes a Managerial Accounting Data Analytics (MADA) framework based on the balanced scorecard theory in a business intelligence context. MADA provides management accountants the ability to utilize comprehensive business analytics to conduct performance measurement and provide decision related information. With MADA, three types of business analytics (descriptive, predictive, and prescriptive) are implemented into four corporate performance measurement perspectives (financial, customer, internal process, and learning and growth) in an enterprise system environment. Other related issues that affect the successful utilization of business analytics within a corporate-wide business intelligence (BI) system, such as data quality and data integrity, are also discussed. This paper contributes to the literature by discussing the impact of business analytics on managerial accounting from an enterprise systems and BI perspective and by providing the Managerial Accounting Data Analytics (MADA) framework that incorporates balanced scorecard methodology.|https://doi.org/10.1016/j.accinf.2017.03.003|https://www.sciencedirect.com/science/article/pii/S1467089517300490||||2017|Impact of business analytics and enterprise systems on managerial accounting|Deniz Appelbaum and Alexander Kogan and Miklos Vasarhelyi and Zhaokai Yan|article|APPELBAUM201729|||International Journal of Accounting Information Systems|14670895||25|||||29806|0,897|Q1|53|21|51|1375|290|50|5,56|65,48|United States|Northern America|2000-2020|Finance (Q1); Information Systems and Management (Q1); Management Information Systems (Q1); Accounting (Q2)|1,009|4.400|5.6E-4|1482787358|818633161
||Big data, Platforms, Technology adoption, Corporate group behaviour, Social network analysis||101570||Due to the importance of big data technology in decision-making, production and service provision, enterprises have adopted various big data technologies and platforms to improve their operational efficiency. However, the number of enterprises that have adopted big data is not promising. The purpose of this study is to explore the current status of big data adoption by Chinese enterprises and to reveal the possible factors that hinder big data adoption from the group behaviour network perspective. Based on a real case survey of 54 big data platforms (BDPs), four types of networks—i.e., the enterprise-platform network, enterprise network, platform network and industry similarity and difference (ISD) network—are constructed and analysed on the basis of social network analysis (SNA). This study finds that among Chinese enterprises, the level and scope of big data adoption are generally low and are imbalanced among industries; the cognitive level and adoption behaviour of enterprises on BDPs are inconsistent, the compatibility of BDPs is different, and the density and distance-based cohesion of networks are weak; although the current big data adoption behaviours of Chinese enterprises have formed some structural features, core-periphery structures and maximal complete cliques are found, and the current network structure has little impact on individual enterprises and platforms; enterprises in the same industry prefer to adopt the same kind of big data technology or platform. Based on these findings, several strategies and suggestions to improve big data adoption are provided.|https://doi.org/10.1016/j.techsoc.2021.101570|https://www.sciencedirect.com/science/article/pii/S0160791X21000452||||2021|Modelling and analysis of big data platform group adoption behaviour based on social network analysis|Zhimei Lei and Yandan Chen and Ming K. Lim|article|LEI2021101570|||Technology in Society|0160791X||65|||||18676|0,819|Q1|51|201|224|14468|1056|222|4,75|71,98|United Kingdom|Western Europe|1979-2020|Business and International Management (Q1); Education (Q1); Human Factors and Ergonomics (Q1); Sociology and Political Science (Q1)|2,735|4.192|0.00214|1487224643|1856981556
||Solid modeling;Data integrity;Volume measurement;Pipelines;Big Data;Data models;Software;Big Data quality characteristics;The V’s;Big Data Quality Measurement Framework;Big Data Pipelines||1579-1586|2021 IEEE 45th Annual Computers, Software, and Applications Conference (COMPSAC)|Big Data is quickly becoming a chief part of the decision-making process in both industry and academia. As more and more institutions begin relying on Big Data to make strategic decisions, the quality of the underlying data comes into question. The quality of Big Data isn’t always transparent and large-scale systems may even lack its visibility, which adversely affects the credibility of the Big Data systems. Continuous monitoring and measurement of data quality is therefore paramount in assessing whether the information can serve its purpose in a particular context (such as Big Data analytics, for example). This research addresses the need for Big Data quality measurement modeling and automation by proposing a novel conceptual quality measurement framework for Big Data (MEGA) with the purpose of assessing the underlying quality characteristics of Big Data (also known as the V’s of Big Data) at each step of the Big Data Pipelines. The theoretical quality measurement models for four of the Big Data V’s (Volume, Variety, Velocity, Veracity) are currently automated; the remaining 6 V’s (Vincularity, Validity, Value, Volatility, Valence and Vitality) will be tackled in our future work. The approach is illustrated on a case study.|10.1109/COMPSAC51774.2021.00235|||||2021|Toward a Novel Measurement Framework for Big Data (MEGA)|Bhardwaj, Dave and Ormandjieva, Olga|inproceedings|9529590||July||07303157|||||||18705|0,216|-|47|0|908|0|1110|834|1,23|0,00|United States|Northern America|1979-2019|Computer Science Applications; Software||||1487468279|838284539
||big data, correlation of cloud logs, cloud log forensics, Cloud computing, integrity, authenticity, confidentiality|42|||Cloud log forensics (CLF) mitigates the investigation process by identifying the malicious behavior of attackers through profound cloud log analysis. However, the accessibility attributes of cloud logs obstruct accomplishment of the goal to investigate cloud logs for various susceptibilities. Accessibility involves the issues of cloud log access, selection of proper cloud log file, cloud log data integrity, and trustworthiness of cloud logs. Therefore, forensic investigators of cloud log files are dependent on cloud service providers (CSPs) to get access of different cloud logs. Accessing cloud logs from outside the cloud without depending on the CSP is a challenging research area, whereas the increase in cloud attacks has increased the need for CLF to investigate the malicious activities of attackers. This paper reviews the state of the art of CLF and highlights different challenges and issues involved in investigating cloud log data. The logging mode, the importance of CLF, and cloud log-as-a-service are introduced. Moreover, case studies related to CLF are explained to highlight the practical implementation of cloud log investigation for analyzing malicious behaviors. The CLF security requirements, vulnerability points, and challenges are identified to tolerate different cloud log susceptibilities. We identify and introduce challenges and future directions to highlight open research areas of CLF for motivating investigators, academicians, and researchers to investigate them.|10.1145/2906149|https://doi.org/10.1145/2906149|New York, NY, USA|Association for Computing Machinery||2016|Cloud Log Forensics: Foundations, State of the Art, and Future Directions|Khan, Suleman and Gani, Abdullah and Wahab, Ainuddin Wahid Abdul and Bagiwa, Mustapha Aminu and Shiraz, Muhammad and Khan, Samee U. and Buyya, Rajkumar and Zomaya, Albert Y.|article|10.1145/2906149|7|may|ACM Comput. Surv.|03600300|1|49|March 2017||||23038|2,079|Q1|163|112|365|15859|6978|365|19,16|141,60|United States|Northern America|1969-2020|Computer Science (miscellaneous) (Q1); Theoretical Computer Science (Q1)|10,790|10.282|0.01438|1490982612|1517405264
||||103107||There is a rapid increase in the adoption of emerging technologies like the Internet of Things (IoT), Unmanned Aerial Vehicles (UAV), Internet of Underground Things (IoUT), Data analytics in the agriculture domain to meet the increased food demand to cater to the increasing population. Agriculture 4.0 is set to revolutionize agriculture productivity by using Precision Agriculture (PA), IoT, UAVs, IoUT, and other technologies to increase agriculture produce for growing demographics while addressing various farm-related issues. This survey provides a comprehensive overview of how multiple technologies such as IoT, UAVs, IoUT, Big Data Analytics, Deep Learning Techniques, and Machine Learning methods can be used to manage various farm-related operations. For each of these technologies, a detailed review is done on how the technology is being used in Agriculture 4.0. These discussions include an overview of relevant technologies, their use cases, existing case studies, and research works that demonstrate the use of these technologies in Agriculture 4.0. This paper also highlights the various future research gaps in the adoption of these technologies in Agriculture 4.0.|https://doi.org/10.1016/j.jnca.2021.103107|https://www.sciencedirect.com/science/article/pii/S1084804521001284||||2021|A survey on the role of Internet of Things for adopting and promoting Agriculture 4.0|Meghna Raj and Shashank Gupta and Vinay Chamola and Anubhav Elhence and Tanya Garg and Mohammed Atiquzzaman and Dusit Niyato|article|RAJ2021103107|||Journal of Network and Computer Applications|10848045||187|||||||||||||||||||||||1492294900|2040591560
SITA'18|Rabat, Morocco|Open Government Data portals, Evaluation, usage, Open Government Data|6||Proceedings of the 12th International Conference on Intelligent Systems: Theories and Applications|Governments are considered as one of the major producers of data. Opening up and publishing this Big Government Data in national portals have significant impact on fostering innovation, improving transparency, public accountability and collaboration. Thus, the expected benefits are hindered by several factors that influence the usage of Open Government Data portals, exploring and investigating these factors is the first step to propose an evaluation approach for OGD portals and promote their usage. In this work, we identified a set of evaluation dimensions that affect OGD portal's usage and fulfillment of users' needs and requirements. According to the identified dimensions, we propose an evaluation of two national OGD portals|10.1145/3289402.3289526|https://doi.org/10.1145/3289402.3289526|New York, NY, USA|Association for Computing Machinery|9781450364621|2018|Exploring Dimensions Influencing the Usage of Open Government Data Portals|Dahbi, Kawtar Younsi and Lamharhar, Hind and Chiadmi, Dalila|inproceedings|10.1145/3289402.3289526|26||||||||||||||||||||||||||||1492811985|42
||CERN, God particle, Large electron–positron collider, Large hadron collider, Quarks, Scientific research, Standard model||85-97|Building Big Data Applications|Scientific research is one area of applications and usage of big data where we can generate lots of data in a single experiment and perform complex analytics on the same in the outcome of that experiment. The most famous example that we can talk about is the usage of all infrastructure technologies in the discovery of the “God particle” or “Higgs boson particle” which is leading us to uncover more exploration around the universe.|https://doi.org/10.1016/B978-0-12-815746-6.00004-1|https://www.sciencedirect.com/science/article/pii/B9780128157466000041||Academic Press|978-0-12-815746-6|2020|4 - Scientific research applications and usage|Krish Krishnan|incollection|KRISHNAN202085||||||||||Krish Krishnan|||||||||||||||||||1493098488|42
||||59-69||This study sets out to assess whether there is a knowledge gap between the research frontier and the consultation business in how transport data are collected, managed and analysed. The consulting business plays an important role in applying data and methods as they typically carry out public tasks in various parts of the transport system, which are becoming more and more specialised. At the same time, big data has emerged with the promise to provide new, more and better information to help understand society and execute policies more efficiently – what we refer to as the data driven transition. We conduct a literature review to identify the state of the art within international research and compare this with results from interviews and with a survey sent to representatives from the Norwegian consultation business. We find that there is a considerable gap between international researchers and the consulting business within the entire process of collection, management and analysis of traffic data, and that this gap is increasing with the emergence of the data driven transition. Finally, we argue that the results are applicable to other countries as well. Action should be taken to keep the consultants up to speed, which will require efforts from several actors, including governmental agencies, the education institutions, the consulting business and researchers.|https://doi.org/10.1016/j.tranpol.2019.05.016|https://www.sciencedirect.com/science/article/pii/S0967070X17305589||||2019|The data driven transport research train is leaving the station. Consultants all aboard?|Hanne Seter and Petter Arnesen and Odd André Hjelkrem|article|SETER201959|||Transport Policy|0967070X||80|||||||||||||||||||||||1493926268|1407180868
KDD '16|San Francisco, California, USA|wisdom of the crowds, data visualization, machine learning tools, analytics, visual workflow|1|411|Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining|With hundreds of thousands of users, RapidMiner is the most frequently used visual workflow platform for machine learning. It covers the full spectrum of analytics from data preparation to machine learning and model validation. In this presentation, I will take you on a tour of machine learning which spans the last 15 years of research and industry applications and share key insights with you about how data scientists perform their daily analysis tasks. These patterns are extracted from mining millions of analytical workflows that have been created with RapidMiner over the past years. This talk will address important questions around the data mining process such as: What are the most frequently used solutions for typical data quality problems? How often are analysts using decision trees or neural networks? And does this behavior change over time or depend on the users experience level?|10.1145/2939672.2945365|https://doi.org/10.1145/2939672.2945365|New York, NY, USA|Association for Computing Machinery|9781450342322|2016|The Wisdom of Crowds: Best Practices for Data Prep &amp; Machine Learning Derived from Millions of Data Science Workflows|Mierswa, Ingo|inproceedings|10.1145/2939672.2945365|||||||||||||||||||||||||||||1493983288|42
||Process mining, data preparation, comparative analysis, health care delivery, patient pathways|18|||Business process analysis and process mining, particularly within the health care domain, remain under-utilized. Applied research that employs such techniques to routinely collected health care data enables stakeholders to empirically investigate care as it is delivered by different health providers. However, cross-organizational mining and the comparative analysis of processes present a set of unique challenges in terms of ensuring population and activity comparability, visualizing the mined models, and interpreting the results. Without addressing these issues, health providers will find it difficult to use process mining insights, and the potential benefits of evidence-based process improvement within health will remain unrealized. In this article, we present a brief introduction on the nature of health care processes, a review of process mining in health literature, and a case study conducted to explore and learn how health care data and cross-organizational comparisons with process-mining techniques may be approached. The case study applies process-mining techniques to administrative and clinical data for patients who present with chest pain symptoms at one of four public hospitals in South Australia. We demonstrate an approach that provides detailed insights into clinical (quality of patient health) and fiscal (hospital budget) pressures in the delivery of health care. We conclude by discussing the key lessons learned from our experience in conducting business process analysis and process mining based on the data from four different hospitals.|10.1145/2629446|https://doi.org/10.1145/2629446|New York, NY, USA|Association for Computing Machinery||2015|Process Mining for Clinical Processes: A Comparative Analysis of Four Australian Hospitals|Partington, Andrew and Wynn, Moe and Suriadi, Suriadi and Ouyang, Chun and Karnon, Jonathan|article|10.1145/2629446|19|jan|ACM Trans. Manage. Inf. Syst.|2158656X|4|5|March 2015||||||||||||||||||||||1494980574|2097885649
GoodIT '22|Limassol, Cyprus|semantic segmentation, human-in-the-loop, archaeology, mesopotamian floodplain|6|378–383|Proceedings of the 2022 ACM Conference on Information Technology for Social Good|In the perspective of landscape archaeology, remote sensing is a very important tool that allows to recognize and locate potential sites, which will then be “groundtruthed” through a surface survey. Remote sensing is, unfortunately, a very time-consuming process that scales terribly with the size of the area under investigation. In this paper we explore the possibility of using semantic segmentation models to detect and highlight the presence of archaeological sites present in the Mesopotamian floodplain. Whereas archaeologists usually combine information from a variety of basemaps, including aerial and satellite photos taken from the 1950s onwards, we investigated the possibility of using an easily accessible online maps (in our case, Bing Maps). Trying to build an accessible and lightweight system also dictated the choice of trying pretrained segmentation models and use transfer learning. The preliminary results obtained (from different models and parameters choices), as well as the dataset, its idiosyncrasies and how we can deal with them are discussed in this paper.|10.1145/3524458.3547121|https://doi.org/10.1145/3524458.3547121|New York, NY, USA|Association for Computing Machinery|9781450392846|2022|When Machines Find Sites for the Archaeologists: A Preliminary Study with Semantic Segmentation Applied on Satellite Imagery of the Mesopotamian Floodplain|Casini, Luca and Orr\`{u}, Valentina and Roccetti, Marco and Marchetti, Nicol\`{o}|inproceedings|10.1145/3524458.3547121|||||||||||||||||||||||||||||1495583698|42
||||1286-1294||Observational data research studying access, utilization, cost, and outcomes of image-guided interventions using publicly available “big data” sets is growing in the interventional radiology (IR) literature. Publicly available data sets offer insight into real-world care and represent an important pillar of IR research moving forward. They offer insights into how IR procedures are being used nationally and whether they are working as intended. On the other hand, large data sources are aggregated using complex sampling frames, and their strengths and weaknesses only become apparent after extensive use. Unintentional misuse of large data sets can result in misleading or sometimes erroneous conclusions. This review introduces the most commonly used databases relevant to IR research, highlights their strengths and limitations, and provides recommendations for use. In addition, it summarizes methodologic best practices pertinent to all data sets for planning and executing scientifically rigorous and clinically relevant observational research.|https://doi.org/10.1016/j.jvir.2022.08.003|https://www.sciencedirect.com/science/article/pii/S1051044322011241||||2022|A Practical Guide to Use of Publicly Available Data Sets for Observational Research in Interventional Radiology|Premal S. Trivedi and Vincent M. Timpone and Rustain L. Morgan and Alexandria M. Jensen and Margaret Reid and P. Michael Ho and Osman Ahmed|article|TRIVEDI20221286|||Journal of Vascular and Interventional Radiology|10510443|11|33|||||17243|0,979|Q1|133|401|1086|6771|2111|715|1,86|16,89|United States|Northern America|1990-2020|Medicine (miscellaneous) (Q1); Radiology, Nuclear Medicine and Imaging (Q1); Cardiology and Cardiovascular Medicine (Q2)|11,063|3.464|0.01078|1497681561|751431221
ICSE '20|Seoul, South Korea||2|256–257|Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: Companion Proceedings|Data analytics application development introduces many challenges including: new roles not in traditional software engineering practices - e.g. data scientists and data engineers; use of sophisticated machine learning (ML) model-based approaches; uncertainty inherent in the models; interfacing with models to fulfill software functionalities; deploying models at scale and rapid evolution of business goals and data sources. We describe our Big Data Analytics Modeling Languages (BiDaML) toolset to bring all stakeholders around one tool to specify, model and document big data applications. We report on our experience applying BiDaML to three real-world large-scale applications. Our approach successfully supports complex data analytics application development in industrial settings.|10.1145/3377812.3390811|https://doi.org/10.1145/3377812.3390811|New York, NY, USA|Association for Computing Machinery|9781450371223|2020|A Practical, Collaborative Approach for Modeling Big Data Analytics Application Requirements|Khalajzadeh, Hourieh and Simmons, Andrew and Abdelrazek, Mohamed and Grundy, John and Hosking, John and He, Qiang and Ratnakanthan, Prasanna and Zia, Adil and Law, Meng|inproceedings|10.1145/3377812.3390811|||||||||||||||||||||||||||||1498850569|42
AutomotiveUI '18|Toronto, ON, Canada|Wizard of Oz, Design Methods, Driver Benchmarking, Driver Evaluation, Interaction Design, Stress|12|298–309|Proceedings of the 10th International Conference on Automotive User Interfaces and Interactive Vehicular Applications|We propose a novel method for reliably inducing stress in drivers for the purpose of generating real-world participant data for machine learning, using both scripted in-vehicle stressor events and unscripted on-road stressors such as pedestrians and construction zones. On-road drives took place in a vehicle outfitted with an experimental display that lead drivers to believe they had prematurely ran out of charge on an isolated road. We describe the elicitation method, course design, instrumentation, data collection procedure and the post-hoc labeling of unplanned road events to illustrate how rich data about a variety of stress-related events can be elicited from study participants on-road. We validate this method with data including psychophysiological measurements, video, voice, and GPS data from (N=20) participants. Results from algorithmic psychophysiological stress analysis were validated using participant self-reports. Results of stress elicitation analysis show that our method elicited a stress-state in 89% of participants.|10.1145/3239060.3239090|https://doi.org/10.1145/3239060.3239090|New York, NY, USA|Association for Computing Machinery|9781450359467|2018|Eliciting Driver Stress Using Naturalistic Driving Scenarios on Real Roads|Baltodano, Sonia and Garcia-Mancilla, Jesus and Ju, Wendy|inproceedings|10.1145/3239060.3239090|||||||||||||||||||||||||||||1501635402|42
||Multiple Imputations, Deep Learning, Imputation of Missing Data, Machine Learning, Internet of Things, Computing Platform for Incomplete data||||Internet of Things (IoT) is enabled by the latest developments in smart sensors, communication technologies, and Internet protocols with broad applications. Collecting data from IoT and generating information from these data become tedious tasks in real-life applications when missing data is encountered in datasets. It is of critical importance to deal with the missing data timely for intelligent decision making. Hence, this survey attempts to provide a structured and comprehensive overview of the research on the imputation of incomplete data in IoT. The paper starts by providing an overview of incomplete data based on the architecture of IoT. Then, it discusses the various strategies to handle the missing data, the assumptions used, the computing platform, and the issues related to them. The paper also explores the application of imputation in the area of IoT. We encourage researchers and data analysts to use known imputation techniques and discuss various issues and challenges. Finally, potential future directions regarding the method are suggested. We believe this survey will provide a better understanding of the research of incomplete data and serve as a guide for future research.|10.1145/3533381|https://doi.org/10.1145/3533381|New York, NY, USA|Association for Computing Machinery||2022|A Comprehensive Survey on Imputation of Missing Data in Internet of Things|Adhikari, Deepak and Jiang, Wei and Zhan, Jinyu and Zhiyuan He and Rawat, Danda B. and Aickelin, Uwe and Khorshidi, Hadi A.|article|10.1145/3533381||may|ACM Comput. Surv.|03600300||||Just Accepted|||23038|2,079|Q1|163|112|365|15859|6978|365|19,16|141,60|United States|Northern America|1969-2020|Computer Science (miscellaneous) (Q1); Theoretical Computer Science (Q1)|10,790|10.282|0.01438|1504706949|1517405264
||Big Data;Government;Standards organizations;Europe;Tools;Big Data;data quality;data checklist;data repository;open science;open government;data science||4844-4853|2018 IEEE International Conference on Big Data (Big Data)|"\"\"\"Big Data readiness\"\" begins at the source where data are first created and extends along a path through an organization to the outside world. This paper focuses on practical solutions to common problems experienced when integrating diverse datasets from disparate sources. Following the Introduction, Section 2 situates Big Data in the larger context of open government, open science, science integrity, and Standards, internationally and in Canada. Section 3 analyses the Big Data problem space, while Section 4 proposes a Big Data solution space. Section 5 proposes eight data checklist modules and suggests implementation strategies to effectively meet a variety of organizational needs. Section 6 summarizes conclusions and describes future work.\""|10.1109/BigData.2018.8622229|||||2018|A Path to Big Data Readiness|Austin, Claire C.|inproceedings|8622229||Dec|||||||||||||||||||||||||||1504895747|42
SIGMOD '13|New York, New York, USA|data accuracy, data cleaning|12|565–576|Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data|The relative accuracy problem is to determine, given tuples t1 and t2 that refer to the same entity e, whether t1[A] is more accurate than t2A, i.e., t1A is closer to the true value of the A attribute of e than t2A. This has been a longstanding issue for data quality, and is challenging when the true values of e are unknown. This paper proposes a model for determining relative accuracy. (1) We introduce a class of accuracy rules and an inference system with a chase procedure, to deduce relative accuracy. (2) We identify and study several fundamental problems for relative accuracy. Given a set Ie of tuples pertaining to the same entity e and a set of accuracy rules, these problems are to decide whether the chase process terminates, is Church-Rosser, and leads to a unique target tuple te composed of the most accurate values from Ie for all the attributes of e. (3) We propose a framework for inferring accurate values with user interaction. (4) We provide algorithms underlying the framework, to find the unique target tuple te whenever possible; when there is no enough information to decide a complete te, we compute top-k candidate targets based on a preference model. (5) Using real-life and synthetic data, we experimentally verify the effectiveness and efficiency of our method.|10.1145/2463676.2465309|https://doi.org/10.1145/2463676.2465309|New York, NY, USA|Association for Computing Machinery|9781450320375|2013|Determining the Relative Accuracy of Attributes|Cao, Yang and Fan, Wenfei and Yu, Wenyuan|inproceedings|10.1145/2463676.2465309|||||||||||||||||||||||||||||1506227838|42
||Data quality management, Visual analytics, Data cleansing||191-197||Data quality management, especially data cleansing, has been extensively studied for many years in the areas of data management and visual analytics. In the paper, we first review and explore the relevant work from the research areas of data management, visual analytics and human-computer interaction. Then for different types of data such as multimedia data, textual data, trajectory data, and graph data, we summarize the common methods for improving data quality by leveraging data cleansing techniques at different analysis stages. Based on a thorough analysis, we propose a general visual analytics framework for interactively cleansing data. Finally, the challenges and opportunities are analyzed and discussed in the context of data and humans.|https://doi.org/10.1016/j.visinf.2018.12.001|https://www.sciencedirect.com/science/article/pii/S2468502X18300573||||2018|Steering data quality with visual analytics: The complexity challenge|Shixia Liu and Gennady Andrienko and Yingcai Wu and Nan Cao and Liu Jiang and Conglei Shi and Yu-Shuen Wang and Seokhee Hong|article|LIU2018191|||Visual Informatics|2468502X|4|2|||||||||||||||||||||||1508183603|1547593947
||Big data, Data ecosystem, Ecology, South Korea, Socio-technical perspective, Big data policy, Big data for development||311-320||From the viewpoint of big data as a socio-technical phenomenon, this study examines the associated assumptions and biases critically and contextually. The research analyzes the big data phenomenon from a socio-technical systems theory perspective: cultural, technological, and scholarly phenomena that rest on the interplay of technology, analysis, and mythology provoking extensive utopian and dystopian rhetoric. It examines the development of big data by reviewing this theory, identifying key components of the big data ecosystem, and explaining how these components are likely to evolve over time. Despite extensive investment and proactive drive, uncertainty exists concerning the evolution of big data and the impact on the new information milieu. Significant concerns recently addressed are in the areas of privacy, data quality, access, curation, preservation, and use. This study provides insight into these challenges and opportunities through the lens of a socio-technical analysis of big data development, which includes social dynamics, political discourse, and technological choices inherent in the design and development of next-generation ICT ecology. The policy implications of big data are addressed using Korean information initiatives to highlight key considerations as the country progresses in this new ecology era.|https://doi.org/10.1016/j.tele.2014.09.006|https://www.sciencedirect.com/science/article/pii/S0736585314000665||||2015|Ecological views of big data: Perspectives and issues|Dong-Hee Shin and Min Jae Choi|article|SHIN2015311|||Telematics and Informatics|07365853|2|32|||||20896|1,567|Q1|66|96|483|6939|3832|438|7,45|72,28|United Kingdom|Western Europe|1984-2020|Communication (Q1); Computer Networks and Communications (Q1); Electrical and Electronic Engineering (Q1); Law (Q1)|5,351|6.182|0.00899|1509737786|1579285336
CHI '22|New Orleans, LA, USA|position mining, data science, human-centered machine learning, model positionality, critical data studies, human-centered data science, Computational reflexivity, annotator fingerprinting|19||Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems|Data science and machine learning provide indispensable techniques for understanding phenomena at scale, but the discretionary choices made when doing this work are often not recognized. Drawing from qualitative research practices, we describe how the concepts of positionality and reflexivity can be adapted to provide a framework for understanding, discussing, and disclosing the discretionary choices and subjectivity inherent to data science work. We first introduce the concepts of model positionality and computational reflexivity that can help data scientists to reflect on and communicate the social and cultural context of a model’s development and use, the data annotators and their annotations, and the data scientists themselves. We then describe the unique challenges of adapting these concepts for data science work and offer annotator fingerprinting and position mining as promising solutions. Finally, we demonstrate these techniques in a case study of the development of classifiers for toxic commenting in online communities.|10.1145/3491102.3501998|https://doi.org/10.1145/3491102.3501998|New York, NY, USA|Association for Computing Machinery|9781450391573|2022|Model Positionality and Computational Reflexivity: Promoting Reflexivity in Data Science|Cambo, Scott Allen and Gergle, Darren|inproceedings|10.1145/3491102.3501998|572||||||||||||||||||||||||||||1511515965|42
||Data usability, Data quality, Decision model and notation, Data quality rule, Data quality assessment, Data quality measurement||113450||To succeed in their business processes, organizations need data that not only attains suitable levels of quality for the task at hand, but that can also be considered as usable for the business. However, many researchers ground the potential usability of the data on its quality. Organizations would benefit from receiving recommendations on the usability of the data before its use. We propose that the recommendation on the usability of the data be supported by a decision process, which includes a context-dependent data-quality assessment based on business rules. Ideally, this recommendation would be generated automatically. Decision Model and Notation (DMN) enables the assessment of data quality based on the evaluation of business rules, and also, provides stakeholders (e.g., data stewards) with sound support for the automation of the whole process of generation of a recommendation regarding usability based on data quality. The main contribution of the proposal involves designing and enabling both DMN-driven mechanisms and a guiding methodology (DMN4DQ) to support the automatic generation of a decision-based recommendation on the potential usability of a data record in terms of its level of data quality. Furthermore, the validation of the proposal is performed through the application of a real dataset.|https://doi.org/10.1016/j.dss.2020.113450|https://www.sciencedirect.com/science/article/pii/S0167923620302050||||2021|DMN4DQ: When data quality meets DMN|Álvaro Valencia-Parra and Luisa Parody and Ángel Jesús Varela-Vaca and Ismael Caballero and María Teresa Gómez-López|article|VALENCIAPARRA2021113450|||Decision Support Systems|01679236||141|||||21933|1,564|Q1|151|115|342|7152|2672|338|7,04|62,19|Netherlands|Western Europe|1985-2020|Arts and Humanities (miscellaneous) (Q1); Developmental and Educational Psychology (Q1); Information Systems (Q1); Information Systems and Management (Q1); Management Information Systems (Q1)|13,580|5.795|0.0081|1511824578|1234879127
MM '16|Amsterdam, The Netherlands|situation recognition, eventshop, micro-reports, situation prediction, disaster|10|938–947|Proceedings of the 24th ACM International Conference on Multimedia|With an increasing amount of diverse heterogeneous data and information, the methodology of multimedia analysis has become increasingly relevant in solving challenging societal problems such as managing emergency situations during disasters. Using cybernetic principles combined with multimedia technology, researchers can develop effective frameworks for using diverse multimedia (including traditional multimedia as well as diverse multimodal) data for situation recognition, and determining and communicating appropriate actions to people stranded during disasters. We present known issues in disaster management and then focus on emergency situations. We show that an emergency management problem is fundamentally a multimedia information assimilation problem for situation recognition and for connecting people's needs to available resources effectively, efficiently, and promptly. Major research challenges for managing emergency situations are identified and discussed. We also present a intelligently detecting evolving environmental situations, and discuss the role of multimedia micro-reports as spontaneous participatory sensing data streams in emergency responses. Given enormous progress in concept recognition using machine learning in the last few years, situation recognition may be the next major challenge for learning approaches in multimedia contextual big data. The data needed for developing such approaches is now easily available on the Web and many challenging research problems in this area are ripe for exploration in order to positively impact our society during its most difficult times.|10.1145/2964284.2976761|https://doi.org/10.1145/2964284.2976761|New York, NY, USA|Association for Computing Machinery|9781450336031|2016|Research Challenges in Developing Multimedia Systems for Managing Emergency Situations|Tang, Mengfan and Pongpaichet, Siripen and Jain, Ramesh|inproceedings|10.1145/2964284.2976761|||||||||||||||||||||||||||||1514493881|42
||Machine learning applications, sofware deployment||||In recent years, machine learning has transitioned from a field of academic research interest to a field capable of solving real-world business problems. However, the deployment of machine learning models in production systems can present a number of issues and concerns. This survey reviews published reports of deploying machine learning solutions in a variety of use cases, industries and applications and extracts practical considerations corresponding to stages of the machine learning deployment workflow. By mapping found challenges to the steps of the machine learning deployment workflow we show that practitioners face issues at each stage of the deployment process. The goal of this paper is to lay out a research agenda to explore approaches addressing these challenges.|10.1145/3533378|https://doi.org/10.1145/3533378|New York, NY, USA|Association for Computing Machinery||2022|Challenges in Deploying Machine Learning: A Survey of Case Studies|Paleyes, Andrei and Urma, Raoul-Gabriel and Lawrence, Neil D.|article|10.1145/3533378||apr|ACM Comput. Surv.|03600300||||Just Accepted|||23038|2,079|Q1|163|112|365|15859|6978|365|19,16|141,60|United States|Northern America|1969-2020|Computer Science (miscellaneous) (Q1); Theoretical Computer Science (Q1)|10,790|10.282|0.01438|1516346645|1517405264
||edge computing, device swarms, Civil engineering|6||||10.1145/3190579|https://doi.org/10.1145/3190579|New York, NY, USA|Association for Computing Machinery||2018|Adaptive and Cost-Effective Collection of High-Quality Data for Critical Infrastructure and Emergency Management in Smart Cities—Framework and Challenges|Bertino, Elisa and Jahanshahi, Mohammad R.|article|10.1145/3190579|1|may|J. Data and Information Quality|19361955|1|10|March 2018||||||||||||||||||||||1517139449|833754770
||Government big data governance, Scenario-based decision-making, Scenario modeling, Model-driven, Data link network, Public safety services||103622||In the public safety service context, government big data governance (GBDG) is a challenging decision-making problem that encompasses uncertainties in the arenas of big data and its complex links. Modeling and collaborating the key scenario information required for GBDG decision-making can minimize system uncertainties. However, existing scenario-building methods are limited by their rigidity as they are employed in various application contexts and the associated high costs of modeling. In this paper, using a design science paradigm, a model-driven scenario modeling approach is proposed to achieve flexible scenario modeling for various applications through the transfer of generic domain knowledge. The key component of the proposed approach is a scenario meta-model that is built from existing literatures and practices by integrating qualitative, quantitative, and meta-modeling analysis. An instantiation mechanism of the scenario meta-model is also proposed to generate customized scenarios under Antecedent-Behavior-Consequence (ABC) theory. Two real-world safety service cases in Wuhan, China were evaluated to find that the proposed approach reduces GBDG decision-making uncertainties significantly by providing key information for GBDG problem identification, solution design, and solution value perception. This scenario-building approach can be further used to develop other GBDG systems for public safety services with reduced uncertainties and complete decision-making functions.|https://doi.org/10.1016/j.im.2022.103622|https://www.sciencedirect.com/science/article/pii/S0378720622000349||||2022|scenario modeling for government big data governance decision-making: Chinese experience with public safety services|Zhao-ge LIU and Xiang-yang LI and Xiao-han ZHU|article|LIU2022103622|||Information & Management|03787206|3|59|||||12303|2,147|Q1|162|130|248|11385|2482|246|8,94|87,58|Netherlands|Western Europe|1977-2020|Information Systems (Q1); Information Systems and Management (Q1); Management Information Systems (Q1)||||1519389173|1945939487
PODS '17|Chicago, Illinois, USA|graph dependencies, built-in predicates, validation, satisfiability, implication, conditional functional dependencies, tgds, egds, axiom system, keys, disjunction|14|403–416|Proceedings of the 36th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems|This paper proposes a class of dependencies for graphs, referred to as graph entity dependencies (GEDs). A GED is a combination of a graph pattern and an attribute dependency. In a uniform format, GEDs express graph functional dependencies with constant literals to catch inconsistencies, and keys carrying id literals to identify entities in a graph.We revise the chase for GEDs and prove its Church-Rosser property. We characterize GED satisfiability and implication, and establish the complexity of these problems and the validation problem for GEDs, in the presence and absence of constant literals and id literals. We also develop a sound and complete axiom system for finite implication of GEDs. In addition, we extend GEDs with built-in predicates or disjunctions, to strike a balance between the expressive power and complexity. We settle the complexity of the satisfiability, implication and validation problems for the extensions.|10.1145/3034786.3056114|https://doi.org/10.1145/3034786.3056114|New York, NY, USA|Association for Computing Machinery|9781450341981|2017|Dependencies for Graphs|Fan, Wenfei and Lu, Ping|inproceedings|10.1145/3034786.3056114|||||||||||||||||||||||||||||1519643397|42
||satisfiability, conditional functional dependencies, implication, TGDs, keys, built-in predicates, Graph dependencies, validation, disjunction, axiom system, EGDs|40|||This article proposes a class of dependencies for graphs, referred to as graph entity dependencies (GEDs). A GED is defined as a combination of a graph pattern and an attribute dependency. In a uniform format, GEDs can express graph functional dependencies with constant literals to catch inconsistencies, and keys carrying id literals to identify entities (vertices) in a graph. We revise the chase for GEDs and prove its Church-Rosser property. We characterize GED satisfiability and implication, and establish the complexity of these problems and the validation problem for GEDs, in the presence and absence of constant literals and id literals. We also develop a sound, complete and independent axiom system for finite implication of GEDs. In addition, we extend GEDs with built-in predicates or disjunctions, to strike a balance between the expressive power and complexity. We settle the complexity of the satisfiability, implication, and validation problems for these extensions.|10.1145/3287285|https://doi.org/10.1145/3287285|New York, NY, USA|Association for Computing Machinery||2019|Dependencies for Graphs|Fan, Wenfei and Lu, Ping|article|10.1145/3287285|5|feb|ACM Trans. Database Syst.|03625915|2|44|June 2019||||||||||||||||||||||1519643397|1726010902
||Big data, Deep learning, Direct numerical simulation, BLASTNet||100087||Many state-of-the-art machine learning (ML) fields rely on large datasets and massive deep learning models (with O(109) trainable parameters) to predict target variables accurately without overfitting. Within combustion, a wealth of data exists in the form of high-fidelity simulation data and detailed measurements that have been accumulating since the past decade. Yet, this data remains distributed and can be difficult to access. In this work, we present a realistic and feasible framework which combines (i) community involvement, (ii) public data repositories, and (iii) lossy compression algorithms for enabling broad access to high-fidelity data via a network-of-datasets approach. This Bearable Large Accessible Scientific Training Network-of-Datasets (BLASTNet) is consolidated on a community-hosted web-platform (at https://blastnet.github.io/), and is targeted towards improving accessibility to diverse scientific data for deep learning algorithms. For datasets that exceed the storage limitations in public ML repositories, we propose employing lossy compression algorithms on high-fidelity data, at the cost of introducing controllable amounts of error to the data. This framework leverages the well-known robustness of modern deep learning methods to noisy data, which we demonstrate is also applicable in combustion by training deep learning models on lossy direct numerical simulation (DNS) data in two completely different ML problems — one in combustion regime classification and the other in filtered reaction rate regression. Our results show that combustion DNS data can be compressed by at least 10-fold without affecting deep learning models, and that the resulting lossy errors can even improve their training. We thus call on the research community to help contribute to opening a bearable pathway towards accessible big data in combustion.|https://doi.org/10.1016/j.jaecs.2022.100087|https://www.sciencedirect.com/science/article/pii/S2666352X22000309||||2022|BLASTNet: A call for community-involved big data in combustion machine learning|Wai Tong Chung and Ki Sung Jung and Jacqueline H. Chen and Matthias Ihme|article|CHUNG2022100087|||Applications in Energy and Combustion Science|2666352X||12|||||||||||||||||||||||1526225327|813020619
https://doi.org/10.1016/j.ijmedinf.2018.03.013|https://www.sciencedirect.com/science/article/pii/S1386505618302466||||2018|Concurrence of big data analytics and healthcare: A systematic review|Nishita Mehta and Anil Pandit|article|MEHTA201857|||International Journal of Medical Informatics|13865056||114||||||||||||||||||||||||||||||1527745046|42
dg.o '20|Seoul, Republic of Korea|Anonymisation, Big Data, Privacy, Governance, Personal Data Protection|11|185–195|The 21st Annual International Conference on Digital Government Research|In a massive processing data era, an emerging impasse has taking scenario: privacy. In this context, personal data receive particular attention, witch its laws and guidelines that ensure better and legal use of data. The General Data Protection Regulation (GDPR) - in the European Union - and the Brazilian General Data Protection Law (LGPD) - in Brazil - lead to anonymisation (and its processes and techniques) as a way to reach secure use of personal data. However, expectations placed on this tool must be reconsidered according to risks and limits of its use, mainly when this technique is applied to Big Data. We discussed whether anonymisation used in conjunction with good data governance practices could provide greater protection for privacy. We conclude that good governance practices can strengthen privacy in anonymous data belonging to a Big Data, and we present a suggestive governance framework aimed at privacy.|10.1145/3396956.3398253|https://doi.org/10.1145/3396956.3398253|New York, NY, USA|Association for Computing Machinery|9781450387910|2020|Big Data, Anonymisation and Governance to Personal Data Protection|Potiguara Carvalho, Artur and Potiguara Carvalho, Fernanda and Dias Canedo, Edna and Potiguara Carvalho, Pedro Henrique|inproceedings|10.1145/3396956.3398253|||||||||||||||||||||||||||||1529952821|42
ESEC/FSE 2020|Virtual Event, USA|AIOps, Incident Management, Cloud Computing|11|1487–1497|Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering|The management of cloud service incidents (unplanned interruptions or outages of a service/product) greatly affects customer satisfaction and business revenue. After years of efforts, cloud enterprises are able to solve most incidents automatically and timely. However, in practice, we still observe critical service incidents that occurred in an unexpected manner and orchestrated diagnosis workflow failed to mitigate them. In order to accelerate the understanding of unprecedented incidents and provide actionable recommendations, modern incident management system employs the strategy of AIOps (Artificial Intelligence for IT Operations). In this paper, to provide a broad view of industrial incident management and understand the modern incident management system, we conduct a comprehensive empirical study spanning over two years of incident management practices at Microsoft. Particularly, we identify two critical challenges (namely, incomplete service/resource dependencies and imprecise resource health assessment) and investigate the underlying reasons from the perspective of cloud system design and operations. We also present IcM BRAIN, our AIOps framework towards intelligent incident management, and show its practical benefits conveyed to the cloud services of Microsoft.|10.1145/3368089.3417055|https://doi.org/10.1145/3368089.3417055|New York, NY, USA|Association for Computing Machinery|9781450370431|2020|Towards Intelligent Incident Management: Why We Need It and How We Make It|Chen, Zhuangbin and Kang, Yu and Li, Liqun and Zhang, Xu and Zhang, Hongyu and Xu, Hui and Zhou, Yangfan and Yang, Li and Sun, Jeffrey and Xu, Zhangwei and Dang, Yingnong and Gao, Feng and Zhao, Pu and Qiao, Bo and Lin, Qingwei and Zhang, Dongmei and Lyu, Michael R.|inproceedings|10.1145/3368089.3417055|||||||||||||||||||||||||||||1534377817|42
||History of data, statistics, scientific data, organizational data, relational data, characteristics of data||69-92|Meeting the Challenges of Data Quality Management|This chapter presents an extended definition of the concept of data, through the lens of history. All forms of data encode information about the real world. Using data always involves interpretation, so it is important to understand how data works, to understand “data as data.” But what we mean by data and how we create and use it in science, statistics, and commerce have changed over time. Many assumptions about data quality are rooted in this evolution. A better understanding of the evolution of data helps us define and manage specific expectations related to data quality.|https://doi.org/10.1016/B978-0-12-821737-5.00004-3|https://www.sciencedirect.com/science/article/pii/B9780128217375000043||Academic Press|978-0-12-821737-5|2022|Chapter 4 - The Data Challenge: The Mechanics of Meaning|Laura Sebastian-Coleman|incollection|SEBASTIANCOLEMAN202269||||||||||Laura Sebastian-Coleman|||||||||||||||||||1534766409|42
||Intelligence Maintenance, Data Quality, Artificial Intelligence, Industry 4.0||55-60||Collecting useful and informative data play an essential role in ensuring the performance of data-driven solutions for intelligent maintenance. However, there is still a lack of methodology to systematically assess the data usefulness (or data suitability) for modeling. This lack of data suitability assessment becomes a more pressing issue in the big data environment where a large volume of machine data is generated at a high velocity. Therefore, there are imperative needs for standardized procedures and systematic solutions that can scan through a large amount of data to quantify the data suitability and locate the useful datasets for model development. To fill in this gap, this paper proposes a novel methodology to evaluate the data suitability for PHM modeling from the aspects of detectability assessment, diagnosability assessment, and prognosability assessment. In the discussion, new assessment procedures and algorithms are proposed by using a series of similarity metrics between data vectors or data distribution. Also, the proposed methods provide both visualization tools and quantitative metrics to assess the data suitability. The effectiveness of the methodology is demonstrated by using real-world examples about the ball screw degradation and boring tool degradation. The results successfully demonstrate the effectiveness and practicality of the proposed methodology and analytics.|https://doi.org/10.1016/j.ifacol.2022.09.183|https://www.sciencedirect.com/science/article/pii/S2405896322013969||||2022|Data Quality and Usability Assessment Methodology for Prognostics and Health Management: A Systematic Framework|Xiaodong Jia and Da-Yan Ji and Takanobu Minami and Jay Lee|article|JIA202255|||IFAC-PapersOnLine|24058963|19|55||5th IFAC Workshop on Advanced Maintenance Engineering, Services and Technologies AMEST 2022|||21100456158|0,308|Q3|72|115|8407|1913|9863|8351|1,13|16,63|Austria|Western Europe|2002-2019|Control and Systems Engineering (Q3)||||1534880424|676980763
||Medical diagnostic imaging;Clinical diagnosis;Electronic medical records;History;Maintenance engineering;Discharges (electric);Hospitals;traditional Chinese Medicine (TCM);clinical research paradigm;clinical research information sharing system;datamation;structured electronic medical record||1-5|2014 IEEE Symposium on Computational Intelligence in Big Data (CIBD)|Under the guidance of clinical research paradigm of traditional Chinese medicine (TCM) in real world, the research group developed the clinical research information sharing system, in which structured electronic medical record system of traditional Chinese medicine is the technology platform of datamation of clinical diagnosis and treatment information. The clinical diagnosis and treatment information can be activated and used effectively only after datamation and truly become the treasures of knowledge of TCM. This paper discusses the implementation process and technologies and methods of TCM clinical information datamation, and take admission records as an example to demonstrate the contents and realization way of datamation, and a brief introduction of the effect of implementation and application of datamation. By making full use of technologies and methods of datamation, strengthening data quality control in the datamation process, greatly improving the quality of TCM clinical research data, to lay a good foundation for establishment of knowledge base through further statistical analysis or data mining of TCM clinical data.|10.1109/CIBD.2014.7011533|||||2014|Methods and technologies of traditional Chinese medicine clinical information datamation in real world|Song, Guanli and Wang, Yinghui and Zhang, Runshun and Liu, Baoyan and Zhou, Xuezhong and Song, Guanbo and Xie, Liang and Huang, Xinghuan|inproceedings|7011533||Dec|||||||||||||||||||||||||||1538605437|42
||Resource description framework;Ontologies;Sparks;Cleaning;Inference algorithms;Automobiles;Jacobian matrices;Big data;Data quality;Inconsistency;Logical inference;RDF data cleaning;Apache Spark ecosystems||74-79|2017 IEEE International Conference on Big Data (Big Data)|We address the problem of dealing with inconsistencies in fusion of big data sources using Resource Description Framework (RDF) and ontologies. We propose a scalable approach ensuring data quality for query answering over big RDF data in a distributed way on a Spark ecosystem. In so doing, the cleaning inconsistent big RDF data approach is built on the following steps (1) modeling consistency rules to detect the inconsistency triples even if it is implicitly hidden including inference and inconsistent rules (2) detecting inconsistency through rule evaluation based on Apache Spark framework to discover the minimally sub-set of inconsistent triples (3) cleaning the inconsistency through finding the best repair for consistent query answering.|10.1109/BigData.2017.8257913|||||2017|Enhancing data quality by cleaning inconsistent big RDF data|Benbernou, Salima and Ouziri, Mourad|inproceedings|8257913||Dec|||||||||||||||||||||||||||1539136578|42
EITCE 2020|Xiamen, China|Machine learning, Sparse matrix, Graph Neural Networks, Personalized recommendation|9|602–610|Proceedings of the 2020 4th International Conference on Electronic Information Technology and Computer Engineering|Personalized recommendation is a key technology to effectively solve the overload of online information and eliminate information islands. It is widely known as an important way to improve the quality of information services. However, cold start, data sparseness, algorithm performance, recommendation accuracy and surprise are still the key issues that restrict users' personalized recommendations. Firstly, we review the development trend of personalized information recommendation algorithms in the past 15 years. And then we propose a new classification method for users' personalized recommendation based on machine learning algorithms with cold start, data sparseness, and the performance of the algorithm as the main goals. On this basis, we summarize and compare the ideas, practices and conclusions of related machine learning algorithms. Finally, we further summarize the main advantages and disadvantages of the 10 kinds of personalized recommendation algorithms from the perspective of classification proposed, and look forward to the development directions, difficulties, focus and methods of personalized recommendation algorithms.|10.1145/3443467.3444711|https://doi.org/10.1145/3443467.3444711|New York, NY, USA|Association for Computing Machinery|9781450387811|2021|A Survey of Personalized Recommendation Based on Machine Learning Algorithms|Tian, Luogeng and Yang, Bailong and Yin, Xinli and Su, Yang|inproceedings|10.1145/3443467.3444711|||||||||||||||||||||||||||||1539978632|42
ICISS 2021|Edinburgh, United Kingdom|SON, Routing, SDN, Deep Learning|7|160–166|2021 The 4th International Conference on Information Science and Systems|“When you are destined for an important appoint-ment, you would obviously opt for the most reliable route instead of the shortest in order to be well prepared”. Modern networking is presently undergoing through a quantum leap. To cope up with ambitious demands and user expectations, it is becoming more complex both structurally and functionally. Software Defined Networking (SDN) happens to be an instance of such advancements. It has significantly leveraged the network programmability, abstraction, and automation. Eventually, with acceptance form all major network infrastructure such as 5G and Cloud, SDN is becoming the standard of future networking. Likewise, Machine Learning (ML) has become the trendiest skill-in-demand recently. With its superiority of analyzing data, makes it applicable for almost every possible domain. The attempt to applying the power of ML in networking has not been too long, it allows the network to be more intelligent and capable enough to take optimal decisions to address some of its native problems. This gives rise to Self- Organized Networking (SON). In this article, Routing using Deep Neural Network (DNN) on top of SDN is addressed. We proposed a Self-organized Knowledge Defined Network (SO-KDN) architecture and an intelligent routing algorithm, that reactively finds the most reliable route, i.e., a route having least probability of fluctuation. This reduces network overhead due to re-routing and optimizes traffic congestion. Experimental data show a mean 90% accurate forecast in reliability prediction.|10.1145/3459955.3460617|https://doi.org/10.1145/3459955.3460617|New York, NY, USA|Association for Computing Machinery|9781450389136|2021|SO-KDN: A Self-Organised Knowledge Defined Networks Architecture for Reliable Routing|Gosh, Saptarshi and EL Boudani, Brahim and Dagiuklas, Tasos and Iqbal, Muddesar|inproceedings|10.1145/3459955.3460617|||||||||||||||||||||||||||||1540375310|42
||Big data analytics, Internet of things, Strategic management, Knowledge-based theory, Dynamics capability theory||103141||Big data analytics (BDA) and the Internet of Things (IoT) tools are considered crucial investments for firms to distinguish themselves among competitors. Drawing on a strategic management perspective, this study proposes that BDA and IoT capabilities can create significant value in business processes if supported by a good level of data quality, which will lead to a better competitive advantage. Responses are collected from 618 European and American firms that use IoT and BDA applications. Partial least squares results reveal that better data quality is needed to unlock the value of IoT and BDA capabilities.|https://doi.org/10.1016/j.im.2019.01.003|https://www.sciencedirect.com/science/article/pii/S0378720617308662||||2020|Leveraging internet of things and big data analytics initiatives in European and American firms: Is data quality a way to extract business value?|Nadine Côrte-Real and Pedro Ruivo and Tiago Oliveira|article|CORTEREAL2020103141|||Information & Management|03787206|1|57||Big data and business analytics: A research agenda for realizing business value|||12303|2,147|Q1|162|130|248|11385|2482|246|8,94|87,58|Netherlands|Western Europe|1977-2020|Information Systems (Q1); Information Systems and Management (Q1); Management Information Systems (Q1)||||1540507824|1945939487
||Data quality, Cyber-physical systems, Service-oriented manufacturing, Workflow nets||34-44||Service-oriented manufacturing (SOM) is a new worldwide manufacturing paradigm, and a cyber-physical system (CPS) is accepted as a strategic choice of SOM enterprises looking to provide bundles of satisfying products and services to customers. The issue of data quality is common in any CPS and poses great challenges to its efficient operation. This paper focuses on defective data generated by the improper operation of physical and cyber components of a service-oriented manufacturing CPS (SMCPS), and develops effective managerial policies to deal with such data. First, formal semantics of workflow nets (WF-nets) are employed to construct process-oriented ontology for the SMCPS. Second, a two-stage optimization model together with algorithms is designed to find optimal policies that balance local and global management objectives. Finally, our model is illustrated through a case. Results show that the proposed control strategy outperforms one-stage control and random control in guaranteeing data quality and saving control costs.|https://doi.org/10.1016/j.compeleceng.2016.08.010|https://www.sciencedirect.com/science/article/pii/S0045790616302099||||2017|Data quality management for service-oriented manufacturing cyber-physical systems|Zhiting Song and Yanming Sun and Jiafu Wan and Peipei Liang|article|SONG201734|||Computers & Electrical Engineering|00457906||64|||||18159|0,630|Q1|64|298|1040|7464|4626|979|4,79|25,05|United Kingdom|Western Europe|1973-1984, 1986-2020|Computer Science (miscellaneous) (Q1); Control and Systems Engineering (Q2); Electrical and Electronic Engineering (Q2)||||1541982962|1285201041
ICBDM 2020|Manchester, United Kingdom|Project Methodologies, Data Analytics, Project characteristics|7|41–47|Proceedings of the 2020 International Conference on Big Data in Management|Developments in big data have led to an increase in data analytics projects conducted by organizations. Such projects aim to create value by improving decision making or enhancing business processes. However, many data analytics projects still fail to deliver the expected value. The use of process models or methodologies is recommended to increase the success rate of these projects. Nevertheless, organizations are hardly using them because they are considered too rigid and hard to implement. The existing methodologies often do not fit the specific project characteristics. Therefore, this research suggests grouping different project characteristics to identify the most appropriate project methodology for a specific type of project. More specifically, this research provides a structured description that helps to determine what type of project methodology works for different types of data analytics projects. The results of six different case studies show that continuous projects would benefit from an iterative methodology.|10.1145/3437075.3437087|https://doi.org/10.1145/3437075.3437087|New York, NY, USA|Association for Computing Machinery|9781450375061|2021|Data Analytics Project Methodologies: Which One to Choose?|Baijens, Jeroen and Helms, Remko and Kusters, Rob|inproceedings|10.1145/3437075.3437087|||||||||||||||||||||||||||||1542639786|42
||analytics, organizational design, data architecture, data, chief data officer, strategy, business intelligence, data integration, data governance, data warehousing, CIO, enterprise architecture, Data management, CDO, BigCo, policy, chief information officer, information systems, data stewardship, enterprise data executive, IT management, conceptual modeling|35|||In a manner similar to most organizations, BigCompany (BigCo) was determined to benefit strategically from its widely recognized and vast quantities of data. (U.S. government agencies make regular visits to BigCo to learn from its experiences in this area.) When faced with an explosion in data volume, increases in complexity, and a need to respond to changing conditions, BigCo struggled to respond using a traditional, information technology (IT) project-based approach to address these challenges. As BigCo was not data knowledgeable, it did not realize that traditional approaches could not work. Two full years into the initiative, BigCo was far from achieving its initial goals. How much more time, money, and effort would be required before results were achieved? Moreover, could the results be achieved in time to support a larger, critical, technology-driven challenge that also depended on solving the data challenges? While these questions remain unaddressed, these considerations increase our collective understanding of data assets as separate from IT projects. Only by reconceiving data as a strategic asset can organizations begin to address these new challenges. Transformation to a data-driven culture requires far more than technology, which remains just one of three required “stool legs” (people and process being the other two). Seven prerequisites to effectively leveraging data are necessary, but insufficient awareness exists in most organizations—hence, the widespread misfires in these areas, especially when attempting to implement the so-called big data initiatives. Refocusing on foundational data management practices is required for all organizations, regardless of their organizational or data strategies.|10.1145/2893482|https://doi.org/10.1145/2893482|New York, NY, USA|Association for Computing Machinery||2016|EXPERIENCE: Succeeding at Data Management—BigCo Attempts to Leverage Data|Aiken, Peter|article|10.1145/2893482|8|may|J. Data and Information Quality|19361955|1–2|7|June 2016||||||||||||||||||||||1544462953|833754770
||Digital twin;Power transmission;System integration;Maintenance engineering;Reliability engineering;Transformers;Real-time systems;digital twin;status evaluation;online evaluation;fault diagnosis;status prediction;transformer||3368-3373|2021 IEEE 5th Conference on Energy Internet and Energy System Integration (EI2)|Traditional status evaluation for main equipment of power transmission and transformation has some shortages, such as low timeliness, low data quality and difficulty for evaluation model construction. Based on digital twin technology system, this paper presents technology of equipment status, and constructs digital twin for power transmission and transformation equipment. According to operation characteristics of power transmission and transformation equipment, fusion and cleansing of perception data is realized. Relying on big data analysis and data mining, status evaluation differentiation, accurate fault diagnosis and status prediction for power transmission and transformation equipment is realized. Further more, this paper analyzes the application of digital twin technology in on-line status evaluation for transformer equipment, expounds specific application of digital twin technology including data governance and model building, and summarizes application prospect of digital twin technology in on-line status evaluation for main equipment of power transmission and transformation.|10.1109/EI252483.2021.9713501|||||2021|Research on Online Status Evaluation Technology for Main Equipment of Power Transmission and Transformation Based on Digital Twin|Liu, Jiaxin and Wang, Shuai and Lu, Xuchen and Li, Tong|inproceedings|9713501||Oct|||||||||||||||||||||||||||1545024379|42
||Agile management, Heterogeneity, Internet-of-things, Smart factory, Smart manufacturing||157-162||The advancements in the industries have paved the way for the distributed establishment of the big data volumes, cyber-physical systems, and industrie 4.0. The perspectives of modules are integrated with the shop-floor monitoring and controlled by computational paradigms, and digital computational spaces. The performance rises after introducing an intelligent and automated manufacturing industry into the next-generation industry. The scope of this paper is to address the state-of-the-art technologies and phases such as digital twins, big data analytics, artificial intelligence, and internet-of-things. The research challenges are examined with attention on data integrity, data quality, data privacy, data availability, data scalability, data transformation, legitimate and monitoring issues, and governance. Lastly, potential research issues that need considerable research efforts are summarized. We believe that this paper is presenting the research directions for researchers in the area of smart industry towards its integration for the advancements of the industrial sector, and agile management. Some surprising development as industry 4.0 integration with socio-technical systems was found in designing the architecture of vertical, horizontal, and end-to-end integration mechanisms.|https://doi.org/10.1016/j.matpr.2020.07.170|https://www.sciencedirect.com/science/article/pii/S2214785320352639||||2021|Big data, industry 4.0 and cyber-physical systems integration: A smart industry context|Harpreet Singh|article|SINGH2021157|||Materials Today: Proceedings|22147853||46||2nd International Conference on Manufacturing Material Science and Engineering|||21100370037|0,341|-|47|3996|10585|88521|14096|10409|1,24|22,15|United Kingdom|Western Europe|2005, 2014-2020|Materials Science (miscellaneous)||||1547440154|400517803
TEEM'21|Barcelona, Spain|21st Century Skills, Connected mathematics, creativity and collaborative work in education, transdisciplinary|2|721–722|Ninth International Conference on Technological Ecosystems for Enhancing Multiculturality (TEEM'21)|Human knowledge is highly connected and under continuous and collaborative evolution, where all of us can co-create and contribute. But the execution of our standardized educational systems mostly offers a standardized experience and teaching practices that do not reflect this point. This is particularly acute in teachers and students with mathematics. In this research, I will review some strategies to help teachers to effectively use connected mathematics with the real and daily world, promoting transdisciplinary work with an emphasis on creativity and collaboration, and using the Game-Based Assessment methodology combined with the unique environment surrounding each teacher.|10.1145/3486011.3486555|https://doi.org/10.1145/3486011.3486555|New York, NY, USA|Association for Computing Machinery|9781450390668|2021|A Methodology Exploration to Motivate Teachers to Place Mathematics at the Center of a Transdisciplinary Experience Using Videogames and Big Data Combined with the 21st Century Skills|Rocuts, Schweitzer and Alier, Marc|inproceedings|10.1145/3486011.3486555|||||||||||||||||||||||||||||1548734555|42
||Urban flood susceptibility, News media data, Flood locations, Named entity recognition, Data quality control||128312||Flood susceptibility assessment for identifying flood-prone areas plays a significant role in flood hazard mitigation. Machine learning is an optional assessment method because of its high objectivity and computational efficiency, but how to get enough and accurate information of historical flood locations to train the machine learning models has been a key problem. In recent years, news media data from both news websites and social media accounts has emerged as a promising source for natural science studies. However, the application of news media data in urban flood susceptibility assessment is still inadequate. This study proposed an approach to fill this gap. Firstly, flood locations were extracted from news media data based on a named entity recognition (NER) model. Then, a frequency or distance-based data quality control method was employed to improve the representativeness of the extracted flooded locations. Finally, flood conditioning factors with information of historical flood locations were input into a Support Vector Machine (SVM) model for flood susceptibility assessment. We took the central city of Dalian, China as a case study. The T-test results show that there was no significant difference between the distributions of most flood conditioning factors at the flood locations from the news media data and the official planning report. In the obtained flood susceptibility map, the high flood susceptibility areas got a recall of 90% compared with the high flood hazard areas in the planning report. Performing data quality control in the frequency-based method can improve the precision of the flood susceptibility map by up to 5%, while the distance-based method is ineffective. This study provides an example and offers the value of applying new data sources and modern deep learning techniques for urban flood management.|https://doi.org/10.1016/j.jhydrol.2022.128312|https://www.sciencedirect.com/science/article/pii/S0022169422008848||||2022|Extracting historical flood locations from news media data by the named entity recognition (NER) model to assess urban flood susceptibility|Shengnan Fu and Heng Lyu and Ze Wang and Xin Hao and Chi Zhang|article|FU2022128312|||Journal of Hydrology|00221694||612|||||50089|1,684|Q1|226|1336|2587|87508|15250|2563|5,76|65,50|Netherlands|Western Europe|1949, 1963-2020|Water Science and Technology (Q1)|73,620|5.722|0.04972|1549708624|788080428
MobiCom '19|Los Cabos, Mexico|3d mapping, crowdsensing, gnss snr measurements|15||The 25th Annual International Conference on Mobile Computing and Networking|3D maps of urban environments are useful in various fields ranging from cellular network planning to urban planning and climatology. These models are typically constructed using expensive techniques such as manual annotation with 3D modeling tools, extrapolated from satellite or aerial photography, or using specialized hardware with depth sensing devices. In this work, we show that 3D urban maps can be extracted from standard GNSS data, by analyzing the received satellite signals that are attenuated by obstacles, such as buildings. Furthermore, we show that these models can be extracted from low-accuracy GNSS data, crowdsourced opportunistically from standard smartphones during their user's uncontrolled daily commute trips, unleashing the potential of applying the principle to wide areas. Our proposal incorporates position inaccuracies in the calculations, and accommodates different sources of variability of the satellite signals' SNR. The diversity of collection conditions of crowdsourced GNSS positions is used to mitigate bias and noise from the data. A binary classification model is trained and evaluated on multiple urban scenarios using data crowdsourced from over 900 users. Our results show that the generalization accuracy for a Random Forest classifier in typical urban environments lies between 79% and 91% on 4 m wide voxels, demonstrating the potential of the proposed method for building 3D maps for wide urban areas.|10.1145/3300061.3345456|https://doi.org/10.1145/3300061.3345456|New York, NY, USA|Association for Computing Machinery|9781450361699|2019|Extracting 3D Maps from Crowdsourced GNSS Skyview Data|Rodrigues, Jo\~{a}o G. P. and Aguiar, Ana|inproceedings|10.1145/3300061.3345456|55||||||||||||||||||||||||||||1551200735|42
ICPE '22|Beijing, China|data sets, distance metrics, time series, time series similarity, workload analysis|8|89–96|Proceedings of the 2022 ACM/SPEC on International Conference on Performance Engineering|Benchmarking is a core element in the toolbox of most systems researchers and is used for analyzing, comparing, and validating complex systems. In the quest for reliable benchmark results, a consensus has formed that a significant experiment must be based on multiple runs. To interpret these runs, mean and standard deviation are often used. In case of experiments where each run produces a time series, applying and comparing the mean is not easily applicable and not necessarily statistically sound. Such an approach ignores the possibility of significant differences between runs with a similar average. In order to verify this hypothesis, we conducted a survey of 1,112 publications of selected performance engineering and systems conferences canvassing open data sets from performance experiments. The identified 3 data sets purely rely on average and standard deviation. Therefore, we propose a novel analysis approach based on similarity analysis to enhance the reliability of performance evaluations. Our approach evaluates 12 (dis-)similarity measures with respect to their applicability in analysing performance measurements and identifies four suitable similarity measures. We validate our approach by demonstrating the increase in reliability for the data sets found in the survey.|10.1145/3489525.3511699|https://doi.org/10.1145/3489525.3511699|New York, NY, USA|Association for Computing Machinery|9781450391436|2022|Same, Same, but Dissimilar: Exploring Measurements for Workload Time-Series Similarity|"\"Leznik, Mark and Grohmann, Johannes and Kliche, Nina and Bauer, Andr\\'{e} and Seybold, Daniel and Eismann, Simon and Kounev, Samuel and Domaschka, J\"\"{o}rg\""|inproceedings|10.1145/3489525.3511699|||||||||||||||||||||||||||||1551687823|42
ICIMTECH 21|Jakarta, Indonesia||5||The Sixth International Conference on Information Management and Technology|NOTICE OF RETRACTION: While investigating potential publication-related misconduct in connection with the ICIMTech 2021 Conference Proceedings, serious concerns were raised that cast doubt on the integrity of the peer-review process and all papers published in the Proceedings of this Conference. The integrity of the entire Conference has been called into question. As a result, of its investigation, ACM has decided to retract the Entire Conference Proceedings and all related papers from the ACM Digital Library.None of the papers from this Proceeding should be cited in the literature because of the questionable integrity of the peer review process for this Conference.|10.1145/3465631.3465664|https://doi.org/10.1145/3465631.3465664|New York, NY, USA|Association for Computing Machinery|9781450385015|2021|Application Strategies of Medical Big Data in Health Economic Management|Yu, Xiaomu and Yin, Yuelin|inproceedings|10.1145/3465631.3465664|33||||||||||||||||||||||||||||1553449828|42
||Digital trace data, Data quality, GitHub||113133||In recent years an increasing number of academic disciplines, including IS, have sourced digital trace data for their research. Notwithstanding the potential of such data in (re)investigations of various phenomena of interest that would otherwise be difficult or impossible to study using other sources of data, we view the quality of digital trace data as an underappreciated issue in IS research. To initiate a discussion of how to evaluate and report on the quality of digital trace data in IS research, we couch our arguments within the broader tradition of research on data quality. We explain how the uncontrolled nature of digital trace data creates unique challenges for IS researchers, who need to collect, store, retrieve, and transform those data for the purpose of numerical analysis. We then draw parallels with concepts and patterns commonly used in data analysis projects and argue that, although IS researchers probably apply such concepts and patterns, this is not reported in publications, undermining the reader's ability to assess the reliability, statistical power and replicability of the findings. Using the case of GitHub to illustrate such challenges, we develop a preliminary set of guidelines to help researchers consider and report on the quality of the digital trace data they use in their research. Our work contributes to the debate on data quality and provides relevant recommendations for scholars and IS journals at a time when a growing number of publications are relying on digital trace data.|https://doi.org/10.1016/j.dss.2019.113133|https://www.sciencedirect.com/science/article/pii/S0167923619301629||||2019|Reflections on quality requirements for digital trace data in IS research|Gregory Vial|article|VIAL2019113133|||Decision Support Systems|01679236||126||Perspectives on Numerical Data Quality in IS Research|||21933|1,564|Q1|151|115|342|7152|2672|338|7,04|62,19|Netherlands|Western Europe|1985-2020|Arts and Humanities (miscellaneous) (Q1); Developmental and Educational Psychology (Q1); Information Systems (Q1); Information Systems and Management (Q1); Management Information Systems (Q1)|13,580|5.795|0.0081|1553965239|1234879127
||Big data, Forecasting, Literature review, Prediction models, Information||100289||With the boom in Internet techniques and computer science, a variety of big data have been introduced into forecasting research, bringing new knowledge and improving prediction models. This paper is the first attempt to conduct a literature review on full-scale big data in forecasting research. By source, big data in forecasting research fell into user-generated content data (from the users on social media in texts, photos, etc.), device-monitored data (by meteorological monitors, smart meters, GPS, etc.) and activity log data (for web searching/visiting, online/offline marketing, clinical treatments, laboratory experiments, etc.). Different data types, bearing distinctive information and characteristics, dominated different forecasting tasks, required different analysis technologies and improved different forecasting models. This survey provides an overall review of big data-based forecasting research, details what (regarding data types and sources), where (forecasting hotspots) and how (analysis and forecasting methods used) big data improved prediction, and offers insights into future prospects.|https://doi.org/10.1016/j.bdr.2021.100289|https://www.sciencedirect.com/science/article/pii/S2214579621001064||||2022|Big Data in Forecasting Research: A Literature Review|Ling Tang and Jieyi Li and Hongchuan Du and Ling Li and Jun Wu and Shouyang Wang|article|TANG2022100289|||Big Data Research|22145796||27|||||21100356018|0,565|Q2|25|11|76|547|324|69|4,06|49,73|United States|Northern America|2014-2020|Computer Science Applications (Q2); Information Systems (Q2); Information Systems and Management (Q2); Management Information Systems (Q2)|586|3.578|0.00126|1554609173|1627174784
||Task analysis;Painting;Training data;Quality assurance;Training;Machine learning;Crowdsourcing;Quality Assurance;Self Correction;Machine Teaching||3522-3528|2018 IEEE International Conference on Big Data (Big Data)|"\"Can people learn from machines behavior in microtask based crowdsourcing? Can we train the machines as our mentor even without domain expertise? In this paper, we investigate how the task results improve concerning quality during and after presenting machine prediction as a reference answer in self-correction. Four reference types were examined in the experiment; Correct, Random, Machine prediction trained by correct answers, and that trained by human answers. Learning effects were observed only in presenting machine prediction, although those accuracy rates were far from correct (100%). Moreover, there were no learning effects in \"\"Correct\"\" and \"\"Random\"\". This suggests the following hypothesis: Since machine learners make some \"\"models\"\" for the problem, it is easier for humans to interpret the outputs of machine learners than the results without via them; it is more difficult to interpret not only random answers but also the correct answers in a case where the perfect interpretation of the problem is difficult. Furthermore, some workers answered with higher accuracy rate than machines in the post-test. Therefore, this strategy can be expected to be useful for bootstrapping solutions in the situation where unknown problems occur without expertise or at a low cost.\""|10.1109/BigData.2018.8622435|||||2018|A Learning Effect by Presenting Machine Prediction as a Reference Answer in Self-correction|Matsubara, Masaki and Kobayashi, Masaki and Morishima, Atsuyuki|inproceedings|8622435||Dec|||||||||||||||||||||||||||1556350286|42
CSAI2019|Normal, IL, USA|Ubiquitous Power IoT, Information Security, Data Splitting|6|252–257|Proceedings of the 2019 3rd International Conference on Computer Science and Artificial Intelligence|"\"As one of the national key infrastructures, the ubiquitous power Internet of Things (IoT) provides a convenient method for large-scale power information collection. The widespread transmission of massive power information using data mining techniques for large amounts of data can yield valuable information. Therefore, hacker attacks are endless, posing a threat to the security of the state, society, collectives and individuals. In this paper, we propose a secure transmission scheme of power information, named \"\"SSD\"\" (Split &amp; Signature &amp; Disturbing). In the scheme, after the data is split, it will be anonymized and selected for different paths to be transferred to the destination node. After recombination, the data will be restored. The SSD ensures the indistinguishability and security of the sensitive data by data splitting and disturbing method, and protects the anonymity of individual identities by group signature. The experimental results show that the individual prediction/actual data similarity approaches 0%, and the similarity ratio of the category data (three types in the experiment) is 37.32%, which can be judged to be basically non-correlated.\""|10.1145/3374587.3374631|https://doi.org/10.1145/3374587.3374631|New York, NY, USA|Association for Computing Machinery|9781450376273|2020|A Secure Transmission Scheme of Sensitive Power Information in Ubiquitous Power IoT|Luo, Jingtang and Yao, Shiying and Gou, Jijun and Shuai, Lisha and Cao, Yu|inproceedings|10.1145/3374587.3374631|||||||||||||||||||||||||||||1556982760|42
||Big data analytic (BDA), Technology adoption, Retail industry, Data volume, Data variety, Data velocity, Diffusion of innovations model, RBV theory||103129||Big data analytics (BDA) adoption has gained attention in both practical and theoretical circles owing to the opportunities and advantages that can be reaped from it. In theory, the majority of researchers have evidenced the benefits of BDA, although barriers to its adoption have also been mentioned. This study draws upon the technology-organisation-environment framework and resource-based view theory to propose an integrated model that examines the drivers and impact of BDA adoption in the retail industry in Jordan. The proposed single model encapsulates the aspects of BDA adoption and performance. The study makes use of an online questionnaire survey to collect the required data, and the research model is eventually validated based on 132 responses gathered from the retail industry in Jordan. The findings highlight two major observations. The first is that relative advantage, organisational readiness, top management support, government support, data variety and data velocity all have a significant influence over BDA adoption. The second observation is that a significant association exists between BDA adoption and firm performance, providing information on the way firms can enhance their BDA adoption for enhanced performance. This study contributes to literature dedicated to examining BDA in terms of its drivers and impact on performance and can be used as a reference in developing nations.|https://doi.org/10.1016/j.jretconser.2022.103129|https://www.sciencedirect.com/science/article/pii/S0969698922002223||||2023|Drivers and impact of big data analytic adoption in the retail industry: A quantitative investigation applying structural equation modeling|Abdalwali Lutfi and Mahmaod Alrawad and Adi Alsyouf and Mohammed Amin Almaiah and Ahmad Al-Khasawneh and Akif Lutfi Al-Khasawneh and Ahmad Farhan Alshira'h and Malek Hamed Alshirah and Mohamed Saad and Nahla Ibrahim|article|LUTFI2023103129|||Journal of Retailing and Consumer Services|09696989||70|||||22992|1,568|Q1|89|346|555|27784|4555|550|7,77|80,30|United Kingdom|Western Europe|1994-2021|Marketing (Q1)|10,506|7.135|0.00876|1560529056|296517282
||industrial analytics, anomaly detection, predictive maintenance, hydraulic pump, stochastic modeling||1-6||In this contribution, a data-driven approach towards the prediction of maintenance for the critical component of an injection molding machine is presented. We present our path from exploring and cleaning the data towards the implementation of a prediction algorithm based on kernel density estimation. We give first analytical evidence of the algorithms potential. Moreover, we compare the approach described here with our previous work where we went a model-based approach and present advantages and disadvantages of the two approaches. We try to contribute to a non-comprehensive guide on the implementation of predictive maintenance systems for industrial mass production facilities.|https://doi.org/10.1016/j.ifacol.2019.12.364|https://www.sciencedirect.com/science/article/pii/S2405896319322645||||2019|Is Big Data About to Retire Expert Knowledge? A Predictive Maintenance Study|Domingo Llorente Rivera and Markus R. Scholz and Christoph Bühl and Markus Krauss and Klaus Schilling|article|RIVERA20191|||IFAC-PapersOnLine|24058963|24|52||5th IFAC Symposium on Telematics Applications TA 2019|||21100456158|0,308|Q3|72|115|8407|1913|9863|8351|1,13|16,63|Austria|Western Europe|2002-2019|Control and Systems Engineering (Q3)||||1560907875|676980763
ICCIP '16|Singapore, Singapore|data fusion and exchange, intelligent power distribution and utilization, information model, multi-source and heterogeneous|5|38–42|Proceedings of the 2nd International Conference on Communication and Information Processing|With the development of smart grid and big data technologies, the stability and economy of distribution network operation are enhanced effectively. Intelligent power distribution and utilization (IPDU) big data platform, which exchanges operation data with other related distribution network management systems, makes decisions for demand side management, power system and distributed energy operation strategies by analyzing the big data. In order to solve the data fusion and exchange problems among all information systems, we proposed a kind of general information model for multi-source heterogeneous big data. In addition, a data fusion and exchange mechanism is established based on circle buffer to ensure the data quality. Finally, this paper demonstrates the effective of the method of IPDU big data fusion method by the example of distribution network reconfiguration. The method proposed in this paper can satisfy the data exchanging demands of future smart grid and demand side management, and it also has good confluent and extensible feature.|10.1145/3018009.3018040|https://doi.org/10.1145/3018009.3018040|New York, NY, USA|Association for Computing Machinery|9781450348195|2016|Integration and Exchange Method of Multi-Source Heterogeneous Big Data for Intelligent Power Distribution and Utilization|Xu, Gang and Wu, Shunyu and Xie, Pengfei|inproceedings|10.1145/3018009.3018040|||||||||||||||||||||||||||||1568513914|42
||Big data, integrated infrastructure asset management, master data management, smart city, smart infrastructure||939-947||In recent years, many governments have launched various smart city or smart infrastructure initiatives to improve the quality of citizens’ life and help city managers / planners optimize the operation and management of urban infrastructures. By deploying internet of things (IoT) to infrastructure systems, high-volume and high-variety of data pertinent to the condition and performance of infrastructure systems along with the behaviors of citizens can be gathered, processed, integrated and analyzed through cloud-based infrastructure asset management systems, ubiquitous mobile applications and big data analytics platforms. Nonetheless, how to fully exploit the value of ‘big infrastructure data’ is still a key challenge facing most stakeholders. Unless data is shared by different infrastructure systems in an interoperable and consistent manner, it is difficult to realize the smart infrastructure concept for efficient smart city planning, not to mention about developing appropriate resilience and sustainable programs. To unlock the value of big infrastructure data for smart, sustainable and resilient city planning, a master data management (MDM) solution is proposed in this paper. MDM has been adopted in the business sector to orchestrate operational and analytical big data applications. In order to derive a suitable MDM solution for smart, sustainable and resilient city planning, commercial and open source MDM systems, smart city standards, smart city concept models, smart community infrastructure frameworks, semantic web technologies will be critically reviewed, and feedback and requirements will be gathered from experts who are responsible for developing smart, sustainable and resilient city programs. A case study which focuses on the building and transportation infrastructures of a selected community in Hong Kong will be conducted to pilot the proposed MDM solution.|https://doi.org/10.1016/j.proeng.2017.08.034|https://www.sciencedirect.com/science/article/pii/S1877705817331569||||2017|A Master Data Management Solution to Unlock the Value of Big Infrastructure Data for Smart, Sustainable and Resilient City Planning|S. Thomas Ng and Frank J. Xu and Yifan Yang and Mengxue Lu|article|NG2017939|||Procedia Engineering|18777058||196||Creative Construction Conference 2017, CCC 2017, 19-22 June 2017, Primosten, Croatia|||18700156717|0,320|-|74|0|5873|0|9870|5804|1,88|0,00|Netherlands|Western Europe|2009-2019|Engineering (miscellaneous)||||1575138861|506091674
||Biodiversity informatics, Data-cleaning, SDM performance, MaxEnt, Australian mammals, Big-data||139-145||The recent availability of species occurrence data from numerous sources, standardized and connected within a single portal, has the potential to answer fundamental ecological questions. These aggregated big biodiversity databases are prone to numerous data errors and biases. The data-user is responsible for identifying these errors and assessing if the data are suitable for a given purpose. Complex technical skills are increasingly required for handling and cleaning biodiversity data, while biodiversity scientists possessing these skills are rare. Here, we estimate the effect of user-level data cleaning on species distribution model (SDM) performance. We implement several simple and easy-to-execute data cleaning procedures, and evaluate the change in SDM performance. Additionally, we examine if a certain group of species is more sensitive to the use of erroneous or unsuitable data. The cleaning procedures used in this research improved SDM performance significantly, across all scales and for all performance measures. The largest improvement in distribution models following data cleaning was for small mammals (1g–100g). Data cleaning at the user level is crucial when using aggregated occurrence data, and facilitating its implementation is a key factor in order to advance data-intensive biodiversity studies. Adopting a more comprehensive approach for incorporating data cleaning as part of data analysis, will not only improve the quality of biodiversity data, but will also impose a more appropriate usage of such data.|https://doi.org/10.1016/j.ecoinf.2016.06.001|https://www.sciencedirect.com/science/article/pii/S1574954116300577||||2016|Quantifying the value of user-level data cleaning for big data: A case study using mammal distribution models|Tomer Gueta and Yohay Carmel|article|GUETA2016139|||Ecological Informatics|15749541||34|||||3100147401|0,774|Q1|55|109|273|5588|964|271|3,43|51,27|Netherlands|Western Europe|2006-2020|Modeling and Simulation (Q1); Applied Mathematics (Q2); Computational Theory and Mathematics (Q2); Computer Science Applications (Q2); Ecological Modeling (Q2); Ecology (Q2); Ecology, Evolution, Behavior and Systematics (Q2)|2,893|3.142|0.00332|1578147310|339174395
|||12|1138–1149||This paper gives an overview of Turn Data Management Platform (DMP). We explain the purpose of this type of platforms, and show how it is positioned in the current digital advertising ecosystem. We also provide a detailed description of the key components in Turn DMP. These components cover the functions of (1) data ingestion and integration, (2) data warehousing and analytics, and (3) real-time data activation. For all components, we discuss the main technical and research challenges, as well as the alternative design choices. One of the main goals of this paper is to highlight the central role that data management is playing in shaping this fast growing multi-billion dollars industry.|10.14778/2536222.2536238|https://doi.org/10.14778/2536222.2536238||VLDB Endowment||2013|Overview of Turn Data Management Platform for Digital Advertising|Elmeleegy, Hazem and Li, Yinan and Qi, Yan and Wilmot, Peter and Wu, Mingxi and Kolay, Santanu and Dasdan, Ali and Chen, Songting|article|10.14778/2536222.2536238||aug|Proc. VLDB Endow.|21508097|11|6|August 2013||||21100199855|0,946|Q1|134|246|598|12401|3759|566|5,50|50,41|United States|Northern America|2008-2020|Computer Science (miscellaneous) (Q1)|7,198|2.047|0.00891|1582152805|1216159931
||||e773-e774|||https://doi.org/10.1016/j.ijrobp.2020.07.224|https://www.sciencedirect.com/science/article/pii/S0360301620316436||||2020|Radiotherapy (RT) Patterns Of Practice Variability Identified As A Challenge To Real-World Big Data: Recommendations From The Learning From Analysis Of Multicenter Big Data Aggregation (LAMBDA) Consortium|A.L. Caissie and M.L. Mierzwa and C.D. Fuller and M. Rajaraman and A. Lin and A.M. McDonald and R.A. Popple and Y. Xiao and L. {van Dijk} and P. Balter and H. Fong and H. Ping and M. Kovoor and J. Lee and A. Rao and M.K. Martel and R.F. Thompson and B. Merz and J. Yao and C. Mayo|article|CAISSIE2020e773|||International Journal of Radiation Oncology*Biology*Physics|03603016|3, Supplement|108||Proceedings of the American Society for Radiation Oncology|||17191|2,117|Q1|248|584|1807|14085|6588|1167|3,48|24,12|United States|Northern America|1975-2020|Cancer Research (Q1); Oncology (Q1); Radiation (Q1); Radiology, Nuclear Medicine and Imaging (Q1)|50,525|7.038|0.03941|1583066793|1023425957
||web based surveys, smartphone surveys, respondent interaction, respondent burden||289-296||Web and smartphone surveys are increasingly being used to collect travel information. This workshop explored respondent interaction with these tools, covering a range of research concerns. While smartphone surveys facilitate real-time passive collection of continuous data, thereby reducing respondent burden, their use raises many issues common with those present in web surveys. These include survey design, sample representativeness, privacy, respondent burden, data quality and validation. Workshop participants considered possible areas for future research on these issues and others such as provision of feedback to respondents, linking with big data and focusing on attitudinal and behavioural motivations.|https://doi.org/10.1016/j.trpro.2015.12.025|https://www.sciencedirect.com/science/article/pii/S2352146515003166||||2015|Workshop Synthesis: Respondent/Survey Interaction in a World of Web and Smartphone Apps|João de Abreu e Silva and Mark Davis|article|SILVA2015289|||Transportation Research Procedia|23521465||11||Transport Survey Methods: Embracing Behavioural and Technological Changes Selected contributions from the 10th International Conference on Transport Survey Methods 16-21 November 2014, Leura, Australia|||||||||||||||||||||1584379135|83400274
||Big data;Data models;Computers;Data mining;Industries;Algorithm design and analysis;Distributed databases;Web Big Data;Data Consistency;Web Data Management;Data Quality Assessment;Data Analysis||73-79|2015 Third International Conference on Advanced Cloud and Big Data|The vigorous growth of big data has triggered both opportunities and challenges in business and industry. However, Web big data distributed in diverse sources with multiple data structures frequently conflict with each other, i.e. inconsistency in cross-source Web big data. In this paper, we propose a state-of-the-art architecture of auto-discovering inconsistency with Web big data. Our contributions include: (1) we classify the inconsistency features to formalize inconsistency data and establish an algebraic operation system, (2) we propose three algorithms to auto-discover inconsistency, including constraint-based, SDA-based and HPDM-based method and (3) we conduct experiments on real-world dataset to compare aforesaid schemes with Oracle-based inconsistency detection framework. The empirical results show that our methods outperform traditional framework both on accuracy and efficiency under Web big data.|10.1109/CBD.2015.22|||||2015|An Automatic Discovery Framework of Cross-Source Data Inconsistency for Web Big Data|Yang, Sha and Yu, Wei and Hu, Yahui and Wang, Kai and Wang, Jun and Li, Shijun|inproceedings|7435456||Oct|||||||||||||||||||||||||||1584938570|42
BCB '19|Niagara Falls, NY, USA|anomaly detection, negative selection algorithm, rna-seq, sample quality, ribosome profiling, machine learning|9|457–465|Proceedings of the 10th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics|RNA-seq and Ribo-seq are popular techniques for quantifying cellular transcription and translation. These experiments use next-generation sequencing to produce genome-wide high-resolution snapshots of the total populations of mRNAs and translating ribosomes within the investigated samples. When performed in concert, these experiments yield valuable information about protein synthesis rates and translational efficiency. Due to their intricate experimental protocols and demanding data processing requirements, quality control and analysis of such experiments are often challenging. Therefore, methods for accurately assessing data quality, and for identifying contaminated samples, are greatly needed. In the following we use a novel negative selection inspired algorithm called Boundary Detection Using Nearest Neighbors (BDUNN), for the identification of corrupted samples. Our algorithm constructs a detector set and reduced training set that defines the boundaries between normal data points and potential anomalies. Subsequently, a nearest neighbor algorithm is used to classify unseen observations. We compare the performance of BDUNN with other popular negative selection and one-class classification algorithms, and show that BDUNN is capable of accurately and efficiently detecting anomalies in standard anomaly detection datasets and simulated RNA-seq and Ribo-seq data sets. Furthermore, we have implemented our method within an existing R Shiny platform for analyzing RNA-seq an Ribo-seq datasets, which permits downstream analysis of anomalous samples.|10.1145/3307339.3342169|https://doi.org/10.1145/3307339.3342169|New York, NY, USA|Association for Computing Machinery|9781450366663|2019|Using a Novel Negative Selection Inspired Anomaly Detection Algorithm to Identify Corrupted Ribo-Seq and RNA-Seq Samples|Perkins, Patrick and Heber, Steffen|inproceedings|10.1145/3307339.3342169|||||||||||||||||||||||||||||1591492144|42
||Big data, Oil and gas, Information technologies, Data||117-121||The upstream oil and gas industry has been contending with massive data sets and monolithic files for many years, but “Big Data” is a relatively new concept that has the potential to significantly re-shape the industry. Despite the impressive amount of value that is being realized by Big Data technologies in other parts of the marketplace, however, much of the data collected within the oil and gas sector tends to be discarded, ignored, or analyzed in a very cursory way. This viewpoint examines existing data management practices in the upstream oil and gas industry, and compares them to practices and philosophies that have emerged in organizations that are leading the way in Big Data. The comparison shows that, in companies that are widely considered to be leaders in Big Data analytics, data is regarded as a valuable asset—but this is usually not true within the oil and gas industry insofar as data is frequently regarded there as descriptive information about a physical asset rather than something that is valuable in and of itself. The paper then discusses how the industry could potentially extract more value from data, and concludes with a series of policy-related questions to this end.|https://doi.org/10.1016/j.enpol.2015.02.020|https://www.sciencedirect.com/science/article/pii/S0301421515000932||||2015|Data as an asset: What the oil and gas sector can learn from other industries about “Big Data”|Robert K. Perrons and Jesse W. Jensen|article|PERRONS2015117|||Energy Policy|03014215||81|||||29403|2,093|Q1|217|679|2165|40582|14111|2135|6,29|59,77|United Kingdom|Western Europe|1973-2020|Energy (miscellaneous) (Q1); Management, Monitoring, Policy and Law (Q1)|60,369|6.142|0.04319|1591723662|1660925188
SAICSIT '19|Skukuza, South Africa|Systematic Literature Review, Skills, Data Science, Competency|8||Proceedings of the South African Institute of Computer Scientists and Information Technologists 2019|The paper presents a systematic literature review of the literature on the competencies that are essential to develop a globally competitive workforce in the field of data science. The systematic review covers a wide range of literature but focuses primarily, but not exclusively, on the computing, information systems, management, and organisation science literature. The paper uses a broad research search strategy covering four separate electronic databases. The search strategy led the researchers to scan 139 titles, abstracts and keywords. Sixty potentially relevant articles were identified, of which 42 met the quality criteria and contributed to the analysis. A critical appraisal checklist assessed the validity of each empirical study. The researchers grouped the findings under six broad competency themes: organisational, technical, analytical, ethical and regulatory, cognitive and social. Thematic analysis was used to develop a unified model of data science competency based on the evidence of the findings. This model will be applied to case studies and survey research in future studies. A unified data science competency model, supported by empirical evidence, is crucial in closing the skills gap, thereby improving the quality and competitiveness of the South Africa's data science workforce. Researchers are encouraged to contribute to the further conceptual development of data science competency.|10.1145/3351108.3351110|https://doi.org/10.1145/3351108.3351110|New York, NY, USA|Association for Computing Machinery|9781450372657|2019|Data Science Competency in Organisations: A Systematic Review and Unified Model|Hattingh, Mari\'{e} and Marshall, Linda and Holmner, Marlene and Naidoo, Rennie|inproceedings|10.1145/3351108.3351110|1||||||||||||||||||||||||||||1591725382|42
||Battery life prediction, RVM optimization prediction, Parameter transfer: RUL probability prediction||103342||This paper proposes the architecture of the combination of the battery management system (BMS) and the cloud big data platform. Firstly, BMS measures and extracts the mean voltage falloff (MVF). A regression model of capacity and MVF based on historical data is established with generalized Box-Cox Transformation and least squares. The capacity and MVF are uploaded to the cloud big data platform, and then the mean and variance of the MVF is predicted based on the relevance vector machine, thereby realizing the 2σ range prediction of the lithium battery's state of health and the probability density function prediction of the remaining useful life. This paper makes two contributions to the data-driven prediction method. First, the edge-cloud collaborative computing architecture combining BMS and cloud is proposed, which effectively utilizes the advantages of BMS data quality and cloud computing power. Second, through the combination of relevance vector machine with particle swarm optimization and horizontal parameter transfer, the number of samples required for model learning is reduced to 30% and has better accuracy and robustness. Through the verification of NASA data, the results show that the average error is less than 2.18%.|https://doi.org/10.1016/j.est.2021.103342|https://www.sciencedirect.com/science/article/pii/S2352152X21010331||||2021|Remaining useful life prediction with probability distribution for lithium-ion batteries based on edge and cloud collaborative computation|Yong Zhou and Huanghui Gu and Teng Su and Xuebing Han and Languang Lu and Yuejiu Zheng|article|ZHOU2021103342|||Journal of Energy Storage|2352152X||44|||||21100400826|1,088|Q1|42|860|831|44642|5383|828|6,87|51,91|Netherlands|Western Europe|2015-2020|Electrical and Electronic Engineering (Q1); Energy Engineering and Power Technology (Q1); Renewable Energy, Sustainability and the Environment (Q2)|7,765|6.583|0.00926|1592370961|593635875
CAIN '22|Pittsburgh, Pennsylvania|data sovereignty, AI engineering, lessons learned, collaborative AI|12|193–204|Proceedings of the 1st International Conference on AI Engineering: Software Engineering for AI|The establishment of collaborative AI pipelines, in which multiple organizations share their data and models, is often complicated by lengthy data governance processes and legal clarifications. Data sovereignty solutions, which ensure data is being used under agreed terms and conditions, are promising to overcome these problems. However, there is limited research on their applicability in AI pipelines. In this study, we extended an existing AI pipeline at Mondragon Corporation, in which sensor data is collected and subsequently forwarded to a data quality service provider with a data sovereignty component. By systematically reflecting and generalizing our experiences during the twelve-month action research project, we formulated ten lessons learned, four benefits, and three barriers to data-sovereign AI pipelines that can inform further research and custom implementations. Our results show that a data sovereignty component can help reduce existing barriers and increase the success of collaborative data science initiatives.|10.1145/3522664.3528593|https://doi.org/10.1145/3522664.3528593|New York, NY, USA|Association for Computing Machinery|9781450392754|2022|Data Sovereignty for AI Pipelines: Lessons Learned from an Industrial Project at Mondragon Corporation|Altendeitering, Marcel and Pampus, Julia and Larrinaga, Felix and Legaristi, Jon and Howar, Falk|inproceedings|10.1145/3522664.3528593|||||||||||||||||||||||||||||1592764250|42
||||1-1|2016 International Conference on Information Technology Systems and Innovation (ICITSI)|The following topics are dealt with: SDLC SPASI v. 4.0. business process; information extraction; statistics indicator tables; rule generalizations; ontology; conventional learning system; ICT-based learning; job training system; time-series data; RAID; software-based accelerator; virtualization environment; enterprise architecture government organization; TOGAF ADM; SONA; e- library; modified quantitative models for performance measurement system method; business process improvement; district government innovation service case study; government organization; m-government implementation evaluation; trusted Big Data; official statistics study case; data profiling; data quality improvement; secure internet access; copyright protection; color images; transform domain; luminance component; information network architecture; local government; software as a service; expert system; meningitis disease; certainty factor method; digital asset management system; broadcasting organizations; e-portofolio definition; system security requirement identification; electronic payment system; Internet-based long distance education; operational model data governance; requirement engineering; open government information network development; process capability assessment; information security management; information security governance; national cyber physical systems; e-learning readiness; remote control system; serial communications mobile; microcontroller; knowledge sharing; indonesia higher educational institutions; cultural heritage metadata; geo linked open data; NUSANTARA: knowledge management system; adaptive personalized learning system; interactive learning media; RPP ICT; government human capital management; knowledge management tools utilization; knowledge management readiness; analytic hierarchy process; government institutions; usability testing; scrum methodology; assistant information system; automatic arowana raiser; pSPEA2; strength Pareto evolutionary algorithm 2; early diagnosis expert system deficiency; digital forensic; user acceptance; human resource information system; automated plasmodium detection; malaria diagnosis; thin blood smear image; 3D virtual game; MOODLE; SLOODLE; open simulator case study; color-based segmentation; feature detection; ball post; goal post; mobile soccer robot game field; smart farming; real time q-log-based feature normalization; distant speech recognition; Monte Carlo localization; robot operating system; finite element method; 3D DC resistivity modeling; multi GPU; breast cancer lesions; adaptive thresholding; morphological operation; gamification framework; online training; collaborative working system; classification breast cancer ultrasound images; posterior features; three-wheeled omnidirectional robot controller; public services satisfaction; sentiment analysis; color blind test quantification; RGB primary color cluster; ERP modules requirement; micro, small and medium enterprise fashion industry; small culinary enterprises; business system requirement; small craft companies ; power analysis attack; DES and IT value model.|10.1109/ICITSI.2016.7858181|||||2016|[Title page]||inproceedings|7858181||Oct|||||||||||||||||||||||||||1595845966|42
||Big data;Context;Quality management;Frequency measurement;Organizations;Big Data;Data quality;Data profiling;Data cleansing;data quality rules;dimensions;metrics||1-9|2015 International Conference on Computing, Communication and Security (ICCCS)|Data quality management systems are thoroughly researched topics and have resulted in many tools and techniques developed by both academia and industry. However, the advent of Big Data might pose some serious questions pertaining to the applicability of existing data quality concepts. There is a debate concerning the importance of data quality for Big Data; one school of thought argues that high data quality methods are essential for deriving higher level analytics while another school of thought argues that data quality level will not be so important as the volume of Big Data would be used to produce patterns and some amount of dirty data will not mask the analytic results which might be derived. This paper aims to investigate various components and activities forming part of data quality management such as dimensions, metrics, data quality rules, data profiling and data cleansing. The result list existing challenges and future research areas associated with Big Data for data quality management.|10.1109/CCCS.2015.7374131|||||2015|Overview of data quality challenges in the context of Big Data|Juddoo, Suraj|inproceedings|7374131||Dec|||||||||||||||||||||||||||1597451323|42
||mapping, clustering, scan statistics, Hotspot, statistical rigor|38|||Mapping of spatial hotspots, i.e., regions with significantly higher rates of generating cases of certain events (e.g., disease or crime cases), is an important task in diverse societal domains, including public health, public safety, transportation, agriculture, environmental science, and so on. Clustering techniques required by these domains differ from traditional clustering methods due to the high economic and social costs of spurious results (e.g., false alarms of crime clusters). As a result, statistical rigor is needed explicitly to control the rate of spurious detections. To address this challenge, techniques for statistically-robust clustering (e.g., scan statistics) have been extensively studied by the data mining and statistics communities. In this survey, we present an up-to-date and detailed review of the models and algorithms developed by this field. We first present a general taxonomy for statistically-robust clustering, covering key steps of data and statistical modeling, region enumeration and maximization, and significance testing. We further discuss different paradigms and methods within each of the key steps. Finally, we highlight research gaps and potential future directions, which may serve as a stepping stone in generating new ideas and thoughts in this growing field and beyond.|10.1145/3487893|https://doi.org/10.1145/3487893|New York, NY, USA|Association for Computing Machinery||2022|Statistically-Robust Clustering Techniques for Mapping Spatial Hotspots: A Survey|Xie, Yiqun and Shekhar, Shashi and Li, Yan|article|10.1145/3487893|36|jan|ACM Comput. Surv.|03600300|2|55|March 2023||||23038|2,079|Q1|163|112|365|15859|6978|365|19,16|141,60|United States|Northern America|1969-2020|Computer Science (miscellaneous) (Q1); Theoretical Computer Science (Q1)|10,790|10.282|0.01438|1599494916|1517405264
Progress in Medicinal Chemistry||Big Data, Artificial intelligence, Drug discovery, Biology, Chemistry, Clinical trials||277-356||Interpretation of Big Data in the drug discovery community should enhance project timelines and reduce clinical attrition through improved early decision making. The issues we encounter start with the sheer volume of data and how we first ingest it before building an infrastructure to house it to make use of the data in an efficient and productive way. There are many problems associated with the data itself including general reproducibility, but often, it is the context surrounding an experiment that is critical to success. Help, in the form of artificial intelligence (AI), is required to understand and translate the context. On the back of natural language processing pipelines, AI is also used to prospectively generate new hypotheses by linking data together. We explain Big Data from the context of biology, chemistry and clinical trials, showcasing some of the impressive public domain sources and initiatives now available for interrogation.|https://doi.org/10.1016/bs.pmch.2017.12.003|https://www.sciencedirect.com/science/article/pii/S0079646817300243||Elsevier||2018|Chapter Five - Big Data in Drug Discovery|Nathan Brown and Jean Cambruzzi and Peter J. Cox and Mark Davies and James Dunbar and Dean Plumbley and Matthew A. Sellwood and Aaron Sim and Bryn I. Williams-Jones and Magdalena Zwierzyna and David W. Sheppard|incollection|BROWN2018277||||00796468||57||||David R. Witty and Brian Cox|||||||||||||||||||1600064515|2036198599
||Fault detection;Data integrity;Data models;Semantics;Decision trees;Self-organizing feature maps;Inspection;Big Data;Data quality tests;Explainable learning;Interactive learning;Unsupervised learning||200-205|2019 IEEE International Conference on Big Data (Big Data)|Data quality tests validate heterogeneous data to detect violations of syntactic and semantic constraints. The specification of these constraints can be incomplete because domain experts typically specify them in an ad hoc manner. Existing automated test approaches can generate false alarms and do not explain the constraint violations while reporting faulty data records. In previous work, we proposed ADQuaTe, which is an automated data quality test approach that uses an unsupervised deep learning techni que (1) to discover constraints from big datasets that may have been missed by experts, and (2) to label as suspicious those records that violate the constraints. These records are grouped and explanations for constraint violations are presented to domain experts who determine whether or not the groups are actually faulty. This paper presents ADQuaTe2, which extends ADQuaTe to use an interactive learning technique that incorporates expert feedback to retrain the learning model and improve the accuracy of constraint discovery and fault detection. We evaluate the effectiveness of the approach on real-world datasets from a health data warehouse and a plant diagnosis database. We also use datasets with known faults from the UCI repository to evaluate the improvement in the accuracy of the approach after incorporating ground truth knowledge.|10.1109/BigData47090.2019.9006446|||||2019|An Interactive Data Quality Test Approach for Constraint Discovery and Fault Detection|Homayouni, Hajar and Ghosh, Sudipto and Ray, Indrakshi and Kahn, Michael G|inproceedings|9006446||Dec|||||||||||||||||||||||||||1600560274|42
ICCDA 2019|Kahului, HI, USA|Big Data, Scala, Singular Value Decomposition (SVD), Latent Semantic Analysis(LSA), Spark, Natural Language Processing(NLP)|8|55–62|Proceedings of the 2019 3rd International Conference on Compute and Data Analysis|Job markets are experiencing an exponential growth in data alongside the recent explosion of big data in various domains including health, security and finance. Staying current with job market trends entails collecting, processing and analyzing huge amounts of data. A typical challenge with analyzing job listings is that they vary drastically with regards to verbiage, for instance a given job title or skill can be referred to using different words or industry jargons. As a result, it becomes incumbent to go beyond words present in job listings and carry out analysis aimed at discovering latent structures and trends in job listings. In this paper, we present a systematic approach of uncovering latent trends in job markets using big data technologies (Apache Spark and Scala) and distributed semantic techniques such as latent semantic analysis (LSA). We show how LSA can uncover patterns/relationships/trends that will otherwise remain hidden if using traditional text mining techniques that rely only on word frequencies in documents.|10.1145/3314545.3314566|https://doi.org/10.1145/3314545.3314566|New York, NY, USA|Association for Computing Machinery|9781450366342|2019|Using Spark and Scala for Discovering Latent Trends in Job Markets|Mbah, Raymond Blanch K. and Rege, Manjeet and Misra, Bhabani|inproceedings|10.1145/3314545.3314566|||||||||||||||||||||||||||||1601016818|42
||Big Data, Smart Data, Performance Management||857-862||In the context of digitalization, some companies are considering a transition to Industry 4.0 to ensure greater flexibility, productivity and responsiveness. The implementation of a relevant performance management system is then a real necessity to measure the degree of achievement of these objectives. In the era of Industry 4.0, the potential access to large amounts of data, i.e. Big Data, poses new challenges to the design and implementation of these systems. With the exponential growth of data generated from different sources, there is a need for extensive exploitation of data for performance management. Given the large volume of data, the speed at which it is generated and the variety of data sources, the manufacturing sector is facing with the challenge of creating value from large data sets. This paper introduces some potential benefits of Big Data for business and in particular its role in performance management systems. However, the key idea is that Big Data are not always neither available nor necessary. Authors focus on the concept of smart data, the result of the transformation of Big Data, and define a set of necessary and sufficient conditions the data should satisfy to be considered as Smart. The paper presents some methods of smart data extraction. Such smart data will be used to feed the performance management system in order to obtain more accurate, timely and representative key performance indicators.|https://doi.org/10.1016/j.ifacol.2021.08.100|https://www.sciencedirect.com/science/article/pii/S2405896321008491||||2021|From Big Data to Smart Data: Application to performance management|Amel Souifi and Zohra Cherfi Boulanger and Marc Zolghadri and Maher Barkallah and Mohamed Haddar|article|SOUIFI2021857|||IFAC-PapersOnLine|24058963|1|54||17th IFAC Symposium on Information Control Problems in Manufacturing INCOM 2021|||21100456158|0,308|Q3|72|115|8407|1913|9863|8351|1,13|16,63|Austria|Western Europe|2002-2019|Control and Systems Engineering (Q3)||||1601277925|676980763
|||8|54–61||Bias in Web data and use taints the algorithms behind Web-based applications, delivering equally biased results.|10.1145/3209581|https://doi.org/10.1145/3209581|New York, NY, USA|Association for Computing Machinery||2018|Bias on the Web|Baeza-Yates, Ricardo|article|10.1145/3209581||may|Commun. ACM|00010782|6|61|June 2018||||||||||||||||||||||1601768123|647144465
||Agile enterprise data warehousing, surface solutions, backfilling the architecture, shadow IT, data virtualization, big data, Hadoop, HDFS, Map/Reduce, Hive||293-327|Agile Data Warehousing for the Enterprise|Without investing in exotic data modeling techniques, EDW teams can achieve fast delivery using “surface solutions.” Surface solutions allow developers to first solve business problems with data taken from landing areas and then steadily “backfill” the DW/BI reference architecture to provide progressively more complete and robust solutions. Teams can create surface solutions by leveraging shadow IT, using data virtualization, and tapping a big data platform. When leveraging shadow IT, the EDW team delivers progressively richer data sets to departmental staff members, who build their own temporary BI solutions using that information. The data virtualization strategy relies on a “superoptimizer” that can create views across databases and data types, even including semistructured data as needed. The big data strategy employs a new category of products such as Hadoop’s HDFS, MapReduce, and Hive to provide access to new data, whether it be very large, poorly structured, and/or just unfamiliar to IT and the business users.|https://doi.org/10.1016/B978-0-12-396464-9.00013-8|https://www.sciencedirect.com/science/article/pii/B9780123964649000138|Boston|Morgan Kaufmann|978-0-12-396464-9|2016|Chapter 13 - Surface Solutions Using Data Virtualization and Big Data|Ralph Hughes|incollection|HUGHES2016293||||||||||Ralph Hughes|||||||||||||||||||1604917650|42
WiPSCE '14|Berlin, Germany|secondary school, analysis, databases, curricula, standards, data management, characterization|8|29–36|Proceedings of the 9th Workshop in Primary and Secondary Computing Education|"\"In the last few years, the focus of data management has changed from handling relatively small amounts of data, often in relational databases, to managing large amounts of data using various different database types. In many secondary school curricula, data management is mainly considered from a \"\"database\"\" perspective. However, in contrast to the developments in computer science research and practice, the new and changing aspects of data management have hardly been discussed with respect to CS education. We suggest re-evaluating the focus and relevance of the established database syllabi, to discuss the educational value of the newly arising developments and to prevent the teaching of outdated concepts. In this paper, we will contrast current educational standards and curricula with an up-to-date characterization of data management in order to identify gaps between the principles and concepts of data management that are considered as important today from a professional point of view on the one side, and the emphasis in current CS education on the other side.The findings of this analysis will provide a basis for aligning the concepts taught in CS education with the developments in data management research and practice, as well as for re-evaluating the educational value of these concepts.\""|10.1145/2670757.2670779|https://doi.org/10.1145/2670757.2670779|New York, NY, USA|Association for Computing Machinery|9781450332507|2014|A Comparison of the Field Data Management and Its Representation in Secondary CS Curricula|Grillenberger, Andreas and Romeike, Ralf|inproceedings|10.1145/2670757.2670779|||||||||||||||||||||||||||||1606977234|42
||Big data analytics, IoT, Energy harvesting||155-164||Current advancements and growth in the arena of the Internet of Things (IoT) is providing great potential in the novel epoch of healthcare. The future of healthcare is expansively promising, as it advances the excellence of life and health of humans, involving several health regulations. Continual increases of multifaceted IoT devices in healthcare is beset by challenges, such as powering IoT terminal nodes used for health monitoring, data processing, smart decisions, and event management. In this paper, we propose a healthcare architecture which is based on an analysis of energy harvesting for health monitoring sensors and the realization of Big Data analytics in healthcare. The rationale of the proposed architecture is two-fold: (1) comprehensive conceptual framework for energy harvesting for health monitoring sensors; and (2) data processing and decision management for healthcare. The proposed architecture is a three-layered architecture that comprises: (1) energy harvesting and data generation; (2) data pre-processing; and (3) data processing and application. The proposed scheme highlights the effectiveness of energy-harvesting based IoT in healthcare. In addition, it also proposes a solution for smart health monitoring and planning. We also utilized consistent datasets on the Hadoop server to validate the proposed architecture based on threshold limit values (TLVs). The study demonstrates that the proposed architecture offers substantial and immediate value to the field of smart health.|https://doi.org/10.1016/j.suscom.2017.10.009|https://www.sciencedirect.com/science/article/pii/S2210537917302238||||2018|Energy-harvesting based on internet of things and big data analytics for smart health monitoring|Muhammad Babar and Ataur Rahman and Fahim Arif and Gwanggil Jeon|article|BABAR2018155|||Sustainable Computing: Informatics and Systems|22105379||20|||||19700201455|0,591|Q1|27|78|178|3347|766|169|4,50|42,91|United States|Northern America|2011-2020|Computer Science (miscellaneous) (Q1); Electrical and Electronic Engineering (Q2)||||1611130012|1556024950
||data cleansing, big data, data quality||731-738||Massive amounts of data are available for the organization which will influence their business decision. Data collected from the various resources are dirty and this will affect the accuracy of prediction result. Data cleansing offers a better data quality which will be a great help for the organization to make sure their data is ready for the analyzing phase. However, the amount of data collected by the organizations has been increasing every year, which is making most of the existing methods no longer suitable for big data. Data cleansing process mainly consists of identifying the errors, detecting the errors and corrects them. Despite the data need to be analyzed quickly, the data cleansing process is complex and time-consuming in order to make sure the cleansed data have a better quality of data. The importance of domain expert in data cleansing process is undeniable as verification and validation are the main concerns on the cleansed data. This paper reviews the data cleansing process, the challenge of data cleansing for big data and the available data cleansing methods.|https://doi.org/10.1016/j.procs.2019.11.177|https://www.sciencedirect.com/science/article/pii/S1877050919318885||||2019|A Review on Data Cleansing Methods for Big Data|Fakhitah Ridzuan and Wan Mohd Nazmee {Wan Zainon}|article|RIDZUAN2019731|||Procedia Computer Science|18770509||161||The Fifth Information Systems International Conference, 23-24 July 2019, Surabaya, Indonesia|||19700182801|0,334|-|76|1907|6483|38511|13722|6359|2,09|20,19|Netherlands|Western Europe|2010-2020|Computer Science (miscellaneous)||||1613507316|2108686752
||requirements for metrics, data quality metrics, data quality assessment, Data quality|32|||Data quality and especially the assessment of data quality have been intensively discussed in research and practice alike. To support an economically oriented management of data quality and decision making under uncertainty, it is essential to assess the data quality level by means of well-founded metrics. However, if not adequately defined, these metrics can lead to wrong decisions and economic losses. Therefore, based on a decision-oriented framework, we present a set of five requirements for data quality metrics. These requirements are relevant for a metric that aims to support an economically oriented management of data quality and decision making under uncertainty. We further demonstrate the applicability and efficacy of these requirements by evaluating five data quality metrics for different data quality dimensions. Moreover, we discuss practical implications when applying the presented requirements.|10.1145/3148238|https://doi.org/10.1145/3148238|New York, NY, USA|Association for Computing Machinery||2018|Requirements for Data Quality Metrics|Heinrich, Bernd and Hristova, Diana and Klier, Mathias and Schiller, Alexander and Szubartowicz, Michael|article|10.1145/3148238|12|jan|J. Data and Information Quality|19361955|2|9|June 2017||||||||||||||||||||||1617507416|833754770
||Big Data architecture, Forecasting, Nowcasting, Data lifecycle, Socio-economic data, Non-traditional data sources, Non-traditional analysis methods||99-113||The Data Big Bang that the development of the ICTs has raised is providing us with a stream of fresh and digitized data related to how people, companies and other organizations interact. To turn these data into knowledge about the underlying behavior of the social and economic agents, organizations and researchers must deal with such amount of unstructured and heterogeneous data. Succeeding in this task requires to carefully plan and organize the whole process of data analysis taking into account the particularities of the social and economic analyses, which include the wide variety of heterogeneous sources of information and a strict governance policy. Grounded on the data lifecycle approach, this paper develops a Big Data architecture that properly integrates most of the non-traditional information sources and data analysis methods in order to provide a specifically designed system for forecasting social and economic behaviors, trends and changes.|https://doi.org/10.1016/j.techfore.2017.07.027|https://www.sciencedirect.com/science/article/pii/S0040162517310946||||2018|Big Data sources and methods for social and economic analyses|Desamparados Blazquez and Josep Domenech|article|BLAZQUEZ201899|||Technological Forecasting and Social Change|00401625||130|||||14704|2,226|Q1|117|448|1099|35581|10127|1061|9,01|79,42|United States|Northern America|1970-2020|Applied Psychology (Q1); Business and International Management (Q1); Management of Technology and Innovation (Q1)|21,116|8.593|0.02416|1618267976|1949868303
||Data integration, source selection, knowledge extraction|5||||10.1145/3362121|https://doi.org/10.1145/3362121|New York, NY, USA|Association for Computing Machinery||2019|Ethical Dimensions for Data Quality|Firmani, Donatella and Tanca, Letizia and Torlone, Riccardo|article|10.1145/3362121|2|dec|J. Data and Information Quality|19361955|1|12|March 2020||||||||||||||||||||||1619519484|833754770
||Ko-CHENS, Children, Environment, Cohort profile, Birth cohort||358-366||The Korean CHildren's ENvironmental health Study (Ko-CHENS) is a nationwide prospective birth cohort showing the correlation between the environmental exposures and the health effects to prevent the environmental diseases in children, and it provides the guidelines for the environmental hazardous factors, applying the life-course approach to the environmental-health management system. The Ko-CHENS consists of 5000 Core and 65,000 Main Cohorts. The children in the Core Cohort are followed up at 6 months, every year before their admission into the elementary school, and every 3 years from the first year after this admission. The children in the Cohort will be followed up through the data links (Statistics Korea, National Health Insurance Service [NHIS], and Ministry of Education). The individual biospecimens will be analyzed for 19 substances. The long-term-storage biological samples will be used for the further substance analysis. The Ko-CHENS will investigate whether the environmental variables including the perinatal outdoor and indoor factors and the greenness contribute causally to the health outcomes in the children and adolescents. In addition to the individual surveys, the assessments of the outdoor exposures and health outcomes will use the national air-quality monitoring data and claim data of the NHIS, respectively. The two big-data forms of the Ko-CHENS are as follows: The Ko-CHENS data that can be linked with the nationally registered NHIS health-related database, including the medical utilization and the periodic health screening, and the birth/mortality database in the Statistics; the other is the Big-CHENS dataset that is based on the NHIS mother delivery code, for which the follow-up of almost 97% of the total birth population is expected. The Ko-CHENS is a very cost-effective study that fully exploits the existing national big-data systems with the data linkage.|https://doi.org/10.1016/j.envres.2018.12.009|https://www.sciencedirect.com/science/article/pii/S0013935118306388||||2019|Cohort profile: Beyond birth cohort study – The Korean CHildren's ENvironmental health Study (Ko-CHENS)|Kyoung Sook Jeong and Suejin Kim and Woo Jin Kim and Hwan-Cheol Kim and Jisuk Bae and Yun-Chul Hong and Mina Ha and Kangmo Ahn and Ji-Young Lee and Yangho Kim and Eunhee Ha|article|JEONG2019358|||Environmental Research|00139351||172|||||||||||||||||||||||1620373211|2119809022
||Artificial intelligence (AI), Big data analytics, General Data Protection Regulation (GDPR), Fairness, Regulations, Collective rights, Data ethics||257-268||The year 2017 has seen many EU and UK legislative initiatives and proposals to consider and address the impact of artificial intelligence on society, covering questions of liability, legal personality and other ethical and legal issues, including in the context of data processing. In March 2017, the Information Commissioner's Office (UK) updated its big data guidance to address the development of artificial intelligence and machine learning, and to provide (GDPR), which will apply from 25 May 2018. This paper situates the ICO's guidance in the context of wider legal and ethical considerations and provides a critique of the position adopted by the ICO. On the ICO's analysis, the key challenge for artificial intelligence processing personal data is in establishing that such processing is fair. This shift reflects the potential for artificial intelligence to have negative social consequences (whether intended or unintended) that are not otherwise addressed by the GDPR. The question of ‘fairness’ is an important one, to address the imbalance between big data organisations and individual data subjects, with a number of ethical and social impacts that need to be evaluated.|https://doi.org/10.1016/j.clsr.2018.01.004|https://www.sciencedirect.com/science/article/pii/S026736491830044X||||2018|The ICO and artificial intelligence: The role of fairness in the GDPR framework|Michael Butterworth|article|BUTTERWORTH2018257|||Computer Law & Security Review|02673649|2|34|||||28888|0,815|Q1|38|66|242|1245|707|223|3,39|18,86|United Kingdom|Western Europe|1985-2020|Business, Management and Accounting (miscellaneous) (Q1); Computer Networks and Communications (Q1); Law (Q1)||||1624845917|1769124433
||Internet of Things, Mobile computing, Human Data Model, wearable computers, pervasive computing, ubiquitous computing, IoT, data mashups, data management, programmable world|39|||Today, an increasing number of systems produce, process, and store personal and intimate data. Such data has plenty of potential for entirely new types of software applications, as well as for improving old applications, particularly in the domain of smart healthcare. However, utilizing this data, especially when it is continuously generated by sensors and other devices, with the current approaches is complex—data is often using proprietary formats and storage, and mixing and matching data of different origin is not easy. Furthermore, many of the systems are such that they should stimulate interactions with humans, which further complicates the systems. In this article, we introduce the Human Data Model—a new tool and a programming model for programmers and end users with scripting skills that help combine data from various sources, perform computations, and develop and schedule computer-human interactions. Written in JavaScript, the software implementing the model can be run on almost any computer either inside the browser or using Node.js. Its source code can be freely downloaded from GitHub, and the implementation can be used with the existing IoT platforms. As a whole, the work is inspired by several interviews with professionals, and an online survey among healthcare and education professionals, where the results show that the interviewed subjects almost entirely lack ideas on how to benefit the ever-increasing amount of data measured of the humans. We believe that this is because of the missing support for programming models for accessing and handling the data, which can be satisfied with the Human Data Model.|10.1145/3402524|https://doi.org/10.1145/3402524|New York, NY, USA|Association for Computing Machinery||2020|Human Data Model: Improving Programmability of Health and Well-Being Data for Enhanced Perception and Interaction|"\"M\"\"{a}kitalo, Niko and Flores-Martin, Daniel and Flores, Huber and Lagerspetz, Eemil and Christophe, Francois and Ihantola, Petri and Babazadeh, Masiar and Hui, Pan and Murillo, Juan Manuel and Tarkoma, Sasu and Mikkonen, Tommi\""|article|10.1145/3402524|26|sep|ACM Trans. Comput. Healthcare|26911957|4|1|October 2020||||||||||||||||||||||1626701339|1983512862
||Data privacy;Privacy;Support vector machines;Big Data;Classification algorithms;Data models;Data integrity;Privacy;Scalability;Big Data;Spark;Analytics;Privacy Preserving;Performance;Utility;UCI;Composite;Efficiency;Anonymization||228-235|2020 International Conference on Smart Electronics and Communication (ICOSEC)|Privacy is an issue of concern in the electronic era where data has become a primary source of investment for businesses and organizations. The value generated from data is put to use in a number of ways for economic benefit. Customer profiling is one such instance, where data collected is used for targeted marketing, personalized purchase recommendations and customized product deliveries. In such applications, the risk of individual sensitive information disclosure always prevails, affecting the privacy of individuals involved. Hence privacy preserving analysis demands suppressing or transforming data before it is published for analysis, thus curbing data leak. Subsequently, data quality degrades, and operative analytics is affected. With Big data, algorithms that offer a reasonable qualityprivacy trade off need enhancements in terms of efficiency and scalability. In this paper, the work proposed uses a privacy based composite classifier model to analyze the accuracy of classification. The diverse characteristics of algorithms in the composite classifier are found to balance the classification accuracy that is likely to get affected by privacy model. Further, the model's performance with respect to execution time is then evaluated using the parallel computing framework Spark.|10.1109/ICOSEC49089.2020.9215250|||||2020|An Ensemble Learning Approach for Privacy–Quality–Efficiency Trade-Off in Data Analytics|Peethambaran, Geetha and Naikodi, Chandrakant and Suresh, L|inproceedings|9215250||Sep.|||||||||||||||||||||||||||1627013580|42
||Resilience, Big Data, Sustainability, Disaster, Exploratory factor analysis, Confirmatory factor analysis||1108-1118||The purpose of this paper is to propose and test a theoretical framework to explain resilience in supply chain networks for sustainability using unstructured Big Data, based upon 36,422 items gathered in the form of tweets, news, Facebook, WordPress, Instagram, Google+, and YouTube, and structured data, via responses from 205 managers involved in disaster relief activities in the aftermath of Nepal earthquake in 2015. The paper uses Big Data analysis, followed by a survey which was analyzed using content analysis and confirmatory factor analysis (CFA). The results of the analysis suggest that swift trust, information sharing and public–private partnership are critical enablers of resilience in supply chain networks. The current study used cross-sectional data. However the hypotheses of the study can be tested using longitudinal data to attempt to establish causality. The article advances the literature on resilience in disaster supply chain networks for sustainability in that (i) it suggests the use of Big Data analysis to propose and test particular frameworks in the context of resilient supply chains that enable sustainability; (ii) it argues that swift trust, public private partnerships, and quality information sharing link to resilience in supply chain networks; and (iii) it uses the context of Nepal, at the moment of the disaster relief activities to provide contemporaneous perceptions of the phenomenon as it takes place.|https://doi.org/10.1016/j.jclepro.2016.03.059|https://www.sciencedirect.com/science/article/pii/S0959652616301275||||2017|The role of Big Data in explaining disaster resilience in supply chains for sustainability|Thanos Papadopoulos and Angappa Gunasekaran and Rameshwar Dubey and Nezih Altay and Stephen J. Childe and Samuel Fosso-Wamba|article|PAPADOPOULOS20171108|||Journal of Cleaner Production|09596526||142||Special Volume on Improving natural resource management and human health to ensure sustainable societal development based upon insights gained from working within ‘Big Data Environments’|||19167|1,937|Q1|200|5126|10603|304498|103295|10577|9,56|59,40|United Kingdom|Western Europe|1993-2021|Environmental Science (miscellaneous) (Q1); Industrial and Manufacturing Engineering (Q1); Renewable Energy, Sustainability and the Environment (Q1); Strategy and Management (Q1)|170,352|9.297|0.18299|1627022092|1121054297
||Coronavirus disease 2019, Healthcare system, Informatization||24-28||With the application of Internet of Things, big data, cloud computing, artificial intelligence, and other cutting-edge technologies, China's medical informatization is developing rapidly. In this paper, we summaried the role of information technology in healthcare sector's battle against the Coronavirus disease 2019 (COVID-19) from the perspectives of early warning and monitoring, screening and diagnosis, medical treatment and scientific research, analyzes the bottlenecks of the development of information technology in the post-COVID-19 era, and puts forward feasible suggestions for further promoting the construction of medical informatization from the perspectives of sharing, convenience, and safety.|https://doi.org/10.1016/j.imed.2021.03.004|https://www.sciencedirect.com/science/article/pii/S2667102621000097||||2021|Thinking on the informatization development of China's healthcare system in the post-COVID-19 era|Ming Zhang and Danyun Dai and Siliang Hou and Wei Liu and Feng Gao and Dong Xu and Yu Hu|article|ZHANG202124|||Intelligent Medicine|26671026|1|1|||||||||||||||||||||||1627648762|714385722
||Big data technology, Heavy metal pollution, Risk assessment, Index evaluation system, Eastern Tianshan||109585||With the increasing maturity of big data technology, its application in the field of environmental assessment has become an important issue. The mining of mineral resource can affect the balance in the ecological environment surrounding mining areas and the normal life activities of humans through groundwater. To solve this problem, big data-based methods were used to evaluate the pollution risk in mining areas in this article. Based on a spatial big data management framework, in this paper, high-risk areas in eastern Tianshan were quantitatively analyzed and delineated via a new comprehensive heavy metal pollution assessment system. A distributed storage environment, unstructured management method and spatial index coding were used to uniformly manage spatial data in vector and raster formats retrieved from different sources. A system involving 18 geological environment indicators was used to evaluate the risk level in the study area and in delineated areas with a high risk of heavy metal pollution. The results indicated that the proposed framework could efficiently store and process spatial data and realize environmental pollution risk assessment in a big data environment. In addition, some of the high-pollution risk areas identified based on the assessment results were consistent with actual mine locations. Overall, the results show that integrating multi-source geological characteristics through a comprehensive evaluation system based on big data can have a positive effect on improving the accuracy of heavy metal pollution risk assessment. Heavy metal pollution assessment can provide a reference for environmental monitoring and governance of the eastern Tianshan region and offer new solutions to large-scale heavy metal pollution assessment.|https://doi.org/10.1016/j.ecolind.2022.109585|https://www.sciencedirect.com/science/article/pii/S1470160X22010585||||2022|Regional metal pollution risk assessment based on a big data framework: A case study of the eastern Tianshan mining area, China|Yinyi Cheng and Kefa Zhou and Jinlin Wang and Shichao Cui and Jining Yan and Philippe {De Maeyer} and Tim {Van de Voorde}|article|CHENG2022109585|||Ecological Indicators|1470160X||145|||||20292|1,315|Q1|127|1074|2432|74155|12850|2408|5,00|69,05|Netherlands|Western Europe|2001-2021|Decision Sciences (miscellaneous) (Q1); Ecology (Q1); Ecology, Evolution, Behavior and Systematics (Q1)|32,205|4.958|0.03788|1628575920|1147214023
WIMS '17|Amantea, Italy|semantic technologies, applications, linked data quality, named entity linking, mitigation strategies, information extraction|12||Proceedings of the 7th International Conference on Web Intelligence, Mining and Semantics|Advances in research areas such as named entity linking and sentiment analysis have triggered the emergence of knowledge-intensive information extraction methods that combine classical information extraction with background knowledge from the Web. Despite data quality concerns, linked data sources such as DBpedia, GeoNames and Wikidata which encode facts in a standardized structured format are particularly attractive for such applications.This paper addresses the problem of data quality by introducing a framework that elaborates on linked data quality issues relevant to different stages of the background knowledge acquisition process, their impact on information extraction performance and applicable mitigation strategies. Applying this framework to named entity linking and data enrichment demonstrates the potential of the introduced mitigation strategies to lessen the impact of different kinds of data quality problems. An industrial use case that aims at the automatic generation of image metadata from image descriptions illustrates the successful deployment of knowledge-intensive information extraction in real-world applications and constraints introduced by data quality concerns.|10.1145/3102254.3102272|https://doi.org/10.1145/3102254.3102272|New York, NY, USA|Association for Computing Machinery|9781450352253|2017|Mitigating Linked Data Quality Issues in Knowledge-Intense Information Extraction Methods|Weichselbraun, Albert and Kuntschik, Philipp|inproceedings|10.1145/3102254.3102272|17||||||||||||||||||||||||||||1628627698|42
BDCAT '16|Shanghai, China|vs challenges, data lifecycle, big data, data complexity, data management, data organization|7|100–106|Proceedings of the 3rd IEEE/ACM International Conference on Big Data Computing, Applications and Technologies|A huge amount of data is constantly being produced in the world. Data coming from the IoT, from scientific simulations, or from any other field of the eScience, are accumulated over historical data sets and set up the seed for future Big Data processing, with the final goal to generate added value and discover knowledge. In such computing processes, data are the main resource; however, organizing and managing data during their entire life cycle becomes a complex research topic. As part of this, Data LifeCycle (DLC) models have been proposed to efficiently organize large and complex data sets, from creation to consumption, in any field, and any scale, for an effective data usage and big data exploitation.Several DLC frameworks can be found in the literature, each one defined for specific environments and scenarios. However, we realized that there is no global and comprehensive DLC model to be easily adapted to different scientific areas. For this reason, in this paper we describe the Comprehensive Scenario Agnostic Data LifeCycle (COSA-DLC) model, a DLC model which: i) is proved to be comprehensive as it addresses the 6Vs challenges (namely Value, Volume, Variety, Velocity, Variability and Veracity; and ii), it can be easily adapted to any particular scenario and, therefore, fit the requirements of a specific scientific field. In this paper we also include two use cases to illustrate the ease of the adaptation in different scenarios. We conclude that the comprehensive scenario agnostic DLC model provides several advantages, such as facilitating global data management, organization and integration, easing the adaptation to any kind of scenario, guaranteeing good data quality levels and, therefore, saving design time and efforts for the scientific and industrial communities.|10.1145/3006299.3006311|https://doi.org/10.1145/3006299.3006311|New York, NY, USA|Association for Computing Machinery|9781450346177|2016|Towards a Comprehensive Data Lifecycle Model for Big Data Environments|Sinaeepourfard, Amir and Garcia, Jordi and Masip-Bruin, Xavier and Mar\'{\i}n-Torder, Eva|inproceedings|10.1145/3006299.3006311|||||||||||||||||||||||||||||1629222116|42
||Point of interest data, Emission inventory, Cooking emission factors, Dataset, Beijing||120320||Cooking emission inventories always have poor spatial resolutions when applying with traditional methods, making their impacts on ambient air and human health remain obscure. In this study, we created a systematic dataset of cooking emission factors (CEFs) and applied it with a new data source, cooking-related point of interest (POI) data, to build up highly spatial resolved cooking emission inventories from the city scale. Averaged CEFs of six particulate and gaseous species (PM, OC, EC, NMHC, OVOCs, VOCs) were 5.92 ± 6.28, 4.10 ± 5.50, 0.05 ± 0.05, 22.54 ± 20.48, 1.56 ± 1.44, and 7.94 ± 6.27 g/h normalized in every cook stove, respectively. A three-field CEF index containing activity and emission factor species was created to identify and further build a connection with cooking-related POI data. A total of 95,034 cooking point sources were extracted from Beijing, as a study city. In downtown areas, four POI types were overlapped in the central part of the city and radiated into eight distinct directions from south to north. Estimated PM/VOC emissions caused by cooking activities in Beijing were 4.81/9.85 t per day. A 3D emission map showed an extremely unbalanced emission density in the Beijing region. Emission hotspots were seen in Central Business District (CBD), Sanlitun, and Wangjing in Chaoyang District and Willow and Zhongguancun in Haidian District. PM/VOC emissions could be as high as 16.6/42.0 kg/d in the searching radius of 2 km. For PM, the total emissions were 417.4, 389.0, 466.9, and 443.0 t between Q1 and Q4 2019 in Beijing, respectively. The proposed methodology is transferrable to other Chinese cities for deriving enhanced commercial cooking inventories and potentially highlighting the further importance of cooking emissions on air quality and human health.|https://doi.org/10.1016/j.envpol.2022.120320|https://www.sciencedirect.com/science/article/pii/S0269749122015342||||2022|Enhanced commercial cooking inventories from the city scale through normalized emission factor dataset and big data|Pengchuan Lin and Jian Gao and Yisheng Xu and James J. Schauer and Jiaqi Wang and Wanqing He and Lei Nie|article|LIN2022120320|||Environmental Pollution|02697491||315|||||23916|2,136|Q1|227|2109|4204|129684|35383|4169|8,04|61,49|United Kingdom|Western Europe|1970-1980, 1986-2020|Health, Toxicology and Mutagenesis (Q1); Medicine (miscellaneous) (Q1); Pollution (Q1); Toxicology (Q1)|84,491|8.071|0.07982|1629723066|354628351
||Big data, data lake, unstructured data, zone||245-255|Data Stewardship (Second Edition)|Big Data Governance and big data stewardship are not so different from what we’ve been doing prior to the advent of big data and data lakes. Most of the same roles still need to be filled, and accountability for making data decisions is even more important because of the vast quantity of data, the many ways in which it can be changed, and increased consequences of “getting it wrong” due to not only the large quantities of data and metadata, but also the speed at which the data can change.|https://doi.org/10.1016/B978-0-12-822132-7.00010-3|https://www.sciencedirect.com/science/article/pii/B9780128221327000103||Academic Press|978-0-12-822132-7|2021|Chapter 10 - Big Data Stewardship and Data Lakes|David Plotkin|incollection|PLOTKIN2021245|||||||||Second Edition|David Plotkin|||||||||||||||||||1630726776|42
ASONAM '15|Paris, France|health data analysis, predictive modelling, health big data, data quality, data mining applications, data mining|6|1057–1062|Proceedings of the 2015 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining 2015|In this survey, we collect the related information that demonstrate the importance of data mining in healthcare. As the amount of collected health data is increasing significantly every day, it is believed that a strong analysis tool that is capable of handling and analyzing large health data is essential. Analyzing the health datasets gathered by electronic health record (EHR) systems, insurance claims, health surveys, and other sources, using data mining techniques is very complex and is faced with very specific challenges, including data quality and privacy issues. However, the applications of data mining in healthcare, advantages of data mining techniques over traditional methods, special characteristics of health data, and new health condition mysteries have made data mining very necessary for health data analysis.|10.1145/2808797.2809367|https://doi.org/10.1145/2808797.2809367|New York, NY, USA|Association for Computing Machinery|9781450338547|2015|Importance of Data Mining in Healthcare: A Survey|Tekieh, Mohammad Hossein and Raahemi, Bijan|inproceedings|10.1145/2808797.2809367|||||||||||||||||||||||||||||1630874932|42
MMSys'17|Taipei, Taiwan|Evaluation, Gastrointestinal Tract, Interactive, Medical Multimedia System, Medicine, Performance|12|112–123|Proceedings of the 8th ACM on Multimedia Systems Conference|Analysis of medical videos for detection of abnormalities and diseases requires both high precision and recall, but also real-time processing for live feedback and scalability for massive screening of entire populations. Existing work on this field does not provide the necessary combination of retrieval accuracy and performance.; AB@In this paper, a multimedia system is presented where the aim is to tackle automatic analysis of videos from the human gastrointestinal (GI) tract. The system includes the whole pipeline from data collection, processing and analysis, to visualization. The system combines filters using machine learning, image recognition and extraction of global and local image features. Furthermore, it is built in a modular way so that it can easily be extended. At the same time, it is developed for efficient processing in order to provide real-time feedback to the doctors. Our experimental evaluation proves that our system has detection and localisation accuracy at least as good as existing systems for polyp detection, it is capable of detecting a wider range of diseases, it can analyze video in real-time, and it has a low resource consumption for scalability.|10.1145/3083187.3083189|https://doi.org/10.1145/3083187.3083189|New York, NY, USA|Association for Computing Machinery|9781450350020|2017|A Holistic Multimedia System for Gastrointestinal Tract Disease Detection|Pogorelov, Konstantin and Eskeland, Sigrun Losada and de Lange, Thomas and Griwodz, Carsten and Randel, Kristin Ranheim and Stensland, H\r{a}kon Kvale and Dang-Nguyen, Duc-Tien and Spampinato, Concetto and Johansen, Dag and Riegler, Michael and Halvorsen, P\r{a}l|inproceedings|10.1145/3083187.3083189|||||||||||||||||||||||||||||1631462950|42
CCS '18|Toronto, Canada|data privacy, mobile computing, location privacy|16|196–211|Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security|As mobile devices and location-based services become increasingly ubiquitous, the privacy of mobile users' location traces continues to be a major concern. Traditional privacy solutions rely on perturbing each position in a user's trace and replacing it with a fake location. However, recent studies have shown that such point-based perturbation of locations is susceptible to inference attacks and suffers from serious utility losses, because it disregards the moving trajectory and continuity in full location traces. In this paper, we argue that privacy-preserving synthesis of complete location traces can be an effective solution to this problem. We present AdaTrace, a scalable location trace synthesizer with three novel features: provable statistical privacy, deterministic attack resilience, and strong utility preservation. AdaTrace builds a generative model from a given set of real traces through a four-phase synthesis process consisting of feature extraction, synopsis learning, privacy and utility preserving noise injection, and generation of differentially private synthetic location traces. The output traces crafted by AdaTrace preserve utility-critical information existing in real traces, and are robust against known location trace attacks. We validate the effectiveness of AdaTrace by comparing it with three state of the art approaches (ngram, DPT, and SGLT) using real location trace datasets (Geolife and Taxi) as well as a simulated dataset of 50,000 vehicles in Oldenburg, Germany. AdaTrace offers up to 3-fold improvement in trajectory utility, and is orders of magnitude faster than previous work, while preserving differential privacy and attack resilience.|10.1145/3243734.3243741|https://doi.org/10.1145/3243734.3243741|New York, NY, USA|Association for Computing Machinery|9781450356930|2018|Utility-Aware Synthesis of Differentially Private and Attack-Resilient Location Traces|Gursoy, Mehmet Emre and Liu, Ling and Truex, Stacey and Yu, Lei and Wei, Wenqi|inproceedings|10.1145/3243734.3243741|||||||||||||||||||||||||||||1632503804|42
|||6|35–40|DG.O 2022: The 23rd Annual International Conference on Digital Government Research|Data collaboratives (DC) [12, 18] have gained increasing attention in recent years benefitting and nurturing the momentum around the use of Data for Good [11]. However, research on the topic, derived and built upon the fields of collaborative governance, information sharing, and open data [9, 17] is still unmature, lacking a systematic body of knowledge, grounded in empirical evidence [11]. Except few studies, specifically referred to DC [31, 36, 37, 40, 42] [15, 18, 19, 24], most of the literature used in the field is encompassing broader concepts as such DataSharing, DataforGood, or Cross Sectoral Partnership.Given that the empirical field has matured sufficiently to permit more quantitative analysis, the research seeks to go beyond existing qualitative classifications and inductively define data collaborative archetypes, emphasizing their distinctions and peculiarities as a foundation for future research on the topic.The research started from a literature review on DCs, their definition and the dimensions identifying different DC's models. The dataset provided on datacollaboratives.org has been filtered based on the literature review, excluding those instances that do not meet the DCs criteria or for whom online data collection is not feasible. Once the empirical setting was defined, a phase of variables selection and population has been conducted according to different variables. The evaluation of different clustering solutions, using both qualitative and quantitative methodologies, brought to identify five mutually exclusive clusters.Each cluster is described according to 18 variables, allowing the emergence of cluster's specific peculiarities and challenges. Findings are consistent with prior classifications and taxonomies [18, 29] with additional views afforded by a larger number of instances, the use of quantitative methodologies and the analysis of additional variables. Findings demonstrate the coexistence of quite different entities under the concept of DC, each of whose challenges and progress should be examined independently by researchers.Responding to the objective to foster DCs long term sustainability, different research priorities are specified according to identified clusters and an empirical setting for conducting this research is made available. From a practitioner perspective, research's findings may enable those interested in the topic to obtain more comprehensive information about benchmark examples, which is a valuable resource for industry growth. Additionally, the research illustrates&nbsp;the efficacy of categorical variable clustering analysis for inductive exploratory studies in a novel field of research.||https://doi.org/10.1145/3543434.3543442|New York, NY, USA|Association for Computing Machinery|9781450397490|2022|Fostering Data Collaboratives’ Systematisation through Models’ Definition and Research Priorities Setting|Bartolomucci, Federico and Bresolin, Gianluca|inbook|10.1145/3543434.3543442|||||||||||||||||||||||||||||1633491488|42
WI '17|Leipzig, Germany|crowdsourcing management, multiple choice HIT, crowd workers, statistical quality control|7|476–482|Proceedings of the International Conference on Web Intelligence|"\"The big data research topic has grown rapidly for the past decade due to the advent of the \"\"data deluge\"\". Recent advancements in the literature leverage human computing power known as crowdsourcing to manage and harness big data for various applications. However, human involvement in the completion of crowdsourcing tasks is an error-prone process that affects the overall performance of the crowd. Thus, controlling the quality of workers is an essential step for crowdsourcing systems, which due to unavailability of ground-truth data for any task at hand becomes increasingly challenging. To propose a solution to this problem, in this study, we propose OSQC (Online Statistical Quality Control Framework) for managing the performance of workers in crowdsourcing. OSQC ascertains the worker's performance by using a statistical model and then leverages the traditional statistical control techniques to decide whether to retain a worker for crowdsourcing or to evict him. We evaluate our proposed framework on a real dataset and demonstrate how OSQC assists crowdsourcing to maintain its accuracy.\""|10.1145/3106426.3106436|https://doi.org/10.1145/3106426.3106436|New York, NY, USA|Association for Computing Machinery|9781450349512|2017|An Online Statistical Quality Control Framework for Performance Management in Crowdsourcing|Saberi, Morteza and Hussain, Omar K. and Chang, Elizabeth|inproceedings|10.1145/3106426.3106436|||||||||||||||||||||||||||||1635826330|42
||Big data, Data governance, AI, Algorithmic governance, Information sharing, Artificial Intelligence, Trusted frameworks||101493||The rise of Big, Open and Linked Data (BOLD) enables Big Data Algorithmic Systems (BDAS) which are often based on machine learning, neural networks and other forms of Artificial Intelligence (AI). As such systems are increasingly requested to make decisions that are consequential to individuals, communities and society at large, their failures cannot be tolerated, and they are subject to stringent regulatory and ethical requirements. However, they all rely on data which is not only big, open and linked but varied, dynamic and streamed at high speeds in real-time. Managing such data is challenging. To overcome such challenges and utilize opportunities for BDAS, organizations are increasingly developing advanced data governance capabilities. This paper reviews challenges and approaches to data governance for such systems, and proposes a framework for data governance for trustworthy BDAS. The framework promotes the stewardship of data, processes and algorithms, the controlled opening of data and algorithms to enable external scrutiny, trusted information sharing within and between organizations, risk-based governance, system-level controls, and data control through shared ownership and self-sovereign identities. The framework is based on 13 design principles and is proposed incrementally, for a single organization and multiple networked organizations.|https://doi.org/10.1016/j.giq.2020.101493|https://www.sciencedirect.com/science/article/pii/S0740624X20302719||||2020|Data governance: Organizing data for trustworthy Artificial Intelligence|Marijn Janssen and Paul Brous and Elsa Estevez and Luis S. Barbosa and Tomasz Janowski|article|JANSSEN2020101493|||Government Information Quarterly|0740624X|3|37|||||14735|2,121|Q1|103|76|205|5894|1946|193|8,39|77,55|United Kingdom|Western Europe|1984-2020|E-learning (Q1); Law (Q1); Library and Information Sciences (Q1); Sociology and Political Science (Q1)|5,379|7.279|0.00502|1637681414|1582933551
|||12|301–312||The discovery of all unique (and non-unique) column combinations in a given dataset is at the core of any data profiling effort. The results are useful for a large number of areas of data management, such as anomaly detection, data integration, data modeling, duplicate detection, indexing, and query optimization. However, discovering all unique and non-unique column combinations is an NP-hard problem, which in principle requires to verify an exponential number of column combinations for uniqueness on all data values. Thus, achieving efficiency and scalability in this context is a tremendous challenge by itself.In this paper, we devise Ducc, a scalable and efficient approach to the problem of finding all unique and non-unique column combinations in big datasets. We first model the problem as a graph coloring problem and analyze the pruning effect of individual combinations. We then present our hybrid column-based pruning technique, which traverses the lattice in a depth-first and random walk combination. This strategy allows Ducc to typically depend on the solution set size and hence to prune large swaths of the lattice. Ducc also incorporates row-based pruning to run uniqueness checks in just few milliseconds. To achieve even higher scalability, Ducc runs on several CPU cores (scale-up) and compute nodes (scale-out) with a very low overhead. We exhaustively evaluate Ducc using three datasets (two real and one synthetic) with several millions rows and hundreds of attributes. We compare Ducc with related work: Gordian and HCA. The results show that Ducc is up to more than 2 orders of magnitude faster than Gordian and HCA (631x faster than Gordian and 398x faster than HCA). Finally, a series of scalability experiments shows the efficiency of Ducc to scale up and out.|10.14778/2732240.2732248|https://doi.org/10.14778/2732240.2732248||VLDB Endowment||2013|Scalable Discovery of Unique Column Combinations|Heise, Arvid and Quian\'{e}-Ruiz, Jorge-Arnulfo and Abedjan, Ziawasch and Jentzsch, Anja and Naumann, Felix|article|10.14778/2732240.2732248||dec|Proc. VLDB Endow.|21508097|4|7|December 2013||||21100199855|0,946|Q1|134|246|598|12401|3759|566|5,50|50,41|United States|Northern America|2008-2020|Computer Science (miscellaneous) (Q1)|7,198|2.047|0.00891|1639050708|1216159931
UCC '17 Companion|Austin, Texas, USA|cloud computing, reproducibility, docker, xnat, medical data, repeatability|6|3–8|Companion Proceedings of The10th International Conference on Utility and Cloud Computing|Replication of computational experiments is essential for verifiable research. However, it requires a comprehensive and unambiguous description of all employed digital artifacts, in particular data, code and the computational environment. Recently, the FAIR Guiding Principles have been published to support reproducible research. In this paper, a cloud-based biomedical collaboration platform has been evaluated regarding FAIR principles and has been extended to support reproducibility. The FAICE suite is presented, encompassing tools to thoroughly describe and reproduce a computational experiment within the original execution environment as well as within a dynamically configured VM.|10.1145/3147234.3148104|https://doi.org/10.1145/3147234.3148104|New York, NY, USA|Association for Computing Machinery|9781450351959|2017|Towards Reproducible Research in a Biomedical Collaboration Platform Following the FAIR Guiding Principles|Jansen, Christoph and Beier, Maximilian and Witt, Michael and Frey, Sonja and Krefting, Dagmar|inproceedings|10.1145/3147234.3148104|||||||||||||||||||||||||||||1640485965|42
||Location-based big data (LocBigData), Smart cities, Data analytics, State-of-the-art review, Research agenda, Geodata||101712||The growing ubiquity of location/activity sensing technologies and location-based services (LBS) has led to a large volume and variety of location-based big data (LocBigData), such as location tracking or sensing data, social media data, and crowdsourced geographic information. The increasing availability of such LocBigData has created unprecedented opportunities for research on urban systems and human environments in general. In this article, we first review the common types of LocBigData: mobile phone network data, GPS data, Location-based social media data, LBS usage/log data, smart card travel data, beacon log data (WiFi or Bluetooth), and camera imagery data. Secondly, we describe the opportunities fueled by LocBigData for the realization of smart cities, mainly via answering questions ranging from “what happened” and “why did it happen” to “what's likely to happen in the future” and “what to do next”. Thirdly, pitfalls of dealing with LocBigData are summarized, such as high volume/velocity/variety; non-random sampling; messy and not clean data; and correlations rather than causal relationships. Finally, we review the state-of-the-art research trends in this field, and conclude the article with a list of open research challenges and a research agenda for LocBigData research to help achieve the vision of smart and sustainable cities.|https://doi.org/10.1016/j.compenvurbsys.2021.101712|https://www.sciencedirect.com/science/article/pii/S0198971521001198||||2021|Analytics of location-based big data for smart cities: Opportunities, challenges, and future directions|Haosheng Huang and Xiaobai Angela Yao and Jukka M. Krisp and Bin Jiang|article|HUANG2021101712|||Computers, Environment and Urban Systems|01989715||90|||||23269|1,549|Q1|92|85|327|4842|2135|324|6,11|56,96|United Kingdom|Western Europe|1980-1986, 1988-2020|Ecological Modeling (Q1); Environmental Science (miscellaneous) (Q1); Geography, Planning and Development (Q1); Urban Studies (Q1)||||1640923472|1571752529
||Task analysis;Computational modeling;Automobiles;Big Data;Quality of service;Privacy;Bandwidth;Vehicular ad-hoc networks, reverse auction, computing offloading, driverless vehicles||412-416|2019 IEEE Intl Conf on Dependable, Autonomic and Secure Computing, Intl Conf on Pervasive Intelligence and Computing, Intl Conf on Cloud and Big Data Computing, Intl Conf on Cyber Science and Technology Congress (DASC/PiCom/CBDCom/CyberSciTech)|In the era of big data, edge computing is emerged as a promising paradigm to alleviate the pressure on the backbone network and facilitate vehicular services on the road. As edge nodes deployed for vehicular applications, roadside units (RSUs) need to undertake a large number of local computing tasks. However, due to the uncertainty of the vehicular network topology, static RSU deployments are subject to short-term overload and cannot handle various delay-sensitive computing tasks concurrently. To address the problem, we propose a big data driven computing offloading scheme to dispatch idle driverless vehicles to enhance the capacities of RSUs dynamically. First, we present a trust assessment model to evaluate the credibility of driverless vehicles. Then, a multi-attribute reverse auction is applied to maximize the utilities of RSUs and driverless vehicles. In addition, a secure forwarding method is developed to protect the privacy of computing tasks.|10.1109/DASC/PiCom/CBDCom/CyberSciTech.2019.00084|||||2019|Big Data Driven Computing Offloading Scheme with Driverless Vehicles Assistance|Chen, Chengling and Su, Zhou and Li, Weiwei and Wang, Yuntao|inproceedings|8890376||Aug|||||||||||||||||||||||||||1642495236|42
ICEGOV 2020|Athens, Greece|e-government, open government data, digital government, readiness assessment, Open data|7|334–340|Proceedings of the 13th International Conference on Theory and Practice of Electronic Governance|Open data is a movement that has gained worldwide political relevance as a strategy that supports active transparency, access to public information, and the generation of public, social, and economic value. To know the progress and results of open data initiatives, governments, working groups, international organizations, and researchers have proposed indexes and evaluation models. These measurements focus on the evaluation of aspects of the preparation, implementation, and impact of open data initiatives. In Colombia, the national government within the framework of its digital government policy defined the open data project. The progress in data openings is monitored through international indexes and the open government index, which focuses solely on the publication and use of open government data. This research deals with the evaluation of the preparation for the opening of data, in public entities that have not implemented an open data initiative. The study gives a general description of the evaluation of open data at the international and national level, identifies aspects to be considered to measure the preparation, and proposes a conceptual model of evaluation to measure the preparation in open data of a public sector entity. This proposal can be considered as a tool that generates information that supports the design and implementation of an effective open data initiative.|10.1145/3428502.3428548|https://doi.org/10.1145/3428502.3428548|New York, NY, USA|Association for Computing Machinery|9781450376747|2020|Developing a Model to Readiness Assessment of Open Government Data in Public Institutions in Colombia|Osorio-Sanabria, Mariutsi Alexandra and Amaya-Fern\'{a}ndez, Ferney and Gonz\'{a}lez-Zabala, Mayda Patricia|inproceedings|10.1145/3428502.3428548|||||||||||||||||||||||||||||1644998245|42
ASONAM '17|Sydney, Australia||4|589–592|Proceedings of the 2017 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining 2017|Organizations use data to support different business processes. Data may become unclean because of corruptions in the central quality aspects due to factors such as duplicate records, outdated data, inconsistent values, incomplete information, or inaccurate values. Real datasets are usually not available for reasons such as privacy constraints. In the existing systems that generate or corrupt synthetic data, the intrinsic characteristics of data may not satisfy the quality aspects, and the injected types of errors do not corrupt multiple data quality aspects. Also, a lack of common datasets is a primary reason that representative comparisons between algorithms of different data quality management approaches are not possible. To address these issues, we present datumPIPE, a system that allows for the generation of data that satisfies a set of integrity constraints, including functional dependencies (FDs), conditional functional dependencies (CFDs), and inclusion dependencies (INDs). Also, datumPIPE provides the functionality to generate other types of attribute values such as sensors and personal data. It also allows for the corruption of the generated data through the introduction of quality issues in the central data quality aspects.|10.1145/3110025.3120958|https://doi.org/10.1145/3110025.3120958|New York, NY, USA|Association for Computing Machinery|9781450349932|2017|DatumPIPE: Data Generator and Corrupter for Multiple Data Quality Aspects|Al-janabi, Samir and Hamid, Abubaker and Janicki, Ryszard|inproceedings|10.1145/3110025.3120958|||||||||||||||||||||||||||||1645815435|42
||Thesauri;Metadata;Vocabulary;Measurement;Quality assessment;Big Data;quality;linked data;thesauri;AHP;metadata;DQV||37-44|2017 International Conference on High Performance Computing & Simulation (HPCS)|Thesauri are knowledge systems which may ease Big Data access, fostering their integration and re-use. Currently several Linked Data thesauri covering multi-disciplines are available. They provide a semantic foundation to effectively support cross-organization and cross-disciplinary management and usage of Big Data. Thesauri effectiveness is affected by their quality. Diverse quality measures are available taking into account different facets. However, an overall measure is needed to compare several thesauri and to identify those more qualified for a proper reuse. In this paper, we propose a Multi Criteria Decision Making based methodology for the documentation of the quality assessment of linked thesauri as a whole. We present a proof of concept of the Analytic Hierarchy Process adoption to the set of Linked Data thesauri for the Environment deployed in LusTRE. We discuss the step-by-step practice to document the overall quality measurements, generated by the quality assessment, with the W3C promoted Data Quality Vocabulary.|10.1109/HPCS.2017.16|||||2017|Linked Thesauri Quality Assessment and Documentation for Big Data Discovery|Albertoni, Riccardo and De Martino, Monica and Quarati, Alfonso|inproceedings|8035055||July|||||||||||||||||||||||||||1645928911|42
||ARDEV, Big data prediction, FIMT-DD, Tree regression||29-39||Today, big data processing has become a challenging task due to the amount of data collected using various sensors increasingly significantly. To build knowledge and predict the data, traditional data mining methods calculate all numerical attributes into the memory simultaneously. The data stream method is a solution for processing and calculating data. The method streams incrementally in batch form; therefore, infrastructure memory is sufficient to develop knowledge. The existing method for data stream prediction is FIMT-DD (Fast Incremental Model Tree-Drift Detection). Using this method, knowledge is developed in tree form for every instance. In this paper, enhanced FIMT-DD is proposed using ARDEV (Average Restrain Divider of Evaluation Value). ARDEV utilizes the Chernoff bound approach with error evaluation, improvement in learning rate, modification of perceptron rule calculation, and utilization of activation function. Standard FIMT-DD separates the tree formation process and perceptron prediction. The proposed method evaluates and connects the development of the tree for knowledge formation and the perceptron rule for prediction. The prediction accuracy of the proposed method is measured using MAE, RMSE and MAPE. From the experiment performed, the utilization of ARDEV enhancement shows significant improvement in terms of accuracy prediction. Statistically, the overall accuracy prediction improvement is approximately 6.99 % compared to standard FIMT-DD with a traffic dataset.|https://doi.org/10.1016/j.knosys.2019.03.019|https://www.sciencedirect.com/science/article/pii/S0950705119301443||||2019|Average Restrain Divider of Evaluation Value (ARDEV) in data stream algorithm for big data prediction|Ari Wibisono and Devvi Sarwinda|article|WIBISONO201929|||Knowledge-Based Systems|09507051||176|||||24772|1,587|Q1|121|716|1187|36777|11094|1181|9,42|51,36|Netherlands|Western Europe|1987-2020|Artificial Intelligence (Q1); Information Systems and Management (Q1); Management Information Systems (Q1); Software (Q1)|22,261|8.038|0.02794|1647753794|1341138576
|||2||||10.1145/3507467|https://doi.org/10.1145/3507467|New York, NY, USA|Association for Computing Machinery||2022|Introduction to the Special Issue on Data Science for Next Generation Big Data|Shen, Yanyan and Dinh, Anh and Jagadish, H. V.|article|10.1145/3507467|31|mar|ACM/IMS Trans. Data Sci.|26911922|4|2|November 2021||||||||||||||||||||||1648170115|961429963
||Knowledge discovery, Reliability, Trustworthiness, Quality, Big Data mining||30-36||Extracting knowledge from Big Data is the process of transforming this data into actionable information. The exponential growth of data has initiated a myriad of new opportunities, and made data become the most valuable raw material of production for many organizations. Mining Big Data is coupled with some challenges, known as the 3V’s of Big Data: Volume, Variety and Velocity. However, a major challenge that needs to be addressed, and often is ignored in the literature, concerns reliability. Actually, data is agglomerated from multiple disparate sources, and each of Knowledge Discovery (KDD) process steps may be carried out by different organizations. These considerations lead us to ask a critical question that is weather the information we have at each step is reliable enough to proceed to the next one? This paper therefore aims to provide a framework that automatically assesses reliability of the knowledge discovery process. We focus on Linked Open Data (LOD) as a source of data, as it constitutes a relevant data provider in many Big Data applications. However, our framework can also be adapted for unstructured data. This framework will assist scientists to automatically and efficiently measure the reliability of each KDD process stage as well as detect unreliable steps that should be revised. Following this methodology, KDD process will be optimized and therefore produce knowledge with higher quality.|https://doi.org/10.1016/j.procs.2019.01.005|https://www.sciencedirect.com/science/article/pii/S1877050919300055||||2019|Assessing reliability of Big Data Knowledge Discovery process|Hicham Moad Safhi and Bouchra Frikh and Brahim Ouhbi|article|SAFHI201930|||Procedia Computer Science|18770509||148||THE SECOND INTERNATIONAL CONFERENCE ON INTELLIGENT COMPUTING IN DATA SCIENCES, ICDS2018|||19700182801|0,334|-|76|1907|6483|38511|13722|6359|2,09|20,19|Netherlands|Western Europe|2010-2020|Computer Science (miscellaneous)||||1649125817|2108686752
SACMAT '17 Abstracts|Indianapolis, Indiana, USA|usage control, argumentation reasoning, data sharing, policy language, data access, cloud, abductive reasoning|8|231–238|Proceedings of the 22nd ACM on Symposium on Access Control Models and Technologies|"\"Internet of Things environments enable us to capture more and more data about the physical environment we live in and about ourselves. The data enable us to optimise resources, personalise services and offer unprecedented insights into our lives. However, to achieve these insights data need to be shared (and sometimes sold) between organisations imposing rights and obligations upon the sharing parties and in accordance with multiple layers of sometimes conflicting legislation at international, national and organisational levels. In this work, we show how such rules can be captured in a formal representation called \"\"Data Sharing Agreements\"\". We introduce the use of abductive reasoning and argumentation based techniques to work with context dependent rules, detect inconsistencies between them, and resolve the inconsistencies by assigning priorities to the rules. We show how through the use of argumentation based techniques use-cases taken from real life application are handled flexibly addressing trade-offs between confidentiality, privacy, availability and safety.\""|10.1145/3078861.3078876|https://doi.org/10.1145/3078861.3078876|New York, NY, USA|Association for Computing Machinery|9781450347020|2017|Enabling Data Sharing in Contextual Environments: Policy Representation and Analysis|Karafili, Erisa and Lupu, Emil C.|inproceedings|10.1145/3078861.3078876|||||||||||||||||||||||||||||1650660550|42
||intelligent support information system, accuracy rate, Machine learning, big data analysis, lightGBM|21|||Two-dimensional1 arrays of bi-component structures made of cobalt and permalloy elliptical dots with thickness of 25 nm, length 1 mm and width of 225 nm, have been prepared by a self-aligned shadow deposition technique. Brillouin light scattering has been exploited to study the frequency dependence of thermally excited magnetic eigenmodes on the intensity of the external magnetic field, applied along the easy axis of the elements.Scientific information technology has been developed rapidly. Here, the purposes are to make people's lives more convenient and ensure information management and classification. The machine learning algorithm is improved to obtain the optimized Light Gradient Boosting Machine (LightGBM) algorithm. Then, an Android-based intelligent support information management system is designed based on LightGBM for the big data analysis and classification management of information in the intelligent support information management system. The system is designed with modules of employee registration and login, company announcement notice, attendance and attendance management, self-service, and daily tools with the company as the subject. Furthermore, the performance of the constructed information management system is analyzed through simulations. Results demonstrate that the training time of the optimized LightGBM algorithm can stabilize at about 100s, and the test time can stabilize at 0.68s. Besides, its accuracy rate can reach 89.24%, which is at least 3.6% higher than other machine learning algorithms. Moreover, the acceleration efficiency analysis of each algorithm suggests that the optimized LightGBM algorithm is suitable for processing large amounts of data; its acceleration effect is more apparent, and its acceleration ratio is higher than other algorithms. Hence, the constructed intelligent support information management system can reach a high accuracy while ensuring the error, with apparent acceleration effect. Therefore, this model can provide an experimental reference for information classification and management in various fields.|10.1145/3469890|https://doi.org/10.1145/3469890|New York, NY, USA|Association for Computing Machinery||2021|Novel Machine Learning for Big Data Analytics in Intelligent Support Information Management Systems|Lv, Zhihan and Lou, Ranran and Feng, Hailin and Chen, Dongliang and Lv, Haibin|article|10.1145/3469890|7|oct|ACM Trans. Manage. Inf. Syst.|2158656X|1|13|March 2022||||||||||||||||||||||1656368421|2097885649
||cliometrics, Databases, Germany, financial data, economic history|22|||Broad, long-term financial, and economic datasets are scarce resources, particularly in the European context. In this article, we present an approach for an extensible data model that is adaptable to future changes in technologies and sources. This model may constitute a basis for digitized and structured long-term historical datasets for different jurisdictions and periods. The data model covers the specific peculiarities of historical financial and economic data and is flexible enough to reach out for data of different types (quantitative as well as qualitative) from different historical sources, hence, achieving extensibility. Furthermore, we outline a relational implementation of this approach based on historical German firm and stock market data from 1920 to 1932.|10.1145/3531533|https://doi.org/10.1145/3531533|New York, NY, USA|Association for Computing Machinery||2022|Design and Implementation of a Historical German Firm-Level Financial Database|Gram, Dennis and Karapanagiotis, Pantelis and Liebald, Marius and Walz, Uwe|article|10.1145/3531533|19|jun|J. Data and Information Quality|19361955|3|14|September 2022||||||||||||||||||||||1656870896|833754770
Safety and Resilience'18|Seattle, WA, USA|Artificial neural network, Particle Swarm Optimization, AermodSystem, Atmospheric dispersion prediction, Source estimation|5||Proceedings of the 4th ACM SIGSPATIAL International Workshop on Safety and Resilience|Air pollution caused by industrial production has become a serious problem for public health. This challenging problem promotes the development of the research in the air contaminant dispersion (ADS) prediction, for the management of the emission and leak accident. However, conventional ADS models can hardly meet the requirement of both accuracy and efficiency. The data model, like the artificial neural network (ANN) provides a feasible way of forecasting the dispersion with high accuracy and efficiency. However, the construction of the ANN for prediction needs plenty of data, which is impractical to obtain in most emission cases. To address this problem, an ADS simulation software AermodSystem is applied to build the simulated dispersion scenarios and provide synthetic dataset for the model training and test. Based on the synthetic data set, the ANN prediction model is established, and evaluated on the test set, as well as the Gaussian model. Further, these two models are served as the forward dispersion model and combined with the Particle Swarm Optimization (PSO) for source estimation. The results verify the effectiveness of the proposed model and indicate that the ANN together with the AermodSystem as the data generator is feasible in the air contaminant dispersion forecast and the source estimation of a particular case.|10.1145/3284103.3284114|https://doi.org/10.1145/3284103.3284114|New York, NY, USA|Association for Computing Machinery|9781450360449|2018|The Air Contaminant Dispersion Prediction by the Integration of the Neural Network and AermodSystem|Wang, Rongxiao and Chen, Bin and Wang, Yiduo and Zhu, Zhengqiu and Ma, Liang and Qiu, Xiaogang|inproceedings|10.1145/3284103.3284114|11||||||||||||||||||||||||||||1657314495|42
||Topic Model, Ailments Aspects, ATAM, Healthcare Topic Model, Social Recommended Healthcare Results||104690||Data mining and big data computing are the emerging domains in the current era of predictions for societal applications. Millions of people are interested in sharing their views through tweets. Healthcare predictions are one of the attractive researches in big data social mining. Healthcare predictions are derived by implementing topic models by the ailments data. An ailment refers to either illness or sign of a particular health problem. Millions of tweets are collected based on conditions and assessed with ailment topic aspect models. The existing topic model, Latent Dirichlet Allocation (LDA), Latent Semantic Indexing, Probabilistic LSI (PLSI), limits the healthcare results assessment concerning any one of the ailments aspects. Recent ailments topic aspect model (ATAM) overcome the problems of these topic models and delivers the healthcare assessment results concerning the fundamental aspects of ailments data except side-effects analysis of treatments. The scalability performance of ATAM is degraded in showing healthcare results over the massive amounts of health data. A high-performance computing model of ATAM has been developed in the distributed environment to address scalability. Its intelligent model is designed in the cloud and multi-node Hadoop environment to deliver high-performance social computing results for healthcare. Experiments are conducted on many comparative studies is demonstrated between the existing and proposed high-performance models using the massive amount of health-related tweets concerning the ailments aspects.|https://doi.org/10.1016/j.micpro.2022.104690|https://www.sciencedirect.com/science/article/pii/S0141933122002204||||2022|High performance social data computing with development of intelligent topic models for healthcare|K Narasimhulu and K.T. {Meena Abarna}|article|NARASIMHULU2022104690|||Microprocessors and Microsystems|01419331||95|||||15552|0,323|Q3|38|462|428|12804|962|423|2,34|27,71|Netherlands|Western Europe|1978-2020|Artificial Intelligence (Q3); Computer Networks and Communications (Q3); Hardware and Architecture (Q3); Software (Q3)|1,490|1.525|0.00207|1657325790|1912058844
||Big Data Analytics, Manufacturing process, Big Data Analytics capabilities, Business intelligence, Literature review, Multiple case study||106099||Today, we are undoubtedly in the era of data. Big Data Analytics (BDA) is no longer a perspective for all level of the organization. This is of special interest in the manufacturing process with their high capital intensity, time constraints and given the huge amount of data already captured. However, there is a paucity in past literature on BDA to develop better understanding of the capabilities and strategic implications to extract value from BDA. In that vein, the central aim of this paper is to develop a novel model that summarizes the main capabilities of BDA in the context of manufacturing process. This is carried out by relying on the findings of a review of the ongoing research along with a multiple case studies within a leading phosphate derivatives manufacturer to point out the capabilities of BDA in manufacturing processes and outline recommendations to advance research in the field. The findings will help companies to understand the big data analytics capabilities and its potential implications for their manufacturing processes and support them seeking to design more effective BDA-enabler infrastructure.|https://doi.org/10.1016/j.cie.2019.106099|https://www.sciencedirect.com/science/article/pii/S0360835219305686||||2019|Understanding Big Data Analytics for Manufacturing Processes: Insights from Literature Review and Multiple Case Studies|Amine Belhadi and Karim Zkik and Anass Cherrafi and Sha'ri M. Yusof and Said {El fezazi}|article|BELHADI2019106099|||Computers & Industrial Engineering|03608352||137|||||18164|1,315|Q1|128|641|1567|31833|9597|1557|5,97|49,66|United Kingdom|Western Europe|1976-2020|Computer Science (miscellaneous) (Q1); Engineering (miscellaneous) (Q1)||||1663386162|1798521593
ICIIT 2018|Ha Noi, Viet Nam|Big Data, Information property index, Method for confirming information property rights, Confirmation of Information Property|6|59–64|Proceedings of the 2018 International Conference on Intelligent Information Technology|The major premise of big data circulation is to identify the ownership of data resource. This paper summed some feasible techniques and methods for confirming big data property which are data citation technology, data provenance technology, data reversible hiding technology, computer forensic technology and block chain technology. The ownership of information property which from different sizes, different formats and different storage condition on distributed heterogeneous platforms can be confirmed by comprehensive application of these techniques and methods based on the coupling interface between them in the practice of big data.|10.1145/3193063.3193069|https://doi.org/10.1145/3193063.3193069|New York, NY, USA|Association for Computing Machinery|9781450363785|2018|An Overview of Techniques for Confirming Big Data Property Rights|Cheng, Susu and Zhao, Haijun|inproceedings|10.1145/3193063.3193069|||||||||||||||||||||||||||||1664693293|42
||indexing, wireless sensor network (WSN), Internet of things (IoT), large-scale data, discovery, ranking|53|||Network-enabled sensing and actuation devices are key enablers to connect real-world objects to the cyber world. The Internet of Things (IoT) consists of the network-enabled devices and communication technologies that allow connectivity and integration of physical objects (Things) into the digital world (Internet). Enormous amounts of dynamic IoT data are collected from Internet-connected devices. IoT data are usually multi-variant streams that are heterogeneous, sporadic, multi-modal, and spatio-temporal. IoT data can be disseminated with different granularities and have diverse structures, types, and qualities. Dealing with the data deluge from heterogeneous IoT resources and services imposes new challenges on indexing, discovery, and ranking mechanisms that will allow building applications that require on-line access and retrieval of ad-hoc IoT data. However, the existing IoT data indexing and discovery approaches are complex or centralised, which hinders their scalability. The primary objective of this article is to provide a holistic overview of the state-of-the-art on indexing, discovery, and ranking of IoT data. The article aims to pave the way for researchers to design, develop, implement, and evaluate techniques and approaches for on-line large-scale distributed IoT applications and services.|10.1145/3154525|https://doi.org/10.1145/3154525|New York, NY, USA|Association for Computing Machinery||2018|Large-Scale Indexing, Discovery, and Ranking for the Internet of Things (IoT)|Fathy, Yasmin and Barnaghi, Payam and Tafazolli, Rahim|article|10.1145/3154525|29|mar|ACM Comput. Surv.|03600300|2|51|March 2019||||23038|2,079|Q1|163|112|365|15859|6978|365|19,16|141,60|United States|Northern America|1969-2020|Computer Science (miscellaneous) (Q1); Theoretical Computer Science (Q1)|10,790|10.282|0.01438|1665537249|1517405264
AISS '19|Singapore, Singapore|stream data, dynamic desensitization, data desensitization|6||Proceedings of the International Conference on Advanced Information Science and System|With the rapid development of the data mining industry, the value hidden in the massive data has been discovered, but at the same time it has also raised concerns about privacy leakage, leakage of sensitive data and other issues. These problems have also become numerous studies. Among the methods for solving these problems, data desensitization technology has been widely adopted for its outstanding performance. However, with the increasing scale of data and the increasing dimension of data, the traditional desensitization method for static data can no longer meet the requirements of various industries in today's environment to protect sensitive data. In the face of ever-changing data sets of scale and dimension, static desensitization technology relies on artificially designated desensitization rules to grasp the massive data, and it is difficult to control the loss of data connotation. In response to these problems, this paper proposes a real-time dynamic desensitization method based on data flow, and combines the data anonymization mechanism to optimize the data desensitization strategy. Experiments show that this method can efficiently and stably perform real-time desensitization of stream data, and can save more information to support data mining in the next steps.|10.1145/3373477.3373499|https://doi.org/10.1145/3373477.3373499|New York, NY, USA|Association for Computing Machinery|9781450372916|2020|Real-Time Dynamic Data Desensitization Method Based on Data Stream|Tian, Bing and Lv, Shuqing and Yin, Qilin and Li, Ning and Zhang, Yue and Liu, Ziyan|inproceedings|10.1145/3373477.3373499|22||||||||||||||||||||||||||||1665806303|42
|||2||||10.1145/3174791|https://doi.org/10.1145/3174791|New York, NY, USA|Association for Computing Machinery||2018|Editorial: Special Issue on Improving the Veracity and Value of Big Data|Geerts, Floris and Missier, Paolo and Paton, Norman|article|10.1145/3174791|13|mar|J. Data and Information Quality|19361955|3|9|September 2017||||||||||||||||||||||1668766580|833754770
||Metadata;Quality control;Predictive models;Open Educational Resources;Measurement;Data analysis;OER;open educational resources;metadata quality;OER quality;Big data;data analysis;quality prediction||29-31|2020 IEEE 20th International Conference on Advanced Learning Technologies (ICALT)|In the recent decade, online learning environments have accumulated millions of Open Educational Resources (OERs). However, for learners, finding relevant and high quality OERs is a complicated and time-consuming activity. Furthermore, metadata play a key role in offering high quality services such as recommendation and search. Metadata can also be used for automatic OER quality control as, in the light of the continuously increasing number of OERs, manual quality control is getting more and more difficult. In this work, we collected the metadata of 8,887 OERs to perform an exploratory data analysis to observe the effect of quality control on metadata quality. Subsequently, we propose an OER metadata scoring model, and build a metadata-based prediction model to anticipate the quality of OERs. Based on our data and model, we were able to detect high-quality OERs with the F1 score of 94.6%.|10.1109/ICALT49669.2020.00007|||||2020|Quality Prediction of Open Educational Resources A Metadata-based Approach|Tavakoli, Mohammadreza and Elias, Mirette and Kismihók, Gábor and Auer, Sören|inproceedings|9155928||July||2161377X|||||||||||||||||||||||||1668893676|675401151
||big data, Quality assessment, quality management|3||||10.1145/3449056|https://doi.org/10.1145/3449056|New York, NY, USA|Association for Computing Machinery||2021|Editorial: Special Issue on Quality Assessment and Management in Big Data—Part II|Aljawarneh, Shadi and Lara, Juan A.|article|10.1145/3449056|13|may|J. Data and Information Quality|19361955|3|13|September 2021||||||||||||||||||||||1670220940|833754770
BIGDSE '16|Austin, Texas||7|26–32|Proceedings of the 2nd International Workshop on BIG Data Software Engineering|The recent growing interest on highly-available data-intensive applications sparked the need for flexible and portable storage technologies, e.g., NoSQL databases. Unfortunately, the lack of standard interfaces and architectures for NoSQLs makes it difficult and expensive to create portable applications, which results in vendor lock-in. Building on previous work, we aim at providing guaranteed fault-tolerant techniques and supporting architectures to port or migrate data to and across heterogeneous NoSQL technology. To prove the effectiveness of our approach we evaluate it on an industrial case-study. We conclude that our method and supporting architecture offer an efficient and fault-tolerant mechanism for NoSQL portability and interoperation.|10.1145/2896825.2896831|https://doi.org/10.1145/2896825.2896831|New York, NY, USA|Association for Computing Machinery|9781450341523|2016|Providing Big Data Applications with Fault-Tolerant Data Migration across Heterogeneous NoSQL Databases|Scavuzzo, Marco and Tamburri, Damian A. and Di Nitto, Elisabetta|inproceedings|10.1145/2896825.2896831|||||||||||||||||||||||||||||1671832344|42
ICTCS '16|Udaipur, India|Cloud computing, Verification &amp; Identification, Serializability, Transaction Manager, MVCC, Data version validation|6||Proceedings of the Second International Conference on Information and Communication Technology for Competitive Strategies|Over transactional database systems MultiVersion concurrency control is maintained for secure, fast and efficient access to the shared data file implementation scenario. An effective coordination is supposed to be set up between owners and users also the developers &amp; system operators, to maintain inter-cloud &amp; intra-cloud communication Most of the services &amp; application offered in cloud world are real-time, which entails optimized compatibility service environment between master and slave clusters. In the paper, offered methodology supports replication and triggering methods intended for data consistency and dynamicity. Where intercommunication between different clusters is processed through middleware besides slave intra-communication is handled by verification &amp; identification protection. The proposed approach incorporates resistive flow to handle high impact systems that identifies and verifies multiple processes. Results show that the new scheme reduces the overheads from different master and slave servers as they are co-located in clusters which allow increased horizontal and vertical scalability of resources.|10.1145/2905055.2905184|https://doi.org/10.1145/2905055.2905184|New York, NY, USA|Association for Computing Machinery|9781450339629|2016|VIP: Verification and Identification Protective Data Handling Layer Implementation to Achieve MVCC in Cloud Computing|Jangra, Ajay and Singh, Niharika and Lakhina, Upasana|inproceedings|10.1145/2905055.2905184|124||||||||||||||||||||||||||||1672062148|42
||Smart cities, big data analytics, Drone, IoT, Data Management||4123-4131||The smart city has become a persistent need and is no longer just a concept. The concept of smart cities heavily relies on collecting enormous amounts of data. This paper proposes a data-management-based solution for smart city, which is labeled Smart Systems Oriented Big Data Architecture. Big data technologies have become essential to the functioning of cities. The architecture includes complex components to be implemented based on the architectural requirements. A data migration strategy was proposed to handle the various data sources such as IoT devices, video cameras, and drones. The proposed approach also takes into account data processing and data storage. The technical constraints related to data processing in a big-data environment are also studied. We also consider data modeling from a business intelligence point of view and a data science perspective. Our main goal is to favor the facilitation of the daily life practices in the context of a smart city by providing the city administrators with a solution that helps them maintain their city smartly and effectively.|https://doi.org/10.1016/j.procs.2022.09.475|https://www.sciencedirect.com/science/article/pii/S1877050922013710||||2022|Data Architecture and Big Data Analytics in Smart Cities|El Mehdi Ouafiq and Mourad Raif and Abdellah Chehri and Rachid Saadane|article|OUAFIQ20224123|||Procedia Computer Science|18770509||207||Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 26th International Conference KES2022|||19700182801|0,334|-|76|1907|6483|38511|13722|6359|2,09|20,19|Netherlands|Western Europe|2010-2020|Computer Science (miscellaneous)||||1674630951|2108686752
SummerSim '19|Berlin, Germany|data analytics, data acquisition systems, signal processing, data processing pipeline, big data|6||Proceedings of the 2019 Summer Simulation Conference|In recent years, improvements in high-speed Analog-to-Digital Converters (ADC) and sensor technology has encouraged researchers to improve the performance of Data Acquisition (DAQ) systems for scientific experiments which require high speed and continuous data measurements --- in particular, measuring the electronic and magnetic properties of materials using pump-probe experiments at high repetition rates. Experiments at TELBE are capable of acquiring almost 100 Gigabytes of raw data every ten minutes. The DAQ system used at TELBE partitions the raw data into various subdirectories for further parallel processing utilizing the multicore structure of modern CPUs.Furthermore, several other types of processors that accelerate data processing like the GPU and FPGA have emerged to solve the challenges of processing the massive amount of raw data. However, the memory and network bottlenecks become a significant challenge in big data processing, and new scalable programming techniques are needed to solve these challenges. In this contribution, we will outline the design and implementation of our practical software approach for efficient parallel processing of our large data sets at the TELBE user facility.|||San Diego, CA, USA|Society for Computer Simulation International||2019|Research and Implementation of Efficient Parallel Processing of Big Data at TELBE User Facility|Bawatna, Mohammed and Green, Bertram and Kovalev, Sergey and Deinert, Jan-Christoph and Knodel, Oliver and Spallek, Rainer G.|inproceedings|10.5555/3374138.3374194|56||||||||||||||||||||||||||||1676567792|42
EASE '22|Gothenburg, Sweden|ArchiMate, Enterprise modelling, Motivation, Blockchain, Goal, Strategy|8|375–382|Proceedings of the International Conference on Evaluation and Assessment in Software Engineering 2022|Blockchain is an emerging disruptive technology with a great potential to impact business value creation. Yet it is challenging to understand how blockchain can be utilised to improve enterprises’ performance and value creation. A formalised conceptual and enterprise modelling may help bridge the communication gap between domain experts and blockchain system designers. The goal of this study is to explore how can modelling facilitate blockchain-enhanced value chain design from a motivation viewpoint. We report our experience from modelling a blockchain-enhanced seafood supply chain using ArchiMate motivation and strategy models, where stakeholders have diverse concerns and goals. Preliminary results have indicated that such a modelling approach facilitates the common understanding, and the decision and prioritisation of the business strategies, as well as the identification of the blockchain benefits for the enterprises. It may help stakeholders along the value chain to align their business strategies and priorities, and can be an effective tool for value co-creation and enhancing the collaboration and communication among stakeholders and with blockchain application developers.|10.1145/3530019.3531346|https://doi.org/10.1145/3530019.3531346|New York, NY, USA|Association for Computing Machinery|9781450396134|2022|Experience on Using ArchiMate Models for Modelling Blockchain-Enhanced Value Chains|Jiang, Shanshan and R\ae{}der, Truls Bakkejord|inproceedings|10.1145/3530019.3531346|||||||||||||||||||||||||||||1682598335|42
BDET 2018|Chengdu, China|Target data optimization, fuzzy case-based reasoning, big data-streaming, fuzzy decision tree|5|26–30|Proceedings of the 2018 International Conference on Big Data Engineering and Technology|How to extract target data effectively and intelligently is key point in the big data-streaming operation. Data extractions need focus on priority of data selection to reduce impact of 4Vs because of requirement of real-time computation. Corresponding big data-streaming for three-stage extraction system is presented in terms of hierarchal base and fuzzy representations. The proposed approach is based on a combination of clustering, classification and relationships method with fuzzy weighted similarity under hierarchical feature-based model. Moreover, heuristic fuzzy CBR-FDT- algorithms are provided to explore the target data optimization. Successful case study and experiment with simulations demonstrated the performance of the proposed approach.|10.1145/3297730.3297731|https://doi.org/10.1145/3297730.3297731|New York, NY, USA|Association for Computing Machinery|9781450365826|2018|Target Data Optimization Based on Big Data-Streaming for Two-Stage Fuzzy Extraction System|Chen, Rui-Yang|inproceedings|10.1145/3297730.3297731|||||||||||||||||||||||||||||1682714717|42
||Ion beam analysis, Big data, Data quality assurance, Artificial intelligence||111-115||With a growing demand for accurate ion beam analysis on a large number of samples, it becomes an issue of how to ensure the quality standards and consistency over hundreds or thousands of samples. In this sense, a virtual assistant that checks the data quality, emitting certificates of quality, is highly desired. Even the processing of a massive number of spectra is a problem regarding the consistency of the analysis. In this work, we report the design and first results of a virtual layer under implementation in our laboratory. It consists of a series of systems running in the cloud that perform the mentioned tasks and serves as a virtual assistant for member staff and users. We aim to bring the concept of the Internet of Things and artificial intelligence closer to the laboratory to support a new generation of instrumentation.|https://doi.org/10.1016/j.nimb.2020.05.027|https://www.sciencedirect.com/science/article/pii/S0168583X2030272X||||2020|Ion beam analysis and big data: How data science can support next-generation instrumentation|Tiago F. Silva and Cleber L. Rodrigues and Manfredo H. Tabacniks and Hugo D.C. Pereira and Thiago B. Saramela and Renato O. Guimarães|article|SILVA2020111|||Nuclear Instruments and Methods in Physics Research Section B: Beam Interactions with Materials and Atoms|0168583X||478|||||29068|0,429|Q2|119|514|1766|15630|2403|1750|1,42|30,41|Netherlands|Western Europe|1983-2020|Instrumentation (Q2); Nuclear and High Energy Physics (Q3)||||1683120367|1513961722
ICIE '17|Dalian Liaoning, China|Recommendation system, WeChatpublic number, Rank learning algorithm, Meta data model|5||Proceedings of the 6th International Conference on Information Engineering|On the basis of obtaining the data of mass WeChat public1, in order to improve the operational efficiency and quality of WeChat public number. On the basis of the retrieval technology, the quality evaluation model of WeChat public number was established. A sort learning algorithm based on model retrieval is proposed. Use the vector space technology based on the weight of the entry position to retrieve the contents of WeChat public number, and then use the WeChat public number quality evaluation model to sort. The retrieved articles sorted data to recommend to the operator, so that the operator can be faster and more efficient to find their hope to find high quality WeChat number of public articles.|10.1145/3078564.3078572|https://doi.org/10.1145/3078564.3078572|New York, NY, USA|Association for Computing Machinery|9781450352109|2017|Ranking Learning Algorithm of Information Retrieval Based on WeChat Public Numbers|Song, Zekun and Zhang, Lvyang and Liu, Tao and Chen, Ying|inproceedings|10.1145/3078564.3078572|4||||||||||||||||||||||||||||1683289253|42
||Data-driven science, Data-driven modeling, Artificial intelligence, Machine learning, Data-science, Big data||845-855||In the past, data in which science and engineering is based, was scarce and frequently obtained by experiments proposed to verify a given hypothesis. Each experiment was able to yield only very limited data. Today, data is abundant and abundantly collected in each single experiment at a very small cost. Data-driven modeling and scientific discovery is a change of paradigm on how many problems, both in science and engineering, are addressed. Some scientific fields have been using artificial intelligence for some time due to the inherent difficulty in obtaining laws and equations to describe some phenomena. However, today data-driven approaches are also flooding fields like mechanics and materials science, where the traditional approach seemed to be highly satisfactory. In this paper we review the application of data-driven modeling and model learning procedures to different fields in science and engineering.|https://doi.org/10.1016/j.crme.2019.11.009|https://www.sciencedirect.com/science/article/pii/S1631072119301809||||2019|Data-driven modeling and learning in science and engineering|Francisco J. Montáns and Francisco Chinesta and Rafael Gómez-Bombarelli and J. Nathan Kutz|article|MONTANS2019845|||Comptes Rendus Mécanique|16310721|11|347||Data-Based Engineering Science and Technology|||||||||||||||||||||1683851139|1115704971
||Quality of service;Supply chains;Web services;Mathematical model;Prediction methods;Time factors;Corporate acquisitions;Data supply chain;QoS model;processing context;QoS prediction||1-7|2017 IEEE International Conference on Communications (ICC)|Due to the execution paradigm may be different at different invocation time, users obtain different QoS when interacting with the same Data Supply Chain (DSC). However, existing QoS prediction methods seldom took this observation into consideration, which shall decrease the prediction accuracy. In this paper, we propose a context-based QoS prediction method for data supply chain. First, a QoS mathematical model is developed for considering the mass data transmission across elementary sub-chains. Then, two execution paradigms of data supply chain are discussed. Besides, we explored several special context factors of data supply chain (such as invocation time, data source update period and execution paradigm) which influence QoS. By processing such context information, we can obtain the part of data supply chain which is need to execute when the user query occurs and leverage them to predict QoS. Experimental results indicate that our approach improves the prediction accuracy and efficiency of QoS when compared to previous methods.|10.1109/ICC.2017.7997260|||||2017|QoS prediction method for data supply chain based on context|Li, Peng and Luo, Hong and Wu, Tin-Yu and Obaidat, Mohammad S.|inproceedings|7997260||May||19381883|||||||||||||||||||||||||1684634681|276554849
||Big data (BD), Safety big data (SBD), Safety decision-making (SDM), Safety insight (SI), Data-driven||46-56||Safety data and information are the most valuable assets for organizations’ safety decision-making (SDM), especially in the era of big data (BD). In this study, a conceptual framework for SDM based on BD, known as BD-driven SDM, was developed and its detailed structure and elements as well as strategies were presented. Other theoretical and practical contributions include: (a) the description of the meta-process and interdisciplinary research area of BD-driven SDM, (b) the design of six types of general analytics and five types of special analytics for SBD mining according to different requirements of safety management applications, (c) the analysis of influencing factors of BD-driven SDM, and (d) the discussion of advantages and limitations in this research as well as suggestions for future research. The results obtained from this study are of important implications for research and practice on BD-driven SDM.|https://doi.org/10.1016/j.ssci.2018.05.012|https://www.sciencedirect.com/science/article/pii/S0925753518300973||||2018|Big-data-driven safety decision-making: A conceptual framework and its influencing factors|Lang Huang and Chao Wu and Bing Wang and Qiumei Ouyang|article|HUANG201846|||Safety Science|09257535||109|||||12332|1,178|Q1|111|451|1047|26276|5909|1020|5,49|58,26|Netherlands|Western Europe|1991-2020|Public Health, Environmental and Occupational Health (Q1); Safety Research (Q1); Safety, Risk, Reliability and Quality (Q1)|17,184|4.877|0.01488|1688176575|1544923827
IoT '20|"\"Malm\"\"{o}, Sweden\""|data validation, data validation network, industrial internet of things, internet of things, big data, cluster-based data \^{A}\u{a}Validation, distributed hash table|8||Proceedings of the 10th International Conference on the Internet of Things|The generated real-time data on the Internet of Things (IoT) and the ability to gather and manipulate them are positively affecting various fields. One of the main concerns in IoT is how to provide trustworthy data. The data validation network ensures that the generated data by data sources in the IoT are trustworthy. However, the existing data validation network depends on a centralized entity for the selection of data validators. In this paper, a decentralized validator selection model is proposed. The proposed model creates multiple clusters using the distributed hash table (DHT) technique. The selection process of data validators from different clusters in the model is done randomly in a decentralized scheme. It provides a global method of assignment, selection, and verification of the selected validators in the network.|10.1145/3410992.3411027|https://doi.org/10.1145/3410992.3411027|New York, NY, USA|Association for Computing Machinery|9781450387583|2020|Distributed Data Validation Network in IoT: A Decentralized Validator Selection Model|Kamel, Mohammed B. M. and Wallis, Kevin and Ligeti, Peter and Reich, Christoph|inproceedings|10.1145/3410992.3411027|12||||||||||||||||||||||||||||1689005222|42
||Data mining;Manufacturing;Information management;Production;Manufacturing processes;Companies;Analytical models;Bottleneck analysis;manufacturing process;process mining;process modelling;information management system;value stream||24203-24214||One major result of the Industrial Digitalization is the access to a large set of digitalized data and information, i.e. Big Data. The market of analytic tools offers a huge variety of algorithms and software to exploit big datasets. Implementing their advantages into one approach brings better results and empower possibilities for process analysis. Its application in the manufacturing industry requires a high level of effort and remains to be challenging due to product complexity, human-centric processes, and data quality. In this manuscript, the authors combine process mining and value streams methods for analyzing the data from the information management system, applying the approach to the data delivered by one specific manufacturing system. The manufacturing process to be examined is the process of assembling gas meters in the manufacture. This specific and important part of the whole supply-chain process was taken as suitable for the study due to almost full-automated line with data about each process activity of the value-stream in the information system. The paper applies process mining algorithms in discovering a descriptive process model that plays the main role as a basis for further analysis. At the same time, modern techniques of the bottleneck analysis are described, and two new comprehensible methods of bottlenecks detection (TimeLag and Confidence intervals methods), as well as their advantages, will be discussed. Achieved results can be subsequently used for other sources of big data and industrial-compliant Information Management Systems.|10.1109/ACCESS.2022.3152211|||||2022|Screening Process Mining and Value Stream Techniques on Industrial Manufacturing Processes: Process Modelling and Bottleneck Analysis|Rudnitckaia, Julia and Venkatachalam, Hari Santhosh and Essmann, Roland and Hruška, Tomáš and Colombo, Armando Walter|article|9715073|||IEEE Access|21693536||10|||||21100374601|0,587|Q1|127|18036|24267|771081|116691|24200|4,48|42,75|United States|Northern America|2013-2020|Computer Science (miscellaneous) (Q1); Engineering (miscellaneous) (Q1); Materials Science (miscellaneous) (Q2)|105,968|3.367|0.15396|1689795578|1905633267
SAT-CPS '21|Virtual Event, USA|security analytics, industrial IoT, security as a service|10|23–32|Proceedings of the 2021 ACM Workshop on Secure and Trustworthy Cyber-Physical Systems|In Cloud Computing, the cloud serves as a central data hub for the Industrial Internet of Things' (IIoT) data and is deployed in diverse application fields, e.g., Smart Grid or Smart Manufacturing. Therefore, the aggregated and contextualized data is bundled in a central data hub, bringing tremendous cybersecurity advantages. Given the threat landscape in IIoT systems, especially SMEs (small and medium-sized enterprises) need to be prepared regarding their cybersecurity, react quickly, and strengthen their overall cybersecurity. For instance, with the application of machine learning algorithms, security-related data can be analyzed predictively in order to be able to ward off a potential attack at an early stage. Since modern reference architectures for IIoT systems, such as RAMI 4.0 or IIRA, consider cybersecurity approaches on a high level and SMEs lack financial funds and knowledge, this paper conceptualizes a security analytics service used as a security add-on to these reference architectures. Thus, this paper conceptualizes a flexible security analytics service that implements security capabilities with flexible analytical techniques that fit specific SMEs' needs. The security analytics service is also evaluated with a real-world use case.|10.1145/3445969.3450427|https://doi.org/10.1145/3445969.3450427|New York, NY, USA|Association for Computing Machinery|9781450383196|2021|A Flexible Security Analytics Service for the Industrial IoT|"\"Empl, Philip and Pernul, G\"\"{u}nther\""|inproceedings|10.1145/3445969.3450427|||||||||||||||||||||||||||||1691115884|42
Mobility '09|Nice, France|measurement, overlay multicast, UniNet, overlay, TEIN2, multicast, KOREN|7||Proceedings of the 6th International Conference on Mobile Technology, Application &amp; Systems|Nowadays not only many of research works with various international networks are increasing more and more but also commercial works are increasing with different international networks. In this paper, we have constructed the overlay multicast testbed with KOREN and TEIN2 network, and then we also analyze and research many works with the data got from the testbed experiments, and research works for speed of transmission and transmission security when the data is forwarded to several various international network. We work out the process of problem based on several data of experiments. We analyze these problems and propose the research way to other researchers in overlay multicast area, and we also provide these useful results to other researchers in this area.|10.1145/1710035.1710077|https://doi.org/10.1145/1710035.1710077|New York, NY, USA|Association for Computing Machinery|9781605585369|2009|Establishment and Traffic Measurement of Overlay Multicast Testbed in KOREN, THaiREN and TEIN2|Liu, Jing and Cho, Sungchol and Han, Sunyoung and Kim, Keecheon and Ha, YoungGuk and Choe, Jongwon and Kamolphiwong, Sinchai and Choo, Hyunseung and Shin, Yongtae and Kim, Chinchol|inproceedings|10.1145/1710035.1710077|42||||||||||||||||||||||||||||1694976969|42
FAT* '19|Atlanta, GA, USA|data governance, data sharing, algorithmic bias, privacy, data ethics|10|191–200|Proceedings of the Conference on Fairness, Accountability, and Transparency|"\"Data too sensitive to be \"\"open\"\" for analysis and re-purposing typically remains \"\"closed\"\" as proprietary information. This dichotomy undermines efforts to make algorithmic systems more fair, transparent, and accountable. Access to proprietary data in particular is needed by government agencies to enforce policy, researchers to evaluate methods, and the public to hold agencies accountable; all of these needs must be met while preserving individual privacy and firm competitiveness. In this paper, we describe an integrated legal-technical approach provided by a third-party public-private data trust designed to balance these competing interests. Basic membership allows firms and agencies to enable low-risk access to data for compliance reporting and core methods research, while modular data sharing agreements support a wide array of projects and use cases. Unless specifically stated otherwise in an agreement, all data access is initially provided to end users through customized synthetic datasets that offer a) strong privacy guarantees, b) removal of signals that could expose competitive advantage, and c) removal of biases that could reinforce discriminatory policies, all while maintaining fidelity to the original data. We find that using synthetic data in conjunction with strong legal protections over raw data strikes a balance between transparency, proprietorship, privacy, and research objectives. This legal-technical framework can form the basis for data trusts in a variety of contexts.\""|10.1145/3287560.3287577|https://doi.org/10.1145/3287560.3287577|New York, NY, USA|Association for Computing Machinery|9781450361255|2019|Beyond Open vs. Closed: Balancing Individual Privacy and Public Accountability in Data Sharing|Young, Meg and Rodriguez, Luke and Keller, Emily and Sun, Feiyang and Sa, Boyang and Whittington, Jan and Howe, Bill|inproceedings|10.1145/3287560.3287577|||||||||||||||||||||||||||||1695195715|42
||Drought monitoring, Artificial intelligence, Big data, Machine learning, Statistical approach, Remote sensing||101136||Over recent years, the frequency and intensity of droughts have increased and there has been a large drying trend over many parts of the world. Consequently, drought monitoring using big data analytic has gained an explosive interest. Droughts stand among the most damaging natural disasters. It threatens agricultural production, ecological environment, and socio-economic development. For this reason, early warning, accurate evaluation, and efficient prediction are an emergency especially for the nations that are the most menaced by this danger. There are numerous emerging studies addressing big data and its applications in drought monitoring. In fact, big data handle data heterogeneity which is an additive value for the prediction of drought, it offers a view of the different dimensions such as the spatial distribution, the temporal distribution and the severity detection of this phenomenon. Big data analytic and drought are introduced and reviewed in this paper. Besides, this review includes different studies, researches and applications of big data to drought monitoring. Challenges related to data life cycle such as data challenges, data processing challenges and data infrastructure management challenges are also discussed. Finally, we conclude that big data analytic can be beneficial in drought monitoring but there is a need for statistical and artificial intelligence-based approaches.|https://doi.org/10.1016/j.ecoinf.2020.101136|https://www.sciencedirect.com/science/article/pii/S1574954120300868||||2020|A review of drought monitoring with big data: Issues, methods, challenges and research directions|Hanen Balti and Ali {Ben Abbes} and Nedra Mellouli and Imed Riadh Farah and Yanfang Sang and Myriam Lamolle|article|BALTI2020101136|||Ecological Informatics|15749541||60|||||3100147401|0,774|Q1|55|109|273|5588|964|271|3,43|51,27|Netherlands|Western Europe|2006-2020|Modeling and Simulation (Q1); Applied Mathematics (Q2); Computational Theory and Mathematics (Q2); Computer Science Applications (Q2); Ecological Modeling (Q2); Ecology (Q2); Ecology, Evolution, Behavior and Systematics (Q2)|2,893|3.142|0.00332|1697792762|339174395
|||7|||LocWeb and TempWeb 2021 were the eleventh events in their workshop series and took place co-located on 12th April 2021 in conjunction with The Web Conference WWW 2021. They were intended to be held in Ljubljana, Slovenia as a potentially hybrid event, but due to the pandemic, were fully moved online.LocWeb and TempWeb were held as one colocated session with a merged programme and shared topics to explore similarities and introduce attendees to the two related and complementary areas. LocWeb 2021 explored the intersection of location-based analytics and Web architecture with a focus on on Web-scale services and location-aware information access. TempWeb 2021 discussed temporal analytics at a Web scale with experts from science and industry.Date: 12 April, 2021.Websites: https://dhere.de/locweb/locweb2021 and http://temporalweb.net/.|10.1145/3527546.3527555|https://doi.org/10.1145/3527546.3527555|New York, NY, USA|Association for Computing Machinery||2022|Report on the 11th International Workshop on Location and the Web (LocWeb 2021) and the 11th Temporal Web Analytics Workshop (TempWeb2021) at WWW2021|Ahlers, Dirk and Wilde, Erik and Spaniol, Marc and Baeza-Yates, Ricardo and Alonso, Omar|article|10.1145/3527546.3527555|6|mar|SIGIR Forum|01635840|2|55|December 2021||||||||||||||||||||||1698720698|1301506833
ECSAW '15|Dubrovnik, Cavtat, Croatia|retrospectives, contributing student pedagogy, peer assessment, Data analytics, software architecture|6||Proceedings of the 2015 European Conference on Software Architecture Workshops|This experience report covers the collaboration between UCL and NII Tokyo students in development of data analytics research projects: the challenges, contributing student pedagogy and changes to teaching. Students are often taught technology management separate from other computing modules. The teaching team designed a more coherent learning experience linking the technology management teaching more closely to engineering processes, specifically to engage students whose interest lies more with computing. This project has given rise to a re-evaluation of how technology management is taught to undergraduate students, adoption of architecture as a key aspect and inclusion of students with different levels of academic attainment within a class.|10.1145/2797433.2797467|https://doi.org/10.1145/2797433.2797467|New York, NY, USA|Association for Computing Machinery|9781450333931|2015|Pedagogical Lessons from an International Collaborative Big Data Undergraduate Research Project|Collins, Graham and Varilly, Hugh and Yoshinori, Tanabe|inproceedings|10.1145/2797433.2797467|32||||||||||||||||||||||||||||1698981477|42
||NoSQL databases, Query Relaxation, Ontology, MongoDB, AllgroGraph||460-469||The complexity of building biological databases is well-known and ontologies play an extremely important role in biological databases. However, much of the emphasis on the role of ontologies in biological databases has been on the construction of databases. In this paper, we explore a somewhat overlooked aspect regarding ontologies in biological databases, namely, how ontologies can be used to assist better database retrieval. In particular, we show how ontologies can be used to revise user submitted queries for query relaxation. In addition, since our research is conducted at today's “big data” era, our investigation is centered on NoSQL databases which serve as a kind of “representatives” of big data. This paper contains two major parts: First we describe our methodology of building two NoSQL application databases (MongoDB and AllegroGraph) using GO ontology, and then discuss how to achieve query relaxation through GO ontology. We report our experiments and show sample queries and results. Our research on query relaxation on NoSQL databases is complementary to existing work in big data and in biological databases and deserves further exploration.|https://doi.org/10.1016/j.procs.2016.07.120|https://www.sciencedirect.com/science/article/pii/S1877050916313138||||2016|Creating NoSQL Biological Databases with Ontologies for Query Relaxation|Naresh Kumar Gundla and Zhengxin Chen|article|GUNDLA2016460|||Procedia Computer Science|18770509||91||Promoting Business Analytics and Quantitative Management of Technology: 4th International Conference on Information Technology and Quantitative Management (ITQM 2016)|||19700182801|0,334|-|76|1907|6483|38511|13722|6359|2,09|20,19|Netherlands|Western Europe|2010-2020|Computer Science (miscellaneous)||||1700542632|2108686752
BDE 2021|Shanghai, China|Participants, Recruitment, Cybersecurity, Crowdsourcing, Online experiments|12|150–161|The 2021 3rd International Conference on Big Data Engineering|Evaluating the effectiveness of defense technologies mandates the inclusion of a human element, specifically if these technologies target human cognition and emotions. One of the biggest challenges that face researchers in the realm of behavioral cybersecurity is participant recruitment. Researchers often rely on college students, the general public, real-world hackers, or a hard-to-reach population (e.g., professional red teamers) to test the effectiveness of cybersecurity defense techniques. However, recruiting participants from these populations has drawbacks, including but not limited to: high cost, time constraints, and manageability and accessibility issues. This research explored the applicability of using two popular crowdsourcing platforms, Amazon Mechanical Turk and Prolific, to conduct web hacking experiments. Our study is the first to use crowdsourcing platforms to run hacking experiments for scientific purposes. While the recruitment is challenging, the paradigm of existing crowdsourcing platforms can be useful for understanding adversarial behavior, as it facilitates access to a diverse set of participants and allows researchers to conduct longitudinal and cross-cultural assessments. In particular, crowdsourcing platforms offer a great opportunity for cybersecurity researchers to investigate the Oppositional Human Factors (OHFs) in a manageable and flexible way.|10.1145/3468920.3468942|https://doi.org/10.1145/3468920.3468942|New York, NY, USA|Association for Computing Machinery|9781450389426|2021|Conducting Malicious Cybersecurity Experiments on Crowdsourcing Platforms|Aljohani, Asmaa and Jones, James|inproceedings|10.1145/3468920.3468942|||||||||||||||||||||||||||||1704307409|42
DataEd '22|Philadelphia, PA, USA|renewable energy systems, open data, datathon, data education curricula|6|26–31|1st International Workshop on Data Systems Education|Data literacy and the fundamentals of big data management are becoming interdisciplinary in Higher Education curricula, also due to the widespread need of data science skills. This casts the need for presenting novel (and more engaging) learning activities to students. Data hackathons (also known as datathons) represent a viable option to allow students practicing with real use cases and datasets, as well as addressing their learning experiences collaboratively. Moreover, datathons promise to improve soft skills and offer hands-on learning opportunities. Therefore, we present in this paper a datathon on a publicly available dataset about renewable energy systems. The datathon involved students from three data-focused courses of three different M.S. degrees at the University of Salento (Italy). A detailed analysis of the design, implementation and evaluation choices is proposed, along with a series of gathered insights and lessons learned that might help systematizing the introduction and use of datathons in data education.|10.1145/3531072.3535322|https://doi.org/10.1145/3531072.3535322|New York, NY, USA|Association for Computing Machinery|9781450393508|2022|Enhancing Data Education with Datathons: An Experience with Open Data on Renewable Energy Systems|Longo, Antonella and Zappatore, Marco and Martella, Angelo and Rucco, Chiara|inproceedings|10.1145/3531072.3535322|||||||||||||||||||||||||||||1707980181|42
HPCCT '19|Guangzhou, China|Big data, Teaching informatization, Service system|5|185–189|Proceedings of the 2019 3rd High Performance Computing and Cluster Technologies Conference|"\"Under the background of the rapid development of big data technology, the construction of college informationization teaching service system is the basis of the informationization teaching in colleges and universities. That is very important for the success of the informatization in Universities what is service realization model, business logic, architecture and platform conform to the whole development strategy of universities. Information management organization supports the planning, implementation, operation, maintenance and management of business information system. This paper analyzes the reform mode of college education information service system supported by big data technology. Based on the analysis of the reform mode of college informationization teaching service system supported by big data technology, this paper puts forward the design idea of post system based on big data. At the same time, with the case of \"\"big data assisted employment\"\", the post design and adjustment were carried out. The results show that big data assisted employment has greatly improved the efficiency and quality of the school's employment department, providing students with better employment security. Finally, the problems that need to be solved in the informatization teaching service are sorted out.\""|10.1145/3341069.3341086|https://doi.org/10.1145/3341069.3341086|New York, NY, USA|Association for Computing Machinery|9781450371858|2019|The Construction Study of College Informationization Teaching Service System under the Background of Big Data|Pengxi, Li|inproceedings|10.1145/3341069.3341086|||||||||||||||||||||||||||||1709291921|42
||Data integrity;Power quality;Monitoring;Power measurement;Redundancy;Big Data;Business;power qualiy;data quality;big data;data provenance;data assessment||248-252|2018 IEEE 3rd International Conference on Cloud Computing and Big Data Analysis (ICCCBDA)|Currently, on-line monitoring and measuring system of power quality has accumulated a huge amount of data. In the age of big data, those data integrated from various systems will face big data application problems. This paper proposes a data quality assessment system method for on-line monitoring and measuring system of power quality based on big data and data provenance to assess integrity, redundancy, accuracy, timeliness, intelligence and consistency of data set and single data. Specific assessment rule which conforms to the situation of on-line monitoring and measuring system of power quality will be devised to found data quality problems. Thus it will provide strong data support for big data application of power quality.|10.1109/ICCCBDA.2018.8386521|||||2018|Data quality assessment for on-line monitoring and measuring system of power quality based on big data and data provenance theory|Hongxun, Tian and Honggang, Wang and Kun, Zhou and Mingtai, Shi and Haosong, Li and Zhongping, Xu and Taifeng, Kang and Jin, Li and Yaqi, Cai|inproceedings|8386521||April|||||||||||||||||||||||||||1710746854|42
||Big Data;Data integrity;Power grids;History;Real-time systems;Sensors;data quality;electric power data;data quality assessment;big data;framework||289-292|2017 14th Web Information Systems and Applications Conference (WISA)|Since a low-quality data may influence the effectiveness and reliability of applications, data quality is required to be guaranteed. Data quality assessment is considered as the foundation of the promotion of data quality, so it is essential to access the data quality before any other data related activities. In the electric power industry, more and more electric power data is continuously accumulated, and many electric power applications have been developed based on these data. In China, the power grid has many special characteristic, traditional big data assessment frameworks cannot be directly applied. Therefore, a big data framework for electric power data quality assessment is proposed. Based on big data techniques, the framework can accumulate both the real-time data and the history data, provide an integrated computation environment for electric power big data assessment, and support the storage of different types of data.|10.1109/WISA.2017.29|||||2017|A Big Data Framework for Electric Power Data Quality Assessment|Liu, He and Huang, Fupeng and Li, Han and Liu, Weiwei and Wang, Tongxun|inproceedings|8332632||Nov|||||||||||||||||||||||||||1714262703|42
||Systematic mapping study, Entity reconciliation, Heterogeneous databases, Big data||14-27||The entity reconciliation (ER) problem aroused much interest as a research topic in today's Big Data era, full of big and open heterogeneous data sources. This problem poses when relevant information on a topic needs to be obtained using methods based on: (i) identifying records that represent the same real world entity, and (ii) identifying those records that are similar but do not correspond to the same real-world entity. ER is an operational intelligence process, whereby organizations can unify different and heterogeneous data sources in order to relate possible matches of non-obvious entities. Besides, the complexity that the heterogeneity of data sources involves, the large number of records and differences among languages, for instance, must be added. This paper describes a Systematic Mapping Study (SMS) of journal articles, conferences and workshops published from 2010 to 2017 to solve the problem described before, first trying to understand the state-of-the-art, and then identifying any gaps in current research. Eleven digital libraries were analyzed following a systematic, semiautomatic and rigorous process that has resulted in 61 primary studies. They represent a great variety of intelligent proposals that aim to solve ER. The conclusion obtained is that most of the research is based on the operational phase as opposed to the design phase, and most studies have been tested on real-world data sources, where a lot of them are heterogeneous, but just a few apply to industry. There is a clear trend in research techniques based on clustering/blocking and graphs, although the level of automation of the proposals is hardly ever mentioned in the research work.|https://doi.org/10.1016/j.eswa.2017.03.010|https://www.sciencedirect.com/science/article/pii/S0957417417301550||||2017|Entity reconciliation in big data sources: A systematic mapping study|J.G. Enríquez and F.J. Domínguez-Mayo and M.J. Escalona and M. Ross and G. Staples|article|ENRIQUEZ201714|||Expert Systems with Applications|09574174||80|||||24201|1,368|Q1|207|770|1945|42314|17345|1943|8,67|54,95|United Kingdom|Western Europe|1990-2021|Artificial Intelligence (Q1); Computer Science Applications (Q1); Engineering (miscellaneous) (Q1)|55,444|6.954|0.04053|1714281062|1377770283
|||2|57–58|Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice|||https://doi.org/10.1145/3447404.3447409|New York, NY, USA|Association for Computing Machinery|9781450390293|2021|The Internet of Everything—Introducing Privacy|McMenemy, David|inbook|10.1145/3447404.3447409|||||||||1||||||||||||||||||||1716492541|42
||Business Intelligence & Analytics, ETL quality, Data, process quality, Talend Data Integration, Talend Data Quality||676-687||The accuracy and relevance of Business Intelligence & Analytics (BI&A) rely on the ability to bring high data quality to the data warehouse from both internal and external sources using the ETL process. The latter is complex and time-consuming as it manages data with heterogeneous content and diverse quality problems. Ensuring data quality requires tracking quality defects along the ETL process. In this paper, we present the main ETL quality characteristics. We provide an overview of the existing ETL process data quality approaches. We also present a comparative study of some commercial ETL tools to show how much these tools consider data quality dimensions. To illustrate our study, we carry out experiments using an ETL dedicated solution (Talend Data Integration) and a data quality dedicated solution (Talend Data Quality). Based on our study, we identify and discuss quality challenges to be addressed in our future research.|https://doi.org/10.1016/j.procs.2019.09.223|https://www.sciencedirect.com/science/article/pii/S1877050919314097||||2019|Data quality in ETL process: A preliminary study|Manel Souibgui and Faten Atigui and Saloua Zammali and Samira Cherfi and Sadok Ben Yahia|article|SOUIBGUI2019676|||Procedia Computer Science|18770509||159||Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 23rd International Conference KES2019|||19700182801|0,334|-|76|1907|6483|38511|13722|6359|2,09|20,19|Netherlands|Western Europe|2010-2020|Computer Science (miscellaneous)||||1716763204|2108686752
|||9|235–243|DG.O 2022: The 23rd Annual International Conference on Digital Government Research|Over the past decade, Open Government Data (OGD) strategies have become a continuing concern in administrative services. This is even truer than at any time. Given the current situation, data management, specifically consistent data publication, has been central to public institutions. The Covid-19 pandemic has shown that data collected by public administrations could make valuable contributions. However, in Switzerland, the pandemic has highlighted the limitations of public organizations' capability to lead the publication of their data. Based on an ethnography and a literature review, this paper explores how data governance components impact OGD publication process and presents a model of OGD governance. For this purpose, we identify key data governance components necessary to OGD publication - structural, procedural, and relational - and illustrate how OGD challenges rarely arise from the publication of OGD or the open nature of data itself, but a lack of data governance.||https://doi.org/10.1145/3543434.3543438|New York, NY, USA|Association for Computing Machinery|9781450397490|2022|The Impact of Data Governance on OGD Publication – An Ethnographic Odyssey|Marmier, Auriane|inbook|10.1145/3543434.3543438|||||||||||||||||||||||||||||1719030383|42
||Digital Twin, Internet of Things, Autonomous systems, Big data, Machine learning||100383||Digital Twin was introduced over a decade ago, as an innovative all-encompassing tool, with perceived benefits including real-time monitoring, simulation, optimisation and accurate forecasting. However, the theoretical framework and practical implementations of digital twin (DT) are yet to fully achieve this vision at scale. Although an increasing number of successful implementations exist in research and industrial works, sufficient implementation details are not publicly available, making it difficult to fully assess their components and effectiveness, to draw comparisons, identify successful solutions, share lessons, and thus to jointly advance and benefit from the DT methodology. This work first presents a review of relevant DT research and industrial works, focusing on the key DT features, current approaches in different domains, and successful DT implementations, to infer the key DT components and properties, and to identify current limitations and reasons behind the delay in the widespread implementation and adoption of digital twin. This work identifies that the major reasons for this delay are: the fact the DT is still a fast evolving concept; the lack of a universal DT reference framework, e.g. DT standards are scarce and still evolving; problem- and domain-dependence; security concerns over shared data; lack of DT performance metrics; and reliance of digital twin on other fast-evolving technologies. Advancements in machine learning, Internet of Things (IoT) and big data have led to significant improvements in DT features such as real-time monitoring and accurate forecasting. Despite this progress and individual company-based efforts, certain research and implementation gaps exist in the field, which have so far prevented the widespread adoption of the DT concept and technology; these gaps are also discussed in this work. Based on reviews of past work and the identified gaps, this work then defines a conceptualisation of DT which includes its components and properties; these also validate the uniqueness of DT as a concept, when compared to similar concepts such as simulation, autonomous systems and optimisation. Real-life case studies are used to showcase the application of the conceptualisation. This work discusses the state-of-the-art in DT, addresses relevant and timely DT questions, and identifies novel research questions, thus contributing to a better understanding of the DT paradigm and advancing the theory and practice of DT and its allied technologies.|https://doi.org/10.1016/j.jii.2022.100383|https://www.sciencedirect.com/science/article/pii/S2452414X22000516||||2022|Digital Twins: State of the art theory and practice, challenges, and open research questions|Angira Sharma and Edward Kosasih and Jie Zhang and Alexandra Brintrup and Anisoara Calinescu|article|SHARMA2022100383|||Journal of Industrial Information Integration|2452414X||30|||||21100787106|2,042|Q1|24|28|86|2152|1361|83|12,26|76,86|Netherlands|Western Europe|2016-2020|Industrial and Manufacturing Engineering (Q1); Information Systems and Management (Q1)|1,149|10.063|0.00148|1719259003|121356201
||Big Data, Data analytics, Internet of Things, Healthcare, Energy, Transportation, Building automation, Smart Cities||601-614||With the rapid development of the Internet of Things (IoT), Big Data technologies have emerged as a critical data analytics tool to bring the knowledge within IoT infrastructures to better meet the purpose of the IoT systems and support critical decision making. Although the topic of Big Data analytics itself is extensively researched, the disparity between IoT domains (such as healthcare, energy, transportation and others) has isolated the evolution of Big Data approaches in each IoT domain. Thus, the mutual understanding across IoT domains can possibly advance the evolution of Big Data research in IoT. In this work, we therefore conduct a survey on Big Data technologies in different IoT domains to facilitate and stimulate knowledge sharing across the IoT domains. Based on our review, this paper discusses the similarities and differences among Big Data technologies used in different IoT domains, suggests how certain Big Data technology used in one IoT domain can be re-used in another IoT domain, and develops a conceptual framework to outline the critical Big Data technologies across all the reviewed IoT domains.|https://doi.org/10.1016/j.future.2018.04.053|https://www.sciencedirect.com/science/article/pii/S0167739X17316953||||2018|Big Data for Internet of Things: A Survey|Mouzhi Ge and Hind Bangui and Barbora Buhnova|article|GE2018601|||Future Generation Computer Systems|0167739X||87|||||12264|1,262|Q1|119|798|1935|36054|16877|1868|9,11|45,18|Netherlands|Western Europe|1984-2021|Computer Networks and Communications (Q1); Hardware and Architecture (Q1); Software (Q1)||||1719858009|562237118
||Big data, Streaming data, Carbon emission, Environmental management, Bibliometric analysis, Net-work analysis||138984||Climate change and environmental management are issues of global concern. The advent of the era of Big Data has created a new research platform for the assessment of environmental governance and policies. However, little is known about Big Data application to climate change and environmental management research. This paper adopts bibliometric analysis in conjunction with network analysis to systematically evaluate the publications on carbon emissions and environmental management based on Big Data and Streaming Data using R package and VOSviewer software. The analysis involves 274 articles after rigorous screening and includes citation analysis, co-citation analysis, and co-word analysis. Main findings include (1) Carbon emissions and environmental management based on big data and streaming data is an emerging multidisciplinary research topic, which has been applied in the fields of computer science, supply chain design, transportation, carbon price assessment, environmental policy evaluation, and CO2 emissions reduction. (2) This field has attracted the attention of nations which are major contributors to the world economy. In particular, European and American scholars have made the main contributions to this topic, and Chinese researchers have also had great impact. (3) The research content of this topic is primarily divided into four categories, including empirical studies of specific industries, air pollution governance, technological innovation, and low-carbon transportation. Our findings suggest that future research should bring greater depth of practical and modeling analysis to environmental policy assessment based on Big Data.|https://doi.org/10.1016/j.scitotenv.2020.138984|https://www.sciencedirect.com/science/article/pii/S0048969720325018||||2020|Carbon emissions and environmental management based on Big Data and Streaming Data: A bibliometric analysis|Yuan Su and Yanni Yu and Ning Zhang|article|SU2020138984|||Science of The Total Environment|00489697||733|||||25349|1,795|Q1|244|6929|13278|455970|106837|13125|7,96|65,81|Netherlands|Western Europe|1970, 1972-2021|Environmental Chemistry (Q1); Environmental Engineering (Q1); Pollution (Q1); Waste Management and Disposal (Q1)|210,143|7.963|0.23082|1720599039|2019676356
||empirical studies, business goals, empirical software engineering., big data applications, big data requirements engineering|6|1–6||Requirements Engineering (RE) plays an essential role in the software engineering process, being considered as one of the most critical phases of the software development life-cycle. As we might expect, then, the Requirements Engineering would play a similar role in the context of Big Data applications. However, practicing Requirements Engineering is a challenging and complex task. It involves (i) stakeholders with diverse backgrounds and levels of knowledge, (ii) different application domains, (iii) it is expensive and error-prone, (iii) it is important to be aligned with business goals, to name a few. Because it involves such complex activities, a lot has to be understood in order to properly address Requirements Engineering. Especially, when the technology domain (e.g., Big Data) is not yet well explored. In this context, this paper describes a research plan on Requirements Engineering involving the development of Big Data applications. The high-level goal is to investigate: (i) On the technical front, the Requirements Engineering activities with respect to the analysis and specification of Big Data requirements and, (ii) on the management side, the relationship between RE and Business Goals in the development of Big Data Software applications.|10.1145/3178315.3178323|https://doi.org/10.1145/3178315.3178323|New York, NY, USA|Association for Computing Machinery||2018|Requirements Engineering in the Context of Big Data Applications|Arruda, Darlan|article|10.1145/3178315.3178323||mar|SIGSOFT Softw. Eng. Notes|01635948|1|43|January 2018||||||||||||||||||||||1721021748|1286430388
||unstructured Big Data, content management, Big Data analytics, machine learning, text analytics, data quality|19|||The article presents a textual Big Data analytics solution developed in a real setting as a part of a high-capacity document digitization and storage system. A software based on machine learning techniques performs automated extraction and processing of textual contents. The work focuses on performance and data confidence evaluation and describes the approach to computing a set of indicators for textual data quality. It then presents experimental results.|10.1145/3461015|https://doi.org/10.1145/3461015|New York, NY, USA|Association for Computing Machinery||2022|Data and Process Quality Evaluation in a Textual Big Data Archiving System|Fugini, Mariagrazia and Finocchi, Jacopo|article|10.1145/3461015|2|mar|J. Comput. Cult. Herit.|15564673|1|15|February 2022||||||||||||||||||||||1721919969|373691565
DTUC '20|Virtual Event, Tunisia|digital humanities, metadata, data lakes|4||Proceedings of the 2nd International Conference on Digital Tools &amp; Uses Congress|Traditional data in Digital Humanities projects bear various formats (structured, semi-structured, textual) and need substantial transformations (encoding and tagging, stemming, lemmatization, etc.) to be managed and analyzed. To fully master this process, we propose the use of data lakes as a solution to data siloing and big data variety problems. We describe data lake projects we currently run in close collaboration with researchers in humanities and social sciences and discuss the lessons learned running these projects.|10.1145/3423603.3424004|https://doi.org/10.1145/3423603.3424004|New York, NY, USA|Association for Computing Machinery|9781405377539|2020|Data Lakes for Digital Humanities|Darmont, J\'{e}r\^{o}me and Favre, C\'{e}cile and Loudcher, Sabine and No\^{u}s, Camille|inproceedings|10.1145/3423603.3424004|6||||||||||||||||||||||||||||1722951636|42
||Big-data analytics, Ecological-economic-social sustainability, Green practices, SustainableOperations management, Structural equation modelling-artificial neural network, Emerging economies||10-24||Big data analytics is becoming very popular concept in academia as well as in industry. It has come up with new decision tools to design data-driven supply chains. The manufacturing industry is under huge pressure to integrate sustainable practices into their overall business for sustainbale operations management. The purpose of this study is to analyse the predictors of sustainable business performance through big data analytics in the context of developing countries. Data was collected from manufacturing firms those have adopted sustainable practices. A hybrid Structural Equation Modelling - Artificial Neural Network model is used to analyse 316 responses of Indian professional experts. Factor analysis results shows that management and leadership style, state and central-government policy, supplier integration, internal business process, and customer integration have a significant influence on big data analytics and sustainability practices. Furthermore, the results obtained from structural equation modelling were feed as input to the artificial neural network model. The study findings shows that management and leadership style, state and central-government policy as the two most important predictors of big data analytics and sustainability practices. The results provide unique insights into manufacturing firms to improve their sustainable business performance from an operations management viewpoint. The study provides theoretical and practical insights into big data implementation issues in accomplishing sustainability practices in business organisations of emerging economies.|https://doi.org/10.1016/j.jclepro.2019.03.181|https://www.sciencedirect.com/science/article/pii/S0959652619308753||||2019|Linking big data analytics and operational sustainability practices for sustainable business management|Rakesh D. Raut and Sachin Kumar Mangla and Vaibhav S. Narwane and Bhaskar B. Gardas and Pragati Priyadarshinee and Balkrishna E. Narkhede|article|RAUT201910|||Journal of Cleaner Production|09596526||224|||||19167|1,937|Q1|200|5126|10603|304498|103295|10577|9,56|59,40|United Kingdom|Western Europe|1993-2021|Environmental Science (miscellaneous) (Q1); Industrial and Manufacturing Engineering (Q1); Renewable Energy, Sustainability and the Environment (Q1); Strategy and Management (Q1)|170,352|9.297|0.18299|1724260218|1121054297
||Technology acceptance model (TAM), Text mining, Big data, Information quality, Top management support||103120||The popularity of the big data domain has boosted corporate interest in collecting and storing tremendous amounts of consumers’ textual information. However, decision makers are often overwhelmed by the abundance of information, and the usage of text mining (TM) tools is still at its infancy. This study validates an extended technology acceptance model integrating information quality (IQ) and top management support. Results confirm that IQ influences behavioral intentions and TM tools usage, through perceptions of external control, perceived ease of use, and perceived usefulness; top management support also has a key role in determining the usage of TM tools.|https://doi.org/10.1016/j.im.2018.10.006|https://www.sciencedirect.com/science/article/pii/S0378720617308765||||2020|Acceptance of text-mining systems: The signaling role of information quality|Nathalie T.M. Demoulin and Kristof Coussement|article|DEMOULIN2020103120|||Information & Management|03787206|1|57||Big data and business analytics: A research agenda for realizing business value|||12303|2,147|Q1|162|130|248|11385|2482|246|8,94|87,58|Netherlands|Western Europe|1977-2020|Information Systems (Q1); Information Systems and Management (Q1); Management Information Systems (Q1)||||1725294681|1945939487
||Administrative database, Mental Health, Secondary data, Psychiatry, Research design||155-165|Big Data in Psychiatry #x0026; Neurology|Administrative databases (AD) are repositories of administrative and clinical data related to patient contact episodes with all sorts of health facilities (primary care, hospitals, pharmacies, etc.). The use of AD data is increasing in Mental Health research as the advantages of using AD surpass some of the difficulties Mental health researchers find when using data from other sources (clinical trials, cohort studies, etc.). The large number of patients/contact episodes available, the systematic and broad register, and the fact that AD provides real-world data are some of the pros in using AD data. There are some methodological aspects that must be addressed when using this type of databases in order to provide solid and valid results. The possibility of clinical and administrative errors in an AD is a reality when using secondary data in Mental Health Research, and diagnostic code validation studies may be performed to estimate clinical and administrative accuracy. This chapter described in detail the pros and cons of using secondary data in mental health research and specifies the methodological steps a researcher must follow in order to find valid conclusions in AD from a clinical point of view.|https://doi.org/10.1016/B978-0-12-822884-5.00009-X|https://www.sciencedirect.com/science/article/pii/B978012822884500009X||Academic Press|978-0-12-822884-5|2021|Chapter 8 - The use of Big Data in Psychiatry—The role of administrative databases|Manuel Gonçalves-Pinho and Alberto Freitas|incollection|GONCALVESPINHO2021155||||||||||Ahmed A. Moustafa|||||||||||||||||||1725470911|42
||Smart city, Big data, Reference model, Challenge, Consideration||86-99||Cities worldwide are attempting to transform themselves into smart cities. Recent cases and studies show that a key factor in this transformation is the use of urban big data from stakeholders and physical objects in cities. However, the knowledge and framework for data use for smart cities remain relatively unknown. This paper reports findings from an analysis of various use cases of big data in cities worldwide and the authors' four projects with government organizations toward developing smart cities. Specifically, this paper classifies the urban data use cases into four reference models and identifies six challenges in transforming data into information for smart cities. Furthermore, building upon the relevant literature, this paper proposes five considerations for addressing the challenges in implementing the reference models in real-world applications. The reference models, challenges, and considerations collectively form a framework for data use for smart cities. This paper will contribute to urban planning and policy development in the modern data-rich economy.|https://doi.org/10.1016/j.cities.2018.04.011|https://www.sciencedirect.com/science/article/pii/S0264275117308545||||2018|Smart cities with big data: Reference models, challenges, and considerations|Chiehyeon Lim and Kwang-Jae Kim and Paul P. Maglio|article|LIM201886|||Cities|02642751||82|||||16956|1,771|Q1|90|419|732|28409|4866|723|6,19|67,80|United Kingdom|Western Europe|1983-2020|Development (Q1); Sociology and Political Science (Q1); Tourism, Leisure and Hospitality Management (Q1); Urban Studies (Q1)|11,076|5.835|0.01251|1726163082|2090576639
ASONAM '16|Davis, California||8|1103–1110|Proceedings of the 2016 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining|The opportunities to empirically study temporal networks nowadays are immense thanks to Internet of Things technologies along with ubiquitous and pervasive computing that allow a real-time fine-grained collection of social network data. This empowers data analytics and data scientists to reason about complex temporal phenomena, such as disease spread, residential energy consumption, political conflicts etc., using systematic methologies from complex networks and graph spectra analysis. However, a misuse of these methods may result in privacy-intrusive and discriminatory actions that may threaten citizens' autonomy and put their life under surveillance. This paper studies highly sparse temporal networks that model social interactions such as the physical proximity of participants in conferences. When citizens can self-determine the anonymized proximity data they wish to share via privacy-preserving platforms, temporal networks may turn out to be highly sparse and have low quality. This paper shows that even in this challenging scenario of privacy-by-design, significant information can be mined from temporal networks such as the correlation of events happening during a conference or stable groups interacting over time. The findings of this paper contribute to the introduction of privacy-preserving data analytics in temporal networks and their applications.||||IEEE Press|9781509028467|2016|Mining Social Interactions in Privacy-Preserving Temporal Networks|Musciotto, Federico and Delpriori, Saverio and Castagno, Paolo and Pournaras, Evangelos|inproceedings|10.5555/3192424.3192631|||||||||||||||||||||||||||||1727412896|42
||Big data, Multilingual search engine index, Crude oil price forecasting, Google Trends, Artificial intelligence||124178||In the big data era, search engine data (SED) have presented new opportunities for improving crude oil price prediction; however, the existing research were confined to single-language (mostly English) search keywords in SED collection. To address such a language bias and grasp worldwide investor attention, this study proposes a novel multilingual SED-driven forecasting methodology from a global perspective. The proposed methodology includes three main steps: (1) multilingual index construction, based on multilingual SED; (2) relationship investigation, between the multilingual index and crude oil price; and (3) oil price prediction, with the multilingual index as an informative predictor. With WTI spot price as studying samples, the empirical results indicate that SED have a powerful predictive power for crude oil price; nevertheless, multilingual SED statistically demonstrate better performance than single-language SED, in terms of enhancing prediction accuracy and model robustness.|https://doi.org/10.1016/j.physa.2020.124178|https://www.sciencedirect.com/science/article/pii/S037843712030025X||||2020|Forecasting crude oil price with multilingual search engine data|Jingjing Li and Ling Tang and Shouyang Wang|article|LI2020124178|||Physica A: Statistical Mechanics and its Applications|03784371||551|||||29115|0,640|Q2|166|1210|4300|51750|15067|4288|3,60|42,77|Netherlands|Western Europe|1975-2021|Condensed Matter Physics (Q2); Statistics and Probability (Q2)||||1727467617|633566785
||Data privacy;Process control;Data warehouses;Privacy breach;Size measurement;Probabilistic logic;Time measurement;Cloud computing;Data warehouses;Data pri-vacy;Internet privacy;Trust management||1555-1560|2022 IEEE 46th Annual Computers, Software, and Applications Conference (COMPSAC)|The transformation of big data to the cloud requires us to reconsider trust. Trust in all parties involved in the data management, the infrastructure as well as all groups with an access interest. A common way to mitigate the risk of the identification of individuals in case of privacy breaches is anonymization, which consequently also leads to information loss. Depending on the assumed level of confidence, a data processor can control the risk for privacy breaches in changing the point where anonymization gets applied. We examined anonymization points in data warehouse scenarios to evaluate their effects on utility and re-identification risk. Our evaluation showed that data quality can differ up to 4,80% while the re-identification risk is reduced by up to 16,82 %. With still improved quality, the re-identification risk differs up to 53,49 % in another configuration.|10.1109/COMPSAC54236.2022.00247|||||2022|Trade-off between Privacy, Quality and Risk: Anonymization Strategy Evaluation for Data Warehouses|Schiegg, Sascha and Gerl, Armin|inproceedings|9842582||June||07303157|||||||18705|0,216|-|47|0|908|0|1110|834|1,23|0,00|United States|Northern America|1979-2019|Computer Science Applications; Software||||1730004804|838284539
DG.O'21|Omaha, NE, USA||9|189–197|DG.O2021: The 22nd Annual International Conference on Digital Government Research|Anticipating the continues increase in quantity and consequently their importance, data are becoming a new currency. Overall purpose of this paper is to show the link between Big Open Linked Data (BOLD) and national budget planning. The methodology that follows is a qualitative case-study based approach leading to a comparative analysis of five cases. Research problems investigated are the commonalities and differences that may be identified in the handling of national budget data in developing countries, as well as best practices or potential ‘lessons learned’ from international cases of handling budget data as BOLD. In addition, the study investigates how Kosovo as a young developing country may benefit from the experiences of other countries. To create a framework of analysis, international cases are reviewed. Their comparison reveals that there are not that many commonalities among these developing countries in terms of issues and challenges regarding the use of BOLD in national budgeting. As it is mainly country specific, the approach used toward BOLD largely depends on the general landscape of each country. In comparison, although there is some progress made, Kosovo is still behind those countries in terms of applying BOLD.|10.1145/3463677.3463696|https://doi.org/10.1145/3463677.3463696|New York, NY, USA|Association for Computing Machinery|9781450384926|2021|BOLD in National Budget Planning – a Comparison of International Cases: BOLD in National Budget Planning|Geci, Mentor and CsAki, Csaba|inproceedings|10.1145/3463677.3463696|||||||||||||||||||||||||||||1732229621|42
C3S2E '16|Porto, Portugal|GIS, Big-data, spatio-temporal databases, performance, Benchmark, pattern-detection, experimentation, algorithms|4|119–122|Proceedings of the Ninth International C* Conference on Computer Science &amp; Software Engineering|The growing number of different models and approaches for Geographic Information Systems (GIS) brings high complexity when we want to develop new approaches and compare a new GIS algorithm. In order to test and compare different processing models and approaches, in a simple way, we identified the need of defining uniform testing methods, able to compare processing algorithms in terms of performance and accuracy regarding large image processing, algorithms for GIS pattern-detection.Taking into account, for instance, images collected during a done flight or a satellite, it is important to know the processing cost to extract data when applying different processing models and approaches, as well as their accuracy (compare execution time vs. extracted data quality). In this work, we propose a GIS Benchmark (GPII), a benchmark that allows evaluating different approaches to detect/extract selected features from a GIS dataset. Considering a given dataset (or two data-sets, from different years, of the same region), it provides linear methods to compare different performance parameters regarding GIS information, making possible to access the most relevant information in terms of features and processing efficiency. Moreover, our approach to test algorithms makes possible to change the data-set in order to support different purpose algorithms.|10.1145/2948992.2949009|https://doi.org/10.1145/2948992.2949009|New York, NY, USA|Association for Computing Machinery|9781450340755|2016|GPII: A Benchmark for Generic Purpose Image Information|Martins, Pedro and Cec\'{\i}lio, Jos\'{e} and Abbasi, Maryam and Furtado, Pedro|inproceedings|10.1145/2948992.2949009|||||||||||||||||||||||||||||1734014669|42
ICISS '18|Jeju, Republic of Korea|Multi-category words, Corpus, Tagging, Intelligent processing, Big data|5|107–111|Proceedings of the 2018 International Conference on Information Science and System|The application of modern Chinese multi-category words corpus is very wide. With the development of the Internet, data from the corpus is getting bigger and bigger during collection. The data gradually develops so big that the current relational database is difficult to deal with them. This article analyzes the important role of the big data technology in corpu|10.1145/3209914.3234639|https://doi.org/10.1145/3209914.3234639|New York, NY, USA|Association for Computing Machinery|9781450364218|2018|Application of Big Data and Intelligent Processing Technology in Modern Chinese Multi-Category Words Part of Speech Tagging Corpus|Song, Zhendong|inproceedings|10.1145/3209914.3234639|||||||||||||||||||||||||||||1734498582|42
||||1-4|CIBDA 2022; 3rd International Conference on Computer Information and Big Data Applications|With the rapid development of China's telecom industry, the business is more and more network resources being unwound, data growth speed faster and faster, data scale is becoming more and more large, then the data quality problem is increasingly serious, how to efficient use of existing resources, to provide a set of scientific and effective data management solutions, has become a network resources competitiveness indispensable important segment. This paper takes the application of data quality verification system of resource management platform as the main research object, discusses from system design, software architecture, database design, system function application and network resource related business, and puts forward a series of feasible schemes about data governance.||||||2022|Application analysis of data quality verification system based on resource management platform|Mei, Hong and Kang, Zhiwen and Li, Haiou and Liu, Yajun and Chen, Xin and Chen, Yanlei and Song, Guolin and Chao, Yuling and Jang, Wenjie and Cheng, Zhanyu and Xu, Jinpeng and Wu, Yi|inproceedings|9899144||March|||||||||||||||||||||||||||1735387148|42
ICSLT '19|Vienna, Austria|weak logic errors, Unsupervised data cleaning, machine learning, attribute correlation, minimum repair cost|7|45–51|Proceedings of the 5th International Conference on E-Society, e-Learning and e-Technologies|Referring to the supervised learning and unsupervised learning in machine learning, we divide the data cleaning processes into supervised and unsupervised two forms too, and then, we reclassify the data quality problems into canonicalization error, redundancy error, strong logic error and weak logic error according to the characteristics of unsupervised cleaning. For the weak logic errors, we propose a repair framework AC-Framework and an algorithm AC-Repair based on the attribute correlation. When repairing, we first establish a priority queue(PQ) for elements to be repaired according to the minimum cost idea and take the corresponding conflict-free data set(Icf) as a training set to learn the correlation among attributes. Then, we select the first element in PQ list as the candidate element to repair, and recompute the PQ list after one repair round to improve the efficiency. Finally, in order to prevent the algorithm from endless loops, we set a label flag to mark the repaired elements, in this way, every error element will be repaired at most once. In the experimental part, we compare the AC-Repair algorithm with the interpolation-based repair algorithm to verify its validity.|10.1145/3312714.3312717|https://doi.org/10.1145/3312714.3312717|New York, NY, USA|Association for Computing Machinery|9781450362351|2019|Application of Attribute Correlation in Unsupervised Data Cleaning|Li, Pei and Dai, Chaofan and Wang, Wenqian|inproceedings|10.1145/3312714.3312717|||||||||||||||||||||||||||||1736353107|42
||HIV, AIDS, Prevention, Treatment, Continuum, Data, Surveillance||807-815|||https://doi.org/10.1016/j.idc.2019.05.009|https://www.sciencedirect.com/science/article/pii/S0891552019300455||||2019|How Big Data Science Can Improve Linkage and Retention in Care|Aadia I. Rana and Michael J. Mugavero|article|RANA2019807|||Infectious Disease Clinics of North America|08915520|3|33||HIV|||||||||||||||||||||1738361926|1411618133
KDD '18|London, United Kingdom|motion picture industry, user ratings, box office predictions, logistic regression, gradient-boosted trees, recommender systems|10|655–664|Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining|Recent progress in machine learning and related fields like recommender systems open up new possibilities for data-driven approaches. One example is the prediction of a movie's box office revenue, which is highly relevant for optimizing production and marketing. We use individual recommendations and user-based forecast models in a system that forecasts revenue and additionally provides actionable insights for industry professionals. In contrast to most existing models that completely neglect user preferences, our approach allows us to model the most important source for movie success: moviegoer taste and behavior. We divide the problem into three distinct stages: (i) we use matrix factorization recommenders to model each user's taste, (ii) we then predict the individual consumption behavior, and (iii) eventually aggregate users to predict the box office result. We compare our approach to the current industry standard and show that the inclusion of user rating data reduces the error by a factor of 2x and outperforms recently published research.|10.1145/3219819.3219840|https://doi.org/10.1145/3219819.3219840|New York, NY, USA|Association for Computing Machinery|9781450355520|2018|Improving Box Office Result Predictions for Movies Using Consumer-Centric Models|"\"Ruhrl\"\"{a}nder, Rui Paulo and Boissier, Martin and Uflacker, Matthias\""|inproceedings|10.1145/3219819.3219840|||||||||||||||||||||||||||||1741582842|42
||Big data analytics, Risk management capability, Data quality, Alliance relationship stability||103680||Notwithstanding the potential of big data analytics technology for alliance management, there is a lack of understanding of how such digital technology influences alliance relationship stability (ARS). Drawing on the information technology-enabled organizational capabilities (IT-enabled OCs) perspective, this study empirically verifies that big data analytics promotes ARS and risk management capability. Moreover, market risk management capability (MRM) enhances ARS, and data quality moderates the relationship between big data analytics usage (BDU) and MRM. This research reveals the impact mechanism of BDU on the ARS. Implications for management and future research are presented as well.|https://doi.org/10.1016/j.im.2022.103680|https://www.sciencedirect.com/science/article/pii/S0378720622000891||||2022|How big data analytics enables the alliance relationship stability of contract farming in the age of digital transformation|Shunzhi Lin and Jiabao Lin and Feiyun Han and Xin (Robert) Luo|article|LIN2022103680|||Information & Management|03787206|6|59|||||12303|2,147|Q1|162|130|248|11385|2482|246|8,94|87,58|Netherlands|Western Europe|1977-2020|Information Systems (Q1); Information Systems and Management (Q1); Management Information Systems (Q1)||||1742690237|1945939487
||Underwater structures;Bridges;Road transportation;Employee welfare;Structural panels;Water quality;Rail transportation;Railway bridges;Pier underwater structure;Non destructive testing;Disease location;Machine learning||544-549|2022 IEEE International Conference on Electrical Engineering, Big Data and Algorithms (EEBDA)|The total number of railway bridges in China has exceeded twenty thousand. As an important part of the railway line, the existing railway bridge bears great live load, excessive concentrated force, obvious dynamic effect and high requirements for foundation stability. However, because the underwater structure of Railway Pier is constantly affected by many factors such as water scouring and ship collision during its service, it leads to a variety of diseases such as concrete peeling and exposed reinforcement. These diseases are often hidden in the water, which poses a serious threat to the structural safety of the bridge and the safety of people's lives and property. However, the traditional detection method of underwater structure of Railway Pier is affected by water flow, water quality, water depth and other environment, which has the pain points of high detection risk, low efficiency, poor detection data quality and missing disease location, so it is difficult to effectively detect underwater structure diseases; In this context, a multi-dimensional disease detection device is designed and developed for Wuhan Yangtze River Bridge (both highway and railway). The device includes a fixed module on water and an underwater detection module, which can realize safe and efficient detection under the condition of rapids and deep water, solve the problems that it is difficult for personnel and existing equipment to reach the structure to be tested, and the detection effect obtained by existing detection means is not ideal The detection information is not comprehensive, which is difficult to be used for key problems such as follow-up structural technical state evaluation.|10.1109/EEBDA53927.2022.9744889|||||2022|Multi dimensional disease intelligent detection device for underwater pier column structure through machine learning|Yang, Bikai and Bing, Han and Zhao, Haiyang and Gu, Tianshu and Cai, Tianxiao|inproceedings|9744889||Feb|||||||||||||||||||||||||||1743048656|42
ICEMIS '18|Istanbul, Turkey|Datasets, Classification, Feature Selection, Clustering, Intrusion Detection, Measures, Anomaly Detection|9||Proceedings of the Fourth International Conference on Engineering &amp; MIS 2018|Intrusion Detection is the most eminent fields in the network which can also be called as anomaly detection. Various methods used by early research tells that, the kind of measures used to detect the intrusion is not specified. Research has grown extensively in Anomaly intrusion detection by using different data mining techniques. Most researchers have not briefed on the kinds of distances measures used, the classification and feature selection techniques used in identifying intrusion detection. Intrusion detection is classified with problems as Outlier problems, Sparseness problem and Data Distribution. One of the important observations made is, High Dimensional Data Reduction is not performed, and conventional dataset is not used or maintained by any researchers. A survey is performed to identify the type of distance measures used and the type of datasets used in the early research. In this extended survey, the measures like Distance measure, pattern behaviors are used in identifying the Network Intrusion Detection. In this paper, we present the various methods used by authors to obtain feature selection methods. Also, the discussion is towards, Computation of High Dimensional Data, how to decide the Choice of Learning algorithm, Efficient Distance and similarity measures to identify the intrusion detection from different datasets.|10.1145/3234698.3234743|https://doi.org/10.1145/3234698.3234743|New York, NY, USA|Association for Computing Machinery|9781450363921|2018|An Extensive Survey on Intrusion Detection- Past, Present, Future|Nagaraja, Arun and Kumar, T. Satish|inproceedings|10.1145/3234698.3234743|45||||||||||||||||||||||||||||1748629444|42
||Big data, Condition assessment, Machine learning, Nondestructive testing, Pipeline, SWOT||101687||Pipelines carrying energy products play vital roles in economic wealth and public safety, but incidents continue occurring. Condition assessment of pipelines is essential to identify anomalies timely. Advanced sensing technologies obtain informative data for condition assessment, while data analysis by human has limited efficiency, accuracy, and reliability. Advances in machine learning offer exciting opportunities for automated condition assessment with minimum human intervention. This paper reviews machine learning approaches to detect, classify, locate, and quantify pipeline anomalies based on intelligent interpretation of routine operation data, nondestructive testing data, and computer vision data. Statistics and uncertainties of performance metrics of machine learning approaches are discussed. An analysis on strengths, weaknesses, opportunities, and threats (SWOT) is performed. Guides for practitioners to perform automated pipeline condition assessment are recommended. This review provide insights into the machine learning approaches for automated pipeline condition assessment. The SWOT analysis will support decision making in the pipeline industry.|https://doi.org/10.1016/j.aei.2022.101687|https://www.sciencedirect.com/science/article/pii/S1474034622001471||||2022|Review on automated condition assessment of pipelines with machine learning|Yiming Liu and Yi Bao|article|LIU2022101687|||Advanced Engineering Informatics|14740346||53|||||23640|1,107|Q1|81|146|295|8184|1973|289|6,41|56,05|United Kingdom|Western Europe|2002-2020|Artificial Intelligence (Q1); Information Systems (Q1)|4,432|5.603|0.00428|1752871562|876312848
||Data protection, Impact assessment, Data protection impact assessment, Human rights, Human rights impact assessment, Ethical impact assessment, Social impact assessment, General Data Protection Regulation||754-772||The use of algorithms in modern data processing techniques, as well as data-intensive technological trends, suggests the adoption of a broader view of the data protection impact assessment. This will force data controllers to go beyond the traditional focus on data quality and security, and consider the impact of data processing on fundamental rights and collective social and ethical values. Building on studies of the collective dimension of data protection, this article sets out to embed this new perspective in an assessment model centred on human rights (Human Rights, Ethical and Social Impact Assessment-HRESIA). This self-assessment model intends to overcome the limitations of the existing assessment models, which are either too closely focused on data processing or have an extent and granularity that make them too complicated to evaluate the consequences of a given use of data. In terms of architecture, the HRESIA has two main elements: a self-assessment questionnaire and an ad hoc expert committee. As a blueprint, this contribution focuses mainly on the nature of the proposed model, its architecture and its challenges; a more detailed description of the model and the content of the questionnaire will be discussed in a future publication drawing on the ongoing research.|https://doi.org/10.1016/j.clsr.2018.05.017|https://www.sciencedirect.com/science/article/pii/S0267364918302012||||2018|AI and Big Data: A blueprint for a human rights, social and ethical impact assessment|Alessandro Mantelero|article|MANTELERO2018754|||Computer Law & Security Review|02673649|4|34|||||28888|0,815|Q1|38|66|242|1245|707|223|3,39|18,86|United Kingdom|Western Europe|1985-2020|Business, Management and Accounting (miscellaneous) (Q1); Computer Networks and Communications (Q1); Law (Q1)||||1753967802|1769124433
ICEGOV 2020|Athens, Greece|document similarity, AI strategies, topic modelling, NLP, machine learning, Automated Text Analysis|12|100–111|Proceedings of the 13th International Conference on Theory and Practice of Electronic Governance|The primary goal of this paper is to explore how Natural Language Processing techniques (NLP) can assist in reviewing, understanding, and drawing conclusions from text datasets. We explore NLP techniques for the analysis and the extraction of useful information from the text of twelve national strategies on artificial intelligence (AI). For this purpose, we are using a set of machine learning algorithms in order to (a) extract the most significant keywords and summarize each strategy document, (b) discover and assign topics to each document, and (c) cluster the strategies based on their pair-wise similarity. Using the results of the analysis, we discuss the findings and highlight critical issues that emerge from the national strategies for artificial intelligence, such as the importance of the data ecosystem for the development of AI, the increasing considerations about ethical and safety issues, as well as the growing ambition of many countries to lead in the AI race. Utilizing the LDA topic model, we were able to reveal the distributions of thematic sub-topics among the strategic documents. The topic modelling distributions were then used along with other document similarity measures as an input for the clustering of the strategic documents into groups. The results revealed three clusters of countries with a visible differentiation between the strategies of China and Japan on the one hand and the Scandinavian strategies (plus the German and the Luxemburgish) one on the other. The former promote technology and innovation-driven development plans in order to integrate AI with the economy, while the latter share a common view regarding the role of the public sector both as a promoter and investor but also as a user and beneficiary of AI, and give a higher priority to the ethical &amp; safety issues that are connected to the development of AI.|10.1145/3428502.3428514|https://doi.org/10.1145/3428502.3428514|New York, NY, USA|Association for Computing Machinery|9781450376747|2020|What Do Governments Plan in the Field of Artificial Intelligence? Analysing National AI Strategies Using NLP|Papadopoulos, Theodoros and Charalabidis, Yannis|inproceedings|10.1145/3428502.3428514|||||||||||||||||||||||||||||1756203445|42
|||4|74–77||"\"In this position paper we argue that the availability of \"\"big\"\" monitoring data on Cyber-Physical Systems (CPS) is challenging the traditional CPS modeling approaches by violating their fundamental assumptions. However, big data alsobrings unique opportunities in its wake by enabling new modeling and analytics approaches as well as facilitating novel applications. We highlight a few key challenges andopportunities, and outline research directions for addressing them. To provide a proper context, we also summarize CPS modeling approaches, and discuss how modeling and analytics for CPS differs from general purpose IT systems.\""|10.1145/2627534.2627558|https://doi.org/10.1145/2627534.2627558|New York, NY, USA|Association for Computing Machinery||2014|Modeling and Analytics for Cyber-Physical Systems in the Age of Big Data|Sharma, Abhishek B. and Ivan\v{c}i\'{c}, Franjo and Niculescu-Mizil, Alexandru and Chen, Haifeng and Jiang, Guofei|article|10.1145/2627534.2627558||apr|SIGMETRICS Perform. Eval. Rev.|01635999|4|41|March 2014||||26742|0,223|Q3|80|72|323|565|337|301|1,04|7,85|United States|Northern America|1980, 1982, 1984, 1986-1989, 1994, 1996-2020|Computer Networks and Communications (Q3); Software (Q3); Hardware and Architecture (Q4)||||1757495225|302815259
||Big data analytics, Fault prediction, Shop floor, Scheduling||187-194||The current task scheduling mainly concerns the availability of machining resources, rather than the potential errors after scheduling. To minimise such errors in advance, this paper presents a big data analytics based fault prediction approach for shop floor scheduling. Within the context, machining tasks, machining resources, and machining processes are represented by data attributes. Based on the available data on the shop floor, the potential fault/error patterns, referring to machining errors, machine faults and maintenance states, are mined for unsuitable scheduling arrangements before machining as well as upcoming errors during machining. Comparing the data-represented tasks with the mined error patterns, their similarities or differences are calculated. Based on the calculated similarities, the fault probabilities of the scheduled tasks or the current machining tasks can be obtained, and they provide a reference of decision making for scheduling and rescheduling the tasks. By rescheduling high-risk tasks carefully, the potential errors can be avoided. In this paper, the architecture of the approach consisting of three steps in three levels is proposed. Furthermore, big data are considered in three levels, i.e. local data, local network data and cloud data. In order to implement this idea, several key techniques are illustrated in detail, e.g. data attribute, data cleansing, data integration of databases in different levels, and big data analytic algorithms. Finally, a simplified case study is described to show the prediction process of the proposed method.|https://doi.org/10.1016/j.jmsy.2017.03.008|https://www.sciencedirect.com/science/article/pii/S0278612517300389||||2017|Big data analytics based fault prediction for shop floor scheduling|Wei Ji and Lihui Wang|article|JI2017187|||Journal of Manufacturing Systems|02786125||43|||||14966|2,310|Q1|70|155|294|8906|2949|288|10,88|57,46|Netherlands|Western Europe|1982-2020|Control and Systems Engineering (Q1); Hardware and Architecture (Q1); Industrial and Manufacturing Engineering (Q1); Software (Q1)|5,413|8.633|0.00561|1757670556|619364890
||Industries;Technological innovation;Distributed databases;Blockchain;Data models;Internet;Safety;blockchain;data traceability;data quality;data security;data governance;energy Internet;huge data||45-49|2020 International Conference on Big Data & Artificial Intelligence & Software Engineering (ICBASE)|Energy Internet is a major innovation to deal with the environmental crisis and efficient energy management and use in the current society. The important condition to achieve this goal is to summarize, integrate, process and apply the data of various industries in the energy field, and then support the relevant management and decision-making. In this process, how to ensure the authenticity and credibility of data is one of the keys in the construction of energy Internet. Therefore, this paper will study the application scenarios of blockchain technology in data traceability. With the help of the natural characteristics of blockchain, such as decentralized, distributed storage, tamper proof, open and transparent, combined with relevant national standards and international theoretical models, based on the needs of energy Internet data integration and management, this paper will develop a data traceability method suitable for the energy industry, and build a covering energy Data life cycle model of Internet. Through the research of this paper, we can help all localities to establish data traceability mechanism in the energy Internet, to help users to accurately grasp where the data is created, what systems have been transferred, which users have carried out query and modification, and so on, so as to realize the monitoring and control of the whole process of data flow, which helps to improve the credibility of data, and also helps to ensure the safety and quality of data and promote the construction of energy Internet huge data application.|10.1109/ICBASE51474.2020.00017|||||2020|Research on data Traceability Method Based on blockchain Technology|Wei, Li and Dawei, Wang and Lixia, Wang|inproceedings|9403739||Oct|||||||||||||||||||||||||||1757866666|42
BCB '20|Virtual Event, USA|genomics, generative adversarial networks, machine learning, deep learning, data augmentation|6||Proceedings of the 11th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics|Although next generation sequencing technologies have made it possible to quickly generate a large collection of sequences, current genomic data still suffer from small data sizes, imbalances, and biases due to various factors including disease rareness, test affordability, and concerns about privacy and security. In order to address these limitations of genomic data, we develop a Population-scale Genomic Data Augmentation based on Conditional Generative Adversarial Networks (PG-cGAN) to enhance the amount and diversity of genomic data by transforming samples already in the data rather than collecting new samples. Both the generator and discriminator in the PG-CGAN are stacked with convolutional layers to capture the underlying population structure. Our results for augmenting genotypes in human leukocyte antigen (HLA) regions showed that PC-cGAN can generate new genotypes with similar population structure, variant frequency distributions and LD patterns. Since the input for PC-cGAN is the original genomic data without assumptions about prior knowledge, it can be extended to enrich many other types of biomedical data and beyond.|10.1145/3388440.3412475|https://doi.org/10.1145/3388440.3412475|New York, NY, USA|Association for Computing Machinery|9781450379649|2020|Population-Scale Genomic Data Augmentation Based on Conditional Generative Adversarial Networks|Chen, Junjie and Mowlaei, Mohammad Erfan and Shi, Xinghua|inproceedings|10.1145/3388440.3412475|26||||||||||||||||||||||||||||1759257816|42
MSM '15|Alexandria, Virginia|disease modeling, Monte-Carlo, publications, clinical trial, reference modeling, high performance computing|6|47–52|Proceedings of the Symposium on Modeling and Simulation in Medicine|Medical data is becoming increasingly available. Access to such data is generally restricted and researchers cannot access it easily. On the other hand, clinical trial data is freely available and published without restriction for access to the public at the summary level. With proper analysis, it is possible to extract valuable conclusions from such data. This paper will review new methods to look at such public data and will discuss possible future trends.|||San Diego, CA, USA|Society for Computer Simulation International|9781510801028|2015|Modeling Clinical Data from Publications|Barhak, Jacob|inproceedings|10.5555/2873003.2873011|||||||||||||||||||||||||||||1759810030|42
||Data access control, Data flows, Data quality, Data sharing, Information architecture, Information flows, Supply chain||147-161|The Digital Supply Chain|Advances in digitalization present new and emerging Supply Chain (SC) Information Architectures that rely on data and information as vital resources. While the importance of data and information in SCs has long been understood, there is a dearth of research or understanding about the effective governance, control, or management of data ecosystems at the SC level. This chapter examines data architectures through a navigation of the background of database management and data quality research of previous decades. The chapter unfolds the critical architectural elements around data and information sharing in the SC regarding the context, systems, and infrastructure. A review of various frameworks and conceptual models is presented on data and information in SCs, as well as access control policies. The critical importance of data quality and the management of data in the cyber-physical systems are highlighted. Policies for data sharing agreements (DSAs) and access control are discussed and the importance of effective governance in the distributed environments of digitally enabled SCs is emphasized. We extend the concept of data sharing agreements to capture the interplay between the various SC stakeholders around data use. Research gaps and needs relevant to new and emerging SC data and information ecosystems are highlighted.|https://doi.org/10.1016/B978-0-323-91614-1.00009-5|https://www.sciencedirect.com/science/article/pii/B9780323916141000095||Elsevier|978-0-323-91614-1|2022|Chapter 9 - Digital architectures: frameworks for supply chain data and information governance|Konstantina Spanaki and Erisa Karafili and Stella Despoudi|incollection|SPANAKI2022147||||||||||Bart L. MacCarthy and Dmitry Ivanov|||||||||||||||||||1759911763|42
||Artificial intelligence, Industry manufacturing process, Bayesian network, Smart manufacturing, Neural network||||This research takes a case study approach to show the development of a diverse adoption and product strategy distinct from the core manufacturing industry process. It explains the development status in all aspects of smart manufacturing, via the example of ceramic circuit board manufacturing and electronic assembly, and outlines future smart manufacturing plans and processes. This research proposed two experiments using Artificial Intelligence and deep learning are used to demonstrate the problems and solutions regarding methods in manufacturing and factory facilities, respectively. In the first experiment, a Bayesian network inference is used to find the cause of the problem of metal residues between electronic circuits through key process and quality correlations. In the second experiment, a Convolutional Neural Network (CNN) is used to identify false defects that were over-inspected during Automatic Optical Inspection. This improves the manufacturing process by enhancing yield rate and reducing cost. The contributions of the study in circuit board production. Smart manufacturing, with the application of a Bayesian network to an IoT setup, has addressed the problem of residue and redundant conductors on the edge of the ceramic circuit board pattern, and has improved and prevented leakage and high-frequency interference. CNN and deep learning were used to improve the accuracy of the AOI system, reduce the current manual review ratio, save labour costs and provide defect classification as a reference for pre-process improvement.|10.1145/3529098|https://doi.org/10.1145/3529098|New York, NY, USA|Association for Computing Machinery||2022|The Core Industry Manufacturing Process of Electronics Assembly Based on Smart Manufacturing|Chen, Rongli and Chen, Xiaozhong and Wang, Lei and Li, Jianxin|article|10.1145/3529098||aug|ACM Trans. Manage. Inf. Syst.|2158656X||||Just Accepted|||||||||||||||||||||1761693034|2097885649
||Automobiles;Safety;Big Data;Government;Personnel;Data mining;automobile recall;data mining;information cluster||541-545|2019 2nd International Conference on Safety Produce Informatization (IICSPI)|According to multisource quality safety data of defective automobile products, key quality safety factors of defective automobile products are extracted, a defect information indicator system for automobile products is systematically constructed and a correlated graph is established between quality safety factors. Based on the optimization and correlation of the quality safety factor indicator system, Big Data technology is used to design a data structure for multisource quality safety information cluster, develop a data platform for the defect information analysis of automobile products and achieve information clustering and correlation analysis based on multisource quality safety data, providing technical support for the recall management of defective automobile products.|10.1109/IICSPI48186.2019.9095877|||||2019|Application of Data Mining Technology in the Recall of Defective Automobile Products in China ——A Typical Case of the Construction of Digital China|Li, Song and Ning, Sun and Yezhou, Yao and Jingjing, Tian and Wenxue, Zhang and Liang, Chi|inproceedings|9095877||Nov|||||||||||||||||||||||||||1763493359|42
||Big data;Data integration;Accuracy;Distributed databases;Data analysis;Business;Big Data;Data Quality;pre-processing||191-198|2015 IEEE International Congress on Big Data|With the abundance of raw data generated from various sources, Big Data has become a preeminent approach in acquiring, processing, and analyzing large amounts of heterogeneous data to derive valuable evidences. The size, speed, and formats in which data is generated and processed affect the overall quality of information. Therefore, Quality of Big Data (QBD) has become an important factor to ensure that the quality of data is maintained at all Big data processing phases. This paper addresses the QBD at the pre-processing phase, which includes sub-processes like cleansing, integration, filtering, and normalization. We propose a QBD model incorporating processes to support Data quality profile selection and adaptation. In addition, it tracks and registers on a data provenance repository the effect of every data transformation happened in the pre-processing phase. We evaluate the data quality selection module using large EEG dataset. The obtained results illustrate the importance of addressing QBD at an early phase of Big Data processing lifecycle since it significantly save on costs and perform accurate data analysis.|10.1109/BigDataCongress.2015.35|||||2015|Big Data Pre-processing: A Quality Framework|Taleb, Ikbal and Dssouli, Rachida and Serhani, Mohamed Adel|inproceedings|7207219||June||23797703|||||||||||||||||||||||||1763511670|1226158248
||Artificial intelligence (AI), Machine learning (ML), Artificial neural network (ANN), Convolution neural network (CNN), Nephrology||54-61||Artificial intelligence (AI) has been applied widely in almost every area of our daily lives, due to the growth of computing power, advances in methods and techniques, and the explosion of data, it also plays a critical role in academic disciplines, medicine is not an exception. AI can augment the intelligence of clinicians in diagnosis, prognosis, and treatment decisions.Kidney disease causes great economic burden worldwide, with both acute kidney injury and chronic kidney disease bringing about high morbidity and mortality. Outstanding challenges in nephrology may be addressed by leveraging big data and AI.In this review, we summarized advances in machine learning (ML), artificial neural network (ANN), convolution neural network (CNN) and deep learning (DL), with a special focus on acute kidney injury (AKI), chronic kidney disease (CKD), end-stage renal disease (ESRD), dialysis, kidney transplantation and nephropathology. AI may not be anticipated to replace the nephrologists’ medical decision-making for now, but instead assisting them in providing optimal personalized therapy for patients.|https://doi.org/10.1016/j.ceh.2021.11.003|https://www.sciencedirect.com/science/article/pii/S2588914121000083||||2021|Application of artificial intelligence in renal disease|Lijing Yao and Hengyuan Zhang and Mengqin Zhang and Xing Chen and Jun Zhang and Jiyi Huang and Lu Zhang|article|YAO202154|||Clinical eHealth|25889141||4|||||||||||||||||||||||1764347417|1497334845
ICEGOV '18|Galway, Ireland|social benefits, profiling, Genetic Algorithm, Data analytics|10|550–559|Proceedings of the 11th International Conference on Theory and Practice of Electronic Governance|In this paper we present a data-driven profiling approach that we have adopted and implemented for a municipality. Our aim was to make profiles transparent and meaningful for citizens, policymakers and authorities so that they can validate, scrutinize and challenge the profiles. Our approach relies on a Genetic Algorithm (GA) that searches for useful and human understandable group profiles. Furthermore, we discuss some of the challenges encountered, show a selection of the profiles that were found by the GA, and discuss the necessity and a number of ways of validating these profiles in accordance with, e.g., privacy and non-discrimination laws and guidelines before using them in practice.|10.1145/3209415.3209481|https://doi.org/10.1145/3209415.3209481|New York, NY, USA|Association for Computing Machinery|9781450354219|2018|Exploiting Data Analytics for Social Services: On Searching for Profiles of Unlawful Use of Social Benefits|Netten, Niels and Bargh, Mortaza S. and Choenni, Sunil|inproceedings|10.1145/3209415.3209481|||||||||||||||||||||||||||||1765816235|42
ICBDC '19|Guangzhou, China|large-scale complex systems, evaluation, big data, effectiveness|5|72–76|Proceedings of the 4th International Conference on Big Data and Computing|With the advent of the information age, big data technology came into being. The wide application of big data brings new opportunities and challenges to the construction of national defense and military information. Under the background of information-based joint operations characterized by large complex systems, how to scientifically and rationally plan the construction of large complex systems, and maximize the effectiveness of the complex system has become a key concern for system construction decision makers and researchers. This paper combines the application of big data in the construction of large complex systems, and focuses on the evaluation of the effectiveness of large complex systems based on big data, which can be used for reference by relevant researchers.|10.1145/3335484.3335545|https://doi.org/10.1145/3335484.3335545|New York, NY, USA|Association for Computing Machinery|9781450362788|2019|Evaluation of Large-Scale Complex Systems Effectiveness Based on Big Data|Zhi-peng, Sun and Gui-ming, Chen and Hui, Zhang|inproceedings|10.1145/3335484.3335545|||||||||||||||||||||||||||||1772911328|42
||deep learning, strongly and nearly similar entities, block processing, Entity blocking and matching, crowdsourcing, batch and incremental entity resolution workflows|42|||One of the most critical tasks for improving data quality and increasing the reliability of data analytics is Entity Resolution (ER), which aims to identify different descriptions that refer to the same real-world entity. Despite several decades of research, ER remains a challenging problem. In this survey, we highlight the novel aspects of resolving Big Data entities when we should satisfy more than one of the Big Data characteristics simultaneously (i.e., Volume and Velocity with Variety). We present the basic concepts, processing steps, and execution strategies that have been proposed by database, semantic Web, and machine learning communities in order to cope with the loose structuredness, extreme diversity, high speed, and large scale of entity descriptions used by real-world applications. We provide an end-to-end view of ER workflows&nbsp;for&nbsp;Big Data, critically review the pros and cons of existing methods, and conclude with the main open research&nbsp;directions.|10.1145/3418896|https://doi.org/10.1145/3418896|New York, NY, USA|Association for Computing Machinery||2020|An Overview of End-to-End Entity Resolution for Big Data|Christophides, Vassilis and Efthymiou, Vasilis and Palpanas, Themis and Papadakis, George and Stefanidis, Kostas|article|10.1145/3418896|127|dec|ACM Comput. Surv.|03600300|6|53|November 2021||||23038|2,079|Q1|163|112|365|15859|6978|365|19,16|141,60|United States|Northern America|1969-2020|Computer Science (miscellaneous) (Q1); Theoretical Computer Science (Q1)|10,790|10.282|0.01438|1773802447|1517405264
Advances in Child Development and Behavior||Big data, Wearable technology, Algorithm bias, Automatic measurement, Language, Audio recording, Children||1-36|New Methods and Approaches for Studying Child Development|Big data are everywhere. In this chapter, we focus on one source: long-form, child-centered recordings collected using wearable technologies. Because these recordings are simultaneously unobtrusive and encompassing, they may be a breakthrough technology for clinicians and researchers from several diverse fields. We demonstrate this possibility by outlining three applications for the recordings—clinical treatment, large-scale interventions, and language documentation—where we see the greatest potential. We argue that incorporating these recordings into basic and applied research will result in more equitable treatment of patients, more reliable measurements of the effects of interventions on real-world behavior, and deeper scientific insights with less observational bias. We conclude by outlining a proposal for a semistructured online platform where vast numbers of long-form recordings could be hosted and more representative, less biased algorithms could be trained.|https://doi.org/10.1016/bs.acdb.2021.12.001|https://www.sciencedirect.com/science/article/pii/S0065240721000434||JAI||2022|Chapter One - Using big data from long-form recordings to study development and optimize societal impact|Margaret Cychosz and Alejandrina Cristia|incollection|CYCHOSZ20221||||00652407||62||||Rick O. Gilmore and Jeffrey J. Lockman|29439|0,767|Q2|41|18|55|1682|106|8|1,81|93,44|United States|Northern America|1964-1965, 1967, 1969-1976, 1978-1980, 1982, 1984-1985, 1987, 1989, 1991, 1993-1994, 1996, 1999, 2001-2020|Developmental and Educational Psychology (Q2); Pediatrics, Perinatology and Child Health (Q2); Behavioral Neuroscience (Q3)|1,055|2.182|0.00113|1774814820|1000737129
WIMS '17|Amantea, Italy|social networks, graph clustering, vote, majority|10||Proceedings of the 7th International Conference on Web Intelligence, Mining and Semantics|We describe a new method of voting system in social networks environment1. We suggest a sequence of continuous support via a social network after electing representatives or exemplars in the network that is different from the typical majority voting. In other words, this paper suggests the method of elected representatives using network clustering approach to counts voting. On the network structure, sending messages from each node reflects the influence or importance to the representative and that can be readjusted and send back to each node. Where the representatives can be clustered within which the selectivity can be decided through the graph edges. In the experiment our algorithm outperformed conventional approaches in social network synthetic dataset as well as real dataset.|10.1145/3102254.3102268|https://doi.org/10.1145/3102254.3102268|New York, NY, USA|Association for Computing Machinery|9781450352253|2017|An Innovative Majority Voting Mechanism in Interactive Social Network Clustering|Lee, Rich C. and Cuzzocrea, Alfredo and Lee, Wookey and Leung, Carson K.|inproceedings|10.1145/3102254.3102268|14||||||||||||||||||||||||||||1778316872|42
|||7|56–62||A view from computational journalism.|10.1145/2844110|https://doi.org/10.1145/2844110|New York, NY, USA|Association for Computing Machinery||2016|Accountability in Algorithmic Decision Making|Diakopoulos, Nicholas|article|10.1145/2844110||jan|Commun. ACM|00010782|2|59|February 2016||||||||||||||||||||||1778450926|647144465
||Genomics;Bioinformatics;Big Data;Data integrity;Diseases;Data models;Conceptual Modelling;Data Quality;Big Data;Smart Data;Genomics||1-11|2018 12th International Conference on Research Challenges in Information Science (RCIS)|During the last two decades, data generated by Next Generation Sequencing Technologies have revolutionized our understanding of human biology and improved the study on how changes (variations) in the DNA are involved in the risk of suffering a certain disease. A huge amount of genomic data is publicly available and frequently used by the research community in order to extract meaningful and reliable gene-disease relationships. However, management of this exponential growth of data has become a challenge for biologists; under such a big data problem perspective, they are forced to delve into a lake of complex data spread in over thousand heterogeneous repositories, represented in multiple formats and with different levels of quality; but when data are used to solve a concrete problem only a small part of that “data lake” is really significant; this is what we call the “smart” data perspective. Using conceptual models and the principles of data quality management, adapted to the genomic domain, we propose a systematic approach to move from a big data to a smart data perspective. The aim of this approach is to populate an Information System with genomic data which must be accessible, informative and actionable enough to extract valuable knowledge.|10.1109/RCIS.2018.8406658|||||2018|From big data to smart data: A genomic information systems perspective|Palacio, Ana León and López, Óscar Pastor|inproceedings|8406658||May||21511357|||||||20300195007|0,174|-|19|0|150|0|116|146|0,81|0,00|United States|Northern America|2011, 2012, 2013, 2014, 2015|Computer Science Applications; Information Systems; Software||||1782474872|1214021277
||Big data;Metadata;Measurement;Quality assessment;Quality of service;Unified modeling language;Big Data;Quality assessment;Metadata;Quality metrics;quality Metadata;Quality of process;Hybrid quality assessment||418-425|2016 IEEE International Congress on Big Data (BigData Congress)|While the potential benefits of Big Data adoption are significant, and some initial successes have already been realized, there remain many research and technical challenges that must be addressed to fully realize this potential. The Big Data processing, storage and analytics, of course, are major challenges that are most easily recognized. However, there are additional challenges related for instance to Big Data collection, integration, and quality enforcement. This paper proposes a hybrid approach to Big Data quality evaluation across the Big Data value chain. It consists of assessing first the quality of Big Data itself, which involve processes such as cleansing, filtering and approximation. Then, assessing the quality of process handling this Big Data, which involve for example processing and analytics process. We conduct a set of experiments to evaluate Quality of Data prior and after its pre-processing, and the Quality of the pre-processing and processing on a large dataset. Quality metrics have been measured to access three Big Data quality dimensions: accuracy, completeness, and consistency. The results proved that combination of data-driven and process-driven quality evaluation lead to improved quality enforcement across the Big Data value chain. Hence, we recorded high prediction accuracy and low processing time after we evaluate 6 well-known classification algorithms as part of processing and analytics phase of Big Data value chain.|10.1109/BigDataCongress.2016.65|||||2016|An Hybrid Approach to Quality Evaluation across Big Data Value Chain|Serhani, Mohamed Adel and El Kassabi, Hadeel T. and Taleb, Ikbal and Nujum, Alramzana|inproceedings|7584971||June|||||||||||||||||||||||||||1786946816|42
dg.o '16|Shanghai, China|Open data, Readiness assessment, Open government data|7|97–103|Proceedings of the 17th International Digital Government Research Conference on Digital Government Research|More and more cities in China are implementing various open government data initiatives for improving their governance. Little research, however, has been done in evaluating the readiness of individual governments in pursuing such initiatives. This paper presents a case study of the readiness assessment on the adoption of open data programs in Shenzhen based on the open data readiness assessment framework of the World Bank. The result shows that there are several issues including developing an action plan, providing privacy and ownership solutions, designating a unified administration, and implementing consistent data management policies and standards that need to be adequately addressed for the effective adoption of the open data program in the city.|10.1145/2912160.2912179|https://doi.org/10.1145/2912160.2912179|New York, NY, USA|Association for Computing Machinery|9781450343398|2016|Readiness Assessment of Open Government Data Programs: A Case of Shenzhen|Hu, Yanhua and Bai, Xianyang and Sun, Shuyang|inproceedings|10.1145/2912160.2912179|||||||||||||||||||||||||||||1788587103|42
||management, multimedia, IoMT, smart cities, machine learning|29|||Integration of embedded multimedia devices with powerful computing platforms, e.g., machine learning platforms, helps to build smart cities and transforms the concept of Internet of Things into Internet of Multimedia Things (IoMT). To provide different services to the residents of smart cities, the IoMT technology generates big multimedia data. The management of big multimedia data is a challenging task for IoMT technology. Without proper management, it is hard to maintain consistency, reusability, and reconcilability of generated big multimedia data in smart cities. Various machine learning techniques can be used for automatic classification of raw multimedia data and to allow machines to learn features and perform specific tasks. In this survey, we focus on various machine learning platforms that can be used to process and manage big multimedia data generated by different applications in smart cities. We also highlight various limitations and research challenges that need to be considered when processing big multimedia data in real-time.|10.1145/3323334|https://doi.org/10.1145/3323334|New York, NY, USA|Association for Computing Machinery||2019|A Survey on Big Multimedia Data Processing and Management in Smart Cities|Usman, Muhammad and Jan, Mian Ahmad and He, Xiangjian and Chen, Jinjun|article|10.1145/3323334|54|jun|ACM Comput. Surv.|03600300|3|52|May 2020||||23038|2,079|Q1|163|112|365|15859|6978|365|19,16|141,60|United States|Northern America|1969-2020|Computer Science (miscellaneous) (Q1); Theoretical Computer Science (Q1)|10,790|10.282|0.01438|1789733305|1517405264
|||2||||10.1145/3143313|https://doi.org/10.1145/3143313|New York, NY, USA|Association for Computing Machinery||2017|Editor-in-Chief (January 2014-May 2017) Farewell Report|Raschid, Louiqa|article|10.1145/3143313|7|oct|J. Data and Information Quality|19361955|2|9|June 2017||||||||||||||||||||||1790234451|833754770
||Big data, Big data availability, Big data prioritization, Supply chain management, Performance||100470||Given exponential growth in the size of big data, its multi-channel sources and variability in quality that create challenges concerning cost-effective use, firms have invested significantly in databases and analytical tools to inform decision-making. In this regard, one means to avoid the costs associated with producing less than insightful reports and negative effects on performance through wasted resources is prioritizing data in terms of relevance and quality. The aim of this study is to investigate this approach by developing and testing a scale to evaluate Big Data Availability and the role of Big Data Prioritization for more effective use of big data in decision-making and performance. Focusing on the context of supply chain management (SCM), we validate this scale through a survey involving 84 managers. Findings support a positive association between Big Data Availability and its use in SCM decision-making, and suggest that Big Data Prioritization, as conceptualized in the study, has a positive impact on the use of big data in SCM decision-making and SCM performance. Through developing a scale to evaluate association between Big Data Availability and use in SCM decision-making, we make an empirical contribution to value generation from big data.|https://doi.org/10.1016/j.accinf.2020.100470|https://www.sciencedirect.com/science/article/pii/S1467089520300385||||2020|Big data prioritization in SCM decision-making: Its role and performance implications|Carla Wilkin and Aldónio Ferreira and Kristian Rotaru and Luigi Red Gaerlan|article|WILKIN2020100470|||International Journal of Accounting Information Systems|14670895||38||2019 UW CISA Symposium|||29806|0,897|Q1|53|21|51|1375|290|50|5,56|65,48|United States|Northern America|2000-2020|Finance (Q1); Information Systems and Management (Q1); Management Information Systems (Q1); Accounting (Q2)|1,009|4.400|5.6E-4|1791834694|818633161
dg.o '17|Staten Island, NY, USA|Data-Driven Storytelling, Data-Driven Journalism, Open Data, Journalism, YDS Platform, Usable Open Data Platform|6|48–53|Proceedings of the 18th Annual International Conference on Digital Government Research|1Research into Data-Driven Storytelling using Open Data has led to considerable discussion into many possible futures for storytelling and journalism in a Data-Driven world, in particular, into the Open Data directives framed by various governments across the globe as a means of facilitating governments, transparency enabled citizens and journalists to get more insights into government actions and enable deeper and easier monitoring of governments' work. While progress in the development of Open Data platforms (usually funded by national and local governments) has been significant, it is only now that we are beginning to see the emergence of more practical and more applied use of Open Data platforms. Previous works have highlighted the potential for storytelling using Open Data as a source of information for journalistic stories. Nevertheless, there is a paucity of studies into Open Data platform affordances to support Data-Driven Storytelling. In this paper, we elaborate on existing Open Data platforms in terms of support for storytelling and analyse feedback from stakeholder focus groups, to discover what methods and tools can introduce or facilitate the storytelling capabilities of Open Data platforms.|10.1145/3085228.3085283|https://doi.org/10.1145/3085228.3085283|New York, NY, USA|Association for Computing Machinery|9781450353175|2017|Extending Open Data Platforms with Storytelling Features|Brolch\'{a}in, Niall \'{O} and Porwol, Lukasz and Ojo, Adegboyega and Wagner, Tilman and Lopez, Eva Tamara and Karstens, Eric|inproceedings|10.1145/3085228.3085283|||||||||||||||||||||||||||||1793881422|42
||Currencies;Maintenance engineering;Cleaning;Urban areas;Remuneration;Databases;Companies;Data cleaning;data quality management;currency determining;temporal data repairing||1288-1302||Data quality plays a key role in big data management today. With the explosive growth of data from a variety of sources, the quality of data is faced with multiple problems. Motivated by this, we study the multiple data cleaning on incompleteness and inconsistency with currency reasoning and determination in this paper. We introduce a 4-step framework, named ${\sf Imp3C}$Imp3C, for errors detection and quality improvement in incomplete and inconsistent data without timestamps. We achieve an integrated currency determining method to compute the currency orders among tuples, according to currency constraints. Thus, the inconsistent data and missing values are repaired effectively considering the temporal impact. For both effectiveness and efficiency consideration, we carry out inconsistency repair ahead of incompleteness repair. A currency-related consistency distance metric is defined to measure the similarity between dirty tuples and clean ones more accurately. In addition, currency orders are treated as an important feature in the missing imputation training process. The solution algorithms are introduced in detail with case studies. A thorough experiment on three real-life datasets verifies our method ${\sf Imp3C}$Imp3C improves the performance of data repairing with multiple quality problems. ${\sf Imp3C}$Imp3C outperforms the existing advanced methods, especially in the datasets with complex currency orders.|10.1109/TKDE.2020.2992456|||||2022|Leveraging Currency for Repairing Inconsistent and Incomplete Data|Ding, Xiaoou and Wang, Hongzhi and Su, Jiaxuan and Wang, Muxian and Li, Jianzhong and Gao, Hong|article|9086142||March|IEEE Transactions on Knowledge and Data Engineering|15582191|3|34|||||||||||||||||||||||1794300064|1598944404
IIWeb '12|Scottsdale, Arizona, USA|data-intensive computing, semistructured data, cloud computing, hyracks, ASTERIX|4||Proceedings of the Ninth International Workshop on Information Integration on the Web|"\"A growing wealth of digital information is being generated on a daily basis in social networks, blogs, online communities, etc. Organizations and researchers in a wide variety of domains recognize that there is tremendous value and insight to be gained by warehousing this emerging data and making it available for querying, analysis, and other purposes. This new breed of \"\"Big Data\"\" applications poses challenging requirements against data management platforms in terms of scalability, flexibility, manageability, and analysis capabilities. At UC Irvine, we are building a next-generation database system, called ASTERIX, in response to these trends. We present ongoing work that approaches the following questions: How does data get into the system? What primitives should we provide to better cope with dirty/noisy data? How can we support efficient data analysis on spatial data? Using real examples, we show the capabilities of ASTERIX for ingesting data via feeds, supporting set-similarity predicates for fuzzy matching, and answering spatial aggregation queries.\""|10.1145/2331801.2331803|https://doi.org/10.1145/2331801.2331803|New York, NY, USA|Association for Computing Machinery|9781450312394|2012|ASTERIX: Scalable Warehouse-Style Web Data Integration|Alsubaiee, Sattam and Behm, Alexander and Grover, Raman and Vernica, Rares and Borkar, Vinayak and Carey, Michael J. and Li, Chen|inproceedings|10.1145/2331801.2331803|2||||||||||||||||||||||||||||1800122967|42
ICEMC '22|Seoul, Republic of Korea|Accountants, Challenges, Industrial revolution 4.0|8|39–46|Proceedings of the 2022 International Conference on E-Business and Mobile Commerce|In the current industrial era, namely 4.0, it greatly facilitates the industrial world and human work and brings changes in human work adjustments. Where there are many changes in human life, for example in communication and data processing, both for individuals and for companies [1]. The characteristic of the industrial revolution 4.0 is the emergence of many applied technologies, such as the internet of things, cloud computing, Big Data, and artificial intelligence which as a whole can change business models and production patterns, especially in various industrial sectors [4]. In the current era of the industrial revolution, it is a challenge that is not easy and difficult for accountants as there is a lot of information technology present because financial transactions do not only use cash, but also use digital money [1]. The research objectives in this paper are to determine the role of accountants in the era of the industrial revolution 4.0 and to find out how accountants challenge the industrial revolution 4.0. The data in this study were obtained from library sources such as collecting journals, websites, and related party documents, and sources from other media that can support the completeness of research data so that this research can run correctly and according to reality.|10.1145/3543106.3543113|https://doi.org/10.1145/3543106.3543113|New York, NY, USA|Association for Computing Machinery|9781450397162|2022|Challenges of the Accounting Profession in the Era of the Industrial Revolution 4.0|Meiryani, Meiryani and Aprilia, Kanaya Regina and Warganegara, Dezie Leonarda and Yanti, Yanti|inproceedings|10.1145/3543106.3543113|||||||||||||||||||||||||||||1803224914|42
||Storytelling, Big data analytics, Smart service, Customer reference, Customer-supplier relationships||122-134||The emergence of digitally connected products and big data analytics (BDA) in industrial marketing has attracted academic and managerial interest in smart services. However, suppliers' provision of smart services and customers' adoption of these services have received scarce attention in the literature, demonstrating the need to address the changing nature of customer-supplier interactions in the digital era. Responding to prior research calls, this study utilizes ethnographic research and a storytelling lens to advance our knowledge of how stories and BDA can enhance customers' attitudes toward suppliers' smart services, their behavioral intentions and their actual adoption of smart services. The study's findings demonstrate that storytelling is a collective sensemaking and sensegiving process that occurs in interactions between customers and suppliers in which both parties contribute to the story development. The use of BDA in storytelling enhances customer sensemaking of smart services by highlighting the business value extracted from the digitized data of a reference customer. By synthesizing insights from servitization, storytelling, BDA and the customer reference literature, this study offers managers practical guidance regarding how to increase smart service sales. An example of a story used to facilitate customer adoption of a supplier's smart services in the manufacturing sector is provided.|https://doi.org/10.1016/j.indmarman.2019.12.004|https://www.sciencedirect.com/science/article/pii/S0019850118303353||||2020|Telling stories that sell: The role of storytelling and big data analytics in smart service sales|Valeriia Boldosova|article|BOLDOSOVA2020122|||Industrial Marketing Management|00198501||86|||||22792|2,022|Q1|136|314|465|27989|3787|450|6,86|89,14|United States|Northern America|1971-2020|Marketing (Q1)|16,291|6.960|0.00901|1803232010|223518748
DataEd '22|Philadelphia, PA, USA||6|38–43|1st International Workshop on Data Systems Education|In the Spring of 2021, we launched a pilot edition of a new Data Engineering course at Berkeley, targeted at our burgeoning Data Science major. We discuss aspects of the design of our first offering of the course, focusing on fluency of data models, languages and transformation tasks.|10.1145/3531072.3535324|https://doi.org/10.1145/3531072.3535324|New York, NY, USA|Association for Computing Machinery|9781450393508|2022|Piloting Data Engineering at Berkeley|Hellerstein, Joseph M. and Parameswaran, Aditya G.|inproceedings|10.1145/3531072.3535324|||||||||||||||||||||||||||||1803279887|42
||Fog computing, Optimization, Sensors, Data quality||101953||Our work is motivated by the fact that there is an increasing need to perform complex analytics jobs over streaming data as close to the edge devices as possible and, in parallel, it is important that data quality is considered as an optimization objective along with performance metrics. In this work, we develop a solution that trades latency for an increased fraction of incoming data, for which data quality-related measurements and operations are performed, in jobs running over geo-distributed heterogeneous and constrained resources. Our solution is hybrid: on the one hand, we perform search heuristics over locally optimal partial solutions to yield an enhanced global solution regarding task allocations; on the other hand, we employ a spring relaxation algorithm to avoid unnecessarily increased degree of partitioned parallelism. Through thorough experiments, we show that we can improve upon state-of-the-art solutions in terms of our objective function that combines latency and extent of quality checks by up to 2.56X. Moreover, we implement our solution within Apache Storm, and we perform experiments in an emulated setting. The results show that we can reduce the latency in 86.9% of the cases examined, while latency is up to 8 times lower compared to the built-in Storm scheduler, with the average latency reduction being 52.5%.|https://doi.org/10.1016/j.is.2021.101953|https://www.sciencedirect.com/science/article/pii/S0306437921001496||||2022|EQUALITY: Quality-aware intensive analytics on the edge|Anna-Valentini Michailidou and Anastasios Gounaris and Moysis Symeonides and Demetris Trihinas|article|MICHAILIDOU2022101953|||Information Systems|03064379||105|||||12305|0,547|Q2|85|100|271|4739|1027|255|3,39|47,39|United Kingdom|Western Europe|1975-2021|Hardware and Architecture (Q2); Information Systems (Q2); Software (Q2)|2,604|2.309|0.00331|1803610976|303930735
CIKM '17|Singapore, Singapore|user mobility, spatial task assignment, spatial crowdsourcing|10|297–306|Proceedings of the 2017 ACM on Conference on Information and Knowledge Management|With the proliferation of GPS-enabled smart devices and increased availability of wireless network, spatial crowdsourcing (SC) has been recently proposed as a framework to automatically request workers (i.e., smart device carriers) to perform location-sensitive tasks (e.g., taking scenic photos, reporting events). In this paper we study a destination-aware task assignment problem that concerns the optimal strategy of assigning each task to proper worker such that the total number of completed tasks can be maximized whilst all workers can reach their destinations before deadlines after performing assigned tasks. Finding the global optimal assignment turns out to be an intractable problem since it does not imply optimal assignment for individual worker. Observing that the task assignment dependency only exists amongst subsets of workers, we utilize tree-decomposition technique to separate workers into independent clusters and develop an efficient depth-first search algorithm with progressive bounds to prune non-promising assignments. Our empirical studies demonstrate that our proposed technique is quite effective and settle the problem nicely.|10.1145/3132847.3132894|https://doi.org/10.1145/3132847.3132894|New York, NY, USA|Association for Computing Machinery|9781450349185|2017|Destination-Aware Task Assignment in Spatial Crowdsourcing|Zhao, Yan and Li, Yang and Wang, Yu and Su, Han and Zheng, Kai|inproceedings|10.1145/3132847.3132894|||||||||||||||||||||||||||||1803981456|42
BigSpatial '14|Dallas, Texas|MapReduce, data warehouse, database, spatial analytics, GIS|4|11–14|Proceedings of the 3rd ACM SIGSPATIAL International Workshop on Analytics for Big Geospatial Data|The growth of spatial big data has been explosive thanks to cost-effective and ubiquitous positioning technologies, and the generation of data from multiple sources in multi-forms. Such emerging spatial data has high potential to create new insights and values for our life through spatial analytics. However, spatial data analytics faces two major challenges. First, spatial data is both data-and compute-intensive due to the massive amounts of data and the multi-dimensional nature, which requires high performance spatial computing infrastructure and methods. Second, spatial big data sources are often isolated, for example, OpenStreetMap, census data and Twitter tweets are independent data sources. This leads to incompleteness of information and sometimes limited data accuracy, thus limited values from the data. Integrating spatial big data analytics by consolidating multiple data sources provides significant potential for data quality improvement in terms of completeness and accuracy, and much increased values derived from the data. In this paper, we present our vision of a high performance integrated spatial big data analytics framework. We provide a scalable spatial query based data integration engine with MapReduce, and demonstrate integrated spatial data analytics through a few use cases in our preliminary work. We then present our future plan on integrated spatial big data analytics for improving public health research and applications.|10.1145/2676536.2676538|https://doi.org/10.1145/2676536.2676538|New York, NY, USA|Association for Computing Machinery|9781450331326|2014|High Performance Integrated Spatial Big Data Analytics|Chen, Xin and Vo, Hoang and Aji, Ablimit and Wang, Fusheng|inproceedings|10.1145/2676536.2676538|||||||||||||||||||||||||||||1804346133|42
LOPAL '18|Rabat, Morocco|Knowledge, framework, data quality, knowledge Base, Business process, java EE, complexity, completeness, impact, prediction|5||Proceedings of the International Conference on Learning and Optimization Algorithms: Theory and Applications|When we talk about quality, we cannot do without mentioning the cost of quality and non-quality, the cost increases if the quality also increases; to maintain quality in small data is easier than huge data like big data or knowledge base.Companies tend to use the knowledge base to perfect and facilitate their work, thus satisfying the end customer, however the non-quality of these bases will penalize the company, so it is necessary to improve the quality, the question is when and why to improve quality, our proposal is based on the cost and impact of this improvement, if the impact is greater than the cost then it is recommended to improve completeness in our case study.|10.1145/3230905.3230932|https://doi.org/10.1145/3230905.3230932|New York, NY, USA|Association for Computing Machinery|9781450353045|2018|An Approach of Data-Driven Framework Alignment to Knowledge Base|Maqboul, Jaouad and Bounabat, Bouchaib|inproceedings|10.1145/3230905.3230932|40||||||||||||||||||||||||||||1805320361|42
||symposium on applied computing, scoping study, retrospective, requirements engineering, trends, relevance, SAC, systematic mapping study|16|26–41||Context: The activities related to Requirements engineering (RE) are some of the most important steps in software development, since the requirements describe what will be provided in a software system in order to fulfill the stakeholders' needs. In this context, the ACM Symposium on Applied Computing (SAC) has been a primary gathering forum for many RE activities. When studying a research area, it is important to identify the most active groups, topics, the research trends and so forth. Objective: In a previous paper, we investigated how the SAC RE-Track is evolving, by analyzing the papers published in its 8 previous editions. In this paper, we extended the analysis including the papers of the last edition (2016) and a brief resume of all papers published in the nine editions of SAC-RE track. Method: We adopted a research strategy that combines scoping study and systematic review good practices. Results: We investigated the most active countries, institutions and authors, the main topics discussed, the types of the contributions, the conferences and journals that have most referenced SAC RE-Track papers, the phases of the RE process supported by the contributions, the publications with the greatest impact, and the trends in RE. Conclusions: We found 86 papers over the 9 previous SAC RETrack editions, which were analyzed and discussed.|10.1145/2993231.2993234|https://doi.org/10.1145/2993231.2993234|New York, NY, USA|Association for Computing Machinery||2016|A Retrospective Analysis of SAC Requirements: Engineering Track|Vilela, Jessyka and Goncalves, Enyo and Holanda, Ana Carla and Castro, Jaelson and Figueiredo, Bruno|article|10.1145/2993231.2993234||aug|SIGAPP Appl. Comput. Rev.|15596915|2|16|June 2016||||||||||||||||||||||1810571822|2094650087
||Networks-On-Chip, Approximate Computing, Data Compression|12|666–677||The trend of unsustainable power consumption and large memory bandwidth demands in massively parallel multicore systems, with the advent of the big data era, has brought upon the onset of alternate computation paradigms utilizing heterogeneity, specialization, processor-in-memory and approximation. Approximate Computing is being touted as a viable solution for high performance computation by relaxing the accuracy constraints of applications. This trend has been accentuated by emerging data intensive applications in domains like image/video processing, machine learning and big data analytics that allow inaccurate outputs within an acceptable variance. Leveraging relaxed accuracy for high throughput in Networks-on-Chip (NoCs), which have rapidly become the accepted method for connecting a large number of on-chip components, has not yet been explored. We propose APPROX-NoC, a hardware data approximation framework with an online data error control mechanism for high performance NoCs. APPROX-NoC facilitates approximate matching of data patterns, within a controllable value range, to compress them thereby reducing the volume of data movement across the chip.Our evaluation shows that APPROX-NoC achieves on average up to 9% latency reduction and 60% throughput improvement compared with state-of-the-art NoC data compression mechanisms, while maintaining low application error. Additionally, with a data intensive graph processing application we achieve a 36.7% latency reduction compared to state-of-the-art compression mechanisms.|10.1145/3140659.3080241|https://doi.org/10.1145/3140659.3080241|New York, NY, USA|Association for Computing Machinery||2017|APPROX-NoC: A Data Approximation Framework for Network-On-Chip Architectures|Boyapati, Rahul and Huang, Jiayi and Majumder, Pritam and Yum, Ki Hwan and Kim, Eun Jung|article|10.1145/3140659.3080241||jun|SIGARCH Comput. Archit. News|01635964|2|45|May 2017||||||||||||||||||||||1812272659|929361222
||Supply chain management, Big data analytics, Capabilities, Maturity model||416-436||In the era of Big Data, many organisations have successfully leveraged Big Data Analytics (BDA) capabilities to improve their performance. However, past literature on BDA have put limited focus on understanding the capabilities required to extract value from big data. In this context, this paper aims to provide a systematic literature review of BDA capabilities in supply chain and develop the capabilities maturity model. The paper presents the bibliometric and thematic analysis of research papers from 2008 to 2016. This paper contributes in theorizing BDA capabilities in context of supply chain, and provides future direction of research in this field.|https://doi.org/10.1016/j.tre.2017.04.001|https://www.sciencedirect.com/science/article/pii/S1366554516303799||||2018|Understanding big data analytics capabilities in supply chain management: Unravelling the issues, challenges and implications for practice|Deepak Arunachalam and Niraj Kumar and John Paul Kawalek|article|ARUNACHALAM2018416|||Transportation Research Part E: Logistics and Transportation Review|13665545||114|||||20909|2,042|Q1|110|233|507|13126|4018|499|7,63|56,33|United Kingdom|Western Europe|1997-2020|Business and International Management (Q1); Civil and Structural Engineering (Q1); Management Science and Operations Research (Q1); Transportation (Q1)||||1812755150|1729083653
ICISCAE 2021|Dalian, China||6|2041–2046|2021 4th International Conference on Information Systems and Computer Aided Education||10.1145/3482632.3484095|https://doi.org/10.1145/3482632.3484095|New York, NY, USA|Association for Computing Machinery|9781450390255|2021|Environmental Big Data Model and Recognition of Abnormal Emission from Enterprise Data|Wu, Rui and Cheng, Qian and He, Lisong and Cao, Zhenyu|inproceedings|10.1145/3482632.3484095|||||||||||||||||||||||||||||1813823132|42
||Data integrity;Measurement;Prediction algorithms;Anomaly detection;Machine learning algorithms;Task analysis;Interpolation;data quality;data analytics;real time data analytics||101-108|2020 IEEE Second International Conference on Cognitive Machine Intelligence (CogMI)|Data quality is critically important for big data and machine learning applications. Data quality systems can analyze data sets for quality and detection of potential errors. They can also provide remediation to fix problems encountered in analyzing data sets. This paper discusses key features that of data quality analysis systems. We also present new algorithms for efficiently maintaining updated data quality metrics on changing data sets. Our algorithms consider anomalies in data regions in determining how much different regions of data contribute to overall data metrics. We also make intelligent choices of which data metrics to update and how frequently to do so in order to limit the overhead for data quality metric updates.|10.1109/CogMI50398.2020.00022|||||2020|Real-Time Data Quality Analysis|Iyengar, Arun and Patel, Dhaval and Shrivastava, Shrey and Zhou, Nianjun and Bhamidipaty, Anuradha|inproceedings|9319350||Oct|||||||||||||||||||||||||||1815821624|42
||survey, Twitter, panels, geolocation, social media, coverage bias, non-response bias, selection bias|24|||Data from Twitter have been employed in prior research to study the impacts of events. Conventionally, researchers use keyword-based samples of tweets to create a panel of Twitter users who mention event-related keywords during and after an event. However, the keyword-based sampling is limited in its objectivity dimension of data and information quality. First, the technique suffers from selection bias since users who discuss an event are already more likely to discuss event-related topics beforehand. Second, there are no viable control groups for comparison to a keyword-based sample of Twitter users. We propose an alternative sampling approach to construct panels of users defined by their geolocation. Geolocated panels are exogenous to the keywords in users’ tweets, resulting in less selection bias than the keyword panel method. Geolocated panels allow us to follow within-person changes over time and enable the creation of comparison groups. We compare different panels in two real-world settings: response to mass shootings and TV advertising. We first show the strength of the selection biases of keyword panels. Then, we empirically illustrate how geolocated panels reduce selection biases and allow meaningful comparison groups regarding the impact of the studied events. We are the first to provide a clear, empirical example of how a better panel selection design, based on an exogenous variable such as geography, both reduces selection bias compared to the current state of the art and increases the value of Twitter research for studying events. While we advocate for the use of a geolocated panel, we also discuss its weaknesses and application scenario seriously. This article also calls attention to the importance of selection bias in impacting the objectivity of social media data.|10.1145/3185048|https://doi.org/10.1145/3185048|New York, NY, USA|Association for Computing Machinery||2018|Addressing Selection Bias in Event Studies with General-Purpose Social Media Panels|Zhang, Han and Hill, Shawndra and Rothschild, David|article|10.1145/3185048|4|may|J. Data and Information Quality|19361955|1|10|March 2018||||||||||||||||||||||1816081914|833754770
||Machine learning algorithms;Data analysis;Software architecture;Medical services;Computer architecture;Big Data;Service-oriented architecture;Healthcare;Electronic Health Records;Data Collection;Data Analysis;Health Policies;Microservices||283-288|2021 IEEE 34th International Symposium on Computer-Based Medical Systems (CBMS)|The era of big data is surrounded by plenty of challenges, concerning aspects related to data quality, data management, and data analysis. Plenty of these challenges are met in several domains, such as the healthcare domain, where the corresponding healthcare platforms not only have to deal with managing and/or analyzing a tremendous quantity of health data, but also have to accomplish these actions in the most efficient and secure way possible. Towards this direction, medical institutions are paying attention to the replacement of traditional approaches such as the Monolithic and Service Oriented Architecture (SOA), which deal with many difficulties for handling the increasing amount of healthcare data. This paper presents a platform for overcoming these issues, by adopting the Microservice Architecture (MSA), being able to efficiently manage and analyze these vast amounts of data. More specifically, the proposed platform, namely beHEALTHIER, offers the ability to construct health policies out of data of collective knowledge, by utilizing a newly proposed kind of electronic health records (i.e., eXtended Health Records (XHRs)) and their corresponding networks, through the efficient analysis and management of ingested healthcare data. In order to achieve that, beHEALTHIER is architected based upon four (4) discrete and interacting pillars, namely the Data, the Information, the Knowledge and the Actions pillars. Since the proposed platform is based on MSA, it fully utilizes MSA's benefits, achieving fast response times and efficient mechanisms for healthcare data collection, processing, and analysis.|10.1109/CBMS52027.2021.00078|||||2021|beHEALTHIER: A Microservices Platform for Analyzing and Exploiting Healthcare Data|Mavrogiorgou, Argyro and Kleftakis, Spyridon and Mavrogiorgos, Konstantinos and Zafeiropoulos, Nikolaos and Menychtas, Andreas and Kiourtis, Athanasios and Maglogiannis, Ilias and Kyriazis, Dimosthenis|inproceedings|9474741||June||23729198|||||||||||||||||||||||||1816151594|1118491704
PEARC17|New Orleans, LA, USA||4||Proceedings of the Practice and Experience in Advanced Research Computing 2017 on Sustainability, Success and Impact|In this work, we investigate global seismic tomographic models obtained by spectral-element simulations of seismic wave propagation and adjoint methods. Global crustal and mantle models are obtained based on an iterative conjugate-gradient type of optimization scheme. Forward and adjoint seismic wave propagation simulations, which result in synthetic seismic data to make measurements and data sensitivity kernels to compute gradient for model updates, respectively, are performed by the SPECFEM3D_GLOBE package [1] [2] at the Oak Ridge Leadership Computing Facility (OLCF) to study the structure of the Earth at unprecedented levels. Using advances in solver techniques that run on the GPUs on Titan at the OLCF, scientists are able to perform large-scale seismic inverse modeling and imaging. Using seismic data from global and regional networks from global CMT earthquakes, scientists are using SPECFEM3D_GLOBE to understand the structure of the mantle layer of the Earth. Visualization of the generated data sets provide an effective way to understand the computed wave perturbations which define the structure of mantle in the Earth.|10.1145/3093338.3104170|https://doi.org/10.1145/3093338.3104170|New York, NY, USA|Association for Computing Machinery|9781450352727|2017|Pillars of the Mantle: Imaging the Interior of the Earth with Adjoint Tomography|Pugmire, David and Bozda\u{g}, Ebru and Lefebvre, Matthieu and Tromp, Jeroen and Komatitsch, Dmitri and Peter, Daniel and Podhorszki, Norbert and Hill, Judith|inproceedings|10.1145/3093338.3104170|75||||||||||||||||||||||||||||1817366477|42
ICBDC '20|Chengdu, China|Big data, Locomotive system, Application platform, Railway, Key technology|7|6–12|Proceedings of the 5th International Conference on Big Data and Computing|In order to improve the efficiency of locomotive organization and the quality of locomotive operation, this paper analyzes and discusses the big data platform and some key technologies suitable for the big data application of the railway locomotive system. Firstly, the definition of big data of the railway locomotive system is proposed, and the current data characteristics of the railway locomotive system are summarized, then the status quo and demands of big data application of the railway locomotive system are analyzed. Secondly, the overall architecture of the big data platform for the railway locomotive system is proposed. Furthermore, seven application scenarios available for the big data platform are analyzed, including locomotive running organization, high-speed railway, repair, maintenance and other fields. Finally, some key technologies, which consist of data collection system of front-line operations, locomotive equipment portrait analysis, staff portrait analysis, transmission and analysis of locomotive video, intelligent auxiliary driving system, are provided to increase efficiency of the locomotive organization and capability of safety management. The obtained results can play a positive role in the construction and application of big data of the railway locomotive system.|10.1145/3404687.3404693|https://doi.org/10.1145/3404687.3404693|New York, NY, USA|Association for Computing Machinery|9781450375474|2020|Research on the Big Data Platform and Its Key Technologies for the Railway Locomotive System|Xin, Li and Tianyun, Shi and Xiaoning, Ma|inproceedings|10.1145/3404687.3404693|||||||||||||||||||||||||||||1819213357|42
||big data, perioperative nursing, quality care, nursing knowledge, nursing informatics||286-292||Big data are large volumes of digital data that can be collected from disparate sources and are challenging to analyze. These data are often described with the five “Vs”: volume, velocity, variety, veracity, and value. Perioperative nurses contribute to big data through documentation in the electronic health record during routine surgical care, and these data have implications for clinical decision making, administrative decisions, quality improvement, and big data science. This article explores methods to improve the quality of perioperative nursing data and provides examples of how these data can be combined with broader nursing data for quality improvement. We also discuss a national action plan for nursing knowledge and big data science and how perioperative nurses can engage in collaborative actions to transform health care. Standardized perioperative nursing data has the potential to affect care far beyond the original patient.|https://doi.org/10.1016/j.aorn.2016.07.009|https://www.sciencedirect.com/science/article/pii/S0001209216304410||||2016|Big Data and Perioperative Nursing|Bonnie L. Westra and Jessica J. Peterson|article|WESTRA2016286|||AORN Journal|00012092|4|104||Special Focus Issue: Technology|||||||||||||||||||||1820785542|26429988
Intelligent Data-Centric Systems||Big Data, Big Data Architecture Framework (BDAF), Big data infrastructure, NIST Big Data Architecture (BDRA), Cloud computing, Intercloud Architecture Framework (ICAF), Cloud powered design, SlipStream cloud automation platform||21-62|Big Data Analytics for Sensor-Network Collected Intelligence|This chapter describes the general architecture and functional components of the cloud-based big data infrastructure (BDI). The chapter starts with the analysis of emerging Big Data and data intensive technologies and provides the general definition of the Big Data Architecture Framework (BDAF) that includes the following components: Big Data definition, Data Management including data lifecycle and data structures, generically cloud based BDI, Data Analytics technologies and platforms, and Big Data security, compliance, and privacy. The chapter refers to NIST Big Data Reference Architecture (BDRA) and summarizes general requirements to Big Data systems described in NIST documents. The proposed BDI and its cloud-based components are defined in accordance with the NIST BDRA and BDAF. This chapter provides detailed analysis of the two bioinformatics use cases as typical example of the Big Data applications that have being developed by the authors in the framework of the CYCLONE project. The effective use of cloud for bioinformatics applications requires maximum automation of the applications deployment and management that may include resources from multiple clouds and providers. The proposed CYCLONE platform for multicloud multiprovider applications deployment and management is based on the SlipStream cloud automation platform and includes all necessary components to build and operate complex scientific applications. The chapter discusses existing platforms for cloud powered applications development and deployment automation, in particularly referring to the SlipStream cloud automation platform, which allows multicloud applications deployment and management. The chapter also includes a short overview of the existing Big Data platforms and services provided by the major cloud services providers which can be used for fast deployment of customer Big Data applications using the benefits of cloud technologies and global cloud infrastructure.|https://doi.org/10.1016/B978-0-12-809393-1.00002-7|https://www.sciencedirect.com/science/article/pii/B9780128093931000027||Academic Press|978-0-12-809393-1|2017|Chapter 2 - Cloud Computing Infrastructure for Data Intensive Applications|Yuri Demchenko and Fatih Turkmen and Cees {de Laat} and Ching-Hsien Hsu and Christophe Blanchet and Charles Loomis|incollection|DEMCHENKO201721||||||||||Hui-Huang Hsu and Chuan-Yu Chang and Ching-Hsien Hsu|||||||||||||||||||1821229823|42
IDEAS '20|Seoul, Republic of Korea|visualization, affordance, ecological interface design, cybersecurity, cognition|6||Proceedings of the 24th Symposium on International Database Engineering &amp; Applications|Cyber security visualization designers can benefit from human factors engineering concepts and principles to resolve key human factors challenges in visual interface design. We survey human factors concepts and principles that have been applied in the past decade of human factors research. We highlight these concepts and relate them to cybersecurity visualization design. We provide guidelines to help cybersecurity visualization designers address some human factors challenges in the context of interface design. We use ecological interface design approach to present human factors-based principles of interface design for visualization. Cyber security visualization designers will benefit from human factors engineering concepts and principles to resolve key human factors challenges in visual interface design.|10.1145/3410566.3410606|https://doi.org/10.1145/3410566.3410606|New York, NY, USA|Association for Computing Machinery|9781450375030|2020|Guidelines for Cybersecurity Visualization Design|Seong, Younho and Nuamah, Joseph and Yi, Sun|inproceedings|10.1145/3410566.3410606|25||||||||||||||||||||||||||||1824071829|42
||Radiation oncology, Big Data, Predictive model, Machine learning||110-117||Precision medicine relies on an increasing amount of heterogeneous data. Advances in radiation oncology, through the use of CT Scan, dosimetry and imaging performed before each fraction, have generated a considerable flow of data that needs to be integrated. In the same time, Electronic Health Records now provide phenotypic profiles of large cohorts of patients that could be correlated to this information. In this review, we describe methods that could be used to create integrative predictive models in radiation oncology. Potential uses of machine learning methods such as support vector machine, artificial neural networks, and deep learning are also discussed.|https://doi.org/10.1016/j.canlet.2016.05.033|https://www.sciencedirect.com/science/article/pii/S0304383516303469||||2016|Big Data and machine learning in radiation oncology: State of the art and future prospects|Jean-Emmanuel Bibault and Philippe Giraud and Anita Burgun|article|BIBAULT2016110|||Cancer Letters|03043835|1|382|||||29160|2,470|Q1|182|430|1423|27650|11608|1409|7,85|64,30|Ireland|Western Europe|1975-2020|Cancer Research (Q1); Oncology (Q1)|42,174|8.679|0.04013|1825107176|1403614722
||tuple insertions, incremental discovery, Functional dependency, parallelism, discovery algorithm|25|||Functional dependencies (fds) are one of the metadata used to assess data quality and to perform data cleaning operations. However, to pursue robustness with respect to data errors, it has been necessary to devise imprecise versions of functional dependencies, yielding relaxed functional dependencies (rfds). Among them, there exists the class of rfds relaxing on the extent, i.e., those admitting the possibility that an fd holds on a subset of data. In the literature, several algorithms to automatically discover rfds from big data collections have been defined. They achieve good performances with respect to the inherent problem complexity. However, most of them are capable of discovering rfds only by batch processing the entire dataset. This is not suitable in the era of big data, where the size of a database instance can grow with high-velocity, and the insertion of new data can invalidate previously holding rfds. Thus, it is necessary to devise incremental discovery algorithms capable of updating the set of holding rfds upon data insertions, without processing the entire dataset. To this end, in this article we propose an incremental discovery algorithm for rfds relaxing on the extent. It manages the validation of candidate rfds and the generation of possibly new rfd candidates upon the insertion of the new tuples, while limiting the size of the overall search space. Experimental results show that the proposed algorithm achieves extremely good performances on real-world datasets.|10.1145/3397462|https://doi.org/10.1145/3397462|New York, NY, USA|Association for Computing Machinery||2020|Incremental Discovery of Imprecise Functional Dependencies|Caruccio, Loredana and Cirillo, Stefano|article|10.1145/3397462|19|oct|J. Data and Information Quality|19361955|4|12|December 2020||||||||||||||||||||||1826387726|833754770
||Data mining, Robustness, Process modeling, Statistical process monitoring, Big data analytics||107-133||Industrial process data are usually mixed with missing data and outliers which can greatly affect the statistical explanation abilities for traditional data-driven modeling methods. In this sense, more attention should be paid on robust data mining methods so as to investigate those stable and reliable modeling prototypes for decision-making. This paper gives a systematic review of various state-of-the-art data preprocessing tricks as well as robust principal component analysis methods for process understanding and monitoring applications. Afterwards, comprehensive robust techniques have been discussed for various circumstances with diverse process characteristics. Finally, big data perspectives on potential challenges and opportunities have been highlighted for future explorations in the community.|https://doi.org/10.1016/j.arcontrol.2018.09.003|https://www.sciencedirect.com/science/article/pii/S1367578818301056||||2018|Review and big data perspectives on robust data mining approaches for industrial process modeling with outliers and missing data|Jinlin Zhu and Zhiqiang Ge and Zhihuan Song and Furong Gao|article|ZHU2018107|||Annual Reviews in Control|13675788||46|||||27843|1,780|Q1|80|55|145|5080|1435|132|9,15|92,36|United Kingdom|Western Europe|1996-2020|Control and Systems Engineering (Q1); Software (Q1)|2,756|6.091|0.0039|1828607086|1006300012
|||5|55–59|||10.1145/2430456.2430472|https://doi.org/10.1145/2430456.2430472|New York, NY, USA|Association for Computing Machinery||2013|10th International Workshop on Quality in Databases: QDB 2012|Dong, Xin Luna and Dragut, Eduard Constantin|article|10.1145/2430456.2430472||jan|SIGMOD Rec.|01635808|4|41|December 2012||||13622|0,372|Q2|142|39|81|808|127|57|1,67|20,72|United States|Northern America|1969, 1973-1978, 1981-2020|Information Systems (Q2); Software (Q2)|1,819|0.775|7.5E-4|1835115386|962972343
||||189-190|||https://doi.org/10.1016/j.puhe.2015.02.013|https://www.sciencedirect.com/science/article/pii/S0033350615000621||||2015|Big data! Big deal?|P. Mackie and F. Sim and C. Johnman|article|MACKIE2015189|||Public Health|00333506|3|129|||||17697|0,826|Q2|75|433|874|11891|2120|788|2,10|27,46|Netherlands|Western Europe|1888-1913, 1915-2020|Medicine (miscellaneous) (Q2); Public Health, Environmental and Occupational Health (Q2)|7,603|2.427|0.01222|1835327918|1344507812
||Big data, Genomics, Transcriptomics, RNA-seq, Microarray, Exome||84-93||The tremendous expansion of data analytics and public and private big datasets presents an important opportunity for pre-clinical drug discovery and development. In the field of life sciences, the growth of genetic, genomic, transcriptomic and proteomic data is partly driven by a rapid decline in experimental costs as biotechnology improves throughput, scalability, and speed. Yet far too many researchers tend to underestimate the challenges and consequences involving data integrity and quality standards. Given the effect of data integrity on scientific interpretation, these issues have significant implications during preclinical drug development. We describe standardized approaches for maximizing the utility of publicly available or privately generated biological data and address some of the common pitfalls. We also discuss the increasing interest to integrate and interpret cross-platform data. Principles outlined here should serve as a useful broad guide for existing analytical practices and pipelines and as a tool for developing additional insights into therapeutics using big data.|https://doi.org/10.1016/j.bcp.2018.03.014|https://www.sciencedirect.com/science/article/pii/S0006295218301199||||2018|Integrity, standards, and QC-related issues with big data in pre-clinical drug discovery|John F. Brothers and Matthew Ung and Renan Escalante-Chong and Jermaine Ross and Jenny Zhang and Yoonjeong Cha and Andrew Lysaght and Jason Funt and Rebecca Kusko|article|BROTHERS201884|||Biochemical Pharmacology|00062952||152|||||||||||||||||||||||1837929167|435053555
ICBDC '20|Chengdu, China|MapReduce, Big Data Technology, HDFS, Apache Hadoop|9|23–31|Proceedings of the 5th International Conference on Big Data and Computing|In as much as the approaches of the new revolution, machines including transmission media like social media sites, nowadays quantity of data swell hastily. So, size is the core and only facet that leaps the mention of BIG DATA. In this article, an effort to touch a comprehensive view of big data technologies, because of the swift evolution of data by an industry trying the academic press to catch up. This paper also offers a unified explanation of big data as well as the analytics methods. A practical discriminate characteristic of this paper is core analytics associated with unstructured data which is more than 90% of big data. To deal with complicated Big Data problems, great work has been done. This paper analyzes contemporary Big Data technologies. Therein article further strengthens the necessity to formulate new tools for analytics. It bestows not sole an intercontinental overview of big data techniques even though the valuation according to big data Hadoop Ecosystem. It classifies and debates the main technologies feature, challenges, and usage as well.|10.1145/3404687.3404694|https://doi.org/10.1145/3404687.3404694|New York, NY, USA|Association for Computing Machinery|9781450375474|2020|A Comprehensive Overview of BIG DATA Technologies: A Survey|Raza, Muhammad Umair and XuJian, Zhao|inproceedings|10.1145/3404687.3404694|||||||||||||||||||||||||||||1839861961|42
||Force;Sensors;Signal to noise ratio;Capacitance;Electrodes;Machine learning;Electromagnetic interference;Piezoelectric touch panel;touch orientation;force–voltage responsivity;white Gaussian noise;regression model||26373-26381||Touch orientation detection is important for piezoelectric touch panels to stabilize force-voltage responsivities. Current touch orientation estimation techniques utilize machine learning algorithms for orientation classification. However, environmental noise could weaken the data quality which will result in a lowered detection accuracy. To address this issue, in this article, we present a noise robustness technique, in which different levels of noise data are injected into the training data. The performance of “dirty” data trained model exhibits a good performance (average mean absolute error (MAE) of 7.8 degrees) among signal-to-noise ratio (SNR) from 3 dB to 40 dB, indicating that an improved user experience can be obtained.|10.1109/JSEN.2021.3065525|||||2021|A Machine-Learning-Based Touch Orientation Detection Method for Piezoelectric Touch Sensing in Noisy Environment|Lu, Yujiao and Cui, Ziang and Guo, Rong and Xu, Lijun and Gao, Shuo|article|9374986||Dec|IEEE Sensors Journal|15581748|23|21|||||||||||||||||||||||1842803057|1628430050
||sentiment classification, cross-language, topic model|13|432–444||Sentiment classification aims to determine the sentiment polarity expressed in a text. In online customer reviews, the sentiment polarities of words are usually dependent on the corresponding aspects. For instance, in mobile phone reviews, we may expect the long battery time but not enjoy the long response time of the operating system. Therefore, it is necessary and appealing to consider aspects when conducting sentiment classification. Probabilistic topic models that jointly detect aspects and sentiments have gained much success recently. However, most of the existing models are designed to work well in a language with rich resources. Directly applying those models on poor-quality corpora often leads to poor results. Consequently, a potential solution is to use the cross-lingual topic model to improve the sentiment classification for a target language by leveraging data and knowledge from a source language. However, the existing cross-lingual topic models are not suitable for sentiment classification because sentiment factors are not considered therein. To solve these problems, we propose for the first time a novel cross-lingual topic model framework which can be easily combined with the state-of-the-art aspect/sentiment models. Extensive experiments in different domains and multiple languages demonstrate that our model can significantly improve the accuracy of sentiment classification in the target language.|10.1109/TASLP.2015.2512041|https://doi.org/10.1109/TASLP.2015.2512041||IEEE Press||2016|An Unsupervised Cross-Lingual Topic Model Framework for Sentiment Classification|Lin, Zheng and Jin, Xiaolong and Xu, Xueke and Wang, Yuanzhuo and Cheng, Xueqi and Wang, Weiping and Meng, Dan|article|10.1109/TASLP.2015.2512041||mar|IEEE/ACM Trans. Audio, Speech and Lang. Proc.|23299290|3|24|March 2016||||||||||||||||||||||1844514179|1905056144
|||6|1764–1769||Data ingestion is an essential part of companies and organizations that collect and analyze large volumes of data. This paper describes Gobblin, a generic data ingestion framework for Hadoop and one of LinkedIn's latest open source products. At LinkedIn we need to ingest data from various sources such as relational stores, NoSQL stores, streaming systems, REST endpoints, filesystems, etc. into our Hadoop clusters. Maintaining independent pipelines for each source can lead to various operational problems. Gobblin aims to solve this issue by providing a centralized data ingestion framework that makes it easy to support ingesting data from a variety of sources.Gobblin distinguishes itself from similar frameworks by focusing on three core principles: generality, extensibility, and operability. Gobblin supports a mixture of data sources out-of-the-box and can be easily extended for more. This enables an organization to use a single framework to handle different data ingestion needs, making it easy and inexpensive to operate. Moreover, with an end-to-end metrics collection and reporting module, Gobblin makes it simple and efficient to identify issues in production.|10.14778/2824032.2824073|https://doi.org/10.14778/2824032.2824073||VLDB Endowment||2015|Gobblin: Unifying Data Ingestion for Hadoop|Qiao, Lin and Li, Yinan and Takiar, Sahil and Liu, Ziyang and Veeramreddy, Narasimha and Tu, Min and Dai, Ying and Buenrostro, Issac and Surlaker, Kapil and Das, Shirshanka and Botev, Chavdar|article|10.14778/2824032.2824073||aug|Proc. VLDB Endow.|21508097|12|8|August 2015||||21100199855|0,946|Q1|134|246|598|12401|3759|566|5,50|50,41|United States|Northern America|2008-2020|Computer Science (miscellaneous) (Q1)|7,198|2.047|0.00891|1844732790|1216159931
||Precision Livestock Farming, digitalization, Digital Technologies in Livestock Systems, sensor technology, big data, blockchain, data models, livestock agriculture||100408||As the global human population increases, livestock agriculture must adapt to provide more livestock products and with improved efficiency while also addressing concerns about animal welfare, environmental sustainability, and public health. The purpose of this paper is to critically review the current state of the art in digitalizing animal agriculture with Precision Livestock Farming (PLF) technologies, specifically biometric sensors, big data, and blockchain technology. Biometric sensors include either noninvasive or invasive sensors that monitor an individual animal’s health and behavior in real time, allowing farmers to integrate this data for population-level analyses. Real-time information from biometric sensors is processed and integrated using big data analytics systems that rely on statistical algorithms to sort through large, complex data sets to provide farmers with relevant trending patterns and decision-making tools. Sensors enabled blockchain technology affords secure and guaranteed traceability of animal products from farm to table, a key advantage in monitoring disease outbreaks and preventing related economic losses and food-related health pandemics. Thanks to PLF technologies, livestock agriculture has the potential to address the abovementioned pressing concerns by becoming more transparent and fostering increased consumer trust. However, new PLF technologies are still evolving and core component technologies (such as blockchain) are still in their infancy and insufficiently validated at scale. The next generation of PLF technologies calls for preventive and predictive analytics platforms that can sort through massive amounts of data while accounting for specific variables accurately and accessibly. Issues with data privacy, security, and integration need to be addressed before the deployment of multi-farm shared PLF solutions becomes commercially feasible.|https://doi.org/10.1016/j.sbsr.2021.100408|https://www.sciencedirect.com/science/article/pii/S2214180421000131||||2021|Digital Livestock Farming|Suresh Neethirajan and Bas Kemp|article|NEETHIRAJAN2021100408|||Sensing and Bio-Sensing Research|22141804||32|||||21100356802|0,770|Q1|28|80|140|3196|713|140|4,12|39,95|Netherlands|Western Europe|2014-2020|Electrical and Electronic Engineering (Q1); Electronic, Optical and Magnetic Materials (Q1); Biotechnology (Q2); Signal Processing (Q2)||||1846191873|381705718
||Big Data, Artificial intelligence and machine learning in neurotrauma||53-75|Leveraging Biomedical and Healthcare Data|Rapid advances in the collection, storage, and analysis of large volumes of data—Big Data—offer the much-needed help to identify and treat the various pathological conditions triggered by traumatic brain injury (TBI). Big Data (BD) is defined as extremely large, complex, and mostly unstructured data that cannot be analyzed using traditional approaches. BD can be only analyzed by using text mining (TM), artificial intelligence (AI), or machine learning (ML). These approaches can reveal patterns, trends, and associations, critical for understanding the “most complex disease of the most complex organ.” While powerful and successfully tested computational tools are available, using BD approaches in TBI is currently hampered by the limited availability of legacy and/or primary data, by incompatible data formats and standards. This chapter introduces Big Data and Big Data approaches such as text mining, artificial intelligence, and machine learning; outlines the benefits of using BD approaches; and suggests potential solutions that can help using the full potential of BD in TBI. It also identifies necessary changes of how researchers can help ushering in a new era of preclinical and clinical TBI research by recording and storing ALL the data generated and making ALL the data available for BD approaches—text mining, artificial intelligence, and machine learning so new correlations, relationships, and trends can be identified. In turn, these new information will help to develop novel diagnostics, evidence-based treatments, and improve outcomes.|https://doi.org/10.1016/B978-0-12-809556-0.00004-6|https://www.sciencedirect.com/science/article/pii/B9780128095560000046||Academic Press|978-0-12-809556-0|2019|Chapter 4 - Big Data, Artificial Intelligence, and Machine Learning in Neurotrauma|Denes V. Agoston|incollection|AGOSTON201953||||||||||Firas Kobeissy and Ali Alawieh and Fadi A. Zaraket and Kevin Wang|||||||||||||||||||1846679729|42
Handbook of Statistics||Cognitive analytics, Text analytics, Learning analytics, Educational data mining, Cognitive systems, Cognitive computing, Personalized learning, Data science, Machine learning, Big data analytics, Business analytics||169-205|Cognitive Computing: Theory and Applications|This chapter defines analytics and traces its evolution from its origin in 1988 to its current stage—cognitive analytics. We discuss types of learning and describe classes of machine learning algorithms. Given this backdrop, we propose a reference architecture for cognitive analytics and indicate ways to implement the architecture. A few cognitive analytics applications are briefly described. The chapter concludes by indicating current trends and future research direction.|https://doi.org/10.1016/bs.host.2016.07.010|https://www.sciencedirect.com/science/article/pii/S0169716116300517||Elsevier||2016|Chapter 5 - Cognitive Analytics: Going Beyond Big Data Analytics and Machine Learning|V.N. Gudivada and M.T. Irfan and E. Fathi and D.L. Rao|incollection|GUDIVADA2016169||||01697161||35||||Venkat N. Gudivada and Vijay V. Raghavan and Venu Govindaraju and C.R. Rao|17700156445|0,125|Q4|42|19|89|1023|82|1|1,12|53,84|Netherlands|Western Europe|1980, 1982-1985, 1988, 1991, 1993-1994, 1996-1998, 2000-2001, 2003, 2005-2007, 2009, 2012-2020|Applied Mathematics (Q4); Modeling and Simulation (Q4); Statistics and Probability (Q4)||||1847194949|2062690730
||Sustainable supply chain management, Data-driven analysis, Fuzzy Delphi method, Entropy weight method, Fuzzy decision-making trial and evaluation laboratory||105421||This study proposes a data-driven analysis that describes the overall situation and reveals the factors hindering improvement in the sustainable supply chain management field. The literature has presented a summary of the evolution of sustainable supply chain management across attributes. Prior studies have evaluated different parts of the supply chain as independent entities. An integrated systematic assessment is absent in the extant literature and makes it necessary to identify potential opportunities for research direction. A hybrid of data-driven analysis, the fuzzy Delphi method, the entropy weight method and fuzzy decision-making trial and evaluation laboratory is adopted to address uncertainty and complexity. This study contributes to locating the boundary of fundamental knowledge to advance future research and support practical execution. Valuable direction is provided by reviewing the existing literature to identify the critical indicators that need further examination. The results show that big data, closed-loop supply chains, industry 4.0, policy, remanufacturing, and supply chain network design are the most important indicators of future trends and disputes. The challenges and gaps among different geographical regions is offered that provides both a local viewpoint and a state-of-the-art advanced sustainable supply chain management assessment.|https://doi.org/10.1016/j.resconrec.2021.105421|https://www.sciencedirect.com/science/article/pii/S0921344921000288||||2021|Sustainable supply chain management trends in world regions: A data-driven analysis|Feng Ming Tsai and Tat-Dat Bui and Ming-Lang Tseng and Mohd Helmi Ali and Ming K. Lim and Anthony SF Chiu|article|TSAI2021105421|||Resources, Conservation and Recycling|09213449||167|||||||||||||||||||||||1847802403|905300119
||Privacy;Training;Fingerprint recognition;Edge computing;Cloud computing;Indoor localization, Differential privacy, Privacy preserving, Edge computing.||491-496|2019 IEEE SmartWorld, Ubiquitous Intelligence & Computing, Advanced & Trusted Computing, Scalable Computing & Communications, Cloud & Big Data Computing, Internet of People and Smart City Innovation (SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI)|With the popularity of smart devices and the widespread use of the Wi-Fi-based indoor localization, edge computing is becoming the mainstream paradigm of processing massive sensing data to acquire indoor localization service. However, these data which were conveyed to train the localization model unintentionally contain some sensitive information of users/devices, and were released without any protection may cause serious privacy leakage. To solve this issue, we propose a lightweight differential privacy-preserving mechanism for the edge computing environment. We extend ε-differential privacy theory to a mature machine learning localization technology to achieve privacy protection while training the localization model. Experimental results on multiple real-world datasets show that, compared with the original localization technology without privacy-preserving, our proposed scheme can achieve high accuracy of indoor localization while providing differential privacy guarantee. Through regulating the value of ε, the data quality loss of our method can be controlled up to 8.9% and the time consumption can be almost negligible. Therefore, our scheme can be efficiently applied in the edge networks and provides some guidance on indoor localization privacy protection in the edge computing.|10.1109/SmartWorld-UIC-ATC-SCALCOM-IOP-SCI.2019.00125|||||2019|Differential Privacy-Based Indoor Localization Privacy Protection in Edge Computing|Zhang, Xuejun and Chen, Qian and Peng, Xiaohui and Jiang, Xinlong|inproceedings|9060228||Aug|||||||||||||||||||||||||||1848152639|42
||Artificial intelligence, Big data, Climate resilience, Data infrastructure, South Asia, SDG, Technological readiness, Urban transformation||100127||Artificial intelligence (AI) and big data solutions are currently being utilized to offer low cost and efficient solutions in solving pressing urban socio-economic and environmental problems globally. The study found big data and AI have the potentiality to solve the common urban problems in South Asia and upsurge the efficiency of urban industries, increase competitiveness and productivity of the human and natural resources, reduce the cost of urban service delivery, and build climate resilience. The study has assessed the current AI and big data initiatives and technologies in mitigating the urban development challenges and their potentiality for scaling up in South Asian cities. The study also examined the latest innovations in AI and big data solutions for SDG monitoring and implementation in South Asia and their implication for transformational change. The study suggested that South Asia can harness the maximum benefit of AI and big data technologies by building big data and associated IT infrastructure, advancing research and innovations with regional cooperation, enhancing technological readiness, and eliminating week enabling conditions.|https://doi.org/10.1016/j.indic.2021.100127|https://www.sciencedirect.com/science/article/pii/S2665972721000283||||2021|Harnessing artificial intelligence and big data for SDGs and prosperous urban future in South Asia|Md. Arfanuzzaman|article|ARFANUZZAMAN2021100127|||Environmental and Sustainability Indicators|26659727||11|||||||||||||||||||||||1857304933|2099729077
||Big Data;Quality of service;Delays;Data models;Vehicular ad hoc networks;Vehicle-to-everything;Optimization;ITS;mobility prediction;time synchronized data exchanges;data offloading;V2X data exchange.||1-11||Intelligent Transportation Systems (ITS) is a smart-transportation system for road-side assistance and data exchange support by integrating cloud and wireless networks. ITS facilitates vehicle-to-vehicle and vehicle-to-anything (V2X) data exchanges for satisfying user demands. The rate of big data granting to the vehicular users is interrupted by the fundamental attributes such as mobility and link instability of the vehicles. To address the issues in vehicular data exchange big data, this article introduces displacement-aware service endowment scheme with the benefits of data offloading. Displacement-aware big data endowment ensures responsive availability of vehicle request information despite unfavorable location and density factors. The time congruency in V2V and V2X data exchanges are adopted for minimizing data exchange dropouts. In the data offloading phase, extraneous information and big data responses are detained based on data exchange relevance to improve congestion free big data endowment. The distinct methods work in a co-operative manner to improve big data quality of fast configuring smart vehicles to provide reliable big data in smart city environments.|10.1109/TITS.2021.3078753|||||2021|Displacement-Aware Service Endowment Scheme for Improving Intelligent Transportation Systems Data Exchange|Manogaran, Gunasekaran and Nguyen, Tu N.|article|9457165|||IEEE Transactions on Intelligent Transportation Systems|15580016|||||||18378|1,591|Q1|153|595|1026|20731|9523|1015|8,41|34,84|United States|Northern America|2000-2020|Automotive Engineering (Q1); Computer Science Applications (Q1); Mechanical Engineering (Q1)|20,072|6.492|0.02555|1858363530|1512692042
SCA '18|Tetouan, Morocco|Responsive cities, Big Data, Quality of data, Data gathering|8||Proceedings of the 3rd International Conference on Smart City Applications|For the last two decades, data driven cities have emerged as an efficient way of improving the city performance, enhancing life quality, and providing more choices to city planners and decision makers. A significant change in data driven cities in recent years is that much more data are collected from a variety of sources and can be processed into various forms for different stakeholders. The availability of a large amount of data can potentially lead to a revolution in city development, changing the city operation system from a conventional technology-driven system into a more powerful multifunctional data-driven intelligent system. But with more data collected the more questions raised about the optimization and space saving methods, then the quality of data collected and the efficiency of the its treatment. In this paper, we provide a survey on the data driven cities requirements, and the tools made available for the responsive cities to maintain its data.|10.1145/3286606.3286794|https://doi.org/10.1145/3286606.3286794|New York, NY, USA|Association for Computing Machinery|9781450365628|2018|Responsive Cities and Data Gathering: Challenges and Opportunities|Noussair, Lazrak and Jihad, Zahir and Hajar, Mousannif|inproceedings|10.1145/3286606.3286794|17||||||||||||||||||||||||||||1860853078|42
PETRA '19|Rhodes, Greece|controlled data creation, coherent database, accelerometer, smartwatch, sensor fusion, data imputation, mobile device, data fusion|6|587–592|Proceedings of the 12th ACM International Conference on PErvasive Technologies Related to Assistive Environments|Data acquisition of mobile tracking devices often suffers from invalid and non-continuous input data streams. This issue especially occurs with current wearables tracking the user's activity and vital data. Typical reasons include the short battery life and the fact that the body-worn tracking device may be doffed. Other reasons, such as technical issues, can corrupt the data and render it unusable. In this paper, we introduce a data imputation concept which complements and thus fixes incomplete datasets by using a new merging approach that is particularly suitable for assessing activities and vital data. Our technique enables the dataset to become coherent and comprehensive so that it is ready for further analysis. In contrast to previous approaches, our technique enables the controlled creation of continuous data sets that also contain information on the level of uncertainty for possible reconversions, approximations, or later analysis.|10.1145/3316782.3322785|https://doi.org/10.1145/3316782.3322785|New York, NY, USA|Association for Computing Machinery|9781450362320|2019|Presenting a Data Imputation Concept to Support the Continuous Assessment of Human Vital Data and Activities|Haescher, Marian and Matthies, Denys J. C. and Krause, Silvio and Bieber, Gerald|inproceedings|10.1145/3316782.3322785|||||||||||||||||||||||||||||1860932579|42
ICCTA '22|Vienna, Austria|cloud platform expertise, artificial intelligence, Styria, statistical analysis, data science, personnel requirement, data management, cloud computing, machine learning, business intelligence, data driven innovation|7|28–34|Proceedings of the 2022 8th International Conference on Computer Technology Applications|The aim of this paper is to discuss the results of a survey conducted to assess the need for skilled workers in the areas Data Science &amp; Cloud Computing in Styria, Austria. Firstly, the relevant roles and skills in the abovementioned areas had to be selected. Initially, this selection process is described. Consequently, a survey was designed and given to a representative group of companies. The survey includes questions regarding the need for skilled workers with respect to the domains and the selected skills in the areas Data Science &amp; Cloud Computing. Moreover, the respondents were asked about the importance of further education and the necessity of academic education in these areas. Overall, our survey concludes that the requirements for skilled workers in the areas of Data Science and Cloud Computing in Styria will increase significantly in the coming years.|10.1145/3543712.3543749|https://doi.org/10.1145/3543712.3543749|New York, NY, USA|Association for Computing Machinery|9781450396226|2022|Need for Skilled Workers in the Area of Data Science and Cloud Computing in Styria|Raab, Raphaele and Granigg, Wolfgang and Melcher, Michael|inproceedings|10.1145/3543712.3543749|||||||||||||||||||||||||||||1863548503|42
||Design Science, Digital Ecosystem, Sustainability, Multidimensional Artefacts, Data Interpretation||1871-1880||Sustainability is a dynamic, complex and composite data relationship among geographically distributed human and environment ecosystems. The ecosystems may have strong interactions among their elements and processes, but with dynamic implicit boundaries. Multi-scalable and multidimensional ecosystems have significance based on a commonality of basic structural units and domains. We intend to develop a holistic information system for managing different ecosystems within a sustainability framework/context, using an empirical qualitative and quantitative interpretation and analysis of the measured observations. Design Science Research (DSR) approach is aimed at developing an information system using the volumes of unstructured Big Data observations. Collaborating multiple domains, interpreting and evaluating the commonality, uncovering the connectivity among multiple systems are key aspects of the study. The Design Science Information System (DSIS), evolved from DSR approach is used in solving the ecosystem issues associated with multiple domains, in which the sustainability challenges manifest. In this context, we propose a human-environment-economic ecosystem (HEES) framework consisting of human, environment and economic elements and processes. In broad terms, human, environment and economic domains are conceptualized as different players/agents that operate within a range of sustainability scenarios. This approach recognizes the existing constraints of the systems as well as the emerging knowledge of the boundaries of ecosystems and their connectivity. The connectivity and interaction among the systems are analyzed by data mining, visualization and interpretation artefacts within a sustainability policy framework.|https://doi.org/10.1016/j.procs.2017.08.233|https://www.sciencedirect.com/science/article/pii/S1877050917316381||||2017|Big Data Guided Design Science Information System (DSIS) Development for Sustainability Management and Accounting|Shastri L Nimmagadda and Torsten Reiners and and {Gary Burke}|article|NIMMAGADDA20171871|||Procedia Computer Science|18770509||112||Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 21st International Conference, KES-20176-8 September 2017, Marseille, France|||19700182801|0,334|-|76|1907|6483|38511|13722|6359|2,09|20,19|Netherlands|Western Europe|2010-2020|Computer Science (miscellaneous)||||1863880577|2108686752
ICISS 2020|Cambridge, United Kingdom|data currency rule, parallel algorithm, Data currency, dynamic data|5|24–28|Proceedings of the 2020 The 3rd International Conference on Information Science and System|Data currency is a temporal reference of data, which is related to the value of data and affects the results of data analysis and mining. The currency rules that reflect the time series features of data can be used not only for data repairing, but also for data quality evaluation. However, with the rapid growth and dynamic update of data volume, both the forms and algorithms of basic currency rule are facing severe challenges in application. Therefore, based on the research on data currency repairing, we extended the basic currency rule form, and proposed rule extraction and incremental updating algorithms that can run in parallel on dynamic data set. The experimental results show that, compared with non-parallel methods, the efficiency of parallel algorithms is significantly improved.|10.1145/3388176.3388210|https://doi.org/10.1145/3388176.3388210|New York, NY, USA|Association for Computing Machinery|9781450377256|2020|Research on Parallel Data Currency Rule Algorithms|Duan, Xuliang and Guo, Bing and Shen, Yan and Shen, Yuncheng and Dong, Xiangqian and Zhang, Hong|inproceedings|10.1145/3388176.3388210|||||||||||||||||||||||||||||1866936996|42
PCI '16|Patras, Greece|e-Shopping, User Profiling, E-Commerce, Retailing, Mobile shopping|6||Proceedings of the 20th Pan-Hellenic Conference on Informatics|The Internet is overwhelmed by a huge amount of information every day and every user has different interests from another. It is therefore important that this information is filtered and sorted according to their preferences. Thus, the profiling systems exploit particularities and preferences of each user and finally they can be studied or used by other applications or humans. This paper analyzes the methods of collecting data (data gathering), and the ways in which this information can be used - filtered so as to create knowledge. A user profile extraction engine is presented and analyzed.|10.1145/3003733.3003761|https://doi.org/10.1145/3003733.3003761|New York, NY, USA|Association for Computing Machinery|9781450347891|2016|User Profile Extraction Engine|Gatziolis, Kleanthis and Boucouvalas, Anthony C.|inproceedings|10.1145/3003733.3003761|41||||||||||||||||||||||||||||1880501249|42
||Big data, Integration, Feature patterns, Indexing, Semantic analysis||100033||Big Data has received much attention in the multi-domain industry. In the digital and computing world, information is generated and collected at a rate that quickly exceeds the boundaries. The traditional data integration system interconnects the limited number of resources and is built with relatively stable and generally complex and time-consuming design activities. However, the rapid growth of these large data sets creates difficulties in learning heterogeneous data structures for integration and indexing. It also creates difficulty in information retrieval for the various data analysis requirements. In this paper, a probabilistic feature Patterns (PFP) approach using feature transformation and selection method is proposed for efficient data integration and utilizing the features latent semantic analysis (F-LSA) method for indexing the unsupervised multiple heterogeneous integrated cluster data sources. The PFP approach takes the advantage of the features transformation and selection mechanism to map and cluster the data for the integration, and an analysis of the data features context relation using LSA to provide the appropriate index for fast and accurate data extraction. A huge volume of BibText dataset from different publication sources are processed to evaluated to understand the effectiveness of the proposal. The analytical study and the outcome results show the improvisation in integration and indexing of the work.|https://doi.org/10.1016/j.array.2020.100033|https://www.sciencedirect.com/science/article/pii/S2590005620300187||||2020|An efficient integration and indexing method based on feature patterns and semantic analysis for big data|Madhu Mahesh Nashipudimath and Subhash K. Shinde and Jayshree Jain|article|NASHIPUDIMATH2020100033|||Array|25900056||7|||||||||||||||||||||||1884002315|1655382114
||Dynamic data quality, graph systems, big data, relational systems, internet of things|3||||10.1145/2998575|https://doi.org/10.1145/2998575|New York, NY, USA|Association for Computing Machinery||2017|An Introduction to Dynamic Data Quality Challenges|Labouseur, Alan G. and Matheus, Carolyn C.|article|10.1145/2998575|6|jan|J. Data and Information Quality|19361955|2|8|February 2017||||||||||||||||||||||1884670976|833754770
iiWAS '14|Hanoi, Viet Nam|R, RDF, Linked Data|7|217–223|Proceedings of the 16th International Conference on Information Integration and Web-Based Applications &amp; Services|Link Data (LD) initiative has fundamentally changed the way how data are published, distributed, and consumed. It advocates data transparency and accessibility to fulfill the Web of Data vision. Thus far, tens of billions of data items have been made publicly available in machine-understandable forms (e.g. RDF). The sheer size of LD data, however, has not resulted in a significant increase of data consumption and thus a self-sustainable consumption-driven publication. We contend that this is primarily due to the lack of tooling for exploiting LD. A new programming paradigm is necessary to simplify and encourage value-add LD data utilisation.This paper reports an on-going project towards programmable Linked Open Data. We propose to tap into a distributed computing environment underpinning the popular statistical toolkit R. Where possible, native R operators and functions are used in our approach so as to lower the learning curve for experienced data scientists.We believe a report to the relevant community at this stage can help us to collect critical requirements before moving into the next stage of development. The crux of our future work lies in comprehensive and extensive evaluations, in terms of, but not limited to, system performance, system stability, system scalability, programming productivity and user experience.|10.1145/2684200.2684336|https://doi.org/10.1145/2684200.2684336|New York, NY, USA|Association for Computing Machinery|9781450330015|2014|Capri: Programmable Analytics for Linked Data|Hu, Bo and Rodrigues, Eduarda Mendes and Viel, Emeric|inproceedings|10.1145/2684200.2684336|||||||||||||||||||||||||||||1884882577|42
SIGMOD '21|Virtual Event, China|data pipelines, data quality, data lake, data validation|14|1678–1691|Proceedings of the 2021 International Conference on Management of Data|"\"Complex data pipelines are increasingly common in diverse applications such as BI reporting and ML modeling. These pipelines often recur regularly (e.g., daily or weekly), as BI reports need to be refreshed, and ML models need to be retrained. However, it is widely reported that in complex production pipelines, upstream data feeds can change in unexpected ways, causing downstream applications to break silently that are expensive to resolve. Data validation has thus become an important topic, as evidenced by notable recent efforts from Google and Amazon, where the objective is to catch data quality issues early as they arise in the pipelines. Our experience on production data suggests, however, that on string-valued data, these existing approaches yield high false-positive rates and frequently require human intervention. In this work, we develop a corpus-driven approach to auto-validate machine-generated data by inferring suitable data-validation \"\"patterns'' that accurately describe the underlying data-domain, which minimizes false-positives while maximizing data quality issues caught. Evaluations using production data from real data lakes suggest that sj is substantially more effective than existing methods. Part of this technology ships as an Auto-Tag feature in Microsoft Azure Purview.\""|10.1145/3448016.3457250|https://doi.org/10.1145/3448016.3457250|New York, NY, USA|Association for Computing Machinery|9781450383431|2021|Auto-Validate: Unsupervised Data Validation Using Data-Domain Patterns Inferred from Data Lakes|Song, Jie and He, Yeye|inproceedings|10.1145/3448016.3457250|||||||||||||||||||||||||||||1887341262|42
CIKM '17|Singapore, Singapore|entity resolution, data cleaning, parallel computing|4|2543–2546|Proceedings of the 2017 ACM on Conference on Information and Knowledge Management|We describe CleanCloud, a system for cleaning big data based on Map-Reduce paradigm in cloud. Using Map-Reduce paradigm, the system detects and repairs various data quality problems in big data. We demonstrate the following features of CleanCloud: (a) the support for cleaning multiple data quality problems in big data; (b) a visual tool for watching the status of big data cleaning process and tuning the parameters for data cleaning; (c) the friendly interface for data input and setting as well as cleaned data collection for big data. CleanCloud is a promising system that provides scalable and effect data cleaning mechanism for big data in either files or databases.|10.1145/3132847.3133187|https://doi.org/10.1145/3132847.3133187|New York, NY, USA|Association for Computing Machinery|9781450349185|2017|CleanCloud: Cleaning Big Data on Cloud|Wang, Hongzhi and Ding, Xiaoou and Chen, Xiangying and Li, Jianzhong and Gao, Hong|inproceedings|10.1145/3132847.3133187|||||||||||||||||||||||||||||1889810833|42
||Big data analytic, Decision Support System, DVRP, S-GA, Spark||938-947||Recently, the explosion of large amounts of traffic data has guided data scientists to create models with big data for a better decision-making. Big Data applications process and analyze this huge amounts of data (collected from a variety of heterogeneous data sources) that cannot be processed with traditional technologies. In this paper, Big Data frameworks are used for solving an optimization problem known as Dynamic Vehicle Routing Problem (DVRP). Hence, due to the NP-Hardness of the problem and to deal with a large size of data, we develop a parallel Spark Genetic Algorithm named (S-GA). This parallelism aims to take the advantage of Spark’s in-memory computing ability (as a master-slave distribution computing) and GA’s iterations operations. Parallel operations were used for fitness evaluation and genetic operations. Based on the parallel S-GA a decision support system is developed for the DVRP in order to generate the best routes. The experiments show that our proposed architecture is improved due to its capacity when coping with Big Data optimization problems by interconnecting components and deploying on different nodes of a cluster.|https://doi.org/10.1016/j.procs.2020.09.089|https://www.sciencedirect.com/science/article/pii/S1877050920319876||||2020|A real-time Decision Support System for Big Data Analytic: A case of Dynamic Vehicle Routing Problems|Ines Sbai and Saoussen Krichen|article|SBAI2020938|||Procedia Computer Science|18770509||176||Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 24th International Conference KES2020|||19700182801|0,334|-|76|1907|6483|38511|13722|6359|2,09|20,19|Netherlands|Western Europe|2010-2020|Computer Science (miscellaneous)||||1891018095|2108686752
||Cloud computing;Sparks;Big Data;Task analysis;Resource management;Computational modeling;Optimization;Big data;quality of service;elastic resource provisioning;cluster management||1301-1316||Nowadays, the big data paradigm is consolidating its central position in the industry, as well as in society at large. Lots of applications, across disparate domains, operate on huge amounts of data and offer great advantages both for business and research. According to analysts, cloud computing adoption is steadily increasing to support big data analyses and Spark is expected to take a prominent market position for the next decade. As big data applications gain more and more importance over time and given the dynamic nature of cloud resources, it is fundamental to develop an intelligent resource management system to provide Quality of Service guarantees to end-users. This article presents a set of run-time optimization-based resource management policies for advanced big data analytics. Users submit Spark applications characterized by a priority and by a hard or soft deadline. Optimization policies address two scenarios: i) identification of the minimum capacity to run a Spark application within the deadline; ii) re-balance of the cloud resources in case of heavy load, minimising the weighted soft deadline application tardiness. The solution relies on an initial non-linear programming model formulation and a search space exploration based on simulation-optimization procedures. Spark application execution times are estimated by relying on a gamut of techniques, including machine learning, approximated analyses, and simulation. The benefits of the approach are evaluated on Microsoft Azure HDInsight and on a private cloud cluster based on POWER8 by considering the TPC-DS industry benchmark and SparkBench. The results obtained in the first scenario demonstrate that the percentage error of the prediction of the optimal resource usage with respect to system measurement and exhaustive search is in the range 4–29 percent while literature-based techniques present an average error in the range 6–63 percent. Moreover, in the second scenario, the proposed algorithms can address complex problems like computing the optimal redistribution of resources among tens of applications in less than a minute with an error of 8 percent on average. On the same considered tests, literature-based approaches obtain an average error of about 57 percent.|10.1109/TCC.2020.2985682|||||2022|Optimal Resource Allocation of Cloud-Based Spark Applications|Lattuada, Marco and Barbierato, Enrico and Gianniti, Eugenio and Ardagna, Danilo|article|9057697||April|IEEE Transactions on Cloud Computing|21687161|2|10|||||21100338351|1,075|Q1|49|173|278|3991|1406|223|4,85|23,07|United States|Northern America|2013-2020|Computer Networks and Communications (Q1); Computer Science Applications (Q1); Hardware and Architecture (Q1); Information Systems (Q1); Software (Q1)|2,658|5.938|0.00415|1894494586|1555155972
https://doi.org/10.1016/j.isci.2021.103052|https://www.sciencedirect.com/science/article/pii/S2589004221010208||||2021|Comprehensive strategies of machine-learning-based quantitative structure-activity relationship models|Jiashun Mao and Javed Akhtar and Xiao Zhang and Liang Sun and Shenghui Guan and Xinyu Li and Guangming Chen and Jiaxin Liu and Hyeon-Nae Jeon and Min Sung Kim and Kyoung Tai No and Guanyu Wang|article|MAO2021103052|||iScience|25890042|9|24||||||||||||||||||||||||||||||1894795485|42
SAICSIT '20|Cape Town, South Africa|Data Wrangling, Data Cleaning, Smart City Data, Open Data|12|198–209|Conference of the South African Institute of Computer Scientists and Information Technologists 2020|South Africa (S.A.) is currently facing economic and social challenges that could benefit from the implementation of international smart city guidelines. Crucial to transforming a city into a smart city is the collection and access to reliable data. One of the main problems experienced by S.A. cities is the limited access to data, resulting from a traditionally fragmented approach to data collection, sharing and use. Crime-related data is one of the most commonly collected datasets in smart cities. In S.A., crime data is predominantly collected by the S.A. Police Services (SAPS) and security companies. While the latter are not readily available for public use, SAPS crime data is consolidated and disseminated at the national level. Initial data exploration, however, shows that temporal, spatial and structural inconsistencies in the data limits the usefulness of available crime data. In this study, the inconsistencies in SAPS crime data are summarised, and standard data wrangling techniques are implemented and evaluated to clean the data. The study proposes a data wrangling model for S.A. crime data. Furthermore, this study will further developments that could benefit S.A. cities in general as they transform into smart cities.|10.1145/3410886.3410913|https://doi.org/10.1145/3410886.3410913|New York, NY, USA|Association for Computing Machinery|9781450388474|2020|Data Wrangling for South African Smart City Crime Data|Kritzinger, A.K. and Calitz, A.P. and Westraadt, L.|inproceedings|10.1145/3410886.3410913|||||||||||||||||||||||||||||1895993246|42
ICISCAE 2021|Dalian, China|health services, biological characteristics, deep learning, High-reliability, blockchain|5|2514–2518|2021 4th International Conference on Information Systems and Computer Aided Education|A new type of high-reliability intelligent-sensing health service support platform and its key technologies are introduced in this article. By the technologies of automatic data collection, perceptual data removal, abnormal data detection, perceptual heterogeneous data identification, it is realizable to collect, analyze and process the health data of users. In order to solve the security problem in the process of data transmission, biometric identification and blockchain are used to realize the high-reliability transmission. At the end of the paper, a high-reliability intelligent-sensing health service support platform is built. And the implementation and service support process are expounded, indicating that the platform has high practical value.|10.1145/3482632.3487461|https://doi.org/10.1145/3482632.3487461|New York, NY, USA|Association for Computing Machinery|9781450390255|2021|Research on High-Reliability Intelligent-Sensing Health Service Support Platform and Key Technologies Based on Biometrics and Blockchain Security Technology|Tang, Xinzhong and Zhuang, Bing and Yao, Ying and Dong, Xuesong|inproceedings|10.1145/3482632.3487461|||||||||||||||||||||||||||||1896577623|42
||Training;Adaptation models;Data mining;Memory management;Detectors;Data models;Probability;machine learning;data stream;concept drift;big data;incremental learning;forgetting||2147-2152|2015 IEEE International Conference on Systems, Man, and Cybernetics|Mining massive data streams in real-time is one of the contemporary challenges for machine learning systems. Such a domain encompass many of difficulties hidden beneath the term of Big Data. We deal with massive, incoming information that must be processed on-the-fly, with lowest possible response delay. We are forced to take into account time, memory and quality constraints. Our models must be able to quickly process large collection of data and swiftly adapt themselves to occurring changes (shifts and drifts) in data streams. In this paper, we propose a novel version of simple, yet effective Naïve Bayes classifier for mining streams. We add a weighting module, that automatically assigns an importance factor to each object extracted from the stream. The higher the weight, the bigger influence given object exerts on the classifier training procedure. We assume, that our model works in the non-stationary environment with the presence of concept drift phenomenon. To allow our classifier to quickly adapt its properties to evolving data, we imbue it with forgetting principle implemented as weight decay. With each passing iteration, the level of importance of previous objects is decreased until they are discarded from the data collection. We propose an efficient sigmoidal function for modeling the forgetting rate. Experimental analysis, carried out on a number of large data streams with concept drift prove that our weighted Naïve Bayes classifier displays highly satisfactory performance in comparison with state-of-the-art stream classifiers.|10.1109/SMC.2015.375|||||2015|Weighted Naïve Bayes Classifier with Forgetting for Drifting Data Streams|Krawczyk, Bartosz and Wozniak, Michal|inproceedings|7379507||Oct|||||||||||||||||||||||||||1897934909|42
TEEM 2017|C\'{a}diz, Spain|Disruption, Governance, Higher Education Institutions, Learning Analytics|8||Proceedings of the 5th International Conference on Technological Ecosystems for Enhancing Multiculturality|Higher education institutions are at this stage, on the one hand, faced with challenges never seen before and, on the other hand, their action is moving very rapidly into digital learning spaces. These challenges are increasingly complex because of the global competition for resources, students and teachers. In addition, the amount of data produced inside and outside higher education institutions has grown exponentially, so more and more institutions are exploring the potential of Big Data to meet these challenges. In this context, higher education institutions and key stakeholders (students, teachers, and governance) can derive multiple benefits from learning analytics using different data analysis strategies to produce summative, real-time and predictive information and recommendations. However, it may be questioned whether institutions, academic administrative staff as well as including those with responsibility for governance, are prepared for learning analytics? As a response to the question raised in this paper is presented an extension of a disruptive conceptual approach to higher education, using information gathered by IoT and based on Big Data &amp; Cloud Computing and Learning Analytics analysis tools, with the main focus on the stakeholder governance.|10.1145/3144826.3145387|https://doi.org/10.1145/3144826.3145387|New York, NY, USA|Association for Computing Machinery|9781450353861|2017|Learning Analytics as a Core Component for Higher Education Disruption: Governance Stakeholder|Moreira, Fernando and Gon\c{c}alves, Ramiro and Martins, Jos\'{e} and Branco, Frederico and Au-Yong-Oliveira, Manuel|inproceedings|10.1145/3144826.3145387|37||||||||||||||||||||||||||||1898398710|42
||Meters;Low voltage;Substations;Network topology;Power supplies;Distribution networks;Transformers;Low voltage distribution network;Internet of things;topology identification;association analysis;topology verification||2226-2230|2020 5th Asia Conference on Power and Electrical Engineering (ACPEE)|This paper introduces a topology identification method of low-voltage distribution network based on data association analysis. The low-voltage distribution network to be identified is divided into the single distribution transformer power off station areas, multiple distribution transformer station areas caused by 10kV distribution line power outage and the distribution transformer areas without power interruption based on low-voltage distribution network blackout event, restoration power on event and geographic location information. In each type of station area, Tanimoto similarity coefficient is used to calculate the correlation and non-correlation between distribution transformer, branch box, meter box and smart meter in each group, so as to achieve the topology identification of the low-voltage distribution network. And then the identified topology can be verified by combining the topology verification rules of the same distribution transformer station area has the same of outage and live state, outage duration, geographical location, power supply radius and so on. Through the actual case, it is proved that the method proposed in this paper can solve the problems of large amount of calculation, inaccuracy of calculation results, and inability to verify based on the existing big data mining methods. It realizes the efficient and accurate identification of distribution transformer substation topology, and improves the information level and data quality of distribution network.|10.1109/ACPEE48638.2020.9136335|||||2020|Topology identification method of low voltage distribution network based on data association analysis|Zhichun, Yang and Yu, Shen and Fan, Yang and Yang, LEI and Lei, Su and Fangbin, Yan|inproceedings|9136335||June|||||||||||||||||||||||||||1899454648|42
||Big data, Child and adolescent mental health, Data linkage, Electronic healthcare records, Natural language processing||59-79|Shaping the Future of Child and Adolescent Mental Health|Over the past twodecades, healthcare providers across the world have adopted digital methods for capturing clinical and administrative information. Clinicians take contemporaneous records of their interactions with patients, so many health service providers have accrued vast repositories of longitudinally collected data. These data, coupled with advances in data extraction methods, computer processing power, and linkage to nonhealth public services data, now provide child and adolescent mental health researchers unique opportunities for tackling a broad range of clinical questions; especially those where the considerations of scale and generalizability make individually funded studies unaffordable. However, these “big” data have their limitations. Best practice requires clinicians, informaticians, and data scientists to work together, so assumptions over data quality or validity are not misplaced. This chapter explains why the evidence base for child and adolescent mental healthcare needs big data applications as well as conventional research, to move the field forward. This chapter provides illustrations of big data applications to child and adolescent mental healthcare, primarily from England and the United Kingdom, but also offers a section on the global perspective. This chapter also reviews the methodological strengths and weaknesses of big data and describes the ethical and governance implications for their use.|https://doi.org/10.1016/B978-0-323-91709-4.00005-6|https://www.sciencedirect.com/science/article/pii/B9780323917094000056||Academic Press|978-0-323-91709-4|2023|Chapter 3 - Clinical applications of big data to child and adolescent mental health care|Alice Wickersham and Johnny Downs|incollection|WICKERSHAM202359||||||||||Matthew Hodes and Petrus J. {De Vries}|||||||||||||||||||1901159689|42
||Big Data;Quality of service;Wireless networks;5G mobile communication;Resource management;Wireless sensor networks;5G multimedia big data wireless networks;ICN;NFV;SDN;optimal transmit power;statistical delay-bounded QoS;effective capacity;relay selection||1721-1738||The multimedia transmission represents a typical big data application in the fifth-generation (5G) wireless networks. However, supporting multimedia big data transmission over 5G wireless networks imposes many new and open challenges because multimedia big data services are both time-sensitive and bandwidth-intensive over time-varying wireless channels with constrained wireless resources. To overcome these difficulties, in this paper we propose the information-centric virtualization architectures for software-defined statistical delay-bounded quality of service (QoS) provisioning over 5G multimedia big data wireless networks. In particular, our proposed schemes integrate the three 5G-promising candidate techniques to guarantee the statistical delay-bounded QoS for multimedia big data transmissions: 1) information-centric network (ICN), to derive the optimal in-network caching locations for multimedia big data; 2) network functions virtualization (NFV), to abstract the PHY-layer infrastructures into several virtualized networks to derive the optimal multimedia data contents delivery paths; and 3) software-defined networks (SDNs), to dynamically reconfigure wireless resources allocation architectures through the SDN-control plane. Under our proposed architectures, to jointly optimize the implementations of NFV and SDN techniques under ICN architectures, we develop the three virtual network selection and transmit-power allocation schemes to: 1) maximize single user's effective capacity; 2) jointly optimize the aggregate effective capacity and allocation fairness over all users; and 3) coordinate non-cooperative gaming among all users, respectively. By simulations and numerical analyses, we show that our proposed architectures and schemes significantly outperform the other existing schemes in supporting the statistical delay-bounded QoS provisioning over the 5G multimedia big data wireless networks.|10.1109/JSAC.2019.2927088|||||2019|Information-Centric Virtualization for Software-Defined Statistical QoS Provisioning Over 5G Multimedia Big Data Wireless Networks|Zhang, Xi and Zhu, Qixuan|article|8756123||Aug|IEEE Journal on Selected Areas in Communications|15580008|8|37|||||||||||||||||||||||1909593080|996304853
|||14|85–98||Data and metadata in datasets experience many different kinds of change. Values are inserted, deleted or updated; rows appear and disappear; columns are added or repurposed, etc. In such a dynamic situation, users might have many questions related to changes in the dataset, for instance which parts of the data are trustworthy and which are not? Users will wonder: How many changes have there been in the recent minutes, days or years? What kind of changes were made at which points of time? How dirty is the data? Is data cleansing required? The fact that data changed can hint at different hidden processes or agendas: a frequently crowd-updated city name may be controversial; a person whose name has been recently changed may be the target of vandalism; and so on. We show various use cases that benefit from recognizing and exploring such change.We envision a system and methods to interactively explore such change, addressing the variability dimension of big data challenges. To this end, we propose a model to capture change and the process of exploring dynamic data to identify salient changes. We provide exploration primitives along with motivational examples and measures for the volatility of data. We identify technical challenges that need to be addressed to make our vision a reality, and propose directions of future work for the data management community.|10.14778/3282495.3282496|https://doi.org/10.14778/3282495.3282496||VLDB Endowment||2018|Exploring Change: A New Dimension of Data Analytics|Bleifu\ss{}, Tobias and Bornemann, Leon and Johnson, Theodore and Kalashnikov, Dmitri V. and Naumann, Felix and Srivastava, Divesh|article|10.14778/3282495.3282496||oct|Proc. VLDB Endow.|21508097|2|12|October 2018||||21100199855|0,946|Q1|134|246|598|12401|3759|566|5,50|50,41|United States|Northern America|2008-2020|Computer Science (miscellaneous) (Q1)|7,198|2.047|0.00891|1910705277|1216159931
||Systematics;Data analysis;Data integrity;Storage management;Government;Data models;Power grids;Date Evaluation Method;Power Grid;Value- Added Service||288-292|2021 5th International Conference on Power and Energy Engineering (ICPEE)|This research focuses on the data application basic technology for value-added services. In this research, the data quality and data value evaluation methods are studied. The data quality management system from data collection, storage, management and application is formed. Quality and power marketing data quality are analyzed and data value evaluation methods are established. As big data, artificial intelligence and other technologies continue to make breakthroughs, the value of data will become more and more important. Based on the State Grid’s full-service data, the full-scale data analysis across the professional, cross-business, and cross-system will promote the company’s power grid lean and intelligent management level, and will provide more value-added services for the company, government and society.|10.1109/ICPEE54380.2021.9662594|||||2021|A Study on the Business Data Evaluation Method of the Power Grid Value-Added Service|Lu, Jianfei and Li, Suxiu and Zhang, Xinsheng|inproceedings|9662594||Dec|||||||||||||||||||||||||||1910789527|42
MSR '22|Pittsburgh, Pennsylvania|data mining, agile development, open-source software|5|707–711|Proceedings of the 19th International Conference on Mining Software Repositories|Agile software development is nowadays a widely adopted practise in both open-source and industrial software projects. Agile teams typically heavily rely on issue management tools to document new issues and keep track of outstanding ones, in addition to storing their technical details, effort estimates, assignment to developers, and more. Previous work utilised the historical information stored in issue management systems for various purposes; however, when researchers make their empirical data public, it is usually relevant solely to the study's objective. In this paper, we present a more holistic and versatile dataset containing a wealth of information on more than half a million issues from 44 open-source Agile software, making it well-suited to several research avenues, and cross-analyses therein, including effort estimation, issue prioritization, issue assignment and many more. We make this data publicly available on GitHub to facilitate ease of use, maintenance, and extensibility.|10.1145/3524842.3528029|https://doi.org/10.1145/3524842.3528029|New York, NY, USA|Association for Computing Machinery|9781450393034|2022|A Versatile Dataset of Agile Open Source Software Projects|Tawosi, Vali and Al-Subaihin, Afnan and Moussa, Rebecca and Sarro, Federica|inproceedings|10.1145/3524842.3528029|||||||||||||||||||||||||||||1912257930|42
||databases, Deduplication, validation, clustering|27|||The massive volumes of data in biological sequence databases provide a remarkable resource for large-scale biological studies. However, the underlying data quality of these resources is a critical concern. A particular challenge is duplication, in which multiple records have similar sequences, creating a high level of redundancy that impacts database storage, curation, and search. Biological database deduplication has two direct applications: for database curation, where detected duplicates are removed to improve curation efficiency, and for database search, where detected duplicate sequences may be flagged but remain available to support analysis.Clustering methods have been widely applied to biological sequences for database deduplication. Since an exhaustive all-by-all pairwise comparison of sequences cannot scale for a high volume of data, heuristic approaches have been recruited, such as the use of simple similarity thresholds. In this article, we present a comparison between CD-HIT and UCLUST, the two best-known clustering tools for sequence database deduplication. Our contributions include a detailed assessment of the redundancy remaining after deduplication, application of standard clustering evaluation metrics to quantify the cohesion and separation of the clusters generated by each method, and a biological case study that assesses intracluster function annotation consistency to demonstrate the impact of these factors on a practical application of the sequence clustering methods. Our results show that the trade-off between efficiency and accuracy becomes acute when low threshold values are used and when cluster sizes are large. This evaluation leads to practical recommendations for users for more effective uses of the sequence clustering tools for deduplication.|10.1145/3131611|https://doi.org/10.1145/3131611|New York, NY, USA|Association for Computing Machinery||2018|Comparative Analysis of Sequence Clustering Methods for Deduplication of Biological Databases|Chen, Qingyu and Wan, Yu and Zhang, Xiuzhen and Lei, Yang and Zobel, Justin and Verspoor, Karin|article|10.1145/3131611|17|jan|J. Data and Information Quality|19361955|3|9|September 2017||||||||||||||||||||||1914591078|833754770
||Distributed databases;Big data;Lattices;Algorithm design and analysis;Partitioning algorithms;Knowledge discovery;Functional dependencies;Discovering functional dependencies;Knowledge discovery;Big data||426-431|2016 IEEE Second International Conference on Multimedia Big Data (BigMM)|Functional dependencies (FDs) represent potentially novel and interesting patterns existent in relational databases. The discovery of functional dependencies has a wide range of applications such as database design, knowledge discovery, data quality assessment, etc. There has been growing interest in the problem of functional dependencies discovery in the last ten years. However, existing functional dependencies discovery algorithms are mainly applied to centralized small data. It is far more challenging to discover functional dependencies from big data. In this paper, we propose an efficient functional dependencies discovery algorithm, for mining functional dependencies from distributed big data. We prune candidate FDs at each node by local fragmented data and batch verify candidate FDs in parallel. Load balance is taken into account when discovering functional dependencies. Experiments show that the proposed algorithm is effective on real dataset and synthetic dataset.|10.1109/BigMM.2016.63|||||2016|Scalable Functional Dependencies Discovery from Big Data|Tu, Shouzhong and Huang, Minlie|inproceedings|7545062||April|||||||||||||||||||||||||||1914684258|42
||Big Data, Smart Data, Classification, Class noise, Label noise.||135-152||In any knowledge discovery process the value of extracted knowledge is directly related to the quality of the data used. Big Data problems, generated by massive growth in the scale of data observed in recent years, also follow the same dictate. A common problem affecting data quality is the presence of noise, particularly in classification problems, where label noise refers to the incorrect labeling of training instances, and is known to be a very disruptive feature of data. However, in this Big Data era, the massive growth in the scale of the data poses a challenge to traditional proposals created to tackle noise, as they have difficulties coping with such a large amount of data. New algorithms need to be proposed to treat the noise in Big Data problems, providing high quality and clean data, also known as Smart Data. In this paper, two Big Data preprocessing approaches to remove noisy examples are proposed: an homogeneous ensemble and an heterogeneous ensemble filter, with special emphasis in their scalability and performance traits. The obtained results show that these proposals enable the practitioner to efficiently obtain a Smart Dataset from any Big Data classification problem.|https://doi.org/10.1016/j.ins.2018.12.002|https://www.sciencedirect.com/science/article/pii/S0020025518309460||||2019|Enabling Smart Data: Noise filtering in Big Data classification|Diego García-Gil and Julián Luengo and Salvador García and Francisco Herrera|article|GARCIAGIL2019135|||Information Sciences|00200255||479|||||15134|1,524|Q1|184|928|2184|40007|17554|2168|7,89|43,11|United States|Northern America|1968-2021|Artificial Intelligence (Q1); Computer Science Applications (Q1); Control and Systems Engineering (Q1); Information Systems and Management (Q1); Software (Q1); Theoretical Computer Science (Q1)|44,038|6.795|0.04908|1914972496|1633962588
BDIOT2017|London, United Kingdom|Industrial Big Data, Framework, Project Selection, Project Prioritization, Manufacturing|5|6–10|Proceedings of the International Conference on Big Data and Internet of Thing|The application of Big Data Techniques (BDT) in discrete manufacturing appears to be very promising, considering lighthouse projects in this area. In general, the goal is to collect all data from manufacturing systems comprehensively, in order to enable new findings and decision support by means of appropriate Industrial Big Data (IBD) analysis procedures. However, due to limited human and economic resources, potential IBD projects need to get prioritized -- in the best case according to their cost-benefit ratio. Available methods for this purpose are insufficient, due to their limited ability to be operationalized, error-proneness, and lack of scientific evidence. In this paper, we discuss how cost-benefit-analysis frameworks can be applied to the preliminary selection of production use cases for the implementation of BDT in larger production systems. It supports the use case selection process from information about production needs, available BDT, and given condition(s) per use case. This concept paper attempts to consolidate the hitherto fragmented discourse on how to prioritize IBD projects, evaluates the challenges of prioritization in this field, and presents a prioritization concept to overcome these challenges.|10.1145/3175684.3175687|https://doi.org/10.1145/3175684.3175687|New York, NY, USA|Association for Computing Machinery|9781450354301|2017|A Data-Based Method for Industrial Big Data Project Prioritization|Kuschicke, Felix and Thiele, Thomas and Meisen, Tobias and Jeschke, Sabina|inproceedings|10.1145/3175684.3175687|||||||||||||||||||||||||||||1920375938|42
|||2|431–432|Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice|||https://doi.org/10.1145/3447404.3447429|New York, NY, USA|Association for Computing Machinery|9781450390293|2021|Ethics in Automotive User Interface|McMenemy, David|inbook|10.1145/3447404.3447429|||||||||1||||||||||||||||||||1921337569|42
||COVID 19, tracking, society, technology, privacy||1018-1026||Since the start of the Coronavirus disease 2019 (COVID-19) governments and health authorities across the world have find it very difficult in controlling infections. Digital technologies such as artificial intelligence (AI), big data, cloud computing, blockchain and 5G have effectively improved the efficiency of efforts in epidemic monitoring, virus tracking, prevention, control and treatment. Surveillance to halt COVID-19 has raised privacy concerns, as many governments are willing to overlook privacy implications to save lives. The purpose of this paper is to conduct a focused Systematic Literature Review (SLR), to explore the potential benefits and implications of using digital technologies such as AI, big data and cloud to track COVID-19 amongst people in different societies. The aim is to highlight the risks of security and privacy to personal data when using technology to track COVID-19 in societies and identify ways to govern these risks. The paper uses the SLR approach to examine 40 articles published during 2020, ultimately down selecting to the most relevant 24 studies. In this SLR approach we adopted the following steps; formulated the problem, searched the literature, gathered information from studies, evaluated the quality of studies, analysed and integrated the outcomes of studies while concluding by interpreting the evidence and presenting the results. Papers were classified into different categories such as technology use, impact on society and governance. The study highlighted the challenge for government to balance the need of what is good for public health versus individual privacy and freedoms. The findings revealed that although the use of technology help governments and health agencies reduce the spread of the COVID-19 virus, government surveillance to halt has sparked privacy concerns. We suggest some requirements for government policy to be ethical and capable of commanding the trust of the public and present some research questions for future research.|https://doi.org/10.1016/j.procs.2021.01.281|https://www.sciencedirect.com/science/article/pii/S1877050921003306||||2021|The challenge of privacy and security when using technology to track people in times of COVID-19 pandemic|Hermanus J Smidt and Osden Jokonya|article|SMIDT20211018|||Procedia Computer Science|18770509||181||CENTERIS 2020 - International Conference on ENTERprise Information Systems / ProjMAN 2020 - International Conference on Project MANagement / HCist 2020 - International Conference on Health and Social Care Information Systems and Technologies 2020, CENTERIS/ProjMAN/HCist 2020|||19700182801|0,334|-|76|1907|6483|38511|13722|6359|2,09|20,19|Netherlands|Western Europe|2010-2020|Computer Science (miscellaneous)||||1921663889|2108686752
WSDM '22|Virtual Event, AZ, USA|tip of the tongue known item retrieval, known item retrieval|9|48–56|Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining|The tip of the tongue known-item retrieval (TOT-KIR) task involves the 'one-off' retrieval of an item for which a user cannot recall a precise identifier. The emergence of several online communities where users pose known-item queries to other users indicates the inability of existing search systems to answer such queries. Research in this domain is hampered by the lack of large, open or realistic datasets. Prior datasets relied on either annotation by crowd workers, which can be expensive and time-consuming, or generating synthetic queries, which can be unrealistic. Additionally, small datasets make the application of modern (neural) retrieval methods unviable, since they require a large number of data-points. In this paper, we collect the largest dataset yet with 15K query-item pairs in two domains, namely, Movies and Books, from an online community using heuristics, rendering expensive annotation unnecessary while ensuring that queries are realistic. We show that our data collection method is accurate by conducting a data study. We further demonstrate that methods like BM25 fall short of answering such queries, corroborating prior research. The size of the dataset makes neural methods feasible, which we show outperforms lexical baselines, indicating that neural/dense retrieval is superior for the TOT-KIR task.|10.1145/3488560.3498421|https://doi.org/10.1145/3488560.3498421|New York, NY, USA|Association for Computing Machinery|9781450391320|2022|'It's on the Tip of My Tongue': A New Dataset for Known-Item Retrieval|Bhargav, Samarth and Sidiropoulos, Georgios and Kanoulas, Evangelos|inproceedings|10.1145/3488560.3498421|||||||||||||||||||||||||||||1923140153|42
||Cloud computing;Computer science;Big data;Computers;Education;Virtual machining;problem-based learning;constructive alignment;data intensive systems;data science||575-581|2015 IEEE 7th International Conference on Cloud Computing Technology and Science (CloudCom)|Constructive alignment has been shown to elicit higher levels of learning among students. Problem-based Learning is one of the forms of constructive alignment often used in medicine and engineering education. We have applied a PBL-based approach to a graduate (master) course in Data Intensive Systems taught simultaneously through a video link at two universities in Europe and USA. Application of standardized measuring methodology shows inconclusive impact of PBL approach on students' learning. We present survey results and analyze major factors that could have lead to inconclusive result, including: low data quality, general applicability of constructivism in computer science, and issues with hybrid classroom and alternative laboratory environments. Finally, we discuss reversed classroom method as a potential solution to the issues encountered.|10.1109/CloudCom.2015.70|||||2015|Experience with Problem-Based Learning in a Hybrid Classroom|Wiktorski, Tomasz and Hacker, Thomas and Hansen, Raymond A. and Rodgers, Gregory|inproceedings|7396216||Nov|||||||||||||||||||||||||||1927217882|42
||Big Data, Covid-19, systematic literature review, management||1746-1755||2020 was globally greatly affected by the Covid-19 pandemic caused by SARS-CoV-2, which is still today impacting and profoundly changing life globally for people but also for firms. In this context, the need for timely and accurate information has become vital in every area of business management. The spread of the Covid-19 global pandemic has generated an exponential increase and extraordinary volume of data. In this domain, Big Data is one of the digital innovation technologies that can support business organizations during these complex times. Based on these considerations, the aim of this paper is to analyze the managerial literature concerning the issue of Big Data in the management of the Covid-19 pandemic through a systematic literature review. The results show a fundamental role of Big Data in pandemic management for businesses. The paper also provides managerial and theoretical implications.|https://doi.org/10.1016/j.procs.2022.01.375|https://www.sciencedirect.com/science/article/pii/S1877050922003842||||2022|The role of Big Data in the business challenge of Covid-19: a systematic literature review in managerial studies|Michela Piccarozzi and Barbara Aquilani|article|PICCAROZZI20221746|||Procedia Computer Science|18770509||200||3rd International Conference on Industry 4.0 and Smart Manufacturing|||19700182801|0,334|-|76|1907|6483|38511|13722|6359|2,09|20,19|Netherlands|Western Europe|2010-2020|Computer Science (miscellaneous)||||1927506490|2108686752
SSDBM 2021|Tampa, FL, USA|Pattern Detection, Steel Industry., Missing Data, Data Quality|6|214–219|33rd International Conference on Scientific and Statistical Database Management|Missing data (MD) is a prevalent problem and can negatively affect the trustworthiness of data analysis. In industrial use cases, faulty sensors or errors during data integration are common causes for systematically missing values. The majority of MD research deals with imputation, i.e., the replacement of missing values with “best guesses”. Most imputation methods require missing values to occur independently, which is rarely the case in industry. Thus, it is necessary to identify missing data patterns (i.e., systematically missing values) prior to imputation (1) to understand the cause of the missingness, (2) to gain deeper insight into the data, and (3) to choose the proper imputation technique. However, in literature, there is a wide varity of MD patterns without a common formalization. In this paper, we introduce the first formal definition of MD patterns. Building on this theory, we developed a systematic approach on how to automatically detect MD patterns in industrial data. The approach has been developed in cooperation with voestalpine Stahl GmbH, where we applied it to real-world data from the steel industry and demonstrated its efficacy with a simulation study.|10.1145/3468791.3468841|https://doi.org/10.1145/3468791.3468841|New York, NY, USA|Association for Computing Machinery|9781450384131|2021|Missing Data Patterns: From Theory to an Application in the Steel Industry|"\"Bechny, Michal and Sobieczky, Florian and Zeindl, J\"\"{u}rgen and Ehrlinger, Lisa\""|inproceedings|10.1145/3468791.3468841|||||||||||||||||||||||||||||1929342334|42
||Sensors;Task analysis;Wireless sensor networks;Resource management;Data integrity;Wireless communication;Mobile handsets;Mobile crowdsensing (MCS);task allocation;wireless sensor networks (WSNs)||78406-78420||Wireless sensor networks (WSNs) and mobile crowdsensing (MCS) are two important paradigms in urban dynamic sensing. In both sensing paradigms, task allocation is a significant problem, which may affect the completion quality of sensing tasks. In this paper, we give a survey of task allocation in WSNs and MCS from the contrastive perspectives in terms of data quality and sensing cost, which help to better understand related objectives and strategies. We first analyze the different characteristics of two sensing paradigms, which may lead to difference in task allocation issues or strategies. Then, we present some common issues in task allocation with objectives in data quality and sensing cost. Furthermore, we provide reviews of unique task allocation issues in MCS according to its new characteristics. Finally, we identify some potential opportunities for the future research.|10.1109/ACCESS.2019.2896226|||||2019|A Survey of Task Allocation: Contrastive Perspectives From Wireless Sensor Networks and Mobile Crowdsensing|Guo, Wenzhong and Zhu, Weiping and Yu, Zhiyong and Wang, Jiangtao and Guo, Bin|article|8660441|||IEEE Access|21693536||7|||||21100374601|0,587|Q1|127|18036|24267|771081|116691|24200|4,48|42,75|United States|Northern America|2013-2020|Computer Science (miscellaneous) (Q1); Engineering (miscellaneous) (Q1); Materials Science (miscellaneous) (Q2)|105,968|3.367|0.15396|1931885721|1905633267
||||713-716|||https://doi.org/10.1053/j.ajkd.2017.04.008|https://www.sciencedirect.com/science/article/pii/S0272638617306340||||2017|The China Kidney Disease Network (CK-NET): “Big Data—Big Dreams”|Rajiv Saran and Diane Steffick and Jennifer Bragg-Gresham|article|SARAN2017713|||American Journal of Kidney Diseases|02726386|6|69|||||||||||||||||||||||1935542189|54349756
ICCAI '22|Tianjin, China|Supply Chain, High-performance, Federated Learning, Block chain|5|241–245|Proceedings of the 8th International Conference on Computing and Artificial Intelligence|Supply chain is an important commercial pattern which used in industry, agriculture and service widely. However different parties have different information even though they are all in the same supply chain. That would results in information silo and the extra increasing on cost. On one hand, information could not transfer to other parties for itself benefits or intellectual property. On the other hand, other parties want to obtain supplementary information in order to make equitable trade and lower trade risk. A skillful method would be designed and employed for solve above problems in this paper. Federated Learning technology was introduced in order to solve the puzzle between information privacy and information sharing. In the concretely implement process, Vertical Federated Learning (VFL) model was constructed and trained for resolving several problems specially because the overlap of parties are more but the features are small in supply chain forward. Gradient descent methods and loss computation ways were also used in training process in order to advance the performance. A series of experiments were used to evaluate VFL in supply chain. Experimental results revealed that VFL used in supply chain was effective.|10.1145/3532213.3532249|https://doi.org/10.1145/3532213.3532249|New York, NY, USA|Association for Computing Machinery|9781450396110|2022|The High-Performance Solution with Federated Learning in Supply Chain System|Liu, Haitao and Ye, Bo and Qin, Zhi and Zhang, Jing|inproceedings|10.1145/3532213.3532249|||||||||||||||||||||||||||||1939100421|42
BuildSys '17|Delft, Netherlands|clustering, k-anonymity, occupancy privacy, convex optimization|10||Proceedings of the 4th ACM International Conference on Systems for Energy-Efficient Built Environments|The diffusion of low-cost sensor network technologies in smart buildings has enabled the collection of massive amounts of data regarding indoor environments, energy use and occupants, which, in turn, creates opportunities for knowledge- and information-based building management. Driven by benefits mutual to occupants, building managers, and research communities, there is a demand for data publication to foster more sophisticated and robust models and algorithms. Data in the original form, however, contains sensitive information about occupants' behavioral patterns, and publishing such data will violate individuals' privacy. The current practice on publishing building-related datasets relies primarily on policies for dictating which types of data can be published and agreements on the use of published data. This approach alone provides insufficient protection as it does not prevent privacy breaches from occurring in the first place.In this paper, we present PAD, which to our knowledge is the first system that provides a technological solution for publishing building related datasets in a privacy-preserving manner while maintaining high data quality. PAD is able to offer a strong anonymity guarantee by perturbing data records. The unique feature of PAD is that it offers an interface to incorporate dataset users into the loop of data publication and customizes the perturbation such that useful information in the dataset can be better retained. We study the efficacy of PAD using occupancy and plug load data collected in real buildings. The experiments demonstrate that PAD can achieve high resilience to privacy threats without introducing any significant data fidelity penalties.|10.1145/3137133.3137140|https://doi.org/10.1145/3137133.3137140|New York, NY, USA|Association for Computing Machinery|9781450355445|2017|PAD: Protecting Anonymity in Publishing Building Related Datasets|Jia, Ruoxi and Sangogboye, Fisayo Caleb and Hong, Tianzhen and Spanos, Costas and Kj\ae{}rgaard, Mikkel Baun|inproceedings|10.1145/3137133.3137140|4||||||||||||||||||||||||||||1939856129|42
||Big data stream analytic, Distributed evolving algorithm, Scalable real-time data mining, Parallel learning, Rule merging strategy||140-152||In this paper, a big data analytic framework is introduced for processing high-frequency data stream. This framework architecture is developed by combining an advanced evolving learning algorithm namely Parsimonious Network Fuzzy Inference System (PANFIS) with MapReduce parallel computation, where PANFIS has the capability of processing data stream in large volume. Big datasets are learnt chunk by chunk by processors in MapReduce environment and the results are fused by rule merging method, that reduces the complexity of the rules. The performance measurement has been conducted, and the results are showing that the MapReduce framework along with PANFIS evolving system helps to reduce the processing time around 22 percent in average in comparison with the PANFIS algorithm without reducing performance in accuracy.|https://doi.org/10.1016/j.procs.2018.10.514|https://www.sciencedirect.com/science/article/pii/S1877050918322233||||2018|Big Data Analytics based on PANFIS MapReduce|Choiru Za’in and Mahardhika Pratama and Edwin Lughofer and Meftahul Ferdaus and Qing Cai and Mukesh Prasad|article|ZAIN2018140|||Procedia Computer Science|18770509||144||INNS Conference on Big Data and Deep Learning|||19700182801|0,334|-|76|1907|6483|38511|13722|6359|2,09|20,19|Netherlands|Western Europe|2010-2020|Computer Science (miscellaneous)||||1945797521|2108686752
||Knowledge discovery via data analytics, Snail shell process model, KDDA, Big data analytics, Data-driven decision making||1-12||The rapid growth of big data environment imposes new challenges that traditional knowledge discovery and data mining process (KDDM) models are not adequately suited to address. We propose a snail shell process model for knowledge discovery via data analytics (KDDA) to address these challenges. We evaluate the utility of the KDDA process model using real-world analytic case studies at a global multi-media company. By comparing against traditional KDDM models, we demonstrate the need and relevance of the snail shell model, particularly in addressing faster turnaround and frequent model updates that characterize knowledge discovery in the big data environment.|https://doi.org/10.1016/j.dss.2016.07.003|https://www.sciencedirect.com/science/article/pii/S0167923616301233||||2016|A snail shell process model for knowledge discovery via data analytics|Yan Li and Manoj A. Thomas and Kweku-Muata Osei-Bryson|article|LI20161|||Decision Support Systems|01679236||91|||||21933|1,564|Q1|151|115|342|7152|2672|338|7,04|62,19|Netherlands|Western Europe|1985-2020|Arts and Humanities (miscellaneous) (Q1); Developmental and Educational Psychology (Q1); Information Systems (Q1); Information Systems and Management (Q1); Management Information Systems (Q1)|13,580|5.795|0.0081|1946154191|1234879127
||land-surface temperature, thermal field pattern, POI data, GIS, air temperature||102024||The effects of human activities and land cover changes on urban thermal field patterns are closely related to the land surface temperature (LST) and air temperature. At present, the number of studies on the quantitative relationship between these two indexes and the effect of the observational scale on their influence is insufficient. In this study, spatial analysis methods such as geographic modeling were combined with remote sensing images, meteorological data, and points of insert and used to investigate the composition and scale of the factors influencing the temperature field in Beijing. The results showed that there are differences in the positive and negative correlations between LST and air temperature and various influencing factors. At a spatial resolution of 90 m, LST had a strong linear relationship with the average air temperature. Indicators reflecting elements of human activity, such as buildings, roads, and entertainment, were easily measured by meteorological stations at a small scale, and the natural green space ratio could also be easily captured by satellite thermal sensors at small scales. These results have substantial implications for environmental impact assessments in areas experiencing an increasing urban heat island effect due to rapid urbanization.|https://doi.org/10.1016/j.scs.2020.102024|https://www.sciencedirect.com/science/article/pii/S2210670720300111||||2020|Analyzing the Influencing Factors of Urban Thermal Field Intensity Using Big-Data-Based GIS|Huang Huanchun and Yang Hailin and Deng Xin and Hao Cui and Liu Zhifeng and Liu Wei and Zeng Peng|article|HUANCHUN2020102024|||Sustainable Cities and Society|22106707||55|||||19700194105|1,645|Q1|61|705|1286|43818|10974|1284|8,53|62,15|Netherlands|Western Europe|2011-2020|Civil and Structural Engineering (Q1); Geography, Planning and Development (Q1); Renewable Energy, Sustainability and the Environment (Q1); Transportation (Q1)|14,373|7.587|0.01684|1946199373|1912866754
||Big Data, Hadoop Map/Reduce, Transitive Closure, Graph Component||161-190|Entity Information Life Cycle for Big Data|This chapter describes how a distributed processing environment such as Hadoop Map/Reduce can be used to support the CSRUD Life Cycle for Big Data. The examples shown in this chapter use the match key blocking described in Chapter 9 as a data partitioning strategy to perform ER on large datasets. The chapter includes an algorithm for finding the transitive closure of multiple match keys in a distributed processing environment using an iterative algorithm that minimizes the amount of local memory required for each processor. It also outlines a structure for an identity knowledge base in a distributed key-value data store, and describes strategies and distributed processing workflows for capture and update phases of the CSRUD life cycle using both record-based and attribute-based cluster-to-cluster structure projections.|https://doi.org/10.1016/B978-0-12-800537-8.00010-7|https://www.sciencedirect.com/science/article/pii/B9780128005378000107|Boston|Morgan Kaufmann|978-0-12-800537-8|2015|Chapter 10 - CSRUD for Big Data|John R. Talburt and Yinle Zhou|incollection|TALBURT2015161||||||||||John R. Talburt and Yinle Zhou|||||||||||||||||||1947622435|42
||Data auction mechanism, Anti-collusion, Smart contract, Blockchain, Ethereum||386-409||Due to the uncertainty of the value of big data, it is difficult to directly give a reasonable price for big data. Auction is an effective method of distributing goods to the bidder with the highest valuation. Hence, the use of auction strategy can not only guarantee the interests of data sellers, but also conform to market principles. However, existing data auction mechanisms are centralized. It is hard to build trust among sellers, buyers and auctioneers. An open and anonymous online environment may cause entities involved in data auctions to collude to manipulate the results of data auctions. This will cause the price of auction data to fail to reach a fair and truthful level. Therefore, the first anti-collusion data auction mechanism based on smart contract is proposed. Through a well-designed anti-collusion data auction algorithm, mutual distrust and rational buyers and sellers safely participate in the data auction without a trusted third party. The data auction mechanism designed in the smart contract can effectively prevent collusion and realize the fairness and truthfulness of data auction. The webpack in the Truffle Boxes is used to implement the data auction mechanism, and the anti-collusion property of the mechanism has been verified. The source code of the smart contract has been uploaded to GitHub.|https://doi.org/10.1016/j.ins.2020.10.053|https://www.sciencedirect.com/science/article/pii/S0020025520310458||||2021|Anti-collusion data auction mechanism based on smart contract|Wei Xiong and Li Xiong|article|XIONG2021386|||Information Sciences|00200255||555|||||15134|1,524|Q1|184|928|2184|40007|17554|2168|7,89|43,11|United States|Northern America|1968-2021|Artificial Intelligence (Q1); Computer Science Applications (Q1); Control and Systems Engineering (Q1); Information Systems and Management (Q1); Software (Q1); Theoretical Computer Science (Q1)|44,038|6.795|0.04908|1951807781|1633962588
|Tokyo, Japan||||||||New York, NY, USA|Association for Computing Machinery|9781450345569|2016|ICMI '16: Proceedings of the 18th ACM International Conference on Multimodal Interaction||proceedings|10.1145/2993148|||||||||||||||||||||||||||||1955457224|42
||Artificial intelligence, Big data, Data science, Personalized treatment, Radiotherapy, Shared decision making||43-54||Big data are no longer an obstacle; now, by using artificial intelligence (AI), previously undiscovered knowledge can be found in massive data collections. The radiation oncology clinic daily produces a large amount of multisource data and metadata during its routine clinical and research activities. These data involve multiple stakeholders and users. Because of a lack of interoperability, most of these data remain unused, and powerful insights that could improve patient care are lost. Changing the paradigm by introducing powerful AI analytics and a common vision for empowering big data in radiation oncology is imperative. However, this can only be achieved by creating a clinical data science community in radiation oncology. In this work, we present why such a community is needed to translate multisource data into clinical decision aids.|https://doi.org/10.1016/j.radonc.2020.09.054|https://www.sciencedirect.com/science/article/pii/S016781402030829X||||2020|From multisource data to clinical decision aids in radiation oncology: The need for a clinical data science community|Joanna Kazmierska and Andrew Hope and Emiliano Spezi and Sam Beddar and William H. Nailon and Biche Osong and Anshu Ankolekar and Ananya Choudhury and Andre Dekker and Kathrine Røe Redalen and Alberto Traverso|article|KAZMIERSKA202043|||Radiotherapy and Oncology|01678140||153||Physics Special Issue: ESTRO Physics Research Workshops on Science in Development|||||||||||||||||||||1956504174|1137641308
||Algorithm design and analysis;Reliability;Internet;Telecommunications;Big Data;Data models;truth finder;data reliability;entity attribute;data conflict||617-626||The Internet now is a large-scale platform with big data. Finding truth from a huge dataset has attracted extensive attention, which can maintain the quality of data collected by users and provide users with accurate and efficient data. However, current truth finder algorithms are unsatisfying, because of their low accuracy and complication. This paper proposes a truth finder algorithm based on entity attributes (TFAEA). Based on the iterative computation of source reliability and fact accuracy, TFAEA considers the interactive degree among facts and the degree of dependence among sources, to simplify the typical truth finder algorithms. In order to improve the accuracy of them, TFAEA combines the one-way text similarity and the factual conflict to calculate the mutual support degree among facts. Furthermore, TFAEA utilizes the symmetric saturation of data sources to calculate the degree of dependence among sources. The experimental results show that TFAEA is not only more stable, but also more accurate than the typical truth finder algorithms.|10.21629/JSEE.2017.03.21|||||2017|Truth finder algorithm based on entity attributes for data conflict solution|Xu, Xiaolong and Liu, Xinxin and Liu, Xiaoxiao and Sun, Yanfei|article|7978034||June|Journal of Systems Engineering and Electronics|10044132|3|28|||||||||||||||||||||||1956871332|12203926
|||12|1728–1739||While there has been significant focus on collecting and managing data feeds, it is only now that attention is turning to their quality. In this paper, we propose a principled approach to online data quality monitoring in a dynamic feed environment. Our goal is to alert quickly when feed behavior deviates from expectations.We make contributions in two distinct directions. First, we propose novel enhancements to permit a publish-subscribe approach to incorporate data quality modules into the DFMS architecture. Second, we propose novel temporal extensions to standard statistical techniques to adapt them to online feed monitoring for outlier detection and alert generation at multiple scales along three dimensions: aggregation at multiple time intervals to detect at varying levels of sensitivity; multiple lengths of data history for varying the speed at which models adapt to change; and multiple levels of monitoring delay to address lagged data arrival.FIT, or Feed Inspection Tool, is the result of a successful implementation of our approach. We present several case studies outlining the effective deployment of FIT in real applications along with user testimonials.|10.14778/2824032.2824070|https://doi.org/10.14778/2824032.2824070||VLDB Endowment||2015|FIT to Monitor Feed Quality|Dasu, Tamraparni and Shkapenyuk, Vladislav and Srivastava, Divesh and Swayne, Deborah F.|article|10.14778/2824032.2824070||aug|Proc. VLDB Endow.|21508097|12|8|August 2015||||21100199855|0,946|Q1|134|246|598|12401|3759|566|5,50|50,41|United States|Northern America|2008-2020|Computer Science (miscellaneous) (Q1)|7,198|2.047|0.00891|1959364395|1216159931
||Big data analytics, Affordance theory, Socio-technical approach, Organizational transformation, Organizational benefits, Affordance actualization||103121||Drawing on a revelatory case study, we identify four big data analytics (BDA) actualization mechanisms: (1) enhancing, (2) constructing, (3) coordinating, and (4) integrating, which manifest in actions on three socio-technical system levels, i.e., the structure, actor, and technology levels. We investigate the actualization of four BDA affordances at an automotive manufacturing company, i.e., establishing customer-centric marketing, provisioning vehicle-data-driven services, data-driven vehicle developing, and optimizing production processes. This study introduces a theoretical perspective to BDA research that explains how organizational actions contribute to actualizing BDA affordances. We further provide practical implications that can help guide practitioners in BDA adoption.|https://doi.org/10.1016/j.im.2018.10.007|https://www.sciencedirect.com/science/article/pii/S0378720617308522||||2020|Actualizing big data analytics affordances: A revelatory case study|Christian Dremel and Matthias M. Herterich and Jochen Wulf and Jan {vom Brocke}|article|DREMEL2020103121|||Information & Management|03787206|1|57||Big data and business analytics: A research agenda for realizing business value|||12303|2,147|Q1|162|130|248|11385|2482|246|8,94|87,58|Netherlands|Western Europe|1977-2020|Information Systems (Q1); Information Systems and Management (Q1); Management Information Systems (Q1)||||1961652026|1945939487
|||12|829–840||We conduct an experimental analysis of a dataset comprising over 27 million microtasks performed by over 70,000 workers issued to a large crowdsourcing marketplace between 2012--2016. Using this data---never before analyzed in an academic context---we shed light on three crucial aspects of crowdsourcing: (1) Task design---helping requesters understand what constitutes an effective task, and how to go about designing one; (2) Marketplace dynamics --- helping marketplace administrators and designers understand the interaction between tasks and workers, and the corresponding marketplace load; and (3) Worker behavior --- understanding worker attention spans, lifetimes, and general behavior, for the improvement of the crowdsourcing ecosystem as a whole.|10.14778/3067421.3067431|https://doi.org/10.14778/3067421.3067431||VLDB Endowment||2017|Understanding Workers, Developing Effective Tasks, and Enhancing Marketplace Dynamics: A Study of a Large Crowdsourcing Marketplace|Jain, Ayush and Sarma, Akash Das and Parameswaran, Aditya and Widom, Jennifer|article|10.14778/3067421.3067431||mar|Proc. VLDB Endow.|21508097|7|10|March 2017||||21100199855|0,946|Q1|134|246|598|12401|3759|566|5,50|50,41|United States|Northern America|2008-2020|Computer Science (miscellaneous) (Q1)|7,198|2.047|0.00891|1961678523|1216159931
||data representation, noise identification, influence space, ranking factor, Data pre-processing scheme|39|||The extensive growth of data quantity has posed many challenges to data analysis and retrieval. Noise and redundancy are typical representatives of the above-mentioned challenges, which may reduce the reliability of analysis and retrieval results and increase storage and computing overhead. To solve the above problems, a two-stage data pre-processing framework for noise identification and data reduction, called ARIS, is proposed in this article. The first stage identifies and removes noises by the following steps: First, the influence space (IS) is introduced to elaborate data distribution. Second, a ranking factor (RF) is defined to describe the possibility that the points are regarded as noises, then, the definition of noise is given based on RF. Third, a clean dataset (CD) is obtained by removing noise from the original dataset. The second stage learns representative data and realizes data reduction. In this process, CD is divided into multiple small regions by IS. Then the reduced dataset is formed by collecting the representations of each region. The performance of ARIS is verified by experiments on artificial and real datasets. Experimental results show that ARIS effectively weakens the impact of noise and reduces the amount of data and significantly improves the accuracy of data analysis within a reasonable time cost range.|10.1145/3522592|https://doi.org/10.1145/3522592|New York, NY, USA|Association for Computing Machinery||2022|ARIS: A Noise Insensitive Data Pre-Processing Scheme for Data Reduction Using Influence Space|Cai, Jianghui and Yang, Yuqing and Yang, Haifeng and Zhao, Xujun and Hao, Jing|article|10.1145/3522592|110|jul|ACM Trans. Knowl. Discov. Data|15564681|6|16|December 2022||||5800173377|0,728|Q1|59|71|169|3729|816|168|4,54|52,52|United States|Northern America|2007-2020|Computer Science (miscellaneous) (Q1)|1,740|2.713|0.00224|1964119004|1302859451
||Big data, Astronomy, Statistical challenges, Astronomical data analysis, Platforms for big data process||29-58|Big Data in Astronomy|Large digital sky surveys are becoming the dominant source of data in astronomy. There are more than 100 terabytes of data in major archives, and that amount is growing rapidly. A typical sky survey archive has approximately 10 terabytes of image data and a billion detected sources (stars, galaxies, quasars, etc.), with hundreds of measured attributes per source. These surveys span the full range of wavelengths, radio through gamma ray, yet they are just a taste of the much larger datasets to come. Yearly advances in electronics bring new instruments that double the amount of data collected each year and lead to the exponential growth of information in astronomy. Thus, datasets that are orders of magnitude larger, more complex, and more homogeneous than in the past are on the horizon. In comparison, the size of the human genome is about 1 gigabyte and that of the Library of Congress is about 20 terabytes. Truly, astronomy has come to the big data era.|https://doi.org/10.1016/B978-0-12-819084-5.00010-9|https://www.sciencedirect.com/science/article/pii/B9780128190845000109||Elsevier|978-0-12-819084-5|2020|2 - Fundamentals of big data in radio astronomy|Jiale Lei and Linghe Kong|incollection|LEI202029||||||||||Linghe Kong and Tian Huang and Yongxin Zhu and Shenghua Yu|||||||||||||||||||1967710186|42
|||14|1962–1975||The proliferation of big data has brought an urgent demand for privacy-preserving data publishing. Traditional solutions to this demand have limitations on effectively balancing the tradeoff between privacy and utility of the released data. Thus, the database community and machine learning community have recently studied a new problem of relational data synthesis using generative adversarial networks (GAN) and proposed various algorithms. However, these algorithms are not compared under the same framework and thus it is hard for practitioners to understand GAN's benefits and limitations. To bridge the gaps, we conduct so far the most comprehensive experimental study that investigates applying GAN to relational data synthesis. We introduce a unified GAN-based framework and define a space of design solutions for each component in the framework, including neural network architectures and training strategies. We conduct extensive experiments to explore the design space and compare with traditional data synthesis approaches. Through extensive experiments, we find that GAN is very promising for relational data synthesis, and provide guidance for selecting appropriate design solutions. We also point out limitations of GAN and identify future research directions.|10.14778/3407790.3407802|https://doi.org/10.14778/3407790.3407802||VLDB Endowment||2020|Relational Data Synthesis Using Generative Adversarial Networks: A Design Space Exploration|Fan, Ju and Chen, Junyou and Liu, Tongyu and Shen, Yuwei and Li, Guoliang and Du, Xiaoyong|article|10.14778/3407790.3407802||sep|Proc. VLDB Endow.|21508097|12|13|August 2020||||21100199855|0,946|Q1|134|246|598|12401|3759|566|5,50|50,41|United States|Northern America|2008-2020|Computer Science (miscellaneous) (Q1)|7,198|2.047|0.00891|1971011032|1216159931
||software platforms, Wireless sensor networks|37|||Information and communication technologies (ICT) can be instrumental in progressing towards smarter city environments, which improve city services, sustainability, and citizens’ quality of life. Smart City software platforms can support the development and integration of Smart City applications. However, the ICT community must overcome current technological and scientific challenges before these platforms can be widely adopted. This article surveys the state of the art in software platforms for Smart Cities. We analyzed 23 projects concerning the most used enabling technologies, as well as functional and non-functional requirements, classifying them into four categories: Cyber-Physical Systems, Internet of Things, Big Data, and Cloud Computing. Based on these results, we derived a reference architecture to guide the development of next-generation software platforms for Smart Cities. Finally, we enumerated the most frequently cited open research challenges and discussed future opportunities. This survey provides important references to help application developers, city managers, system operators, end-users, and Smart City researchers make project, investment, and research decisions.|10.1145/3124391|https://doi.org/10.1145/3124391|New York, NY, USA|Association for Computing Machinery||2017|Software Platforms for Smart Cities: Concepts, Requirements, Challenges, and a Unified Reference Architecture|Santana, Eduardo Felipe Zambom and Chaves, Ana Paula and Gerosa, Marco Aurelio and Kon, Fabio and Milojicic, Dejan S.|article|10.1145/3124391|78|nov|ACM Comput. Surv.|03600300|6|50|November 2018||||23038|2,079|Q1|163|112|365|15859|6978|365|19,16|141,60|United States|Northern America|1969-2020|Computer Science (miscellaneous) (Q1); Theoretical Computer Science (Q1)|10,790|10.282|0.01438|1971184042|1517405264
|||3|97–99||This forum provides a space to engage with the challenges of designing for intelligent algorithmic experiences. We invite articles that tackle the tensions between research and practice when integrating AI and UX design. We welcome interdisciplinary debate, artful critique, forward-looking research, case studies of AI in practice, and speculative design explorations. --- Juho Kim and Henriette Cramer, Editors|10.1145/3448888|https://doi.org/10.1145/3448888|New York, NY, USA|Association for Computing Machinery||2021|UX of Data: Making Data Available Doesn't Make It Usable|Koesten, Laura and Simperl, Elena|article|10.1145/3448888||mar|Interactions|10725520|2|28|March - April 2021||||4000148705|0,247|Q3|46|125|292|742|503|292|1,82|5,94|United States|Northern America|1994-1995, 1997, 2006-2020|Human-Computer Interaction (Q3)||||1972329576|2035703392
||Delays;Correlation;Quality of service;Jitter;Big data;Ports (Computers)||302-306|2016 12th International Conference on Network and Service Management (CNSM)|Managing QoS in a telecommunications network is a complex process. Effective network design and sizing in conjunction with load balancing, access control and traffic prioritization need to be orchestrated to optimize CAPEX investment, maximize network utilization and ensure that performance metrics and SLAs are met. This work shows how big data analytics were used to improve the management of QoS in an SDN by performing multi-dimensional analysis of Key Performance Indicators (KPIs) and applying machine learning algorithms to discover new correlations, perform root cause analysis and predict traffic congestion.|10.1109/CNSM.2016.7818437|||||2016|Applying big data technologies to manage QoS in an SDN|Jain, Shashwat and Khandelwal, Manish and Katkar, Ashutosh and Nygate, Joseph|inproceedings|7818437||Oct||2165963X|||||||||||||||||||||||||1977733364|982102026
BDCAT '17|Austin, Texas, USA|data analysis, compute cluster, big data, algorithms, resource management, population scale clustering, machine learning, in-memory computing|9|189–197|Proceedings of the Fourth IEEE/ACM International Conference on Big Data Computing, Applications and Technologies|Abstract Genomics data is unstructured and mostly stored on hard disks. It is both technically and culturally residing in big data domain due to the challenges of volume, velocity and variety. Huge volumes of data are generated from diverse sources in different formats and at a high frequency. Appropriate data models are required to accommodate these data formats for analysing and producing required results with a quick response time. Genomics data can be analysed for a variety of purposes. Existing genomics data analysis pipelines are disk I/O intensive and focus on optimizing data processing for individual analysis tasks. Intensive disk I/O operations and focus on optimizing individual analysis tasks are the biggest bottleneck of existing genomics analysis pipelines. Making any updates in genomics data require reading the whole data set again. In this paper, we present a genomics data analysis framework that addresses both the issues of existing genomics analysis pipelines. It reads unstructured genomics data from sources, transforms it in a structured format and stores this data into a NoSQL database. In this way, genomics data can be queried like any other data and an update in the genomics data does not require reading the whole data set. The framework also presents an efficient analysis pipeline for analysing the genomics data for a variety of purposes like genotype clustering, gene expression microarrays, chromosome variations or gene linkage analysis. A case study of genotype clustering is presented to demonstrate and evaluate the effectiveness of the presented framework. Our results show that the framework improves overall performance of the genomics data analysis pipeline by 49% from existing genomics data analysis pipelines. Furthermore, our approach is robust and is able sustain high performance with high system workloads.|10.1145/3148055.3148072|https://doi.org/10.1145/3148055.3148072|New York, NY, USA|Association for Computing Machinery|9781450355490|2017|Genomics Analyser: A Big Data Framework for Analysing Genomics Data|Abdullah, Tariq and Ahmet, Ahmed|inproceedings|10.1145/3148055.3148072|||||||||||||||||||||||||||||1983155630|42
dg.o '20|Seoul, Republic of Korea|Data-driven government, e-government, data governance, data stewardship, data enterprise|9|196–204|The 21st Annual International Conference on Digital Government Research|Comparable to the concept of a data(-driven) enterprise, the concept of a ‘government as data (-driven) enterprise’ is gaining popularity as a data strategy. However, what it implies is unclear. The objective of this paper is to clarify the concept of the government as data (-driven) enterprise, and identify the challenges and drivers that shape future data strategies. Drawing on literature review and expert interviews, this paper provides a rich understanding of the challenges for developing sound future government data strategies. Our analysis shows that two contrary data strategies dominate the debate. On the one hand is the data-driven enterprise strategy that focusses on collecting and using data to improve or enrich government processes and services (internal orientation). On the other hand, respondents point to the urgent need for governments to take on data stewardship, so other parties can use data to develop value for society (external orientation). Since these data strategies are not mutually exclusive, some government agencies will attempt to combine them, which is very difficult to pull off. Nonetheless, both strategies demand a more data minded culture. Moreover, the successful implementation of either strategy requires mature data governance – something most organisations still need to master. This research contributes by providing more depth to these strategies. The main challenge for policy makers is to decide on which strategy best fits their agency's roles and responsibilities and develop a shared roadmap with the external actors while at the same time mature on data governance.|10.1145/3396956.3396975|https://doi.org/10.1145/3396956.3396975|New York, NY, USA|Association for Computing Machinery|9781450387910|2020|Future Government Data Strategies: Data-Driven Enterprise or Data Steward? Exploring Definitions and Challenges for the Government as Data Enterprise|van Donge, W. and Bharosa, N. and Janssen, M. F. W. H. A.|inproceedings|10.1145/3396956.3396975|||||||||||||||||||||||||||||1983295549|42
||Big data analytics, Big data analytics architecture, Big data analytics capabilities, Business value of information technology (IT), Health care||3-13||To date, health care industry has not fully grasped the potential benefits to be gained from big data analytics. While the constantly growing body of academic research on big data analytics is mostly technology oriented, a better understanding of the strategic implications of big data is urgently needed. To address this lack, this study examines the historical development, architectural design and component functionalities of big data analytics. From content analysis of 26 big data implementation cases in healthcare, we were able to identify five big data analytics capabilities: analytical capability for patterns of care, unstructured data analytical capability, decision support capability, predictive capability, and traceability. We also mapped the benefits driven by big data analytics in terms of information technology (IT) infrastructure, operational, organizational, managerial and strategic areas. In addition, we recommend five strategies for healthcare organizations that are considering to adopt big data analytics technologies. Our findings will help healthcare organizations understand the big data analytics capabilities and potential benefits and support them seeking to formulate more effective data-driven analytics strategies.|https://doi.org/10.1016/j.techfore.2015.12.019|https://www.sciencedirect.com/science/article/pii/S0040162516000500||||2018|Big data analytics: Understanding its capabilities and potential benefits for healthcare organizations|Yichuan Wang and LeeAnn Kung and Terry Anthony Byrd|article|WANG20183|||Technological Forecasting and Social Change|00401625||126|||||14704|2,226|Q1|117|448|1099|35581|10127|1061|9,01|79,42|United States|Northern America|1970-2020|Applied Psychology (Q1); Business and International Management (Q1); Management of Technology and Innovation (Q1)|21,116|8.593|0.02416|1983835065|1949868303
AIES '21|Virtual Event, USA|artificial intelligence, public perception|11|627–637|Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society|As the influence and use of artificial intelligence (AI) have grown and its transformative potential has become more apparent, many questions have been raised regarding the economic, political, social, and ethical implications of its use. Public opinion plays an important role in these discussions, influencing product adoption, commercial development, research funding, and regulation. In this paper we present results of an in-depth survey of public opinion of artificial intelligence conducted with 10,005 respondents spanning eight countries and six continents. We report widespread perception that AI will have significant impact on society, accompanied by strong support for the responsible development and use of AI, and also characterize the public's sentiment towards AI with four key themes (exciting, useful, worrying, and futuristic) whose prevalence distinguishes response to AI in different countries.|10.1145/3461702.3462605|https://doi.org/10.1145/3461702.3462605|New York, NY, USA|Association for Computing Machinery|9781450384735|2021|Exciting, Useful, Worrying, Futuristic: Public Perception of Artificial Intelligence in 8 Countries|Kelley, Patrick Gage and Yang, Yongwei and Heldreth, Courtney and Moessner, Christopher and Sedley, Aaron and Kramm, Andreas and Newman, David T. and Woodruff, Allison|inproceedings|10.1145/3461702.3462605|||||||||||||||||||||||||||||1984941546|42
||Data integrity;Pipelines;Data visualization;Big Data;Syntactics;Software;Python;Data Quality Management;Data Profiling;Data Quality Auditing;Python Package;Data Pipeline||1-4|2020 1st International Conference on Big Data Analytics and Practices (IBDAP)|Data Profiling and data quality management become a more significant part of data engineering, which an essential part of ensuring that the system delivers quality information to users. In the last decade, data quality was considered to need more managing. Especially in the big data era that the data comes from many sources, many data types, and an enormous amount. Thus it makes the managing of data quality is more difficult and complicated. The traditional system was unable to respond as needed. The data quality managing software for big data was developed but often found in a high-priced, difficult to customize as needed, and mostly provide as GUI, which is challenging to integrate with other systems. From this problem, we have developed an opensource package for data quality managing. By using Python programming language, Which is a programming language that is widely used in the scientific and engineering field today. Because it is a programming language that is easy to read syntax, small, and has many additional packages to integrate. The software developed here is called “Sakdas” this package has been divided into three parts. The first part deals with data profiling provide a set of data analyses to generate a data profile, and this profile will help to define the data quality rules. The second part deals with data quality auditing that users can set their own data quality rules for data quality measurement. The final part deals with data visualizing that provides data profiling and data auditing report to improve the data quality. The results of the profiling and auditing services, the user can specify both the form of a report for self-review. Or in the form of JSON for use in post-process automation.|10.1109/IBDAP50342.2020.9245455|||||2020|Sakdas: A Python Package for Data Profiling and Data Quality Auditing|Loetpipatwanich, Sakda and Vichitthamaros, Preecha|inproceedings|9245455||Sep.|||||||||||||||||||||||||||1987050411|42
||Maturity model, Network planning, Big data analytics, Airlines, Case study||101721||The evaluation, acquisition and use of newly available big data sources has become a major strategic and organizational challenge for airline network planners. We address this challenge by developing a maturity model for big data readiness for airline network planning. The development of the maturity model is grounded in literature, expert interviews and case study research involving nine airlines. Four airline business models are represented, namely full-service carriers, low-cost airlines, scheduled charter airlines and cargo airlines. The maturity model has been well received with seven change requests in the model development phase. The revised version has been evaluated as exhaustive and useful by airline network planners. The self-assessment of airlines revealed low to medium maturity for most domains. Organizational factors show the lowest average maturity, IT architecture the highest. Full-service carriers seem to be more mature than airlines with different business models.|https://doi.org/10.1016/j.jairtraman.2019.101721|https://www.sciencedirect.com/science/article/pii/S0969699718304988||||2020|Towards a maturity model for big data analytics in airline network planning|Iris Hausladen and Maximilian Schosser|article|HAUSLADEN2020101721|||Journal of Air Transport Management|09696997||82|||||20532|1,220|Q1|75|142|356|7377|1612|349|4,68|51,95|United Kingdom|Western Europe|1994-1995, 1997-2020|Law (Q1); Management, Monitoring, Policy and Law (Q1); Strategy and Management (Q1); Transportation (Q1)|4,929|4.134|0.00394|1987488231|1929604570
||Probabilistic logic;Uncertainty;Data integration;Data integrity;Redundancy;Scalability;Erbium;data integration;information retrieval;NoSQL databases;graph probabilistic dependencies;data science||27-36|2020 IEEE/ACM International Conference on Big Data Computing, Applications and Technologies (BDCAT)|The diversity and proliferation of Knowledge bases have made data integration one of the key challenges in the data science domain. The imperfect representations of entities, particularly in graphs, add additional challenges in data integration. Graph dependencies (GDs) were investigated in existing studies for the integration and maintenance of data quality on graphs. However, the majority of graphs contain plenty of duplicates with high diversity. Consequently, the existence of dependencies over these graphs becomes highly uncertain. In this paper, we proposed graph probabilistic dependencies (GPDs) to address the issue of uncertainty over these large-scale graphs with a novel class of dependencies for graphs. GPDs can provide a probabilistic explanation for dealing with uncertainty while discovering dependencies over graphs. Furthermore, a case study is provided to verify the correctness of the data integration process based on GPDs. Preliminary results demonstrated the effectiveness of GPDs in terms of reducing redundancies and inconsistencies over the benchmark datasets.|10.1109/BDCAT50828.2020.00028|||||2020|Large-scale Data Integration Using Graph Probabilistic Dependencies (GPDs)|Zada, Muhammad Sadiq Hassan and Yuan, Bo and Anjum, Ashiq and Azad, Muhammad Ajmal and Khan, Wajahat Ali and Reiff-Marganiec, Stephan|inproceedings|9302543||Dec|||||||||||||||||||||||||||1988545757|42
||Semantic-aware, Quality assessment, Image big data, IDSTH, SHR||53-65||Data quality (DQ) assessment is essential for realizing the promise of big data by judging the value of data in advance. Relevance, an indispensable dimension of DQ, focusing on “fitness for requirement”, can arouse the user’s interest in exploiting the data source. It has two-level evaluations: (1) the amount of data that meets the user’s requirements; (2) the matching degree of these relevant data. However, there lack works of DQ assessment at dimension of relevance, especially for unstructured image data which focus on semantic similarity. When we try to evaluate semantic relevance between an image data source and a query (requirement), there are three challenges: (1) how to extract semantic information with generalization ability for all image data? (2) how to quantify relevance by fusing the quantity of relevant data and the degree of similarity comprehensively? (3) how to improve assessing efficiency of relevance in a big data scenario by design of an effective architecture? To overcome these challenges, we propose a semantic-aware data quality assessment (SDQA) architecture which includes off-line analysis and on-line assessment. In off-line analysis, for an image data source, we first transform all images into hash codes using our improved Deep Self-taught Hashing (IDSTH) algorithm which can extract semantic features with generalization ability, then construct a graph using hash codes and restricted Hamming distance, next use our designed Semantic Hash Ranking (SHR) algorithm to calculate the importance score (rank) for each node (image), which takes both the quantity of relevant images and the degree of semantic similarity into consideration, and finally rank all images in descending order of score. During on-line assessment, we first convert the user’s query into hash codes using IDSTH model, then retrieve matched images to collate their importance scores, and finally help the user determine whether the image data source is fit for his requirement. The results on public dataset and real-world dataset show effectiveness, superiority and on-line efficiency of our SDQA architecture.|https://doi.org/10.1016/j.future.2019.07.063|https://www.sciencedirect.com/science/article/pii/S0167739X19302304||||2020|Semantic-aware data quality assessment for image big data|Yu Liu and Yangtao Wang and Ke Zhou and Yujuan Yang and Yifei Liu|article|LIU202053|||Future Generation Computer Systems|0167739X||102|||||12264|1,262|Q1|119|798|1935|36054|16877|1868|9,11|45,18|Netherlands|Western Europe|1984-2021|Computer Networks and Communications (Q1); Hardware and Architecture (Q1); Software (Q1)||||1989310282|562237118
NISS2020|Marrakech, Morocco|cost/benefit analysis, Data quality improvement project, data quality assessment and improvement, artificial neural network, cost of data quality|8||Proceedings of the 3rd International Conference on Networking, Information Systems &amp; Security|The technological evolution and the immensity of the data produced, circulated into company makes these data, the real capital of the companies to the detriment of the customers. The erroneous data put the knockout to relationships with customers, the company must address this problem and identify the quality projects on which it must make an effort. In this article, we will present an approach based on qualitative and quantitative analysis to help the decision-makers to target data by its impacts and complexities of process improvement. The Qualitative study will be a survey and a quantitative to learn from survey data to decide the prediction and the completeness of data.|10.1145/3386723.3387850|https://doi.org/10.1145/3386723.3387850|New York, NY, USA|Association for Computing Machinery|9781450376341|2020|Contribution of Artificial Neural Network in Predicting Completeness Through the Impact and Complexity of Its Improvement|Maqboul, Jaouad and Jaouad, Bouchaib Bounabat|inproceedings|10.1145/3386723.3387850|31||||||||||||||||||||||||||||1989966602|42
IDEAS '13|Barcelona, Spain|OLAP over big data, privacy of big data, big data posting, big data|6|198–203|Proceedings of the 17th International Database Engineering &amp; Applications Symposium|Recently, a great deal of interest for Big Data has risen, mainly driven from a widespread number of research problems strongly related to real-life applications and systems, such as representing, modeling, processing, querying and mining massive, distributed, large-scale repositories (mostly being of unstructured nature). Inspired by this main trend, in this paper we discuss three important aspects of Big Data research, namely OLAP over Big Data, Big Data Posting, and Privacy of Big Data. We also depict future research directions, hence implicitly defining a research agenda aiming at leading future challenges in this research field.|10.1145/2513591.2527071|https://doi.org/10.1145/2513591.2527071|New York, NY, USA|Association for Computing Machinery|9781450320252|2013|Big Data: A Research Agenda|Cuzzocrea, Alfredo and Sacc\`{a}, Domenico and Ullman, Jeffrey D.|inproceedings|10.1145/2513591.2527071|||||||||||||||||||||||||||||1991096497|42
|||13|635–647||In the Big Data era, truth discovery has served as a promising technique to solve conflicts in the facts provided by numerous data sources. The most significant challenge for this task is to estimate source reliability and select the answers supported by high quality sources. However, existing works assume that one data source has the same reliability on any kinds of entity, ignoring the possibility that a source may vary in reliability on different domains. To capture the influence of various levels of expertise in different domains, we integrate domain expertise knowledge to achieve a more precise estimation of source reliability. We propose to infer the domain expertise of a data source based on its data richness in different domains. We also study the mutual influence between domains, which will affect the inference of domain expertise. Through leveraging the unique features of the multi-truth problem that sources may provide partially correct values of a data item, we assign more reasonable confidence scores to value sets. We propose an integrated Bayesian approach to incorporate the domain expertise of data sources and confidence scores of value sets, aiming to find multiple possible truths without any supervision. Experimental results on two real-world datasets demonstrate the feasibility, efficiency and effectiveness of our approach.|10.1145/3177732.3177739|https://doi.org/10.1145/3177732.3177739||VLDB Endowment||2018|Domain-Aware Multi-Truth Discovery from Conflicting Sources|Lin, Xueling and Chen, Lei|article|10.1145/3177732.3177739||oct|Proc. VLDB Endow.|21508097|5|11|January 2018||||21100199855|0,946|Q1|134|246|598|12401|3759|566|5,50|50,41|United States|Northern America|2008-2020|Computer Science (miscellaneous) (Q1)|7,198|2.047|0.00891|1991388404|1216159931
||Data quality, Big data, Context-awareness, Data profiling, DQ assessment||548-562||Big data changed the way in which we collect and analyze data. In particular, the amount of available information is constantly growing and organizations rely more and more on data analysis in order to achieve their competitive advantage. However, such amount of data can create a real value only if combined with quality: good decisions and actions are the results of correct, reliable and complete data. In such a scenario, methods and techniques for the Data Quality assessment can support the identification of suitable data to process. If for traditional database numerous assessment methods are proposed, in the Big Data scenario new algorithms have to be designed in order to deal with novel requirements related to variety, volume and velocity issues. In particular, in this paper we highlight that dealing with heterogeneous sources requires an adaptive approach able to trigger the suitable quality assessment methods on the basis of the data type and context in which data have to be used. Furthermore, we show that in some situations it is not possible to evaluate the quality of the entire dataset due to performance and time constraints. For this reason, we suggest to focus the Data Quality assessment only on a portion of the dataset and to take into account the consequent loss of accuracy by introducing a confidence factor as a measure of the reliability of the quality assessment procedure. We propose a methodology to build a Data Quality adapter module, which selects the best configuration for the Data Quality assessment based on the user main requirements: time minimization, confidence maximization, and budget minimization. Experiments are performed by considering real data gathered from a smart city case study.|https://doi.org/10.1016/j.future.2018.07.014|https://www.sciencedirect.com/science/article/pii/S0167739X17329151||||2018|Context-aware data quality assessment for big data|Danilo Ardagna and Cinzia Cappiello and Walter Samá and Monica Vitali|article|ARDAGNA2018548|||Future Generation Computer Systems|0167739X||89|||||12264|1,262|Q1|119|798|1935|36054|16877|1868|9,11|45,18|Netherlands|Western Europe|1984-2021|Computer Networks and Communications (Q1); Hardware and Architecture (Q1); Software (Q1)||||1993017799|562237118
||Time series analysis;Signal processing algorithms;Laplace equations;Cost function;Electrocardiography;Finite difference methods;Big data;Finite difference matrix;Regularization;Biosignal processing;Big data analytics;Conjugate gradient;Discrete Laplace operator;Electrocardiogram;Heartbeat rate||1-6|2016 7th International Conference on Information, Intelligence, Systems & Applications (IISA)|In the biomedical analytics pipeline data preprocessing is the first and crucial step as subsequent results and visualization depend heavily on original data quality. However, the latter often contain a large number of outliers or missing values. Moreover, they may be corrupted by noise of unknown characteristics. This is in many cases aggravated by lack of sufficient information to construct a data cleaning mechanism. Regularization techniques remove erroneous values and complete missing ones while requiring little or no information regarding either data or noise dynamics. This paper examines the theory and practice of a regularization class based on finite differences and implemented through the conjugate gradient method. Moreover, it explores the connection of finite differences to the discrete Laplace operator. The results obtained from applying the proposed regularization techniques to heart rate time series from the MIT-BIH dataset are discussed.|10.1109/IISA.2016.7785346|||||2016|Regularizing large biosignals with finite differences|Drakopoulos, Georgios and Megalooikonomou, Vasileios|inproceedings|7785346||July|||||||||||||||||||||||||||1995149047|42
||Industry 4.0, interpretive structural modeling, digital manufacturing, barriers, MICMAC analysis||85-90||Industry 4.0 has enabled technological integration of cyber physical systems and internet based communication in manufacturing value creation processes. As of now, many people use it as a collective term for advanced technologies, i.e. advanced robotics, artificial intelligence, machine learning, big data analytics, cloud computing, smart sensors, internet of things, augmented reality, etc. This substantially improves flexibility, quality, productivity, cost, and customer satisfaction by transforming existing centralized manufacturing systems towards digital and decentralized one. Despite having potential benefits of industry 4.0, the organizations are facing typical obstacles and challenges in adopting new technologies and successful implementation in their business models. This paper aims to identify potential barriers which may hinder the implementation of industry 4.0 in manufacturing organizations. The identified barriers, through comprehensive literature review and on the basis of opinions collected from industry experts, are: poor value-chain integration, cyber-security challenges, uncertainty about economic benefits, lack of adequate skills in workforce, high investment requirements, lack of infrastructure, jobs disruptions, challenges in data management and data quality, lack of secure standards and norms, and resistance to change. Interpretive Structural Modeling (ISM) is used to establish relationships among these barriers to develop a hierarchical model and MICMAC analysis for further classification of identified barriers for better understanding. An analysis of driving and dependence of the barriers may help in clear understanding of these for successful implementation of Industry 4.0 practices in the organizations.|https://doi.org/10.1016/j.procir.2021.01.010|https://www.sciencedirect.com/science/article/pii/S2212827121000330||||2021|Analysis of Barriers to Industry 4.0 adoption in Manufacturing Organizations: an ISM Approach|Pramod Kumar and Jaiprakash Bhamu and Kuldip Singh Sangwan|article|KUMAR202185|||Procedia CIRP|22128271||98||The 28th CIRP Conference on Life Cycle Engineering, March 10 – 12, 2021, Jaipur, India|||21100243809|0,683|-|65|1456|3191|30451|8225|3145|2,40|20,91|Netherlands|Western Europe|2012-2020|Control and Systems Engineering; Industrial and Manufacturing Engineering||||1996015664|2127027836
||Databases, big data, metabolomics, metabolic phenotyping, data exchange, data warehousing||317-331|Metabolic Phenotyping in Personalized and Public Healthcare|This chapter explains the concept of big data and shows that the analytical data and related meta-data of metabolic phenotyping experiments fall into this category. The various databases that can be used to aid interpretation of such data, comprising general chemical and biochemical data, biochemical pathway specific data, analytical chemistry data to aid metabolite identification, and finally metabolic results, are discussed. The concept of data warehousing is explored in the context of metabolic data sets. Finally, the challenges for successful data storage, data exchange, and data interpretation are discussed.|https://doi.org/10.1016/B978-0-12-800344-2.00011-2|https://www.sciencedirect.com/science/article/pii/B9780128003442000112|Boston|Academic Press|978-0-12-800344-2|2016|Chapter 11 - From Databases to Big Data|Nadine Levin and Reza M. Salek and Christoph Steinbeck|incollection|LEVIN2016317||||||||||Elaine Holmes and Jeremy K. Nicholson and Ara W. Darzi and John C. Lindon|||||||||||||||||||1997438513|42
||Cognition;Resource description framework;Metadata;Data integrity;Tools;Iris;Complexity theory||49-56|2018 International Workshop on Big Data and Information Security (IWBIS)|Data quality is a major issue in the devel- opment of knowledge graphs. Data completeness is a key factor in data quality pertaining to how broad and deep is information contained in knowledge graphs. As for large- scale knowledge graphs (e.g., DBpedia, Wikidata), it is conceivable that given the vast amount of information contained in there, they may be complete for a wide range of topics, such as children of Joko Widodo, cantons of Switzerland, and presidents of Indonesia. Previous research has shown how one can augment knowledge graphs with statements about their completeness, stating which parts of data are complete. Such meta-information can be leveraged to check query completeness, that is, whether the answer returned by a query is complete. Yet, it is still unclear how such a check can be done in practice, especially when many completeness statements are involved. We devise implementation techniques to make completeness reasoning in the presence of large sets of completeness statements feasible, and experimentally evaluate their effectiveness in realistic settings based on the characteristics of real-world knowledge graphs.|10.1109/IWBIS.2018.8471712|||||2018|Comparing Index Structures for Completeness Reasoning|Darari, Fariz and Nutt, Werner and Razniewski, Simon|inproceedings|8471712||May|||||||||||||||||||||||||||1998555396|42
||Crowdsourcing;Artificial intelligence;Big data;Data warehouses;Decision support systems;Databases;Business;Big Data Lake;data warehouses;artificial intelligence;crowdsourcing;data governance;master data management;intelligent systems||70-73||"\"Daniel E. O'Leary examines the notion of the Big Data Lake and contrasts it with decision support-based data warehouses. In addition, some of the risks of the emerging Lake concept that ultimately require data governance are analyzed. O'Leary investigates using different AI and crowdsourcing (human intelligence) applications in that lake in order to integrate disparate data sources, facilitate master data management and analyze data quality. Although data governance often is not seen as a technology issue, it is seen as a critical component of making the Big Data Lake \"\"work\"\".\""|10.1109/MIS.2014.82|||||2014|Embedding AI and Crowdsourcing in the Big Data Lake|O'Leary, Daniel E.|article|6949519||Sep.|IEEE Intelligent Systems|19411294|5|29|||||||||||||||||||||||2002808559|1201856183
||Biomedical informatics, Personalized medicine, Big data analytics, Topological data analysis, TDA, Applied topology, Mapper, Persistent homology, Machine learning||104082||Significant technological advances made in recent years have shepherded a dramatic increase in utilization of digital technologies for biomedicine– everything from the widespread use of electronic health records to improved medical imaging capabilities and the rising ubiquity of genomic sequencing contribute to a “digitization” of biomedical research and clinical care. With this shift toward computerized tools comes a dramatic increase in the amount of available data, and current tools for data analysis capable of extracting meaningful knowledge from this wealth of information have yet to catch up. This article seeks to provide an overview of emerging mathematical methods with the potential to improve the abilities of clinicians and researchers to analyze biomedical data, but may be hindered from doing so by a lack of conceptual accessibility and awareness in the life sciences research community. In particular, we focus on topological data analysis (TDA), a set of methods grounded in the mathematical field of algebraic topology that seeks to describe and harness features related to the “shape” of data. We aim to make such techniques more approachable to non-mathematicians by providing a conceptual discussion of their theoretical foundations followed by a survey of their published applications to scientific research. Finally, we discuss the limitations of these methods and suggest potential avenues for future work integrating mathematical tools into clinical care and biomedical informatics.|https://doi.org/10.1016/j.jbi.2022.104082|https://www.sciencedirect.com/science/article/pii/S1532046422000983||||2022|Topological data analysis in biomedicine: A review|Yara Skaf and Reinhard Laubenbacher|article|SKAF2022104082|||Journal of Biomedical Informatics|15320464||130|||||||||||||||||||||||2005105174|568426795
||Artificial intelligence, Radiotherapy, Auto-segmentation, Auto-planning, Quality assurance||160-171||Radiotherapy is a discipline closely integrated with computer science. Artificial intelligence (AI) has developed rapidly over the past few years. With the explosive growth of medical big data, AI promises to revolutionize the field of radiotherapy through highly automated workflow, enhanced quality assurance, improved regional balances of expert experiences, and individualized treatment guided by multi-omics. In addition to independent researchers, the increasing number of large databases, biobanks, and open challenges significantly facilitated AI studies on radiation oncology. This article reviews the latest research, clinical applications, and challenges of AI in each part of radiotherapy including image processing, contouring, planning, quality assurance, motion management, and outcome prediction. By summarizing cutting-edge findings and challenges, we aim to inspire researchers to explore more future possibilities and accelerate the arrival of AI radiotherapy.|https://doi.org/10.1016/j.semcancer.2022.08.005|https://www.sciencedirect.com/science/article/pii/S1044579X22001912||||2022|Artificial intelligence in radiotherapy|Guangqi Li and Xin Wu and Xuelei Ma|article|LI2022160|||Seminars in Cancer Biology|1044579X||86|||||||||||||||||||||||2006346447|337903391
||Big Data;Quality of service;Time factors;Feature extraction;Computer crime;Computer architecture;big data, cyber security, adaptation, accuracy||81-86|2019 24th International Conference on Engineering of Complex Computer Systems (ICECCS)|Big Data Cyber Security Analytics (BDCA) leverages big data technologies for collecting, storing, and analyzing a large volume of security events data to detect cyber-attacks. Accuracy and response time, being the most important quality concerns for BDCA, are impacted by changes in security events data. Whilst it is promising to adapt a BDCA system's architecture to the changes in security events data for optimizing accuracy and response time, it is important to consider large search space of architectural configurations. Searching a large space of configurations for potential adaptation incurs an overwhelming adaptation time, which may cancel the benefits of adaptation. We present an adaptation approach, QuickAdapt, to enable quick adaptation of a BDCA system. QuickAdapt uses descriptive statistics (e.g., mean and variance) of security events data and fuzzy rules to (re) compose a system with a set of components to ensure optimal accuracy and response time. We have evaluated QuickAdapt for a distributed BDCA system using four datasets. Our evaluation shows that on average QuickAdapt reduces adaptation time by 105× with a competitive adaptation accuracy of 70% as compared to an existing solution.|10.1109/ICECCS.2019.00016|||||2019|QuickAdapt: Scalable Adaptation for Big Data Cyber Security Analytics|Ullah, Faheem and Ali Babar, M.|inproceedings|8882761||Nov|||||||||||||||||||||||||||2008088777|42
https://doi.org/10.1016/S2589-7500(22)00151-0|https://www.sciencedirect.com/science/article/pii/S2589750022001510||||2022|CODE-EHR best-practice framework for the use of structured electronic health-care records in clinical research|Dipak Kotecha and Folkert W Asselbergs and Stephan Achenbach and Stefan D Anker and Dan Atar and Colin Baigent and Amitava Banerjee and Birgit Beger and Gunnar Brobert and Barbara Casadei and Cinzia Ceccarelli and Martin R Cowie and Filippo Crea and Maureen Cronin and Spiros Denaxas and Andrea Derix and Donna Fitzsimons and Martin Fredriksson and Chris P Gale and Georgios V Gkoutos and Wim Goettsch and Harry Hemingway and Martin Ingvar and Adrian Jonas and Robert Kazmierski and Susanne Løgstrup and R Thomas Lumbers and Thomas F Lüscher and Paul McGreavy and Ileana L Piña and Lothar Roessig and Carl Steinbeisser and Mats Sundgren and Benoît Tyl and Ghislaine van Thiel and Kees van Bochove and Panos E Vardas and Tiago Villanueva and Marilena Vrana and Wim Weber and Franz Weidinger and Stephan Windecker and Angela Wood and Diederick E Grobbee|article|KOTECHA2022e757|||The Lancet Digital Health|25897500|10|4||||||||||||||||||||||||||||||2008887646|42
||Databases;Big data;Measurement;Context;Cleaning;Biology;Computers;Data quality;big data;biological data;information quality||2654-2660|2015 IEEE International Conference on Big Data (Big Data)|Though the issues of data quality trace back their origin to the early days of computing, the recent emergence of Big Data has added more dimensions. Furthermore, given the range of Big Data applications, potential consequences of bad data quality can be for more disastrous and widespread. This paper provides a perspective on data quality issues in the Big Data context. it also discusses data integration issues that arise in biological databases and attendant data quality issues.|10.1109/BigData.2015.7364065|||||2015|Data quality issues in big data|Rao, Dhana and Gudivada, Venkat N and Raghavan, Vijay V.|inproceedings|7364065||Oct|||||||||||||||||||||||||||2009369160|42
||Circular bioeconomy, Data pipeline, Digital twin, Process design, Sensor, Wastewater treatment||103152||Rapid urbanization, population increase, emerging contaminants and increasing water scarcity have put a major constraint on the wastewater treatment system. Scarcity of water is steering current way of water recycle, and the drive focus towards resource recovery. Zero waste pathway in circular bioeconomy can bring transformation of wastewater commercialization by adding value with resource recovery. The complex biological reactions, unforeseen microbial behaviours, lack of reliable on-line instrumentation, complex modelling, lack of visualize techniques, low-quality industrial measurements and highly time-varying intensive data-driven operations call for the intelligence techniques and operations. The study is a review of sustainable circularity and intelligent data-driven operations and control of the wastewater treatment plant. Water surveillance and monitoring, circular economy and sustainability, automation pyramid, digital transformation, artificial intelligence, data pipeline, digital twin, data mining, and data-driven visualization, cyber-physical systems and water-energy-health management were reviewed. The deployment of the digital systems has evidently proven to bridges the gap between the data-driven soft sensor, operation and control systems in WWTP. Accurate prediction of the WWTP variables can support process design and control, reduce operation cost, improve system reliability, predictive maintenance and troubleshooting, increase water quality, increase stakeholder's engagement and endorse optimization of the plant performance. This procures the best compliance with international standards and diversification. The inclusion of life cycle environmental or cost management technologies in optimization models is an interesting pathway towards sustainable water treatment in-line with sustainable development goals, circular bioeconomy and industry 4.0.|https://doi.org/10.1016/j.pce.2022.103152|https://www.sciencedirect.com/science/article/pii/S1474706522000468||||2022|Sustainable circularity and intelligent data-driven operations and control of the wastewater treatment plant|Anthony Njuguna Matheri and Belaid Mohamed and Freeman Ntuli and Esther Nabadda and Jane Catherine Ngila|article|MATHERI2022103152|||Physics and Chemistry of the Earth, Parts A/B/C|14747065||126|||||23315|0,724|Q2|82|111|303|5038|923|293|2,87|45,39|United Kingdom|Western Europe|1982, 1991-1992, 1995, 2002-2020|Geochemistry and Petrology (Q2); Geophysics (Q2)|5,794|2.712|0.00279|2011555447|838831279
||Big data, Analytics, Capability development, Qualitative study, Supply chain||113382||Big data analytics (BDA) are gaining importance in all aspects of business management. This is driven by both the presence of large-scale data and management's desire to root decisions in data. Extant research demonstrates that supply chain and operations management functions are among the biggest sources and users of data in the company. Therefore, their decision-making processes would benefit from increased use of BDA technologies. However, there is still a lack of understanding of what determines a company's ability to build BDA capability to gain a competitive advantage. In this study, we attempt to answer this fundamental question by identifying the factors that assist a company in or inhibit it from building its BDA capability and maximizing its gains through BDA technologies. We base our findings on a qualitative analysis of data collected from field visits, interviews with senior management, and secondary resources. We find that, in addition to technical capacity, competitive landscape and intra-firm power dynamics play an important role in building BDA capability and using BDA technologies.|https://doi.org/10.1016/j.dss.2020.113382|https://www.sciencedirect.com/science/article/pii/S0167923620301378||||2020|A note on big data analytics capability development in supply chain|Ashish Kumar Jha and Maher A.N. Agi and Eric W.T. Ngai|article|JHA2020113382|||Decision Support Systems|01679236||138|||||21933|1,564|Q1|151|115|342|7152|2672|338|7,04|62,19|Netherlands|Western Europe|1985-2020|Arts and Humanities (miscellaneous) (Q1); Developmental and Educational Psychology (Q1); Information Systems (Q1); Information Systems and Management (Q1); Management Information Systems (Q1)|13,580|5.795|0.0081|2013769897|1234879127
||Big data, Apache HBase, Apache Spark, ARC, Apache Hadoop, WARC|30|||Web archiving initiatives around the world capture ephemeral Web content to preserve our collective digital memory. However, unlocking the potential of Web archives for humanities scholars and social scientists requires a scalable analytics infrastructure to support exploration of captured content. We present Warcbase, an open-source Web archiving platform that aims to fill this need. Our platform takes advantage of modern open-source “big data” infrastructure, namely Hadoop, HBase, and Spark, that has been widely deployed in industry. Warcbase provides two main capabilities: support for temporal browsing and a domain-specific language that allows scholars to interrogate Web archives in several different ways. This work represents a collaboration between computer scientists and historians, where we have engaged in iterative codesign to build tools for scholars with no formal computer science training. To provide guidance, we propose a process model for scholarly interactions with Web archives that begins with a question and proceeds iteratively through four main steps: filter, analyze, aggregate, and visualize. We call this the FAAV cycle for short and illustrate with three prototypical case studies. This article presents the current state of the project and discusses future directions.|10.1145/3097570|https://doi.org/10.1145/3097570|New York, NY, USA|Association for Computing Machinery||2017|Warcbase: Scalable Analytics Infrastructure for Exploring Web Archives|Lin, Jimmy and Milligan, Ian and Wiebe, Jeremy and Zhou, Alice|article|10.1145/3097570|22|jul|J. Comput. Cult. Herit.|15564673|4|10|October 2017||||||||||||||||||||||2014549586|373691565
||Data integrity;Standards;Measurement;Asset management;Data models;Complexity theory;Data Quality;Asset Management Systems;Policy based Data Management||4025-4031|2019 IEEE International Conference on Big Data (Big Data)|With the growing importance of data in all aspects of the functioning of an enterprise, having good quality of data is crucial in support of business processes. However, there do not exist good metrics to measure the quality of data that is available within an enterprise. While there are several data quality standards, their complexity and their required customization makes them difficult to use in real-world industrial scenarios. In this paper, we discuss the challenges encountered in measuring data quality within asset management systems. We propose a policy-based approach for measuring data quality, and show how such an approach can be customized and interpreted easily by practitioners in the field.|10.1109/BigData47090.2019.9006422|||||2019|A Policy-based Approach for Measuring Data Quality|Grueneberg, K. and Calo, S. and Dewan, P. and Verma, D. and O’Gorman, Tristan|inproceedings|9006422||Dec|||||||||||||||||||||||||||2014655770|42
||Spectrogram;Whales;Sonar equipment;Big Data;Acoustic distortion;Anomaly detection;Anomaly detection;Hydrophone Big data;Quality detection;Multitaper spectrogram;Marine mammal sound;Sperm whale||1-6|2017 IEEE Pacific Rim Conference on Communications, Computers and Signal Processing (PACRIM)|This paper proposes a novel method for anomaly and quality detection of marine mammal sounds using multitaper spectrogram and hydrophone big data. The proposed method is aimed to automatically detect anomaly, such as high-frequency vessel noise, Doppler noise, in sperm whale (SPW) sound as well as the quality of the sound. A new signature function derived from a multi-taper spectrogram is able to detect the anomaly in the data and a new anomaly distortion measure can detect the sound quality into good/bad. The proposed method, is tested with 1905 minutes of data spanning a single year, and using a human operator's annotations. The experimental results reveal that the proposed multitaper spectrogram based approach is efficient in detecting anomaly as well as sperm whale sound quality for hydrophone big data and high detection accuracy (>85%) is achieved for raw input hydrophone data.|10.1109/PACRIM.2017.8121916|||||2017|Marine mammal sound anomaly and quality detection using multitaper spectrogram and hydrophone big data|Sattart, Farook and McQuay, Colter and Driessen, Peter F.|inproceedings|8121916||Aug|||||||||||||||||||||||||||2018199334|42
ICEGOV '17|New Delhi AA, India|exploitation, smart cities, innovation, ambidexterity, e-government, exploration, transformation|9|405–413|Proceedings of the 10th International Conference on Theory and Practice of Electronic Governance|Most cities have limited resources to become a smart city. Yet some cities have been more successful than others in becoming a smart city. This raises the questions why were some cities able to become smart, whereas other were not able to do so? This research is aimed at identifying factors influencing the shift towards becoming a smart city. In this way insight is gained into factors that governments can influence to become a smart city. First, Literature was reviewed to identify dimensions and factors enabling or impeding the process of becoming a smart city. These factors were used to compare two similar type of case studies. The cases took different paths to become a smart city and had different levels of success. This enabled us to identify factors influencing the move towards smart cities. The results reveal that existing infrastructures should be used and extended in such a way that they can facilitate a variety of different applications. Synergy from legacy systems can avoid extra expenditures. Having such an infrastructure in place facilitates the development of new organizational models. These models are developed outside the existing organization structure to avoid hinder from existing practices and organizational structures. This finding suggests that smart cities focussed on structural ambidexterity innovate quicker.|10.1145/3047273.3047386|https://doi.org/10.1145/3047273.3047386|New York, NY, USA|Association for Computing Machinery|9781450348256|2017|How to Become a Smart City? Balancing Ambidexterity in Smart Cities|Matheus, Ricardo and Janssen, Marijn|inproceedings|10.1145/3047273.3047386|||||||||||||||||||||||||||||2018410195|42
|||10|254–263||Understanding binding cores is of fundamental importance in deciphering Protein-DNA TF-TFBS binding and gene regulation. Limited by expensive experiments, it is promising to discover them with variations directly from sequence data. Although existing computational methods have produced satisfactory results, they are one-to-one mappings with no site-specific information on residue/nucleotide variations, where these variations in binding cores may impact binding specificity. This study presents a new representation for modeling binding cores by incorporating variations and an algorithm to discover them from only sequence data. Our algorithm takes protein and DNA sequences from TRANSFAC a Protein-DNA Binding Database as input; discovers from both sets of sequences conserved regions in Aligned Pattern Clusters APCs; associates them as Protein-DNA Co-Occurring APCs; ranks the Protein-DNA Co-Occurring APCs according to their co-occurrence, and among the top ones, finds three-dimensional structures to support each binding core candidate. If successful, candidates are verified as binding cores. Otherwise, homology modeling is applied to their close matches in PDB to attain new chemically feasible binding cores. Our algorithm obtains binding cores with higher precision and much faster runtime (≥1,600x) than that of its contemporaries, discovering candidates that do not co-occur as one-to-one associated patterns in the raw data. Availability: http://www.pami.uwaterloo.ca/~ealee/files/tcbbPnDna2015/Release.zip.|10.1109/TCBB.2015.2474376|https://doi.org/10.1109/TCBB.2015.2474376|Washington, DC, USA|IEEE Computer Society Press||2017|Discovering Protein-DNA Binding Cores by Aligned Pattern Clustering|Lee, En-Shiun Annie and Sze-To, Ho-Yin Antonio and Wong, Man-Hon and Leung, Kwong-Sak and Lau, Terrence Chi-Kong and Wong, Andrew K. C.|article|10.1109/TCBB.2015.2474376||mar|IEEE/ACM Trans. Comput. Biol. Bioinformatics|15455963|2|14|March 2017||||||||||||||||||||||2018662208|1878427007
||Bioinformatics;Genomics;Diseases;DNA;Task analysis;Databases;Infoxication;Genomics;Information Systems;SILE method||1-9|2019 13th International Conference on Research Challenges in Information Science (RCIS)|We live in an age where data acquisition is no longer a problem and the real challenge is how to determine which information is the right one to take important and sometimes difficult decisions. Infoxication (also known as Infobesity or Information Overload) is a term used to describe the difficulty of adapting to new situations and effectively making decisions when there is too much information to manage. With the advent of the Big Data, infoxication is affecting critical domains such as Health Sciences, where tough decisions for patient's health is being taken every day based on heterogeneous, unconnected and sometimes conflicting information. In order to understand the magnitude of the challenge, based on the information publicly available about the genetic causes of the disease and using data quality assessment techniques, we performed an exhaustive analysis of the DNA variations that have been associated to the risk of suffering migraine headache. The same analysis has been repeated 8 months after, and the results have allowed us to exemplify i) how fragile is the information in this domain, ii) the difficulty of finding repositories of contrasted and reliable data, and iii) the need to have information systems that, far from integrating and storing huge volumes of data, are able to support the decision-making process by providing mechanisms agile and flexible enough to be able to adapt to the changing user needs.|10.1109/RCIS.2019.8877003|||||2019|Infoxication in the Genomic Data Era and Implications in the Development of Information Systems|Palacio, Ana León and López, Óscar Pastor|inproceedings|8877003||May||21511357|||||||20300195007|0,174|-|19|0|150|0|116|146|0,81|0,00|United States|Northern America|2011, 2012, 2013, 2014, 2015|Computer Science Applications; Information Systems; Software||||2019784850|1214021277
CrowdMM '14|Orlando, Florida, USA|sports, multimedia retrieval, crowdsourcing, video annotation|6|63–68|Proceedings of the 2014 International ACM Workshop on Crowdsourcing for Multimedia|Recent developments in sport analytics have heightened the interest in collecting data on the behavior of individuals and of the entire team in sports events. Rather than using dedicated sensors for recording the data, the detection of semantic events reflecting a team's behavior and the subsequent annotation of video data is nowadays mostly performed by paid experts. In this paper, we present an approach to generating such annotations by leveraging the wisdom of the crowd. We present the CrowdSport application that allows to collect data for soccer games. It presents crowd workers short video snippets of soccer matches and allows them to annotate these snippets with event information. Finally, the various annotations collected from the crowd are automatically disambiguated and integrated into a coherent data set. To improve the quality of the data entered, we have implemented a rating system that assigns each worker a trustworthiness score denoting the confidence towards newly entered data. Using the DBSCAN clustering algorithm and the confidence score, the integration ensures that the generated event labels are of high quality, despite of the heterogeneity of the participating workers. These annotations finally serve as a basis for a video retrieval system that allows users to search for video sequences on the basis of a graphical specification of team behavior or motion of the individual player. Our evaluations of the crowd-based semantic event detection and video annotation using the Microworkers platform have shown the effectiveness of the approach and have led to results that are in most cases close to the ground truth and can successfully be used for various retrieval tasks.|10.1145/2660114.2660119|https://doi.org/10.1145/2660114.2660119|New York, NY, USA|Association for Computing Machinery|9781450331289|2014|Crowd-Based Semantic Event Detection and Video Annotation for Sports Videos|Sulser, Fabio and Giangreco, Ivan and Schuldt, Heiko|inproceedings|10.1145/2660114.2660119|||||||||||||||||||||||||||||2021246593|42
SIGCSE '21|Virtual Event, USA|data science, curriculum|7|865–871|Proceedings of the 52nd ACM Technical Symposium on Computer Science Education|Many universities are introducing a new major in Data Science into their offering, to reflect the explosive growth in this field and the career opportunities it provides. As a field Data Science has elements from Computer Science and from Statistics, and curricula plans differ widely, both in the balance between the CS and Stats aspects, and also in the emphasis within the computing topics. This paper reports on the curriculum that has been taught for three years now at the University of Sydney. In particular, we describe the approach of a sequence of computing subjects which were developed specifically for the major, in order to bring students over several years to a sophisticated understanding of the data-handling aspects of Data Science. Students also take traditional subjects from both CS (such as Data Structures or AI) and from Statistics (such as Learning from Data and Statistical Inference). The data-centric specially-designed subjects we discuss in this paper are (i) Informatics: Data and Computation (in the first year), (ii) Big Data and Data Diversity (in the second year), and then upper-division subjects on (iii) Data Science Platforms, and (iv) Human-in-the-Loop Data Analytics.|10.1145/3408877.3432457|https://doi.org/10.1145/3408877.3432457|New York, NY, USA|Association for Computing Machinery|9781450380621|2021|A Data-Centric Computing Curriculum for a Data Science Major|"\"Fekete, Alan and Kay, Judy and R\"\"{o}hm, Uwe\""|inproceedings|10.1145/3408877.3432457|||||||||||||||||||||||||||||2022608749|42
||Customer experience, Customer experience management, Customer experience insight, Big data analytics||356-365||Customer experience (CX) has emerged as a sustainable source of competitive differentiation. Recent developments in big data analytics (BDA) have exposed possibilities to unlock customer insights for customer experience management (CXM). Research at the intersection of these two fields is scarce and there is a need for conceptual work that (1) provides an overview of opportunities to use BDA for CXM and (2) guides management practice and future research. The purpose of this paper is therefore to develop a strategic framework for CXM based on CX insights resulting from BDA. Our conceptualisation is comprehensive and is particularly relevant for researchers and practitioners who are less familiar with the potential of BDA for CXM. For managers, we provide a step-by-step guide on how to kick-start or implement our strategic framework. For researchers, we propose some opportunities for future studies in this promising research area.|https://doi.org/10.1016/j.jbusres.2020.01.022|https://www.sciencedirect.com/science/article/pii/S0148296320300345||||2020|Customer experience management in the age of big data analytics: A strategic framework|Maria Holmlund and Yves {Van Vaerenbergh} and Robert Ciuchita and Annika Ravald and Panagiotis Sarantopoulos and Francisco Villarroel Ordenes and Mohamed Zaki|article|HOLMLUND2020356|||Journal of Business Research|01482963||116|||||20550|2,049|Q1|195|878|1342|73221|11507|1315|7,38|83,40|United States|Northern America|1973-2021|Marketing (Q1)|46,935|7.550|0.03523|2022746405|1502892296
ICEGOV '14|Guimaraes, Portugal|business intelligence, ICT salary profile, big data analytics|2|450–451|Proceedings of the 8th International Conference on Theory and Practice of Electronic Governance|This paper elucidates the opportunity on expanding the on-going preparation of salary profile of information communications technology (ICT) professionals of Malaysia into a big data analytics (BDA) activity. The current activity is based on structured database provided by the online job service providers. In essence, BDA framework entailing 5 Vs namely, volume, variety, velocity, veracity and value used in gauging the gaps and potential areas to consider in next stages.|10.1145/2691195.2691196|https://doi.org/10.1145/2691195.2691196|New York, NY, USA|Association for Computing Machinery|9781605586113|2014|Towards Big Data Analytics Framework: ICT Professionals Salary Profile Compilation Perspective|Ramasamy, Ramachandran|inproceedings|10.1145/2691195.2691196|||||||||||||||||||||||||||||2023558297|42
WABD 2021|Fuzhou, China|Personalized service, Marketing, Big data|4|79–82|2021 Workshop on Algorithm and Big Data|In the era of big data, under the conditions of rapid economic development in our country, various enterprises have also vigorously carried out marketing. In the context of big data, marketing research should be strengthened to effectively improve market. Market issues ensure that marketing has improved its status in the era of big data. This article has conducted a research and analysis on marketing in the context of big data. And then the opportunities and challenges of marketing in the context of big data has been explained, which gradually optimize the marketing implementation effect. The challenges faced by marketing has been understood which ensures that the big data model plays its best role in it.|10.1145/3456389.3456390|https://doi.org/10.1145/3456389.3456390|New York, NY, USA|Association for Computing Machinery|9781450389945|2021|Opportunities and Challenges of Marketing in the Context of Big Data|Cao, Shuangshuang|inproceedings|10.1145/3456389.3456390|||||||||||||||||||||||||||||2023698804|42
||datasets, work practice, computer vision, values in design, machine learning|37|||Data is a crucial component of machine learning. The field is reliant on data to train, validate, and test models. With increased technical capabilities, machine learning research has boomed in both academic and industry settings, and one major focus has been on computer vision. Computer vision is a popular domain of machine learning increasingly pertinent to real-world applications, from facial recognition in policing to object detection for autonomous vehicles. Given computer vision's propensity to shape machine learning research and impact human life, we seek to understand disciplinary practices around dataset documentation - how data is collected, curated, annotated, and packaged into datasets for computer vision researchers and practitioners to use for model tuning and development. Specifically, we examine what dataset documentation communicates about the underlying values of vision data and the larger practices and goals of computer vision as a field. To conduct this study, we collected a corpus of about 500 computer vision datasets, from which we sampled 114 dataset publications across different vision tasks. Through both a structured and thematic content analysis, we document a number of values around accepted data practices, what makes desirable data, and the treatment of humans in the dataset construction process. We discuss how computer vision datasets authors value efficiency at the expense of care; universality at the expense of contextuality; impartiality at the expense of positionality; and model work at the expense of data work. Many of the silenced values we identify sit in opposition with social computing practices. We conclude with suggestions on how to better incorporate silenced values into the dataset creation and curation process.|10.1145/3476058|https://doi.org/10.1145/3476058|New York, NY, USA|Association for Computing Machinery||2021|Do Datasets Have Politics? Disciplinary Values in Computer Vision Dataset Development|Scheuerman, Morgan Klaus and Hanna, Alex and Denton, Emily|article|10.1145/3476058|317|oct|Proc. ACM Hum.-Comput. Interact.||CSCW2|5|October 2021||||||||||||||||||||||2028084067|42
||||S206|||https://doi.org/10.1016/j.jval.2021.11.1002|https://www.sciencedirect.com/science/article/pii/S1098301521027972||||2022|POSB319 Predictive Analytic Techniques and Big Data for Improved Health Outcomes in the Context of Value Based Health Care and Coverage Decisions: A Scoping Review|A Miracolo and M Mills and P Kanavos|article|MIRACOLO2022S206|||Value in Health|10983015|1, Supplement|25||Emerging Frontiers and Opportunities|||22377|1,859|Q1|103|211|572|8179|2291|532|3,67|38,76|United Kingdom|Western Europe|1998-2020|Health Policy (Q1); Medicine (miscellaneous) (Q1); Public Health, Environmental and Occupational Health (Q1)|12,642|5.725|0.01786|2028892194|748206584
||Time series, Inter-country relations, Spatio-temporal data mining, Mass media events, GDELT||86-96||The development of theories and techniques for big data analytics offers tremendous possibility for investigating large-scale events and patterns that emerge over space and time. In this research, we utilize a unique open dataset “The Global Data on Events, Location and Tone” (GDELT) to model the image of China in mass media, specifically, how China has related to the rest of the world and how this connection has evolved upon time. The results of this research contribute to both the methodological and the empirical perspectives: We examined the effectiveness of the dynamic time warping (DTW) distances in measuring the differences between long-term mass media data. We identified four types of connection strength patterns between China and its top 15 related countries. With that, the distance decay effect in mass media is also examined and compared with social media and public transportation data. While using multiple datasets and focusing on mass media, this study generates valuable input regarding the interpretation of the diplomatic and regional correlation for the nation of China. It also provides methodological references for investigating international relations in other countries and regions in the big data era.|https://doi.org/10.1016/j.compenvurbsys.2016.10.012|https://www.sciencedirect.com/science/article/pii/S0198971516303313||||2017|Exploring inter-country connection in mass media: A case study of China|Yihong Yuan and Yu Liu and Guixing Wei|article|YUAN201786|||Computers, Environment and Urban Systems|01989715||62|||||23269|1,549|Q1|92|85|327|4842|2135|324|6,11|56,96|United Kingdom|Western Europe|1980-1986, 1988-2020|Ecological Modeling (Q1); Environmental Science (miscellaneous) (Q1); Geography, Planning and Development (Q1); Urban Studies (Q1)||||2032933780|1571752529
AIPR 2018|Beijing, China|NLP, Electronic health record, machine learning, SVM, ICD-10|6|101–106|Proceedings of the 2018 International Conference on Artificial Intelligence and Pattern Recognition|The electronic health record (EHR) analysis has become an increasingly important application for artificial intelligence (AI) algorithms to leverage the insight from the big data for improving the quality of human healthcare. In a lot of Chinese EHR analysis applications, it is very important to categorize the patients' diseases according to the medical coding standard. In this paper, we develop NLP and machine learning algorithms to automatically categorize each patient's individual diseases into the ICD-10 coding standard. Experimental results show that the support vector machine algorithm (SVM) accomplishes very promising classification results.|10.1145/3268866.3268877|https://doi.org/10.1145/3268866.3268877|New York, NY, USA|Association for Computing Machinery|9781450365246|2018|Categorization of Patient Disease into ICD-10 with NLP and SVM for Chinese Electronic Health Record Analysis|Zhong, Junmei and Gao, Chuangui and Yi, Xiu|inproceedings|10.1145/3268866.3268877|||||||||||||||||||||||||||||2033444856|42
ASSE' 22|Macau, Macao||6|81–86|2022 3rd Asia Service Sciences and Software Engineering Conference|With the development of the economy and the improvement of people's quality of life, the public demand for food taste has gradually changed to the demand for food safety. In order to better facilitate the government to strengthen food safety supervision and protect people's food safety, it is necessary for the government to realize information interaction with enterprises related to the food supply chain and ensure the traceability of food flowing into the market by exchanging and sharing the data of food safety information. With the rapid promotion and popularization of various mobile terminals in the Internet era, the data analysis technology based on artificial intelligence technology is more accurate, which makes the value contained in the data more and more important to people. At present, many fields need the opening and sharing of big data, but there is no reliable data sharing environment in the field of food supervision, and it is still difficult to ensure the traceability of data related to food safety. Blockchain has unique advantages of decentralization and distribution, which can help break the current obstacles of big data sharing and exchange and achieve a high degree of data sharing, interconnection and exchange. Based on the blockchain technology, this paper studies the food safety data sharing and exchange mechanism, combines the blockchain with the distributed file system, constructs the data connection model, stores the shared information on the blockchain, then introduces IPFs and zigzag coding, designs the corresponding control method, and establishes a reliable data sharing and exchange mechanism. The analysis shows that the data sharing and exchange mechanism proposed in this paper can meet the needs of food safety data sharing and exchange.|10.1145/3523181.3523193|https://doi.org/10.1145/3523181.3523193|New York, NY, USA|Association for Computing Machinery|9781450387453|2022|Research on Food Safety Data Sharing and Exchange Mechanism|Yu, Jie and Li, Danning and Chen, Kai and Huang, Wei and Qin, Meiyuan and Qin, Xianjin|inproceedings|10.1145/3523181.3523193|||||||||||||||||||||||||||||2034019641|42
||||1322-1327|2016 IEEE International Conference on Big Data (Big Data)|As sensors spread across almost every industry, the Internet of Things (IoT) is going to trigger an era of big data. However, the abundance of available sensing data causes new challenges when building IoT applications. One main challenge is how to select proper data from large amount of sensing data for learning useful information efficiently. Existing approaches require developers to manage data for each specific application, which is very time consuming since the developers may not have enough knowledge about the dynamic changing data quality of different sensors. In this paper, we propose a data management middleware to learn the correlations between time series sensor data without prior knowledge. The learned correlation is then applied to select the useful sensor and reconstruct the incorrect data. To generalize the correlation models for each application, we utilize the dynamic feedback from the application to update the data selection and reconstruction. We evaluate our data management middleware in smart grids. The evaluation results show that our middleware can achieve better application performance with the help of dynamic feedback, data reconstruction and data selection.|10.1109/BigData.2016.7840737|||||2016|Application-driven sensing data reconstruction and selection based on correlation mining and dynamic feedback|Huang, Zhichuan and Xie, Tiantian and Zhu, Ting and Wang, Jianwu and Zhang, Qingquan|inproceedings|7840737||Dec|||||||||||||||||||||||||||2034742272|42
||Data integrity;Conferences;Time series analysis;Banking;Big Data;Anomaly detection;Business;customer segmentation;clustering;association rules mining;anomaly detection||4392-4401|2020 IEEE International Conference on Big Data (Big Data)|This paper presents a novel big data analytics framework for creating explainable personas for retail and business banking customers. These personas are essential to better tailor financial products and improve customer retention. This framework is comprised of several components including anomaly detection, binning and aggregation of contextual data, clustering of transaction time series, and mining association rules that map contextual data to cluster identifiers. Leveraging rich transaction and contextual data available from nearly 60,000 retail and 90,000 business customers of a financial institution, we empirically evaluate this framework and describe how the identified association rules can be used to explain and refine existing customer classes, and identify new customer classes and various data quality issues. We also analyze the performance of the proposed framework and show that it can easily scale to millions of banking customers.|10.1109/BigData50022.2020.9378483|||||2020|Large-scale Data-driven Segmentation of Banking Customers|Hossain, Md Monir and Sebestyen, Mark and Mayank, Dhruv and Ardakanian, Omid and Khazaei, Hamzeh|inproceedings|9378483||Dec|||||||||||||||||||||||||||2036098930|42
||Biomedical imaging;Informatics;Bioinformatics;Big data;Cancer;Decision making||1-2|2015 IEEE 39th Annual Computer Software and Applications Conference|"\"Rapid advancements in biotechnologies such as -omic (genomics, proteomics, metabolomics, lipidomics etc.), next generation sequencing, bio-nanotechnologies, molecular imaging, and mobile sensors etc. accelerate the data explosion in biomedicine and health wellness. Multiple nations around the world have been seeking novel effective ways to make sense of \"\"big data\"\" for evidence-based, outcome-driven, and affordable 5P (Patient-centric, Predictive, Preventive, Personalized, and Precise) healthcare. My main research focus is on multi-modal and multi-scale (i.e. molecular, cellular, whole body, individual, and population) biomedical data analytics for discovery, development, and delivery, including translational bioinformatics in biomarker discovery for personalized care; imaging informatics in histopathology for clinical diagnosis decision support; bionanoinformatics for minimally-invasive image-guided surgery; critical care informatics in ICU for real-time evidence-based decision making; and chronic care informatics for patient-centric health. In this talk, first, I will highlight major challenges in biomedical and health informatics pipeline consisting of data quality control, information feature extraction, advanced knowledge modeling, decision making, and proper action taking through feedback. Second, I will present informatics methodological research in (i) data integrity and integration; (ii) case-based reasoning for individualized care; and (iii) streaming data analytics for real-time decision support using a few mobile health case studies (e.g. Sickle Cell Disease, asthma, pain management, rehabilitation, diabetes etc.). Last, there is big shortage of data scientists and engineers who are capable of handling Big Data. In addition, there is an urgent need to educate healthcare stakeholders (i.e. patients, physicians, payers, and hospitals) on how to tackle these grant challenges. I will discuss efforts such as patient-centric educational intervention, community-based crowd sourcing, and Biomedical Data Analytics MOOC development. Our research has been supported by NIH, NSF, Georgia Research Alliance, Georgia Cancer Coalition, Emory-Georgia Tech Cancer Nanotechnology Center, Children's Health Care of Atlanta, Atlanta Clinical and Translational Science Institute, and industrial partners such as Microsoft Research and HP.\""|10.1109/COMPSAC.2015.343|||||2015|Biomedical Big Data Analytics for Patient-Centric and Outcome-Driven Precision Health|Wang, May D.|inproceedings|7273312||July||07303157||3|||||18705|0,216|-|47|0|908|0|1110|834|1,23|0,00|United States|Northern America|1979-2019|Computer Science Applications; Software||||2039549722|838284539
||Big data analytics, Artificial intelligence, Marketing performance, Knowledge-based view||120986||A universal trend in advanced manufacturing countries is defining Industry 4.0, industrialized internet and future factories as a recent wave, which may transform the production and its related services. Further, big data analytics has emerged as a game changer in the business world due to its uses for increasing accuracy in decision-making and enhancing performance of sustainable industry 4.0 applications. This study intends to emphasize on how to support Industry 4.0 with knowledge based view. For the same, a conceptual model is framed and presented with essential components that are required for a real world implementation. The study used qualitative analysis and was guided by a knowledge-based theoretical framework. Thematic analysis resulted in the identification of a number of emergent categories. Key findings highlight significant gaps in conventional decision-making systems and demonstrate how big data enhances firms’ strategic and operational decisions as well as facilitates informational access for improved marketing performance. The resulting proposed model can provide managers with a reference point for using big data to line up firms’ activities for more effective marketing efforts and presents a conceptual basis for further empirical studies in this area.|https://doi.org/10.1016/j.techfore.2021.120986|https://www.sciencedirect.com/science/article/pii/S0040162521004182||||2021|Big data and firm marketing performance: Findings from knowledge-based view|Shivam Gupta and Théo Justy and Shampy Kamboj and Ajay Kumar and Eivind Kristoffersen|article|GUPTA2021120986|||Technological Forecasting and Social Change|00401625||171|||||14704|2,226|Q1|117|448|1099|35581|10127|1061|9,01|79,42|United States|Northern America|1970-2020|Applied Psychology (Q1); Business and International Management (Q1); Management of Technology and Innovation (Q1)|21,116|8.593|0.02416|2041428511|1949868303
||Business intelligence, data quality, real estate, predictive analytics|16|||This article builds on previous work in the area of real-world applications of Business Intelligence (BI) technology. It illustrates the analysis, modeling, and framework design of a BI solution with high data quality to provide reliable analytics and decision support in the Jordanian real estate market. The motivation is to provide analytics dashboards to potential investors about specific segments or units in the market. The article ekxplains the design of a BI solution, including background market and technology investigation, problem domain requirements, solution architecture modeling, design and testing, and the usability of descriptive and predictive features. The resulting framework provides an effective BI solution with user-friendly market insights for investors with little or no market knowledge. The solution features predictive analytics based on established Machine Learning modeling techniques, analyzed and contrasted to select the optimum methodology and model combination for predicting market behavior to empower inexperienced users.|10.1145/3422669|https://doi.org/10.1145/3422669|New York, NY, USA|Association for Computing Machinery||2021|Business Intelligence Framework Design and Implementation: A Real-Estate Market Case Study|Fraihat, Salam and Salameh, Walid A. and Elhassan, Ammar and Tahoun, Bushra Abu and Asasfeh, Maisa|article|10.1145/3422669|10|jun|J. Data and Information Quality|19361955|2|13|June 2021||||||||||||||||||||||2043719496|833754770
||Covid-19, Moving pattern, Mobile spatial statistics, Population concentration, Big data||100212||At the time of writing, the world is facing the new coronavirus pandemic, which has been declared one of the most dangerous disasters of the 21st century. All nations and communities have applied many countermeasures to control the spread of the epidemic. In terms of countermeasures, lockdowns and reductions of social activities are meant to flatten the curve of infection. Nevertheless, to date, there has been no evaluation of the effectiveness of these methods. Thus, the present study aims to interpret the change in the population density of Sapporo city in the emergency's period declaration using big data obtained from mobile spatial statistics. The results indicate that, in the time of refraining from traveling, the city's residents have been more likely to stay home and less likely to travel to the center area. This has led to a decrease of up to 90% of the population density in crowded areas. The study's outcomes partly explain the statement of reducing 70%–80% of contact between people in line with the purpose of the emergency declaration. Moreover, these findings establish the primary step for further analysis of estimating the efficiency of policy in controlling the epidemic.|https://doi.org/10.1016/j.trip.2020.100212|https://www.sciencedirect.com/science/article/pii/S2590198220301238||||2020|Changes in urban mobility in Sapporo city, Japan due to the Covid-19 emergency declarations|Mikiharu Arimura and Tran Vinh Ha and Kota Okumura and Takumi Asada|article|ARIMURA2020100212|||Transportation Research Interdisciplinary Perspectives|25901982||7|||||21100943534|0,383|Q2|10|171|64|8370|114|62|1,78|48,95|United Kingdom|Western Europe|2019-2020|Automotive Engineering (Q2); Civil and Structural Engineering (Q3); Management Science and Operations Research (Q3); Transportation (Q3)||||2045724923|2103768449
||Current research information systems, CRIS, Research information systems, RIS, Research information, Data sources, Data quality, Extraction transformation load, ETL, Data analysis, Data profiling, Science system, Standardization||50-56||The success or failure of a RIS in a scientific institution is largely related to the quality of the data available as a basis for the RIS applications. The most beautiful Business Intelligence (BI) tools (reporting, etc.) are worthless when displaying incorrect, incomplete, or inconsistent data. An integral part of every RIS is thus the integration of data from the operative systems. Before starting the integration process (ETL) of a source system, a rich analysis of source data is required. With the support of a data quality check, causes of quality problems can usually be detected. Corresponding analyzes are performed with data profiling to provide a good picture of the state of the data. In this paper, methods of data profiling are presented in order to gain an overview of the quality of the data in the source systems before their integration into the RIS. With the help of data profiling, the scientific institutions can not only evaluate their research information and provide information about their quality, but also examine the dependencies and redundancies between data fields and better correct them within their RIS.|https://doi.org/10.1016/j.ijinfomgt.2018.02.007|https://www.sciencedirect.com/science/article/pii/S0268401218300975||||2018|Analyzing data quality issues in research information systems via data profiling|Otmane Azeroual and Gunter Saake and Eike Schallehn|article|AZEROUAL201850|||International Journal of Information Management|02684012||41|||||15631|2,770|Q1|114|224|382|20318|5853|373|16,16|90,71|United Kingdom|Western Europe|1970, 1986-2021|Artificial Intelligence (Q1); Computer Networks and Communications (Q1); Information Systems (Q1); Information Systems and Management (Q1); Library and Information Sciences (Q1); Management Information Systems (Q1); Marketing (Q1)|12,245|14.098|0.01167|2050550842|747927863
||Reliability;Mathematical model;Data integration;Bayes methods;Equations;Uncertainty||1-8|2014 14th UK Workshop on Computational Intelligence (UKCI)|The recent development of information techniques, especially the state-of-the-art “big data” solutions, enables the extracting, gathering, and processing large amount of toxicity information from multiple sources. Facilitated by this technology advance, a framework named integrated testing strategies (ITS) has been proposed in the predictive toxicology domain, in an effort to intelligently jointly use multiple heterogeneous toxicity data records (through data fusion, grouping, interpolation/extrapolation etc.) for toxicity assessment. This will ultimately contribute to accelerating the development cycle of chemical products, reducing animal use, and decreasing development costs. Most of the current study in ITS is based on a group of consensus processes, termed weight of evidence (WoE), which quantitatively integrate all the relevant data instances towards the same endpoint into an integrated decision supported by data quality. Several WoE implementations for the particular case of toxicity data fusion have been presented in the literature, which are collectively studied in this paper. Noting that these uncertainty handling methodologies are usually not simply developed from conventional probability theory due to the unavailability of big datasets, this paper first investigates the mathematical foundations of these approaches. Then, the investigated data integration models are applied to a representative case in the predictive toxicology domain, with the experimental results compared and analysed.|10.1109/UKCI.2014.6930153|||||2014|Integration strategies for toxicity data from an empirical perspective|Yang, Longzhi and Neagu, Daniel|inproceedings|6930153||Sep.||21627657|||||||||||||||||||||||||2053823284|1045621306
||Databases;Tools;Measurement;Maintenance engineering;Reliability;Engines;Arrays;Databases;data cube;big data;lessons learned||1-5|2018 Annual Reliability and Maintainability Symposium (RAMS)|This experience paper describes some lessons learned using “big data.” Managers want to make data driven decisions, and many companies spend substantial effort and resources to develop collection methods, record facts, and store records in large data warehouses. Additional resources take this collection of data and produce reports, which are then used to support decision making. We work with customer premises equipment, and our databases track nearly 50 million serial numbers. As reliability engineers, we use this data as the basis of analysis to assess field performance of the equipment the databases track. In 2016, we began to use a standard tool to serve as a definitive repository and an engine to do preliminary postprocessing. This database uses data dimensions, where each dimension is an array. It is convenient to think of the lefthand column as a set of labels and the cells to the right as measures (either collected data or computed metrics) for each label. The advantage of creating dimensions is that-rather than working with individual data items and the relationships between them-a dimension preprocesses data into a set that has relationships with other sets. This models the “real world” more closely. A cube is just a set of one or more dimensions. Using a cube allows complex questions to be asked and answered in ways that relational databases do not. We have been using this data structure to support analysis of customer premises equipment, typically set top boxes, modems, and similar equipment that is leased by the provider to customers at their residences and businesses. Tools that support cubes offer several advantages. It is possible to do analyses in a cube that are difficult in a relational database that does not support the “logical ecology” that a cube does By moving up and down the data hierarchy, it is possible to see relationships on the screen, and outputs can be saved to other more powerful post processing tools for more detailed analysis Cubes support faster and less error prone analysis This paper describes these points and illustrate them with a simple example. Our objective is to illustrate the concepts rather than work through a detailed problem. Our work to date suggests that it is critical to manage data quality in a broad sense so that the resulting reports and analysis are trustworthy. We have had a generally positive experience with this technology and found that it benefits the business by allowing processes to be modeled. This refines our understanding of the meaning of the various process metrics, and in some cases, we have been able to recommend changes to policy.|10.1109/RAM.2018.8463091|||||2018|Product Reliability and Databases: Lessons Learned|Bantug, Derek and Franklin, Paul and Boone, Ted|inproceedings|8463091||Jan||25770993|||||||||||||||||||||||||2054104058|137887340
||pre-large concept, privacy protection, federated learning, Frequent itemset mining, Internet of Behaviour, industrial collaborative data mining||||Rapid advancement of industrial internet of things (IoT) technology has changed the supply chain network to an open system to meet the high demand for individualized products and provide better customer experiences. However the open-system supply chain has forced many small and midsize enterprises (SMEs) to adopt vertical integration by being divided into smaller companies with a distinctive business for each SME but a central alliance to produce a range of products and gain competencies. Therefore, existing models do not guarantee the protection of data privacy of individual SMEs. Moreover, especially for the IoT environment, collecting data in a secure way and revealing valuable knowledge in an IoT network is difficult. How to share data in a secure framework is of paramount importance in the internet of behavior field. In this article, a privacy-preserving data-mining framework is proposed for joint-venture industrial collaborative activities by combining federated learning and a ”pre-large concept” of data-mining techniques. The novelty of the proposed approach is that, while mining high-utility itemsets from multiple datasets, it does not require direct data sharing. In the proposed method, the federated-learning framework can learn from aggregated learning parameters without scanning all data from different sets. The pre-large concept in this approach reduces the amount of scanning into different datasets. Thus, the approach makes it possible to train federated learning more quickly while protecting the privacy of individual data owners. The approach has been tested on real industrial datasets in a collaborative environment. Extensive experimental results show that the approach achieves high accuracy compared with conventional data-mining techniques while preserving the privacy of datasets.|10.1145/3532090|https://doi.org/10.1145/3532090|New York, NY, USA|Association for Computing Machinery||2022|A Privacy Frequent Itemsets Mining Framework for Collaboration in IoT Using Federated Learning|Wu, Jimmy Ming-Tai and Teng, Qian and Huda, Shamsul and Chen, Yeh-Cheng and Chen, Chien-Ming|article|10.1145/3532090||may|ACM Trans. Sen. Netw.|15504859||||Just Accepted|||4700152843|0,598|Q2|67|44|118|2352|447|117|3,52|53,45|United States|Northern America|2005-2020|Computer Networks and Communications (Q2)|1,365|2.253|9.9E-4|2061518303|1384228366
||Big Data;Data models;Banking;Neural networks;Industries;Data integrity;Computational modeling;Big Data;Data Noise;Data Quality;Prediction and Machine Learning||791-792|2020 IEEE 7th International Conference on Data Science and Advanced Analytics (DSAA)|Big data has been transformed into knowledge by information systems to add value in businesses. Enterprises relying on it benefit from risk management to a certain extent. The value, however, depends on the quality of data. The quality needs to be verified before any use of the data. Specifically, measuring the quality by simulating the real life situation and even forecast it accurately turns into a hot topic. In recent years, there have been numerous researches on the measurement and assessment of data quality. These are yet to utilize a scientific computational method for the measurement and prediction. Current methods either fail to make an accurate prediction or do not consider the correlation and time sequence factors of the data. To address this, we design a model to extend machine learning technique to business applications predicting this. Firstly, we implement the model to detect data noises from a risk dataset according to an international data quality standard from banking industry and then estimate their impacts with Gaussian and Bayesian methods. Secondly, we direct sequential learning in multiple deep neural networks for the prediction with an attention mechanism. The model is experimented with various network methodologies to show the predictive power of machine learning technique and is evaluated by validation data to confirm the model effectiveness. The model is scalable to apply to any industries utilizing big data other than the banking industry.|10.1109/DSAA49011.2020.00119|||||2020|Big Data Quality Prediction on Banking Applications: Extended Abstract|Wong, Ka Yee. and Wong, Raymond K.|inproceedings|9260067||Oct|||||||||||||||||||||||||||2062493599|42
ESEC/FSE 2020|Virtual Event, USA|AI system, Industrial Artificial Intelligence, AI software, Continuous Experimentation|4|1513–1516|Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering|Moving from experiments to industrial level AI software development requires a shift from understanding AI/ ML model attributes as a standalone experiment to know-how integrating and operating AI models in a large-scale software system. It is a growing demand for adopting state-of-the-art software engineering paradigms into AI development, so that the development efforts can be aligned with business strategies in a lean and fast-paced manner. We describe AI development as an “unknown unknown” problem where both business needs and AI models evolve over time. We describe a holistic view of an iterative, continuous approach to develop industrial AI software basing on business goals, requirements and Minimum Viable Products. From this, five areas of challenges are presented with the focus on experimentation. In the end, we propose a research agenda with seven questions for future studies.|10.1145/3368089.3417039|https://doi.org/10.1145/3368089.3417039|New York, NY, USA|Association for Computing Machinery|9781450370431|2020|Continuous Experimentation on Artificial Intelligence Software: A Research Agenda|Nguyen-Duc, Anh and Abrahamsson, Pekka|inproceedings|10.1145/3368089.3417039|||||||||||||||||||||||||||||2062844969|42
SIGMOD '22|Philadelphia, PA, USA|data integration, data collection, responsible ai, distribution tailoring, data equity, fair ML|7|2458–2464|Proceedings of the 2022 International Conference on Management of Data|Data integration has been extensively studied by the data management community and is a core task in the data pre-processing step of ML pipelines. When the integrated data is used for analysis and model training, responsible data science requires addressing concerns about data quality and bias. We present a tutorial on data integration and responsibility, highlighting the existing efforts in responsible data integration along with research opportunities and challenges. In this tutorial, we encourage the community to audit data integration tasks with responsibility measures and develop integration techniques that optimize the requirements of responsible data science. We focus on three critical aspects: (1) the requirements to be considered for evaluating and auditing data integration tasks for quality and bias; (2) the data integration tasks that elicit attention to data responsibility measures and methods to satisfy these requirements; and, (3) techniques, tasks, and open problems in data integration that help achieve data responsibility.|10.1145/3514221.3522567|https://doi.org/10.1145/3514221.3522567|New York, NY, USA|Association for Computing Machinery|9781450392495|2022|Responsible Data Integration: Next-Generation Challenges|Nargesian, Fatemeh and Asudeh, Abolfazl and Jagadish, H. V.|inproceedings|10.1145/3514221.3522567|||||||||||||||||||||||||||||2062976946|42
CIKM '22|Atlanta, GA, USA|pre-training, sequential recommendation, contrastive learning|10|1925–1934|Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management|Recently, pre-training based approaches are proposed to leverage self-supervised signals for improving the performance of sequential recommendation. However, most of existing pre-training recommender systems simply model the historical behavior of a user as a sequence, while lack of sufficient consideration on temporal interaction patterns that are useful for modeling user behavior.In order to better model temporal characteristics of user behavior sequences, we propose a Temporal Contrastive Pre-training method for Sequential Recommendation (TCPSRec for short). Based on the temporal intervals, we consider dividing the interaction sequence into more coherent subsequences, and design temporal pre-training objectives accordingly. Specifically, TCPSRec models two important temporal properties of user behavior, i.e., invariance and periodicity. For invariance, we consider both global invariance and local invariance to capture the long-term preference and short-term intention, respectively. For periodicity, TCPSRec models coarse-grained periodicity and fine-grained periodicity at the subsequence level, which is more stable than modeling periodicity at the item level. By integrating the above strategies, we develop a unified contrastive learning framework with four specially designed pre-training objectives for fusing temporal information into sequential representations. We conduct extensive experiments on six real-world datasets, and the results demonstrate the effectiveness and generalization of our proposed method.|10.1145/3511808.3557468|https://doi.org/10.1145/3511808.3557468|New York, NY, USA|Association for Computing Machinery|9781450392365|2022|Temporal Contrastive Pre-Training for Sequential Recommendation|Tian, Changxin and Lin, Zihan and Bian, Shuqing and Wang, Jinpeng and Zhao, Wayne Xin|inproceedings|10.1145/3511808.3557468|||||||||||||||||||||||||||||2063126714|42
||Big data analytics capabilities, Information governance, Incremental innovation, Radical innovation, Environmental uncertainty, FIMIX-PLS||103361||The age of big data analytics is now here, with companies increasingly investing in big data initiatives to foster innovation and outperform competition. Nevertheless, while researchers and practitioners started to examine the shifts that these technologies entail and their overall business value, it is still unclear whether and under what conditions they drive innovation. To address this gap, this study draws on the resource-based view (RBV) of the firm and information governance theory to explore the interplay between a firm’s big data analytics capabilities (BDACs) and their information governance practices in shaping innovation capabilities. We argue that a firm’s BDAC helps enhance two distinct types of innovative capabilities, incremental and radical capabilities, and that information governance positively moderates this relationship. To examine our research model, we analyzed survey data collected from 175 IT and business managers. Results from partial least squares structural equation modelling analysis reveal that BDACs have a positive and significant effect on both incremental and radical innovative capabilities. Our analysis also highlights the important role of information governance, as it positively moderates the relationship between BDAC’s and a firm’s radical innovative capability, while there is a nonsignificant moderating effect for incremental innovation capabilities. Finally, we examine the effect of environmental uncertainty conditions in our model and find that information governance and BDACs have amplified effects under conditions of high environmental dynamism.|https://doi.org/10.1016/j.im.2020.103361|https://www.sciencedirect.com/science/article/pii/S0378720620302998||||2020|The role of information governance in big data analytics driven innovation|Patrick Mikalef and Maria Boura and George Lekakos and John Krogstie|article|MIKALEF2020103361|||Information & Management|03787206|7|57|||||12303|2,147|Q1|162|130|248|11385|2482|246|8,94|87,58|Netherlands|Western Europe|1977-2020|Information Systems (Q1); Information Systems and Management (Q1); Management Information Systems (Q1)||||2064650715|1945939487
|||||||||New York, NY, USA|Association for Computing Machinery|9781450390606|2021|Computing Competencies for Undergraduate Data Science Curricula|ACM Data Science Task Force|book|10.1145/3453538|||||||||||||||||||||||||||||2064921798|42
||||652-660||The objective of this paper is to identify the extent to which real world data (RWD) is being utilized, or could be utilized, at scale in drug development. Through screening peer-reviewed literature, we have cited specific examples where RWD can be used for biomarker discovery or validation, gaining a new understanding of a disease or disease associations, discovering new markers for patient stratification and targeted therapies, new markers for identifying persons with a disease, and pharmacovigilance. None of the papers meeting our criteria was specifically geared toward novel targets or indications in the biopharmaceutical sector; the majority were focused on the area of public health, often sponsored by universities, insurance providers or in combination with public health bodies such as national insurers. The field is still in an early phase of practical application, and is being harnessed broadly where it serves the most direct need in public health applications in early, rare and novel disease incidents. However, these exemplars provide a valuable contribution to insights on the use of RWD to create novel, faster and less invasive approaches to advance disease understanding and biomarker discovery. We believe that pharma needs to invest in making better use of Electronic Health Records and the need for more precompetitive collaboration to grow the scale of this ‘big denominator’ capability, especially given the needs of precision medicine research.|https://doi.org/10.1016/j.drudis.2017.12.002|https://www.sciencedirect.com/science/article/pii/S1359644617305950||||2018|Real world big data for clinical research and drug development|Gurparkash Singh and Duane Schulthess and Nigel Hughes and Bart Vannieuwenhuyse and Dipak Kalra|article|SINGH2018652|||Drug Discovery Today|13596446|3|23|||||||||||||||||||||||2065752562|933393058
||Bridges;Analytical models;Data analysis;Data integrity;Decision making;Data visualization;Big Data;Unstructured data;data usability;unstructured data analytics;pragmatic data quality||1-6|2021 International Conference on Computer & Information Sciences (ICCOINS)|A high volume of unstructured data is being generated from diverse and heterogeneous sources. The unstructured data analytics process is used to extract valuable insights from these unstructured data sources but unlocking useful and usable information is critical for analytics. Despite advancements in technologies, data preparation requires an inordinate amount of time in unstructured data manipulation into a usable form. Although several data manipulation and preparation techniques have been proposed for unstructured big data, relatively limited research has addressed the usability issues of unstructured data. This study identifies the usability issues of unstructured big data for the analytical process to bridge the identified gap. The usability enhancement model has been proposed for unstructured big data to facilitate the subjective and objective efficacy of unstructured big data for data preparation and manipulation activities. Moreover, concept mapping is an essential element to improve the usability of unstructured big data incorporated in the proposed model with usability rules. These rules reduce the usability gap between data availability and its usefulness for an intended purpose. The proposed research model will help to improve the efficiency of unstructured big data analytics.|10.1109/ICCOINS49721.2021.9497187|||||2021|Towards Improved Data Analytics Through Usability Enhancement of Unstructured Big Data|Adnan, Kiran and Akbar, Rehan and Wang, Khor Siak|inproceedings|9497187||July|||||||||||||||||||||||||||2065813834|42
||ISCTSC, transport, survey methodology, big data||1-15||This document presents an introduction to the ISCTSC Special Issue of Transport Research Procedia. It synthesizes the discussions held at the 11th International Conference on Transport Survey Methods, and describes the contents of the selected contributions. This conference has been held in different countries from all over the world, involving an increasing group of enthusiastic and generous specialists, willing to share their knowledge. This 11th conference was an opportunity to discuss the state of the art on transport survey methods, but also to question the way transport surveys are conducted in the era of big data. We took the opportunity to identify the main challenges, and the most important questions.|https://doi.org/10.1016/j.trpro.2018.10.001|https://www.sciencedirect.com/science/article/pii/S2352146518301522||||2018|Transport survey methods - in the era of big data facing new and old challenges|Patrick Bonnel and Marcela A. Munizaga|article|BONNEL20181|||Transportation Research Procedia|23521465||32||Transport Survey Methods in the era of big data:facing the challenges|||||||||||||||||||||2067182040|83400274
||Big Data;Data integrity;Data mining;Feature extraction;Data models;Measurement;Quality assessment;Big Data;Data Quality;Unstructured Data;Quality of Unstructured Big Data||69-74|2018 International Conference on Innovations in Information Technology (IIT)|Big Data has gained an enormous momentum the past few years because of the tremendous volume of generated and processed Data from diverse application domains. Nowadays, it is estimated that 80% of all the generated data is unstructured. Evaluating the quality of Big data has been identified to be essential to guarantee data quality dimensions including for example completeness, and accuracy. Current initiatives for unstructured data quality evaluation are still under investigations. In this paper, we propose a quality evaluation model to handle quality of Unstructured Big Data (UBD). The later captures and discover first key properties of unstructured big data and its characteristics, provides some comprehensive mechanisms to sample, profile the UBD dataset and extract features and characteristics from heterogeneous data types in different formats. A Data Quality repository manage relationships between Data quality dimensions, quality Metrics, features extraction methods, mining methodologies, data types and data domains. An analysis of the samples provides a data profile of UBD. This profile is extended to a quality profile that contains the quality mapping with selected features for quality assessment. We developed an UBD quality assessment model that handles all the processes from the UBD profiling exploration to the Quality report. The model provides an initial blueprint for quality estimation of unstructured Big data. It also, states a set of quality characteristics and indicators that can be used to outline an initial data quality schema of UBD.|10.1109/INNOVATIONS.2018.8605945|||||2018|Big Data Quality Assessment Model for Unstructured Data|Taleb, Ikbal and Serhani, Mohamed Adel and Dssouli, Rachida|inproceedings|8605945||Nov||23255498|||||||||||||||||||||||||2067675932|73226646
ESEC/FSE 2018|Lake Buena Vista, FL, USA|Empirical study, static analysis, third party library|12|319–330|Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering|Intensive dependencies of a Java project on third-party libraries can easily lead to the presence of multiple library or class versions on its classpath. When this happens, JVM will load one version and shadows the others. Dependency conflict (DC) issues occur when the loaded version fails to cover a required feature (e.g., method) referenced by the project, thus causing runtime exceptions. However, the warnings of duplicate classes or libraries detected by existing build tools such as Maven can be benign since not all instances of duplication will induce runtime exceptions, and hence are often ignored by developers. In this paper, we conducted an empirical study on real-world DC issues collected from large open source projects. We studied the manifestation and fixing patterns of DC issues. Based on our findings, we designed Decca, an automated detection tool that assesses DC issues' severity and filters out the benign ones. Our evaluation results on 30 projects show that Decca achieves a precision of 0.923 and recall of 0.766 in detecting high-severity DC issues. Decca also detected new DC issues in these projects. Subsequently, 20 DC bug reports were filed, and 11 of them were confirmed by developers. Issues in 6 reports were fixed with our suggested patches.|10.1145/3236024.3236056|https://doi.org/10.1145/3236024.3236056|New York, NY, USA|Association for Computing Machinery|9781450355735|2018|Do the Dependency Conflicts in My Project Matter?|Wang, Ying and Wen, Ming and Liu, Zhenwei and Wu, Rongxin and Wang, Rui and Yang, Bo and Yu, Hai and Zhu, Zhiliang and Cheung, Shing-Chi|inproceedings|10.1145/3236024.3236056|||||||||||||||||||||||||||||2069199654|42
ICBDE '22|Shanghai, China|university public opinion, Big data, data analysis, public opinion system|6|121–126|Proceedings of the 5th International Conference on Big Data and Education|Higher educational institutions rely on reputation to attract and earn the legitimacy of the public. However, the emerging trend of public misinformation due to the chunk of information available on the internet affects public opinion formation (POF) about universities. Therefore, narrowing down on the fallouts in POF can provide evidence for managerial and policy interventions. This paper explores five-layer POF and its application with big data in university public opinion monitoring and processing. It reveals that big data technology can be actively employed to safeguard the image of university public opinion formation, but this can be inhibited by the lack of commitment to integrating multiple stakeholders on a common platform, privacy, and security concerns. The paper recommends collaboration between universities and the government to increase momentum on big data with requisite actions for mutual benefits.|10.1145/3524383.3524442|https://doi.org/10.1145/3524383.3524442|New York, NY, USA|Association for Computing Machinery|9781450395793|2022|Research on the Application of Big Data in University's Public Opinion Monitoring and Processing|Cai, Mingjun and Sam, Francis and Asante Boadi, Evans|inproceedings|10.1145/3524383.3524442|||||||||||||||||||||||||||||2069666857|42
||Intelligent manufacturing, Cognitive manufacturing, Cognitive framework, Computable digital twin, Multi-dimensional, OAR model||469-485||With the production system shifting to a multi-variety and small-batch production mode, the production process faces more user requirements, changes, and uncertainties. To solve the above problems, it is necessary to obtain the status and trend changes information and provide information support for the optimization of decision-making and dynamic adjustment of the production system. However, the production system cognition faces the problems of state coupling, state dynamic transfer and transition, and multi-system interweaving, which makes the production system cognition face huge challenges. Combining technologies such as the Internet of Things, industrial big data, and artificial intelligence, cognitive manufacturing can realize dynamic cognition of the production process, support dynamic adjustment, and become a promising way to solve the dynamic changes and uncertainties of production systems. In addition, as a formal expression of information processing and knowledge learning process in cognitive informatics, the Object-Attribute-Relation (OAR) model can effectively guide the construction of the production process cognitive mechanism. Therefore, this paper proposes a multi-dimensional cognitive framework based on OAR model of the human cognitive world for the dynamic cognitive needs of production system. The framework carries out dynamic cognition from the three dimensions of the manufacturing unit, production situation, and production system, and builds the continuous cognitive abilities from the three dimensions of analysis, decision-making, and learning. By integrating intelligent algorithms in the fields of artificial intelligence, a computable digital twin model is constructed as a carrier to provide the cognitive enabling technologies and capabilities for the production system. Finally, the feasibility of the proposed framework is illustrated by the developed computational digital twin platform. The computable digital twin platform provides the production system with important cognitive capabilities such as states perception, trend prediction, optimization decision-making, and knowledge learning, to support the dynamic cognition and optimization decision-making of the production system, and lay a technical foundation for adaptive production and cognitive manufacturing.|https://doi.org/10.1016/j.jmsy.2022.09.019|https://www.sciencedirect.com/science/article/pii/S0278612522001686||||2022|A multi-dimensional cognitive framework for cognitive manufacturing based on OAR model|Tengyuan Jiang and Jingtao Zhou and Jianhua Zhao and Mingwei Wang and Shusheng Zhang|article|JIANG2022469|||Journal of Manufacturing Systems|02786125||65|||||14966|2,310|Q1|70|155|294|8906|2949|288|10,88|57,46|Netherlands|Western Europe|1982-2020|Control and Systems Engineering (Q1); Hardware and Architecture (Q1); Industrial and Manufacturing Engineering (Q1); Software (Q1)|5,413|8.633|0.00561|2070069728|619364890
EATIS '20|Aveiro, Portugal|syphilis, healthcare surveillance, epidemiology, big data|6||Proceedings of the 10th Euro-American Conference on Telematics and Information Systems|"\"For many decades society did need to monitor and assess the standard of living of the population. In the 1950s, the United Nations (UN) saw this need and proposed 12 areas that should be evaluated, the first of which is listed under \"\"Health and Demography\"\", which focuses on what is expressed as the level of a population's health. Decades have passed and great results have been gained from similar initiatives such as reducing mortality from infectious diseases and even eradicating some others. In the age of the digital society, needs have grown. Monitoring demands that once perished from data to become concrete now suffer from the opposite effect, the excess of data from everywhere. Healthcare systems around the world use many different information systems, collecting and generating hundreds of data at unimaginable speed. We are billions of people on the planet and most of us are connected to the virtual world, sharing information, experiences and events with some kind of cloud. In this information age, the ability to aggregate and process this data is a major factor in raising public health to a new level. The development of tools capable of analyzing a large volume of data in seconds and producing knowledge for targeted decision making can help in the fight against specific diseases, in the process of continuing education of professionals, in the formation of new professionals, in the elaboration of new policies. with the specific locoregional look, in the analysis of hidden trends in front of so much information faced in everyday life and other possibilities. The present work proposes an architecture capable of storing and manipulating seeking to standardize the variables in order to allow to correlate this large amount of data in a systematic way, providing to several services and researchers the possibility of consuming health, social, economic and educational data for the promotion of public health.\""|10.1145/3401895.3402092|https://doi.org/10.1145/3401895.3402092|New York, NY, USA|Association for Computing Machinery|9781450377119|2021|A Big Data Architecture to a Multiple Purpose in Healthcare Surveillance: The Brazilian Syphilis Case|Silva, Rodrigo Dantas da and de Ara\'{u}jo, Jean Jar Pereira and de Paiva, \'{A}lvaro Ferreira Pires and de Medeiros Valentim, Ricardo Alexsandro and Coutinho, Karilany Dantas and de Paiva, Jailton Carlos and Roussanaly, Azim and Boyer, Anne|inproceedings|10.1145/3401895.3402092|58||||||||||||||||||||||||||||2072422062|42
||Internet of Things, Cyber-physical systems, Cloud computing, Analytics, Big data||247-259||Big data production in industrial Internet of Things (IIoT) is evident due to the massive deployment of sensors and Internet of Things (IoT) devices. However, big data processing is challenging due to limited computational, networking and storage resources at IoT device-end. Big data analytics (BDA) is expected to provide operational- and customer-level intelligence in IIoT systems. Although numerous studies on IIoT and BDA exist, only a few studies have explored the convergence of the two paradigms. In this study, we investigate the recent BDA technologies, algorithms and techniques that can lead to the development of intelligent IIoT systems. We devise a taxonomy by classifying and categorising the literature on the basis of important parameters (e.g. data sources, analytics tools, analytics techniques, requirements, industrial analytics applications and analytics types). We present the frameworks and case studies of the various enterprises that have benefited from BDA. We also enumerate the considerable opportunities introduced by BDA in IIoT. We identify and discuss the indispensable challenges that remain to be addressed, serving as future research directions.|https://doi.org/10.1016/j.future.2019.04.020|https://www.sciencedirect.com/science/article/pii/S0167739X18313645||||2019|The role of big data analytics in industrial Internet of Things|Muhammad Habib {ur Rehman} and Ibrar Yaqoob and Khaled Salah and Muhammad Imran and Prem Prakash Jayaraman and Charith Perera|article|URREHMAN2019247|||Future Generation Computer Systems|0167739X||99|||||12264|1,262|Q1|119|798|1935|36054|16877|1868|9,11|45,18|Netherlands|Western Europe|1984-2021|Computer Networks and Communications (Q1); Hardware and Architecture (Q1); Software (Q1)||||2074446125|562237118
||Quality of service;Predictive models;Optimization;Computational modeling;Mathematical model;Data models;Web services;Big data;latent factor model;missing data prediction;quality-of-service (QoS);second-order solver;service computing sparse matrices;Web service||1216-1228||Generating highly accurate predictions for missing quality-of-service (QoS) data is an important issue. Latent factor (LF)-based QoS-predictors have proven to be effective in dealing with it. However, they are based on first-order solvers that cannot well address their target problem that is inherently bilinear and nonconvex, thereby leaving a significant opportunity for accuracy improvement. This paper proposes to incorporate an efficient second-order solver into them to raise their accuracy. To do so, we adopt the principle of Hessian-free optimization and successfully avoid the direct manipulation of a Hessian matrix, by employing the efficiently obtainable product between its Gauss-Newton approximation and an arbitrary vector. Thus, the second-order information is innovatively integrated into them. Experimental results on two industrial QoS datasets indicate that compared with the state-of-the-art predictors, the newly proposed one achieves significantly higher prediction accuracy at the expense of affordable computational burden. Hence, it is especially suitable for industrial applications requiring high prediction accuracy of unknown QoS data.|10.1109/TCYB.2017.2685521|||||2018|Incorporation of Efficient Second-Order Solvers Into Latent Factor Models for Accurate Prediction of Missing QoS Data|Luo, Xin and Zhou, MengChu and Li, Shuai and Xia, YunNi and You, Zhu-Hong and Zhu, QingSheng and Leung, Hareton|article|7900340||April|IEEE Transactions on Cybernetics|21682275|4|48|||||21100274221|3,109|Q1|124|542|1065|18740|13312|997|11,19|34,58|United States|Northern America|2013-2020|Computer Science Applications (Q1); Control and Systems Engineering (Q1); Electrical and Electronic Engineering (Q1); Human-Computer Interaction (Q1); Information Systems (Q1); Software (Q1)|24,753|11.448|0.05214|2074718063|3104217
ACM TUR-C '17|Shanghai, China|course architecture, big data, data science|6||Proceedings of the ACM Turing 50th Celebration Conference - China|Big data is one of the hottest topic in not only academic but also enterprise, which provide grate requirements for the people with knowledge and experiences of big data. However, current education architecture of computer science could not provide sufficient training for big data. For the education for people suitable for big data era, we attempt to design a novel course architecture. Such course architecture will not change the skeleton of traditional course architecture of computer science but just add content and subjects that is adaptive for big data. In this paper, we discuss the goal, architecture and content of the course architecture.|10.1145/3063955.3063968|https://doi.org/10.1145/3063955.3063968|New York, NY, USA|Association for Computing Machinery|9781450348737|2017|The Design of Course Architecture for Big Data|Wang, Hongzhi and Gao, Hong and Yin, Shenjun and Zhu, Jie|inproceedings|10.1145/3063955.3063968|13||||||||||||||||||||||||||||2076507443|42
GoodIT '22|Limassol, Cyprus|HTML tags, SMEs, Innovation, Website data|7|338–344|Proceedings of the 2022 ACM Conference on Information Technology for Social Good|The paper explores the possibility to employ the source code of corporate websites as an information source for research in innovation studies. Research in this area is generally based on studies that collect data on patents or official data sources. Our paper links the standard economic information of the firm with web-based data and joins the ongoing debate with a threefold contribution. First, whereas the majority of the literature focused on the linguistic content of web-pages, we mostly use HTML tags. Second, we propose a method to assess the quality of the linkage of Web data to firm-level information. Third, we show that the data retrieved from corporate websites can aid to identify ‘innovative SMEs’.|10.1145/3524458.3547246|https://doi.org/10.1145/3524458.3547246|New York, NY, USA|Association for Computing Machinery|9781450392846|2022|Unconventional Data for Policy: Using Big Data for Detecting Italian Innovative SMEs|Bottai, Carlo and Crosato, Lisa and Domenech, Josep and Guerzoni, Marco and Liberati, Caterina|inproceedings|10.1145/3524458.3547246|||||||||||||||||||||||||||||2080374497|42
||big data, enhanced recovery, epidemiology, orthopedic surgery, postoperative outcome, study design||510-512|||https://doi.org/10.1016/j.bja.2020.01.012|https://www.sciencedirect.com/science/article/pii/S0007091220300593||||2020|Real-world evaluation of enhanced recovery after surgery: big data under the microscope|Daniel I. McIsaac|article|MCISAAC2020510|||British Journal of Anaesthesia|00070912|5|124|||||21858|2,589|Q1|181|486|1260|12638|5321|684|4,33|26,00|United Kingdom|Western Europe|1923-2020|Anesthesiology and Pain Medicine (Q1)|27,510|9.166|0.02901|2086632524|728858816
||Firm performance, Big data, Hotel industry, Fuzzy logic, Structural equation modelling||100921||Big data has increasingly appeared as a frontier of opportunity in enhancing firm performance. However, it still is in early stages of introduction and many enterprises are still un-decisive in its adoption. The aim of this study is to propose a theoretical model based on integration of Human-Organization-Technology fit and Technology-Organization-Environment frameworks to identify the key factors affecting big data adoption and its consequent impact on the firm performance. The significant factors are gained from the literature and the research model is developed. Data was collected from top managers and/or owners of SMEs hotels in Malaysia using online survey questionnaire. Structural Equation Modelling (SEM) is used to assess the developed model and Adaptive Neuro-Fuzzy Inference Systems (ANFIS) technique is used to prioritize adoption factors based on their importance levels. The results showed that relative advantage, management support, IT expertise, and external pressure are the most important factors in the technological, organizational, human, and environmental dimensions. The results further revealed that technology is the most important influential dimension. The outcomes of this study can assist the policy makers, businesses and governments to make well-informed decisions in adopting big data.|https://doi.org/10.1016/j.elerap.2019.100921|https://www.sciencedirect.com/science/article/pii/S1567422319300985||||2020|The impact of big data on firm performance in hotel industry|Elaheh Yadegaridehkordi and Mehrbakhsh Nilashi and Liyana Shuib and Mohd {Hairul Nizam Bin Md Nasir} and Shahla Asadi and Sarminah Samad and Nor {Fatimah Awang}|article|YADEGARIDEHKORDI2020100921|||Electronic Commerce Research and Applications|15674223||40|||||15057|1,184|Q1|74|108|188|6016|1347|188|7,27|55,70|Netherlands|Western Europe|2002-2020|Computer Networks and Communications (Q1); Computer Science Applications (Q1); Management of Technology and Innovation (Q1); Marketing (Q1)|4,026|6.014|0.00318|2087961804|817436666
||Big data, Research roadmap, Societal externalities, Skills development, Standardisation||74-86||With its rapid growth and increasing adoption, big data is producing a substantial impact in society. Its usage is opening both opportunities such as new business models and economic gains and risks such as privacy violations and discrimination. Europe is in need of a comprehensive strategy to optimise the use of data for a societal benefit and increase the innovation and competitiveness of its productive activities. In this paper, we contribute to the definition of this strategy with a research roadmap to capture the economic, social and ethical, legal and political benefits associated with the use of big data in Europe. The present roadmap considers the positive and negative externalities associated with big data, maps research and innovation topics in the areas of data management, processing, analytics, protection, visualisation, as well as non-technical topics, to the externalities they can tackle, and provides a time frame to address these topics in order to deliver social impact, skills development and standardisation. Finally, it also identifies what sectors will be most benefited by each of the research efforts. The goal of the roadmap is to guide European research efforts to develop a socially responsible big data economy, and to allow stakeholders to identify and meet big data challenges and proceed with a shared understanding of the societal impact, positive and negative externalities and concrete problems worth investigating in future programmes.|https://doi.org/10.1016/j.techsoc.2018.03.005|https://www.sciencedirect.com/science/article/pii/S0160791X17300131||||2018|The societal impact of big data: A research roadmap for Europe|Martí Cuquet and Anna Fensel|article|CUQUET201874|||Technology in Society|0160791X||54|||||18676|0,819|Q1|51|201|224|14468|1056|222|4,75|71,98|United Kingdom|Western Europe|1979-2020|Business and International Management (Q1); Education (Q1); Human Factors and Ergonomics (Q1); Sociology and Political Science (Q1)|2,735|4.192|0.00214|2088515053|1856981556
||Sensitivity;Data integrity;Web pages;Feature extraction;Character recognition;llegal websites;website identification;sensitive feature Similarity;Mixed Elements of websites||9-12|2022 International Conference on Computation, Big-Data and Engineering (ICCBE)|In order to identify illegal websites efficiently, a recognition method based on feature similarity of multiple web page mixed elements is proposed. This method constructs the website feature vector based on the web page text content, access path, and website association platform attribute. The method calculates the similarity between the website to be judged and the historical website in the case base. Then, the average similarity of top k cases is selected as the basis for judging the possibility of illegal websites. The experiment shows that the judgment based on similarity can achieve better results in the context of fewer historical cases and better case data quality. Therefore, the method is suitable for scenes where it is difficult to obtain typical illegal website cases.|10.1109/ICCBE56101.2022.9888158|||||2022|Recognition of Illegal Websites Based on Similarity of Sensitive Features of Mixed Elements|Xiong, Jianying|inproceedings|9888158||May|||||||||||||||||||||||||||2090933526|42
dg.o '18|Delft, The Netherlands|the right to know, public-private cooperation, data management, reuse of PSI, public data, public sector information, legal right management, openness|10||Proceedings of the 19th Annual International Conference on Digital Government Research: Governance in the Data Age|With data revolution, data is emerging as a new raw material. As the importance of data has increased, interest in the availability of public sector information (PSI) has also grown. PSI created in the public sector comprises public attributes and directly impacts national administration and citizen's lives.Korea has almost the highest level of Information and Communication Technology (ICT) infrastructure and considerable data through government-led policies. As a result of such policies, Korea has demonstrated excellent results in the United Nation's e-government survey, ITU's ICT development index, and OECD's public data openness index. Paradoxically, however, this history and experience is a stumbling block to a new era. PSI, which is a basic resource for realizing the value of openness, sharing, cooperation, and communication, should be actively managed and opened by government to provide support for reuse in the sense that the government is its main producer and manager. However, no matter how good the quality of PSI through data management is and how excellent policies and institutions are established, if the private sector cannot actively use it, it is useless. What is the role of government and law in the context of changing the way data is managed and blurring sectoral boundaries?This paper aims to propose core challenges by analyzing the case of Korea in order to derive a basis for discussions to coordinate public and private cooperation and legal relations in the process. To begin with, we analyze the changes in the management environment of data and PSI and identify the role of government and law in responding to changes in the legal rights. Then, we discuss how Korea responds to change, examines related policies by function and discussions on the data law, which seem to have the greatest effect on government's role, and suggests essential tasks to change its role accordingly.|10.1145/3209281.3209297|https://doi.org/10.1145/3209281.3209297|New York, NY, USA|Association for Computing Machinery|9781450365260|2018|Role of Law as a Guardian of the Right to Use Public Sector Information: Case Study of Korean Government|Yoon, Sang-Pil and Joo, Moon-Ho and Kwon, Hun-Yeong|inproceedings|10.1145/3209281.3209297|83||||||||||||||||||||||||||||2091527024|42
||Big data, Analytic infrastructure, Competence set, Deduction graph, Supply chain innovation||223-233||Today, firms can access to big data (tweets, videos, click streams, and other unstructured sources) to extract new ideas or understanding about their products, customers, and markets. Thus, managers increasingly view data as an important driver of innovation and a significant source of value creation and competitive advantage. To get the most out of the big data (in combination with a firm׳s existing data), a more sophisticated way of handling, managing, analysing and interpreting data is necessary. However, there is a lack of data analytics techniques to assist firms to capture the potential of innovation afforded by data and to gain competitive advantage. This research aims to address this gap by developing and testing an analytic infrastructure based on the deduction graph technique. The proposed approach provides an analytic infrastructure for firms to incorporate their own competence sets with other firms. Case studies results indicate that the proposed data analytic approach enable firms to utilise big data to gain competitive advantage by enhancing their supply chain innovation capabilities.|https://doi.org/10.1016/j.ijpe.2014.12.034|https://www.sciencedirect.com/science/article/pii/S0925527314004289||||2015|Harvesting big data to enhance supply chain innovation capabilities: An analytic infrastructure based on deduction graph|Kim Hua Tan and YuanZhu Zhan and Guojun Ji and Fei Ye and Chingter Chang|article|TAN2015223|||International Journal of Production Economics|09255273||165|||||19165|2,406|Q1|185|327|891|21712|8124|881|8,31|66,40|Netherlands|Western Europe|1991-2021|Business, Management and Accounting (miscellaneous) (Q1); Economics and Econometrics (Q1); Industrial and Manufacturing Engineering (Q1); Management Science and Operations Research (Q1)|32,606|7.885|0.0228|2092599704|850534974
||neural networks, datasets, document summarization, Transformer, language models||||Long documents such as academic articles and business reports have been the standard format to detail out important issues and complicated subjects that require extra attention. An automatic summarization system that can effectively condense long documents into short and concise texts to encapsulate the most important information would thus be significant in aiding the reader’s comprehension. Recently, with the advent of neural architectures, significant research efforts have been made to advance automatic text summarization systems, and numerous studies on the challenges of extending these systems to the long document domain have emerged. In this survey, we provide a comprehensive overview of the research on long document summarization and a systematic evaluation across the three principal components of its research setting: benchmark datasets, summarization models, and evaluation metrics. For each component, we organize the literature within the context of long document summarization and conduct an empirical analysis to broaden the perspective on current research progress. The empirical analysis includes a study on the intrinsic characteristics of benchmark datasets, a multi-dimensional analysis of summarization models, and a review of the summarization evaluation metrics. Based on the overall findings, we conclude by proposing possible directions for future exploration in this rapidly growing field.|10.1145/3545176|https://doi.org/10.1145/3545176|New York, NY, USA|Association for Computing Machinery||2022|An Empirical Survey on Long Document Summarization: Datasets, Models and Metrics|Koh, Huan Yee and Ju, Jiaxin and Liu, Ming and Pan, Shirui|article|10.1145/3545176||jun|ACM Comput. Surv.|03600300||||Just Accepted|||23038|2,079|Q1|163|112|365|15859|6978|365|19,16|141,60|United States|Northern America|1969-2020|Computer Science (miscellaneous) (Q1); Theoretical Computer Science (Q1)|10,790|10.282|0.01438|2093343099|1517405264
||Emotional recognition, Data collection, Data acquisition, Edge devices, Location, Collection cost|12|595–606||This paper develops a real-time and reliable data collection system for big scale emotional recognition systems. Based on the data sample set collected in the initialization stage and by considering the dynamic migration of emotional recognition data, we design an adaptive Kth average device clustering algorithm for migration perception. We define a sub-modulus weight function, which minimizes the sum of the weights of the subsets covered by a cover to achieve high-precision device positioning. Combining the energy of the data collection devices and the energy of the wireless emotional device, we balance the data collection efficiency and energy consumption, and define a minimum access number problem based on energy and storage space constraints. By designing an approximate algorithm to solve the approximate minimum Steiner point problem, the continuous collection of emotional recognition data and the connectivity of data acquisition devices are guaranteed under the energy constraint of wireless devices. We validate the proposed algorithms through simulation experiments using different emotional recognition systems and different data scale. Furthermore, we analyze the proposed algorithms in terms of topology for devices classification, location accuracy, and data collection efficiency by comparing with the Bayesian classifier-based expectation maximization algorithm, the background difference-based moving target detection arithmetic averaging algorithm, and the Hungarian algorithm for solving the assignment problem.|10.1007/s00779-019-01217-0|https://doi.org/10.1007/s00779-019-01217-0|Berlin, Heidelberg|Springer-Verlag||2019|Data Collection Scheme with Minimum Cost and Location of Emotional Recognition Edge Devices|Jin, Yong and Qian, Zhenjiang and Chen, Shunjiang|article|10.1007/s00779-019-01217-0||jul|Personal Ubiquitous Comput.|16174909|3–4|23|Jul 2019||||22315|0,416|Q2|88|160|297|6581|891|276|2,87|41,13|United Kingdom|Western Europe|1997-2020|Computer Science Applications (Q2); Hardware and Architecture (Q2); Management Science and Operations Research (Q3)|2,743|3.006|0.00196|2093909500|1080502182
||Image recognition;Data visualization;Breast;Image segmentation;data mining;clustering;big data;performance improvement;implementation||43-46|2015 International Conference on Communication Networks (ICCN)|The big data environment is used to support the huge amount of data processing. In this environment tons (i.e. Giga bytes, Tera bytes) of data is processed. Therefore the various online applications where the huge data request are generated are treated using the big data i.e. facebook, google. In this presented work the big data environment is studied and investigated how the data is consumed using the big data and how the supporting tools are working with the Hadoop storage. Furthermore, for keen understanding and investigation, a cluster analysis technique more specifically the K-mean clustering algorithm is implemented through the Hadoop and MapReduce. The clustering is a part of big data analytics where the unlabelled data is processed and utilized to make groups of the data. In addition of that it is observed the traditional k-mean algorithm is not much suitably works with the Hadoop and MapReduce thus small amount of modification is performed on the data processing technique. In addition of that during cluster analysis various issues are found in traditional k-means i.e. fluctuating accuracy, outliers and empty cluster. Therefore a new clustering algorithm with modification on traditional approach of k-means clustering is proposed and implemented. That approach first enhances the data quality by removing the outlier points in datasets and then the bi-part method is used to perform the clustering. The proposed clustering technique implemented using the JAVA, Hadoop and MapReduce finally the performance of the proposed clustering approach is evaluated and compared with the traditional k-means clustering algorithm. The obtained performance shows the effective results and enhanced accuracy of cluster formation with the removal of the de-efficiency. Thus the proposed work is adoptable for the big data environment with improving the performance of clustering.|10.1109/ICCN.2015.9|||||2015|Analysis and performance improvement of K-means clustering in big data environment|Rathore, Purva and Shukla, Deepak|inproceedings|7507493||Nov|||||||||||||||||||||||||||2094104625|42
||Industries;Pathogens;Pandemics;Data integrity;Collaboration;Data models;Numerical models||137-141||"\"The flavor of this \"\"Impact\"\" department is somewhat different. In a pandemic, everybody has to come together. In April 2020, a call went out in the United Kingdom for groups to informally form and collaborate to study this brutal pathogen in whatever way they could. The five authors of this article, old friends from the geophysical industry with decades of experience in numerical modeling and big data, formed such a group.\""|10.1109/MS.2021.3056642|||||2021|Open Collaboration, Data Quality, and COVID-19|Hampson, Gary and Hargreaves, Neil and Jakubowicz, Helmut and Williams, Gareth and Hatton, Les|article|9407288||May|IEEE Software|19374194|3|38|||||||||||||||||||||||2094942222|1030259557
||Monitoring;Internet of Things;Random forests;Logic gates;Feature extraction;Clustering algorithms;Neural networks;Communication message;power Internet of Things (IoT) system;Q learning;random forest (RF)||9450-9459||As the power system develops from informatization to intelligence. Research on data services based on the Internet of Things (IoT) focuses more on application functions, but the research on the data quality of the IoT itself is insufficient. Long-term continuous operation of the big data IoT system has the risk of performance degradation or even partial fault, which leads to a decrease in the availability of collected data for intelligent analysis. In this article, based on the power IoT message data, the characteristics are established through a variety of improved detection methods, and then the abnormal data type is obtained through Q learning and fusion of the random forest (RF) identification features. Finally, the topology of the specific power user IoT system is combined with kernel principal component analysis (KPCA) + improved RF algorithm getting the abnormal location of the IoT. The results show that the research method has a significantly higher positioning accuracy (from 61% to 97%) than the traditional RF method, and the combination method has more advantages in parameter adjustment and classification accuracy than directly using a multilayer perceptron (MLP).|10.1109/JIOT.2021.3058563|||||2021|Fault Diagnosis of Power IoT System Based on Improved Q-KPCA-RF Using Message Data|Jiang, Haoyu and Chen, Kai and Ge, Quanbo and Wang, Yun and Xu, Jinqiang and Li, Chunxi|article|9352014||June|IEEE Internet of Things Journal|23274662|11|8|||||21100338350|2,075|Q1|97|1163|1594|40380|20461|1555|12,37|34,72|United States|Northern America|2014-2020|Computer Networks and Communications (Q1); Computer Science Applications (Q1); Hardware and Architecture (Q1); Information Systems (Q1); Information Systems and Management (Q1); Signal Processing (Q1)|21,151|9.471|0.03208|2096731166|1395601152
||Automotive, Manufacturing, Data Analytics, Big Data, Optimization||120-127||Companies of the manufacturing industry face increasing process complexity. To remain competitive, increasing the knowledge concerning innovative manufacturing processes is necessary. In other areas, data analytics methods have been successfully applied for this purpose. Currently, their application in large scale manufacturing is hampered by insufficient data availability. Therefore, this study presents a solution approach that enables adaptive data availability by establishing a data-use-case-matrix (DUCM), which allows use case prioritization to support dimensioning of control systems and IT infrastructures. In order to support technology development, further proposed is a scalable implementation of the prioritized use cases starting in early prototyping phases.|https://doi.org/10.1016/j.promfg.2018.06.017|https://www.sciencedirect.com/science/article/pii/S2351978918305341||||2018|Enabling Data Analytics in Large Scale Manufacturing|Achim Kampker and Heiner Heimes and Ulrich Bührer and Christoph Lienemann and Stefan Krotil|article|KAMPKER2018120|||Procedia Manufacturing|23519789||24||4th International Conference on System-Integrated Intelligence: Intelligent, Flexible and Connected Systems in Products and Production|||21100792109|0,504|Q2|43|1390|3628|27760|8346|3572|1,79|19,97|Netherlands|Western Europe|2015-2020|Artificial Intelligence (Q2); Industrial and Manufacturing Engineering (Q2)||||2099029879|896540749
ICASIT 2021|Changsha, China||5|815–819|2021 International Conference on Aviation Safety and Information Technology|In the background of today's big data era, the popularization of computer network and information technology helps us to understand and disseminate the hot information in the current society more quickly. By quickly understanding these hot issues, we can better supervise and prevent these problems in our life. In recent years, the problem of food safety appears frequently in our field of vision, which makes people have to regard food safety as a hot issue in today's social development. The state and relevant food safety supervision departments are also paying attention to the food safety problems. In order to better supervise food safety issues in this era of big data, this paper will analyze and study food safety issues with the help of popular technologies in the new era, such as artificial intelligence technology and big data technology, so as to formulate a new scheme to meet the needs of people in the new era for food safety supervision. Through the research, it can be found that a series of methods proposed in this paper can effectively provide new ideas for food safety big data visual analysis research method based on artificial intelligence.|10.1145/3510858.3511394|https://doi.org/10.1145/3510858.3511394|New York, NY, USA|Association for Computing Machinery|9781450390422|2022|Research on Visual Analysis Method of Food Safety Big Data Based on Artificial Intelligence|Chen, Xiaoyu|inproceedings|10.1145/3510858.3511394|||||||||||||||||||||||||||||2102234876|42
EGOSE '14|St. Petersburg, Russian Federation|Policy Impact Evaluation, Fuzzy Cognitive Maps, Policy Making, Open Data, Prosperity Indicators|5|70–74|Proceedings of the 2014 Conference on Electronic Governance and Open Society: Challenges in Eurasia|The aim of this paper is to provide an overview of (the theory and practice of) prosperity indicators for assessing the impact of governmental policies and the data sources associated to their calculation, touching also on the broad theme of Open Data which opens up new horizons for the calculation and exploitation of Social Indicators. Following a quick overview of the basics of prosperity indicators, their basic methodological principles and their typology, a presentation of the Policy Compass project approach and the description of its pilot application in St. Petersburg are provided, which are tackling the above mentioned issue with the provision of a powerful ICT platform.|10.1145/2729104.2729134|https://doi.org/10.1145/2729104.2729134|New York, NY, USA|Association for Computing Machinery|9781450334013|2014|Assessing Governmental Policies' Impact through Prosperity Indicators and Open Data|Kokkinakos, Panagiotis and Koutras, Costas and Markaki, Ourania and Koussouris, Sotirios and Trutnev, Dmitrii and Glikman, Yuri|inproceedings|10.1145/2729104.2729134|||||||||||||||||||||||||||||2102949262|42
TECPS 2017|Santa Barbara, CA, USA|IoT, MBT, elasticity, MDE, testing, Cloud, uncertainty|4|5–8|Proceedings of the 1st ACM SIGSOFT International Workshop on Testing Embedded and Cyber-Physical Systems|Today's cyber-physical systems (CPS) span IoT and cloud-based datacenter infrastructures, which are highly heterogeneous with various types of uncertainty. Thus, testing uncertainties in these CPS is a challenging and multidisciplinary activity. We need several tools for modeling, deployment, control, and analytics to test and evaluate uncertainties for different configurations of the same CPS. In this paper, we explain why using state-of-the art model-driven engineering (MDE) and model-based testing (MBT) tools is not adequate for testing uncertainties of CPS in IoT Cloud infrastructures. We discus how to combine them with techniques for elastic execution to dynamically provision both CPS under test and testing utilities to perform tests in various IoT Cloud infrastructures.|10.1145/3107091.3107093|https://doi.org/10.1145/3107091.3107093|New York, NY, USA|Association for Computing Machinery|9781450351126|2017|Testing Uncertainty of Cyber-Physical Systems in IoT Cloud Infrastructures: Combining Model-Driven Engineering and Elastic Execution|Truong, Hong-Linh and Berardinelli, Luca|inproceedings|10.1145/3107091.3107093|||||||||||||||||||||||||||||2103015243|42
||||S90|||https://doi.org/10.1016/j.jval.2019.04.303|https://www.sciencedirect.com/science/article/pii/S1098301519304954||||2019|PCN181 THE NATIONAL CANCER BIG DATA PLATFORM OF CHINA: VISION AND STATUS|Z.G. Hui and Q. Guo and W.Z. Shi and M.C. Gong and C. Liu and H. Xu and H. Li|article|HUI2019S90|||Value in Health|10983015||22||ISPOR 2019: Rapid. Disruptive. Innovative: A New Era in HEOR|||22377|1,859|Q1|103|211|572|8179|2291|532|3,67|38,76|United Kingdom|Western Europe|1998-2020|Health Policy (Q1); Medicine (miscellaneous) (Q1); Public Health, Environmental and Occupational Health (Q1)|12,642|5.725|0.01786|2103757491|748206584
||Surgical outcomes, Developing countries, Caribbean, Safe surgery, Quality improvement, Big data||27-32||Staggering statistics regarding the global burden of disease due to lack of surgical care worldwide has been gaining attention in the global health literature over the last 10 years. The Lancet Commission on Global Surgery reported that 16.9 million lives were lost due to an absence of surgical care in 2010, equivalent to 33% of all deaths worldwide. Although data from low- and middle-income countries (LMICs) are limited, recent investigations, such as the African Surgical Outcomes Study, highlight that despite operating on low risk patients, there is increased postoperative mortality in LMICs versus higher-resource settings, a majority of which occur secondary to seemingly preventable complications like surgical site infections. We propose that implementing creative, low-cost surgical outcomes monitoring and select quality improvement systems proven effective in high-income countries, such as surgical infection prevention programs and safety checklists, can enhance the delivery of safe surgical care in existing LMIC surgical systems. While efforts to initiate and expand surgical access and capacity continues to deserve attention in the global health community, here we advocate for creative modifications to current service structures, such as promoting a culture of safety, employing technology and mobile health (mHealth) for patient data collection and follow-up, and harnessing partnerships for information sharing, to create a framework for improving morbidity and mortality in responsible, scalable, and sustainable ways.|https://doi.org/10.1016/j.ijsu.2019.07.036|https://www.sciencedirect.com/science/article/pii/S174391911930189X||||2019|Ensuring safe surgical care across resource settings via surgical outcomes data & quality improvement initiatives|Belain Eyob and Marissa A. Boeck and Patrick FaSiOen and Shamir Cawich and Michael D. Kluger|article|EYOB201927|||International Journal of Surgery|17439191||72||Endoscopic Surgery|||130156|1,315|Q1|61|698|1214|12301|4685|959|4,38|17,62|Netherlands|Western Europe|2003-2020|Medicine (miscellaneous) (Q1); Surgery (Q1)|16,011|6.071|0.01876|2110655351|1553638612
Mobihoc '19|Catania, Italy|Sensor Fusion, Internet of Things, Deep Learning|10|151–160|Proceedings of the Twentieth ACM International Symposium on Mobile Ad Hoc Networking and Computing|"\"In recent years, significant research efforts have been spent towards building intelligent and user-friendly IoT systems to enable a new generation of applications capable of performing complex sensing and recognition tasks. In many of such applications, there are usually multiple different sensors monitoring the same object. Each of these sensors can be regarded as an information source and provides us a unique \"\"view\"\" of the observed object. Intuitively, if we can combine the complementary information carried by multiple sensors, we will be able to improve the sensing performance. Towards this end, we propose DeepFusion, a unified multi-sensor deep learning framework, to learn informative representations of heterogeneous sensory data. DeepFusion can combine different sensors' information weighted by the quality of their data and incorporate cross-sensor correlations, and thus can benefit a wide spectrum of IoT applications. To evaluate the proposed DeepFusion model, we set up two real-world human activity recognition testbeds using commercialized wearable and wireless sensing devices. Experiment results show that DeepFusion can outperform the state-of-the-art human activity recognition methods.\""|10.1145/3323679.3326513|https://doi.org/10.1145/3323679.3326513|New York, NY, USA|Association for Computing Machinery|9781450367646|2019|DeepFusion: A Deep Learning Framework for the Fusion of Heterogeneous Sensory Data|Xue, Hongfei and Jiang, Wenjun and Miao, Chenglin and Yuan, Ye and Ma, Fenglong and Ma, Xin and Wang, Yijiang and Yao, Shuochao and Xu, Wenyao and Zhang, Aidong and Su, Lu|inproceedings|10.1145/3323679.3326513|||||||||||||||||||||||||||||2111615470|42
ICASIT 2021|Changsha, China||6|1–6|2021 International Conference on Aviation Safety and Information Technology|As our country's economic development enters a new normal, the original manufacturing development model can no longer meet the needs of current economic development, and it is urgent to accelerate the transformation of the manufacturing industry. At present, the country's supply-side structural reforms are deepening, and listed manufacturing companies are the most important backbone in terms of scale and innovation opportunities. Data mining technology is used to study the impact of quality control on corporate performance. Listed companies have a positive impact on the further realization of transformation and upgrading and the improvement of corporate performance. This article aims to study the manufacturing audit quality analysis model based on data mining technology, and adopts the analysis method of the combination of supervisory research and empirical analysis, from the perspective of supervisory research, summarizes the theory of internal audit quality and company performance in the research process, and summarizes predecessors' research results and research ideas. The experimental data in this article shows that the average quality of internal audit information disclosure is 3.1156, indicating that the audit disclosure status of listed companies selected by the Shenzhen Stock Exchange is good, and to a certain extent reflects the quality level of some internal controls.|10.1145/3510858.3510863|https://doi.org/10.1145/3510858.3510863|New York, NY, USA|Association for Computing Machinery|9781450390422|2022|Manufacturing Audit Quality Analysis Model Based on Data Mining Technology|Sun, Fangyu|inproceedings|10.1145/3510858.3510863|||||||||||||||||||||||||||||2117983830|42
|||21|926–946||Omic data analyses pose great informatics challenges. As an emerging subfield of bioinformatics, omics informatics focuses on analyzing multi-omic data efficiently and effectively, and is gaining momentum. There are two underlying trends in the expansion of omics informatics landscape: the explosion of scattered individual omics informatics tools with each of which focuses on a specific task in both single- and multi- omic settings, and the fast-evolving integrated software platforms such as workflow management systems that can assemble multiple tools into pipelines and streamline integrative analysis for complicated tasks. In this survey, we give a holistic view of omics informatics, from scattered individual informatics tools to integrated workflow management systems. We not only outline the landscape and challenges of omics informatics, but also sample a number of widely used and cutting-edge algorithms in omics data analysis to give readers a fine-grained view. We survey various workflow management systems WMSs, classify them into three levels of WMSs from simple software toolkits to integrated multi-omic analytical platforms, and point out the emerging needs for developing intelligent workflow management systems. We also discuss the challenges, strategies and some existing work in systematic evaluation of omics informatics tools. We conclude by providing future perspectives of emerging fields and new frontiers in omics informatics.|10.1109/TCBB.2016.2535251|https://doi.org/10.1109/TCBB.2016.2535251|Washington, DC, USA|IEEE Computer Society Press||2017|Omics Informatics: From Scattered Individual Software Tools to Integrated Workflow Management Systems|Ma, Tianle and Zhang, Aidong|article|10.1109/TCBB.2016.2535251||jul|IEEE/ACM Trans. Comput. Biol. Bioinformatics|15455963|4|14|July 2017||||||||||||||||||||||2119211692|1878427007
||Statistical process monitoring, Big data, Smart manufacturing, Feature extraction, Internet of things||35-43||With ever-accelerating advancement of information, communication, sensing and characterization technologies, such as industrial Internet of Things (IoT) and high-throughput instruments, it is expected that the data generated from manufacturing will grow exponentially, generating so called ‘big data’. One of the focuses of smart manufacturing is to create manufacturing intelligence from real-time data to support accurate and timely decision-making. Therefore, big data analytics is expected to contribute significantly to the advancement of smart manufacturing. In this work, a roadmap of statistical process monitoring (SPM) is presented. Most recent developments in SPM are briefly reviewed and summarized. Specific challenges and potential solutions in handling manufacturing big data are discussed. We suggest that process characteristics or feature based SPM, instead of process variable based SPM, is a promising route for next generation SPM and could play a significant role in smart manufacturing. The advantages of feature based SPM are discussed to support the suggestion and future directions in SPM are discussed in the context of smart manufacturing.|https://doi.org/10.1016/j.jprocont.2017.06.012|https://www.sciencedirect.com/science/article/pii/S0959152417301257||||2018|Statistical process monitoring as a big data analytics tool for smart manufacturing|Q. Peter He and Jin Wang|article|HE201835|||Journal of Process Control|09591524||67||Big Data: Data Science for Process Control and Operations|||14414|1,102|Q1|114|136|413|5639|1820|408|4,39|41,46|United Kingdom|Western Europe|1991-2020|Computer Science Applications (Q1); Control and Systems Engineering (Q1); Industrial and Manufacturing Engineering (Q1); Modeling and Simulation (Q1)|7,446|3.666|0.00525|2119915588|26970220
||Big Data;Machinery;Feature extraction;Condition monitoring;Data integrity;Fault diagnosis;Cleaning;Condition-monitoring big data;data cleaning;data quality;incorrect data;local outlier factor (LOF)||2326-2336||The presence of incorrect data leads to the decrease of condition-monitoring big data quality. As a result, unreliable or misleading results are probably obtained by analyzing these poor-quality data. In this paper, to improve the data quality, an incorrect data detection method based on an improved local outlier factor (LOF) is proposed for data cleaning. First, a sliding window technique is used to divide data into different segments. These segments are considered as different objects and their attributes consist of time-domain statistical features extracted from each segment, such as mean, maximum and peak-to-peak value. Second, a kernel-based LOF (KLOF) is calculated using these attributes to evaluate the degree of each segment being incorrect data. Third, according to these KLOF values and a threshold value, incorrect data are detected. Finally, a simulation of vibration data generated by a defective rolling element bearing and three real cases concerning a fixed-axle gearbox, a wind turbine, and a planetary gearbox are used to verify the effectiveness of the proposed method, respectively. The results demonstrate that the proposed method is able to detect both missing segments and abnormal segments, which are two typical incorrect data, effectively, and thus is helpful for big data cleaning of machinery condition monitoring.|10.1109/TIE.2019.2903774|||||2020|An Incorrect Data Detection Method for Big Data Cleaning of Machinery Condition Monitoring|Xu, Xuefang and Lei, Yaguo and Li, Zeda|article|8667006||March|IEEE Transactions on Industrial Electronics|15579948|3|67|||||||||||||||||||||||2121597756|1063852316
CompSysTech '14|Ruse, Bulgaria|networking, data analysis, big data, public data, open data|15|25–39|Proceedings of the 15th International Conference on Computer Systems and Technologies|"\"Open data is seen as a promising source of new business, especially in the SME sector, in the form of new products, services and innovative solutions. High importance is seen also in fostering citizens' participation in political and social life and increasing the transparency of public authorities. The forerunners of the open data movement in the public sector are the USA and the UK, which started to open their public data resources in 2009. The first European Union open data related directive was drawn up as early as 2003; however progress in putting the idea into practice has been slow and adoptions by the wider member states are placed in the early 2010s. The beneficial use of open data in real applications has progressed hand in hand with the improvement of other ICT-related technologies. The (raw) data itself has no high value. The economic value comes from a balanced combination of high quality open (data) resources combined with the related value chain. This paper builds up a \"\"big picture\"\" of the role of open data in current society. The approach is analytical and it clarifies the topic from the viewpoints of both opportunities and challenges. The paper covers both general aspects related to open data and results of the research and regional development project conducted by the authors.\""|10.1145/2659532.2659594|https://doi.org/10.1145/2659532.2659594|New York, NY, USA|Association for Computing Machinery|9781450327534|2014|Open Data: Opportunities and Challenges|"\"Jaakkola, Hannu and M\"\"{a}kinen, Timo and Etel\\\"\"{a}aho, Anna\""|inproceedings|10.1145/2659532.2659594|||||||||||||||||||||||||||||2126180094|42
||Countermeasures., Attacks, Federated Learning, Blockchain||||Federated learning (FL) is experiencing fast booming in recent years, which is jointly promoted by the prosperity of machine learning and Artificial Intelligence along with the emerging privacy issues. In the FL paradigm, a central server and local end devices maintain the same model by exchanging model updates instead of raw data, with which the privacy of data stored on end devices is not directly revealed. In this way, the privacy violation caused by the growing collection of sensitive data can be mitigated. However, the performance of FL with a central server is reaching a bottleneck while new threats are emerging simultaneously. There are various reasons, among which the most significant ones are centralized processing, data falsification, and lack of incentives. To accelerate the proliferation of FL, blockchain-enabled FL has attracted substantial attention from both academia and industry. A considerable number of novel solutions are devised to meet the emerging demands of diverse scenarios. Blockchain-enabled FL provides both theories and techniques to improve the performances of FL from various perspectives. In this survey, we will comprehensively summarize and evaluate existing variants of blockchain-enabled FL, identify the emerging challenges, and propose potentially promising research directions in this under-explored domain.|10.1145/3524104|https://doi.org/10.1145/3524104|New York, NY, USA|Association for Computing Machinery||2022|Blockchain-Enabled Federated Learning: A Survey|Qu, Youyang and Uddin, Md Palash and Gan, Chenquan and Xiang, Yong and Gao, Longxiang and Yearwood, John|article|10.1145/3524104||mar|ACM Comput. Surv.|03600300||||Just Accepted|||23038|2,079|Q1|163|112|365|15859|6978|365|19,16|141,60|United States|Northern America|1969-2020|Computer Science (miscellaneous) (Q1); Theoretical Computer Science (Q1)|10,790|10.282|0.01438|2127718720|1517405264
||Big Data;Organizations;Radio frequency;Predictive models;Support vector machines;Data models;Analytical models;Deep people analytics;employee attrition;retention;prediction;interpretation;policies recommendation||60447-60458||In the era of data science and big data analytics, people analytics help organizations and their human resources (HR) managers to reduce attrition by changing the way of attracting and retaining talent. In this context, employee attrition presents a critical problem and a big risk for organizations as it affects not only their productivity but also their planning continuity. In this context, the salient contributions of this research are as follows. Firstly, we propose a people analytics approach to predict employee attrition that shifts from a big data to a deep data context by focusing on data quality instead of its quantity. In fact, this deep data-driven approach is based on a mixed method to construct a relevant employee attrition model in order to identify key employee features influencing his/her attrition. In this method, we started thinking `big' by collecting most of the common features from the literature (an exploratory research) then we tried thinking `deep' by filtering and selecting the most important features using survey and feature selection algorithms (a quantitative method). Secondly, this attrition prediction approach is based on machine, deep and ensemble learning models and is experimented on a large-sized and a medium-sized simulated human resources datasets and then a real small-sized dataset from a total of 450 responses. Our approach achieves higher accuracy (0.96, 0.98 and 0.99 respectively) for the three datasets when compared previous solutions. Finally, while rewards and payments are generally considered as the most important keys to retention, our findings indicate that `business travel', which is less common in the literature, is the leading motivator for employees and must be considered within HR policies to retention.|10.1109/ACCESS.2021.3074559|||||2021|From Big Data to Deep Data to Support People Analytics for Employee Attrition Prediction|Yahia, Nesrine Ben and Hlel, Jihen and Colomo-Palacios, Ricardo|article|9409047|||IEEE Access|21693536||9|||||21100374601|0,587|Q1|127|18036|24267|771081|116691|24200|4,48|42,75|United States|Northern America|2013-2020|Computer Science (miscellaneous) (Q1); Engineering (miscellaneous) (Q1); Materials Science (miscellaneous) (Q2)|105,968|3.367|0.15396|2129917254|1905633267
dg.o '18|Delft, The Netherlands|adoption, government, blockchain, public service, literature review|9||Proceedings of the 19th Annual International Conference on Digital Government Research: Governance in the Data Age|The ability of blockchain technology to record transactions on distributed ledgers offers new opportunities for governments to improve transparency, prevent fraud, and establish trust in the public sector. However, blockchain adoption and use in the context of e-Government is rather unexplored in academic literature. In this paper, we systematically review relevant research to understand the current research topics, challenges and future directions regarding blockchain adoption for e-Government. The results show that the adoption of blockchain-based applications in e-Government is still very limited and there is a lack of empirical evidence. The main challenges faced in blockchain adoption are predominantly presented as technological aspects such as security, scalability and flexibility. From an organizational point of view, the issues of acceptability and the need of new governance models are presented as the main barriers to adoption. Moreover, the lack of legal and regulatory support is identified as the main environmental barrier of adoption. Based on the challenges presented in the literature, we propose future research questions that need to be addressed to inform how the public sector should approach the blockchain technology adoption.|10.1145/3209281.3209317|https://doi.org/10.1145/3209281.3209317|New York, NY, USA|Association for Computing Machinery|9781450365260|2018|Challenges of Blockchain Technology Adoption for E-Government: A Systematic Literature Review|Batubara, F. Rizal and Ubacht, Jolien and Janssen, Marijn|inproceedings|10.1145/3209281.3209317|76||||||||||||||||||||||||||||2133700675|42
||Feature selection, Filters, Preprocessing, High dimensionality, Classification, Data analysis||365-375||The task of choosing the appropriate classifier for a given scenario is not an easy-to-solve question. First, there is an increasingly high number of algorithms available belonging to different families. And also there is a lack of methodologies that can help on recommending in advance a given family of algorithms for a certain type of datasets. Besides, most of these classification algorithms exhibit a degradation in the performance when faced with datasets containing irrelevant and/or redundant features. In this work we analyze the impact of feature selection in classification over several synthetic and real datasets. The experimental results obtained show that the significance of selecting a classifier decreases after applying an appropriate preprocessing step and, not only this alleviates the choice, but it also improves the results in almost all the datasets tested.|https://doi.org/10.1016/j.neucom.2021.05.107|https://www.sciencedirect.com/science/article/pii/S0925231221011127||||2022|How important is data quality? Best classifiers vs best features|Laura Morán-Fernández and Verónica Bólon-Canedo and Amparo Alonso-Betanzos|article|MORANFERNANDEZ2022365|||Neurocomputing|09252312||470|||||24807|1,085|Q1|143|1653|3586|78823|25554|3535|7,08|47,68|Netherlands|Western Europe|1989-2020|Artificial Intelligence (Q1); Computer Science Applications (Q1); Cognitive Neuroscience (Q2)|46,751|5.719|0.06669|2134616494|411767096
||Big Data;Pipelines;Ontologies;Metadata;Google;Quality-driven policies;Big Data pipeline;ontology-based quality checking;Big Data confidentiality||2974-2979|2017 IEEE International Conference on Big Data (Big Data)|Policy making has the strict requirement to rely on quantitative and high quality information. This paper will address the data quality issue for policy making by showing how to deal with Big Data quality in the different steps of a processing pipeline, with a focus on the integration of Big Data sources with traditional sources. In this respect, a relevant role is played by metadata and in particular by ontologies. Integration systems relying on ontologies enable indeed a formal quality evaluation of inaccuracy, inconsistency and incompleteness of integrated data. The paper will finally describe data confidentiality as a Big Data quality dimension, showing the main issues to be faced for its assurance.|10.1109/BigData.2017.8258267|||||2017|My (fair) big data|Catarci, Tiziana and Scannapieco, Monica and Console, Marco and Demetrescu, Camil|inproceedings|8258267||Dec|||||||||||||||||||||||||||2136156662|42
||Sensors;Contracts;Task analysis;Conferences;Big Data;Edge computing;Smart cities;Mobile crowd sensing;trust scheme;optimal contract||1-5|2018 International Conference on Selected Topics in Mobile and Wireless Networking (MoWNeT)|Mobile crowd sensing networks (MCSNs) have emerged as a promising paradigm to provide various sensing services. With the increasing number of mobile users, how to develop an effective scheme to provide the high-quality and secure sensing data becomes a new challenge. In this paper, we propose a contract theory based scheme to provide sensing service in MCSNs. At first, with the analysis of the interaction experience between the crowd sensing platform and mobile user, a trust scheme is introduced to guarantee the quality of sensing data by considering the direct trust and indirect trust. Next, according to the transaction between crowd sensing platform and mobile user, an optimal contract based on incentive scheme is designed to stimulate mobile users to participate in crowd sensing network, where the contract item can not only maximize the platform utility, but also satisfy individual rationality and incentive compatibility. Finally, the numerical results show that the proposal outperforms the conventional schemes.|10.1109/MoWNet.2018.8428903|||||2018|Contract Theory Based Incentive Scheme for Mobile Crowd Sensing Networks|Dai, Minghui and Su, Zhou and Wang, Yuntao and Xu, Qichao|inproceedings|8428903||June|||||||||||||||||||||||||||2137524020|42
||Data governance, Industry 4.0, data bus architecture, cloud computing, IoT, big data||614-621||The fourth industrial revolution considers data as a business asset and therefore this is placed as a central element of the software architecture (data as a service) that will support the horizontal and vertical digitalization of industrial processes. The large volume of data that the environment generates, its heterogeneity and complexity, as well as its reuse for later processes (e.g. analytics, IA) requires the adoption of policies, directives and standards for its right governance. Furthermore, the issues related to the use of resources in the cloud computing must be taken into account with the aim of meeting the requirements of performance and security of the different processes. This article, in the absence of frameworks adapted to this new architecture, proposes an initial schema for developing an effective data governance programme for third generation platforms, that means, a conceptual tool which guides organizations to define, design, develop and deploy services aligned with its vision and business goals in I4.0 era.|https://doi.org/10.1016/j.procs.2019.04.082|https://www.sciencedirect.com/science/article/pii/S1877050919305447||||2019|Towards a Data Governance Framework for Third Generation Platforms|Juan Yebenes and Marta Zorrilla|article|YEBENES2019614|||Procedia Computer Science|18770509||151||The 10th International Conference on Ambient Systems, Networks and Technologies (ANT 2019) / The 2nd International Conference on Emerging Data and Industry 4.0 (EDI40 2019) / Affiliated Workshops|||19700182801|0,334|-|76|1907|6483|38511|13722|6359|2,09|20,19|Netherlands|Western Europe|2010-2020|Computer Science (miscellaneous)||||2138015380|2108686752
||Data quality evaluation process, Data quality certification, Data quality management, ISO/IEC 25012, ISO/IEC 25024, ISO/IEC 25040||110938||The most successful organizations in the world are data-driven businesses. Data is at the core of the business of many organizations as one of the most important assets, since the decisions they make cannot be better than the data on which they are based. Due to this reason, organizations need to be able to trust their data. One important activity that helps to achieve data reliability is the evaluation and certification of the quality level of organizational data repositories. This paper describes the results of the application of a data quality evaluation and certification process to the repositories of three European organizations belonging to different sectors. We present findings from the point of view of both the data quality evaluation team and the organizations that underwent the evaluation process. In this respect, several benefits have been explicitly recognized by the involved organizations after achieving the data quality certification for their repositories (e.g., long-term organizational sustainability better internal knowledge of data, and a more efficient management of data quality). As a result of this experience, we have also identified a set of best practices aimed to enhance the data quality evaluation process.|https://doi.org/10.1016/j.jss.2021.110938|https://www.sciencedirect.com/science/article/pii/S0164121221000352||||2021|Data quality certification using ISO/IEC 25012: Industrial experiences|Fernando Gualo and Moisés Rodriguez and Javier Verdugo and Ismael Caballero and Mario Piattini|article|GUALO2021110938|||Journal of Systems and Software|01641212||176|||||19309|0,642|Q1|109|183|619|11845|3058|590|4,94|64,73|United States|Northern America|1979, 1981-2021|Hardware and Architecture (Q1); Information Systems (Q2); Software (Q2)|6,579|2.829|0.00727|2139768524|1111852116
||Big data analytics, Forecasting, Innovation, Online review crowdsourcing, Consumer goods companies, Digital data||338-352||The advent and development of digital technologies have brought about a proliferation of online consumer reviews (OCRs), i.e., real-time customers’ evaluations of products, services, and brands. Increasingly, e-commerce platforms are using them to gain insights from customer feedback. Meanwhile, a new generation of big data analytics (BDA) companies are crowdsourcing large volumes of OCRs by means of controlled ad hoc online experiments and advanced machine learning (ML) techniques to forecast demand and determine the market potential for new products in several industries. We illustrate how this process is taking place for consumer goods companies by exploring the case of UK digital BDA company, SoundOut. Based on an in-depth qualitative analysis, we develop the consumer goods company innovation (CGCI) conceptual framework, which illustrates how digital BDA firms help consumer goods companies to test new products before they are launched on the market, and innovate. Theoretical and managerial implications are discussed.|https://doi.org/10.1016/j.jbusres.2020.09.012|https://www.sciencedirect.com/science/article/pii/S0148296320305956||||2020|Exploring how consumer goods companies innovate in the digital age: The role of big data analytics companies|Marcello M. Mariani and Samuel {Fosso Wamba}|article|MARIANI2020338|||Journal of Business Research|01482963||121|||||20550|2,049|Q1|195|878|1342|73221|11507|1315|7,38|83,40|United States|Northern America|1973-2021|Marketing (Q1)|46,935|7.550|0.03523|2139782576|1502892296
||AHP, Big data analytics, Barriers to BDA, Delphi, Information and communication technology (ICT), Manufacturing supply chains||1063-1075||Recently, big data (BD) has attracted researchers and practitioners due to its potential usefulness in decision-making processes. Big data analytics (BDA) is becoming increasingly popular among manufacturing companies as it helps gain insights and make decisions based on BD. However, there many barriers to the adoption of BDA in manufacturing supply chains. It is therefore necessary for manufacturing companies to identify and examine the nature of each barrier. Previous studies have mostly built conceptual frameworks for BDA in a given situation and have ignored examining the nature of the barriers to BDA. Due to the significance of both BD and BDA, this research aims to identify and examine the critical barriers to the adoption of BDA in manufacturing supply chains in the context of Bangladesh. This research explores the existing body of knowledge by examining these barriers using a Delphi-based analytic hierarchy process (AHP). Data were obtained from five Bangladeshi manufacturing companies. The findings of this research are as follows: (i) data-related barriers are most important, (ii) technology-related barriers are second, and (iii) the five most important components of these barriers are (a) lack of infrastructure, (b) complexity of data integration, (c) data privacy, (d) lack of availability of BDA tools and (e) high cost of investment. The findings can assist industrial managers to understand the actual nature of the barriers and potential benefits of using BDA and to make policy regarding BDA adoption in manufacturing supply chains. A sensitivity analysis was carried out to justify the robustness of the barrier rankings.|https://doi.org/10.1016/j.cie.2018.04.013|https://www.sciencedirect.com/science/article/pii/S0360835218301505||||2019|Barriers to big data analytics in manufacturing supply chains: A case study from Bangladesh|Md. Abdul Moktadir and Syed Mithun Ali and Sanjoy Kumar Paul and Nagesh Shukla|article|MOKTADIR20191063|||Computers & Industrial Engineering|03608352||128|||||18164|1,315|Q1|128|641|1567|31833|9597|1557|5,97|49,66|United Kingdom|Western Europe|1976-2020|Computer Science (miscellaneous) (Q1); Engineering (miscellaneous) (Q1)||||2141445105|1798521593
